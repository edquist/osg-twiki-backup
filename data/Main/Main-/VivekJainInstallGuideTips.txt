%META:TOPICINFO{author="VivekJain" date="1120605159" format="1.0" version="1.3"}%
%META:TOPICPARENT{name="VivekJain"}%
-- Main.VivekJain - 05 Jul 2005


%TOC%

---++Introduction

Here are some *tips/comments* for setting up a grid node as per instructions in the OSG Install Guide. When I started this project, I had very, very limited knowledge about system adminstration, so these comments may be useful for novices (like me). Feel free to [[mailto:vj@bnl.gov e-mail]] me.

---++ Installing the cluster

---+++ Setting up the ROCKS cluster

We used ROCKS 3.3 [[http://www.rocksclusters.org/Rocks/ ROCKS]] to setup the cluster; the installation documentation is very well written.
Since we were setting up a test grid node, we used some old CPU's which were lying around. I mention this because, we ended up booting the compute nodes using a floppy disk (the CD drives were not working and the NIC cards were too old to know about a PXE boot). We downloaded the relevant software from [[http://www.rom-o-matic.net/ here]] - you have to know the type of your NIC.

The ROCKS headnode is multi-homed. One NIC is connected to the external world and the other to the internal network. The latter has the IP address - 10.1.1.1. We have three compute nodes. When we go to a bigger cluster, we will probably have to do more sophisticated things for setting up NFS.

---+++ OSG-GateKeeper

Here we took a box and installed Scientific Linux 3 on it. Initially, we had connected this only to the external world, but it was recommended by [[Main.TerrenceMartin][Terrence]] that we also connect it to the same private network as the ROCKS headnode; the GK's internal card is connected on the "same side" of the switch as the ROCKS compute nodes. 

The internal NIC had a IP address of 10.1.1.2. For the DNS address, we used 10.1.1.1, i.e., the ROCKS headnode. It is necessary to have the GK talk directly to the compute nodes - makes the operation of CONDOR more robust. I will talk more about this [[#MyHeadnode][here]].

---++ Following the OSG Install Guide

Now come my comments/tips on the install guide - [[Integration.OSGCEInstallGuide]]. For the most part, the guide is well documented. Some stuff needs to be better explained, at least for a novice.

---+++ Pre-installation Checklist

Most of the stuff listed here is pretty straightforward and well documented. One comment:

---++++ Firewalls

	If you follow the link, you will come to the separate section on [[Integration.OSGCEInstallGuide#Firewalls][Firewalls]]. As it is currently written, it appears that information regarding setting environment variables like GLOBUS_TCP_PORT_RANGE only applies to the case of host-based firewalls. This is not *true*. It applies equally to the case where you have an external firewall "surrounding the nodes". 

You need to set the variables *GLOBUS_TCP_PORT_RANGE* and *GLOBUS_TCP_SOURCE_RANGE* (both have the same range, _e.g.,_ 40000-41000).

	* In */etc/xinetd.d/gsiftp,gsiftp2,globus-gatekeeper* - both variables need to be set
	* Wherever you have installed the VDT software, below that will be sub-dirs, globus and vdt
		* In *globus/etc/globus-job-manager.conf*, you need to set *GLOBUS_TCP_PORT_RANGE*
		* In *vdt/etc/vdt-local-setup.sh (.csh)*, you can set both these varibles (_won't hurt_)

Remember to change the configuration files on the firewall to allow traffic on these ports, as well as the other ports specified in the documentation. 

---++++ User accounts

I installed the VDT software under /usr/local/grid and the plan is to export this to the ROCKS headnode and compute nodes. Thus, I put the user accounts in /usr/local/grid/users/. To make a new user account, do

useradd -d /usr/local/grid/users/&lt;username&gt; &lt;username&gt;

---+++ Installation Procedure

Again the documentation is quite self-explanatory. Some comments:

---++++ Certificates

Once you get your User certificate, you will also need other certificates which are for the host as well as for services on the host. In some cases, it is better to wait for the certificate before proceeding. I needed a host certificate as well as service certificates for LDAP and http. It may save time if you were to apply for all these certificates at once (%BLUE% Is this possible. Need to check %ENDCOLOR%).

---+++ Configuration and setup of OSG CE Services

Again the documentation is quite self-explanatory. Some comments:

---++++ Configuring [[Integration.MonALISA][MonALISA]]

The !MonALISA daemon monitors the state of the ROCKS compute nodes through the ROCKS headnode. The latter uses Ganglia for collecting information. It was suggested that it would be better for this communication to occur via the internal network. So, when you are configuring !MonALISA, you will be asked the host name on which Ganglia is running. Instead of giving the fully qualified domain name, I used ahepg1h.local (my ROCKS headnode is called __ahepg1h.tld_). Even if you don't answer correctly, you can go back and edit the file $VDT_LOCATION/MonaLisa/Service/VDTFarm/vdtFarm.conf
Make sure in this line, it says ahepg1h.local instead of the FQDN

*PN_vdt{monIGangliaTCP, ahepg1h.local, 8649}%30

On the ROCKS headnode (_ahepg1h_), you have to edit */etc/gmond.conf* and set *trusted_hosts* to point to the Gatekeeper node. Again, you want to use the internal network. I say *trusted_hosts osg-gk.local* - on the internal network, the gatekeeper is known as *osg-gk*, even though to the external world, it is known as _aheposgk.tld_. I'll talk about how to set this up [[#MyHeadnode][here]].


#MyHeadnode ta ta...

