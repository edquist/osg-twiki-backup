%META:TOPICINFO{author="DerekWeitzel" date="1339538551" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="DerekWeitzel"}%
---+ Planning document for Storage

---++ Outline
[[Education.UserSchool12Schedule][Timeline page]]


---+++ Goals:
   * Students should be able to identify five strategies for managing data for HTC jobs (local storage, job sandbox, prestaging, caching, remote I/O)
   * Build from the ground up, local storage -> job sandbox -> prestaging -> caching -> remote I/O
   * Students should be able to compare/contrast the approaches, and select an appropriate approach for a given problem.
   * Run jobs using pre-staging and job sandboxes (local was run on Alain’s day)
   * Run jobs using caching and job sandboxes
   * Extra-credit: Remote I/O with Parrot (may overlap with Alain a bit?)

---+++ 9:00 - 9:30 Interactive Lecture 1: Adding Data and Storage to the HTC picture
   * Discuss archetypical cases of how data is used in HTC.
   * Introduce the five strategies for managing data.

<!-- Find strong analogies to real-life.  Law-firm caching analogy: getting folders from the basement. -->

---+++ 9:30 - 10:30 Planning for distributed storage (interactive)
   * Break up into pairs.  Students will be given a few example problems.  They will be asked to determine, for the different solution approaches to the problems, the choke points and I/O load incurred at the choke points.
   * Show how to use sandbox in this case to only transfer what an application needs.  And only transfer back what it needs back.
 

9:50 - 10:10
Discussion / Lecture 2
- Discuss results of the first hands-on session
 - Review solutions
 - Review the "cost of complexity" for data management.
 - Cover reliability of the solutions.
- Discuss how shared storage resources differ from shared compute resources
- Lecture on basic concepts of presaging data with SRM and GridFTP.

10:10 - 10:30
Break

10:30 - 11:15
Hands-on with SRM and GridFTP
- Verify they can use SRM and GridFTP for the BestMan at the site.
- Stage data to a single endpoint, submit jobs.
- While jobs are running, stage data to multiple sites.
- Submit jobs at the remaining sites.

11:15 - 11:35
- Wrap-up previous Hands-On.  What was simple?  What was difficult?  Mention Globus Online and experimental data movers.
- Talk about caching with wget/curl.

11:35 - 12:00
Hands-on with caching
- Upload database to a web server somewhere (to investigate: maybe to web-server built in to schedd?)
- Re-run jobs from before, across multiple sites, with the cache
- Extra credit 1: modify workflow to use parrot and caching.
- Extra credit 2: Take another one of the summer school's applications to use storage more intelligently.


-- Main.DerekWeitzel - 12 Jun 2012
