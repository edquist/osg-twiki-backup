%META:TOPICINFO{author="VivekJain" date="1120677994" format="1.0" version="1.5"}%
%META:TOPICPARENT{name="VivekJain"}%
-- Main.VivekJain - 05 Jul 2005


%TOC%

---++Introduction

Here are some *tips/comments* for setting up a grid node as per instructions in the OSG Install Guide. When I started this project, I had very, very limited knowledge about system adminstration, so these comments may be useful for novices (like me). Feel free to [[mailto:vj@bnl.gov e-mail]] me.

---++ Installing the cluster

We first set up the cluster using [[http://www.rocksclusters.org/Rocks/ ROCKS 3.3]] (Makalu). This includes the ROCKS headnode and compute nodes. We then set up the Gatekeeper node; this node contains all the VDT software. 

---+++ Setting up the ROCKS cluster

The ROCKS installation documentation is very well written. Since we were setting up a test grid node, we used some old CPU's which were lying around. I mention this because, we ended up booting the compute nodes using a floppy disk (the CD drives were not working and the NIC cards were too old to know about a PXE boot). We downloaded the relevant software from [[http://www.rom-o-matic.net/ here]] - you have to know the type of your NIC.

For installing the ROCKS cluster, we only used the base roll and the Kernel+HPC roll. All other software came from VDT.

The ROCKS headnode is multi-homed. One NIC is connected to the external world and the other to the internal network. The latter has the IP address - 10.1.1.1. We have three compute nodes. When we go to a bigger cluster, we will probably have to do more sophisticated things for setting up NFS.

Make sure that _eth0_ is connected to the internal network (_eth1_ is connected to the outside world).

---+++ OSG-GateKeeper

#MyGatekeeper Here we took a box and installed Scientific Linux 3 on it. Initially, we had connected this only to the external world, but it was recommended by [[Main.TerrenceMartin][Terrence]] that we also connect it to the same private network as the ROCKS headnode; the GK's internal card is connected on the "same side" of the switch as the ROCKS compute nodes. 

The internal NIC had a IP address of 10.1.1.2. For the DNS address, we used 10.1.1.1, i.e., the ROCKS headnode. It is necessary to have the GK talk directly to the compute nodes - makes the operation of CONDOR more robust. I will talk more about this [[#MyHeadnode][here]].

Make sure that _eth0_ is connected to the internal network (_eth1_ is connected to the outside world).

---++ Following the OSG Install Guide

Now come my comments/tips on the install guide - [[Integration.OSGCEInstallGuide]]. For the most part, the guide is well documented. Some stuff needs to be better explained, at least for a novice.

---+++ Pre-installation Checklist

Most of the stuff listed here is pretty straightforward and well documented. One comment:

---++++ Firewalls

	If you follow the link, you will come to the separate section on [[Integration.OSGCEInstallGuide#Firewalls][Firewalls]]. As it is currently written, it appears that information regarding setting environment variables like GLOBUS_TCP_PORT_RANGE only applies to the case of host-based firewalls. This is not *true*. It applies equally to the case where you have an external firewall "surrounding the nodes". 

You need to set the variables *GLOBUS_TCP_PORT_RANGE* and *GLOBUS_TCP_SOURCE_RANGE* (both have the same range, _e.g.,_ 40000-41000).

	* In */etc/xinetd.d/gsiftp,gsiftp2,globus-gatekeeper* - both variables need to be set
	* Wherever you have installed the VDT software, below that will be sub-dirs, globus and vdt
		* In *globus/etc/globus-job-manager.conf*, you need to set *GLOBUS_TCP_PORT_RANGE*
		* In *vdt/etc/vdt-local-setup.sh (.csh)*, you can set both these varibles (_won't hurt_)

Remember to change the configuration files on the firewall to allow traffic on these ports, as well as the other ports specified in the documentation. 

---++++ User accounts

I installed the VDT software under /usr/local/grid and the plan is to export this to the ROCKS headnode and compute nodes. Thus, I put the user accounts in /usr/local/grid/users/. To make a new user account, do

useradd -d /usr/local/grid/users/&lt;username&gt; &lt;username&gt;

Remember, to set up the user accounts on the ROCKS headnode too. Here, it doesn't matter where the home directory is. Just use the command,

useradd &lt;username&gt;

The only requirement is that the user ID's should match on the two nodes.

---+++ Installation Procedure

Again the documentation is quite self-explanatory. Some comments:

---++++ Certificates

#MyCertificates Once you get your User certificate, you will also need other certificates which are for the host as well as for services on the host. In some cases, it is better to wait for the certificate before proceeding. I needed a host certificate as well as service certificates for LDAP and http. It may save time if you were to apply for all these certificates at once (%BLUE% Is this possible? Need to check %ENDCOLOR%).

To apply for the certificates, go to *https://pki1.doegrids.org* and follow instructions.

---+++ Configuration and setup of OSG CE Services

Again the documentation is quite self-explanatory. Some comments:

---++++ Configuring [[Integration.MonALISA][MonALISA]]

The !MonALISA daemon monitors the state of the ROCKS compute nodes through the ROCKS headnode. The latter uses Ganglia for collecting information. It was suggested that it would be better for this communication to occur via the internal network. So, when you are configuring !MonALISA, you will be asked the host name on which Ganglia is running. Instead of giving the fully qualified domain name, I used ahepg1h.local (my ROCKS headnode is called __ahepg1h.tld_). Even if you don't answer correctly, you can go back and edit the file $VDT_LOCATION/MonaLisa/Service/VDTFarm/vdtFarm.conf
Make sure in this line, it says ahepg1h.local instead of the FQDN

*PN_vdt{monIGangliaTCP, ahepg1h.local, 8649}%30

On the ROCKS headnode (_ahepg1h_), you have to edit */etc/gmond.conf* and set *trusted_hosts* to point to the Gatekeeper node. Again, you want to use the internal network. I say *trusted_hosts osg-gk.local* - on the internal network, the gatekeeper is known as *osg-gk*, even though to the external world, it is known as _aheposgk.tld_. I'll talk about how to set this up [[#MyHeadnode][here]].

---++++ Authorizing other users 

Since I have a test node, for now I am using the simplest scheme, *Grid3*, to authorize others users. Basically, I have grid-mapfile which gets updated regularly with user names.

*Tip* One thing I discovered was that if a user is in more than two VO's, only one entry is made in the grid-mapfile. This depends on the order of VO's in the file $VDT_LOCATION/edg/etc/edg-mkgridmap.conf. So, if your node doesn't allow the jobs from the first VO, then this user will not be allowed to run jobs on your node, even though they may belong to another VO which is authorized to run jobs.

You also need to do chkconfig --add edg-gridmapfile-upgraded, so that the daemon comes up at boot time.

---++++ Configuring CONDOR

This took the longest time to tweak. [[Main.TerrenceMartin][Terrence]] made many suggestions and we finally got it working. Here are the steps I followed. I did all this tweaking after I had set up the system and discovered that CONDOR wasn't working

---+++++ %BLUE% Setup the Gatekeeper on the internal network %ENDCOLOR%

#MyHeadnode As mentioned previously, the OSG Gatekeeper needs to communicate with both the ROCKS headnode (for [[Integration.MonALISA][MonALISA]]) and the ROCKS compute nodes (for CONDOR). It was suggested that we set it up so that it uses the internal network as much as possible. Some details are [[#MyGatekeeper][here]].

On the ROCKS headnode,

	* Make a file, */var/named/rocks.domain.local* and put the line "osg-gk A 10.1.1.2" in it.
	* cd /var/named
	* dbreport dns > rocks.domain
	* Make a file, */var/named/reverse.rocks.domain.local* and put the line "2.1.1 PTR osg-gk.local." in it.
	* dbreport dns reverse > reverse.rocks.domain
	* /etc/init.d/named reload

These steps allow osg-gk.local, osg-gk, 10.1.1.2 to be resolved. Remember, that for the Gatekeeper, the DNS server is the ROCKS headnode.

---+++++ %BLUE% Exporting directories from the Gatekeeper %ENDCOLOR%

I installed the VDT software below */usr/local/grid*. Some of the software needs to be visible from the ROCKS compute nodes, so we have to export these sub-dirs.

	On the _Gatekeeper_,

	* Edit */etc/exports* and add "/usr/local/grid 10.0.0.0/255.0.0.0(rw,no_root_squash)" - not clear if no_root_squash is necessary. We put it in while trying to fix a bug.
	* Export via "exportfs -r"  (refreshes the kernel)
	* "service nfs start" (Start NFS on the Gatekeeper) 
	* If you also do "chkconfig nfs on" and "chkconfig nfslock on", it will keep NFS on all the time

On the _ROCKS headnode_,

	* Edit */etc/auto.master* and add the line "/usr/local/grid /etc/auto.osg -- timeout 600"
	* Edit */etc/auto.osg* and add lines like "app osg-gk.local:/usr/local/grid/app". Similarly add more lines where _app_ is replaced by _data,scratch,users,condor_
	* Make the directory */usr/local/grid*
	* Issue the command "cluster-fork mkdir /usr/local/grid" - this makes the sub-dir on all the ROCKS compute nodes
	* Since the ROCKS headnode communicates to the compute nodes via 411, restart this service. Issue the following commands, 
		* "make -C /var/411 clean"
		* "make -C /var/411"
		* "cluster-fork /opt/rocks/bin/411get" - forces the compute nodes to get the latest 411 information
		* "cluster-fork service autofs reload"

---+++++ %BLUE% To start CONDOR at boot %ENDCOLOR%

To ensure that CONDOR starts up at boot, 

	* Make the directory */etc/condor/* 
	* Issue the command, "ln -s /usr/local/grid/condor/etc/condor_config /etc/condor"

---+++++ %BLUE% Other CONDOR configuration files to be changed %ENDCOLOR%

We also need to edit a few CONDOR configuration files on the %RED% Gatekeeper %ENDCOLOR%

Edit */usr/local/grid/condor/etc/condor_config*

	* Set "RELEASE_DIR = /usr/local/grid/condor"
	* Set "LOCAL_DIR = $(RELEASE_DIR)/local.$(HOSTNAME)"
	* Set "LOCAL_CONFIG_FILE = $(LOCAL_DIR)/condor_config.local"
	* Set "CONDOR_HOST = osg-gk.local"
	* Set "UID_DOMAIN =albany.edu"
	* Set "TRUST_UID_DOMAIN" = True
	* Set "FILESYSTEM_DOMAIN" = True
	* Set "USE_NFS = True"  (can't hurt)

Edit */usr/local/grid/condor/local.aheposgk/condor_config.local*. I think this file gets created when we initially ran CONDOR as per instructions in the Install guide (Remember, I did all this tweaking after I had set up the system and discovered that CONDOR wasn't working).

	* Set variables _CONDOR_HOST, RELEASE_DIR, LOCAL_DIR, UID_DOMAIN, FILESYSTEM_DOMAIN, USE_NFS__ as above.
	* Set "NETWORK_INTERFACE = 10.1.1.2" (do this at the very top of the file, just after CONDOR_HOST)
	* Set the variable "DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, SCHEDD" (I didn't want the gatekeeper to run jobs, so I removed STARTD from the list

After all this tweaking, do "condor_reconfig" on the Gatekeeper. Re-reads the config. file(s).

Wait, we are not done yet. We need to make local config files for the ROCKS compute nodes. I did it by hand (to test the system), but one can do issue a cluster-fork command from the ROCKS headnode and do all compute nodes at once. In any case, I did the following.

On one of my %RED% ROCKS compute nodes (compute-0-0.local) %ENDCOLOR%, I did the following,

	* In /usr/local/grid/ do "mkdir condor/local.compute-0.0/" (%BLUE% Do I have to make them on all compute nodes by hand? %ENDCOLOR%)
	* "touch condor_config.local" within this sub-dir
	* Issue the command, "/usr/local/grid/condor/condor_configure --install_dir = /usr/local/grid/condor/ --type=execute --central-manager=osg-gk.local --owner=condor" - this sets up the configuration file (%BLUE% This step could be done by cluster-fork, but I am not sure %ENDCOLOR%)
	* Issue the command, "EXPORT CONDOR_CONFIG = /usr/local/grid/condor/etc/condor_config" followed by "condor/sbin/condor_master" - This turns CONDOR on this node
	* For the other nodes, I did, "cp -rp local.compute-0-0/ local.compute-0-1/", etc. and start CONDOR on those nodes. (%BLUE% Is there a better way? %ENDCOLOR%

This should get it working...

---++ Testing some of this software

I set up another machine where I installed VDT-Client software using the command 

*pacman -get http://www.cs.wisc.edu/vdt/vdt-136-cache:VDT-Client *

I put my User Certificate on this machine and made the .pem files as explained on the website mentioned [[#MyCertificates][above]]

Make sure that this machine does not have a firewall, or the Gatekeeper will not be able to talk to it.

