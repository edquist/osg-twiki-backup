%META:TOPICINFO{author="KyleGross" date="1225985950" format="1.1" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+!!<nop>%TOPIC%
%TOC%

%STARTINCLUDE%

---++ Minimum Requirements

There are two main areas that need to be properly configured for interoperability to work.  The first is to properly configure the Generic Information Provider.  The second is to have the Compute Nodes set up in a certain manner.

---+++ Generic Information Provider Setup

An entry for each VirtualOrganizations/VOInfo supported is needed as a <nop>GlueCEUniqueID.  This entry describes the queue available to a VO.  Please note that jobmanager-condor-cms does not exist nor does it have to be created.  This is a shorthand for the Resource Broker used in the LCG testbed that means when a job is submitted, that it should be submitted to rsgrid3.its.uiowa.edu:2119/jobmanager-condor with an attributed in the RSL set to (queue=cms).  (Also, one important thing to note is that this shorthand is necessary for interoperability)

<pre>GlueCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-cms</pre>

For each queue advertised there has to be a minimal set of attributes.

<pre>GlueCEStateStatus: Production</pre>

The <nop>GlueCEStateStatus needs to equal Production, the GIP should automatically configure this to be Production.  This attribute is basically the equivalent of the site state bit for the LCG.  I think the other value is 'Closed' which indicates that jobs should not be scheduled at this site.

<pre>GlueCEAccessControlBaseRule: VirtualOrganizations/VOInfo:cms</pre>

The <nop>GlueCEAccessControlBaseRule should be in the format 'VO: vo_name' where vo_name is the Virtual Organization which has access to this queueing policy.

---+++ Worker Node Setup

The LCG testbed does not assume that the home directories of users are shared accross nodes.  It also does not have an equivalent of the OSG NFS mounted $TMP directory.  (There is a directory $VO_CMS_SW_DIR where the CMS software is stored and NFS mounted on all compute nodes, but it is only writable by very few grid users)

Therefore, data is loaded onto and removed from the Compute Nodes using globus-url-copy.  In order for this to occur the following conditions must be met on the compute nodes.

   * Outbound access must be available
   * $GLOBUS_LOCATION must exist
   * globus-url-copy must be available
   * grid-proxy-info must be available (the proxy is checked for validity by the wrapper script)

In order for globus-url-copy and grid-proxy-info to work, the CA signing policies must be available on the worker nodes in one of the following locations.

   * env. var. X509_CERT_DIR
   * $HOME/.globus/certificates
   * /etc/grid-security/certificates
   * $GLOBUS_LOCATION/share/certificates

Note: It is the authors suggestion to put them in $GLOBUS_LOCATION/share/certificates if it is shared accross the nodes and link /etc/grid-security/certificates to this directory on the headnode.  X509_CERT_DIR is set in $VDT_LOCATION/setup.sh but is not initially available to jobs running on compute nodes.

-- Main.RansomBriggs - 22 Jun 2005

%STOPINCLUDE%