%META:TOPICINFO{author="RobGardner" date="1213284874" format="1.1" reprev="1.30" version="1.30"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Background
The goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters.  Scientists organized into virtual organizations (VOs) and requiring High Throughput Computing are the users of the interface and consumers of the CPU cycles and storage. 

Your site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them. 

As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support.  The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. As end-user scientists send jobs into your cluster's batch system, your CE  receives them and parses them out to Worker Nodes (WN) for execution. Some VOs and scientists require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE) on which more OSG storage-specific software is installed. A site is not required to provide both a CE and an SE.

---++ Site Policies

OSG expects you to clearly specify your site's policies regarding resource access.  Please write them on a web page, make this page part of your site registration, and make it available via VORS (you will answer a question about this during configuration) and the OSG information management system, OIM.  We encourage you to allow all virtual organizations registered with the OSG at least "opportunistic use" of your resources. You may need to preempt those jobs when higher priority jobs come around. The scientists using the OSG generally prefer having access to your site subject to preemption over having no access at all.

---++ Compute Element (CE)

The standard installation includes a GRAM and Gridftp interface; both are currently required on the same CE host for the Condor-G client to successfully stage out (smallish) output files that are defined as part of the job's JDL.  The CE host needs to include a sizeable local disk (~100GB or more) as scientists tend to specify a few (smallish) input and output files as part of their JDL. Those files, in addition to stdout and stderr of all jobs submitted via the CE, are spooled via the CE out to your cluster. 

You must determine your security policy with regard to Unix ID management on the cluster.  You may choose group accounts and/or dynamic accounts for all users.

You must choose the OS (most if not all OSG clusters are some RHEL derivative), the batch system (Condor, PBS, LSF, and SGE are presently supported), and the network architecture (default assumption is public/private with NAT - you will need to advertise your architecture by changing some settings by hand if yours isn't like this) of your cluster. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster (NFS-lite). 

The CE hosts information provider(s) and monitoring services, most of which are configured correctly by default.  We require all OSG sites to deploy Gratia, the OSG accounting system. Your site thus sends accounting records to OSG.  Aggregated summaries of this information can be viewed via the 
[[http://gratia-osg.fnal.gov:8880/gratia-reporting/][GRATIA displays]].

There are other services the run off the CE that are required:
   * CEMon which provides the basis for the information services in OSG.
   * RSV the Resource Service Validation system which schedules execution of local "probes" of your CE (and SE), and reports the results up to the GOC.  This is important for service availablity monitoring of OSG sites.
  * The package of tools available to the OSG job environment: the OSG worker-node client package.  Many site admins install this package on a single server and NFS export it to compute nodes.  

---++ Storage Element (SE)
There are two types of storage element services provided by OSG which implement SRM v2.  See pointers to instructions for these services:
   * [[DCache]]  - generally useful for aggregating disks from lots of servers
   * [[Bestman]] - sits in front of NFS, GPFS, Lustre distributed filesystems.  There is also a version which supports xrootd filesystems. 

These services are supported by the OSG Storage group.  Please email osg-storage@opensciencegrid.org for installation and support questions for these services.


<!-- 

%INCLUDE{ "Integration.StorageElementAdmins" section="SEintro" }%

-->


<!-- The SE is currently much less well defined than a  CE. Scientists expect three types of areas: 

   * an area into which they can install application software releases via the CE; this must be read-only accessible from all batch slots on your cluster. 
   * an area to which they can stage data using gftp; this must be readable from all batch slots. 
   * an area to which they can stage output files from all batch slots, for later asynchronous retrieval using gftp. 


In addition, the following must be available at each batch slot: 

   * a set of "client tools" that are part of the OSG software stack. These can be either exported from the CE host, from a separate host, or be deployed locally on each compute node. 
   * an area that is strictly local to each batch slot, from within which they run their job(s) during the time they have "leased" the batch slot. You or your batch system is expected to clean this area after the scientists release the batch slot to you. 
-->

---++ Optional site services
There are additionally a number of optional site services you may wish to install, including:
   * GUMS - the Grid User Management Service, for managing Grid accounts on your cluster
   * Squid - a web caching service
   * Syslog-ng - a logging service that can be used to forward CE logfiles to the GOC for troubleshooting purposes.
   * Monalisa - a monitoring program

---++ VO management services
There are two services that are used by VO administrators for management of VO information:
   * VOMS the VO management service
   * VOMRS the administrative interface to VOMS

Usually these are installed and operated by a designated central group affiliated with a given VO.

---++ OSG user software
The OSG user software consists in
   * OSG-Client (which is basically VDT-Client) and include tools like Condor-G for submission and management of jobs on the OSG.
   * Installed on every OSG site is a package called Worker-node client that has Globus client tools, wget, Myproxy, LCG-Utils (a set of partially supported client tools from gLite), etc.

%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.RobGardner %BR%
%REVIEW% Main.AlainRoy - 20 Nov 2007 %BR%
%REVCOM%  %BR%
%REVFLAG%   %BR%



%META:TOPICMOVED{by="RobGardner" date="1192827019" from="Integration/ITB_0_7.OverviewServicesOSG" to="Integration/ITB_0_7.OverviewOfServicesInOSG"}%
