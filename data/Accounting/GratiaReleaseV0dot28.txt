%META:TOPICINFO{author="JohnWeigand" date="1193676884" format="1.1" version="1.19"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! Gratia Services V0.28 - 2007/10/27

%TOC%

%STARTINCLUDE%

---+ Release Notes
This release (v0.28) incorporates the following changes and updates ...
<dl>
<dt style="font-weight: bold; font-size: larger">Since VDT 1.8.1b release (v0.27.5)</dt>
<dd>
<ul>
  <li>Collector
  <ul>
    <li>Improvements for !MetricRecords.</li>
    <li>Upgrade hibernate to v3.2.5.</li>
  </ul></li>
  <li>Reporting
  <ul>
    <li>Upgrade to a new version of the report viewer: BIRT v2.0 -> v2.2.1
    <ul>
      <li>Link to allow reports to be bookmarked.</li>
      <li>PDF generation available.</li>
      <li>VO multi-select now available in one report ("Usage Metrics - For VO(s)").</li>
    </ul></li>
    <li>All drill through reports and should work now; and download =.csv= should be fixed everywhere.</li>
  </ul></li>
</ul>
</dd>
<dt style="font-weight: bold; font-size: larger">Between VDT 1.8.1b and the last version running on the production servers (v0.26.6)</dt>
<dd>
<ul>
  <li>Administration
  <ul>
    <li>Detailed view available for main status page.</li>
  </ul></li>
  <li>Collector
  <ul>
    <li>Support for probe handshaking.</li>
  </ul></li>
  <li>Reporting
  <ul>
    <li>Fix calendar pop-up.</li>
    <li>Fix "See Table" functionality for some reports.</li>
  </ul></li>
</ul>
</dd>
<dt style="font-weight: bold; font-size: larger">Between v0.26.6 and VDT 1.6.1 (v0.25)</dt>
<dd>
<ul>
  <li>Administration
  <ul>
    <li>Minor improvements to the: probe status; VO; and replication control pages.</li>
  </ul></li>
  <li>Collector
  <ul>
    <li>Fix history clean-up.</li>
    <li>Replication now more versatile.</li>
    <li>Update hibernate to v3.2.4sp1.</li>
    <li>VOName correction improved.</li>
  </ul></li>
  <li>Reporting
  <ul>
    <li>Introduce ranked reports.</li>
    <li>Generalized usage reports to enable selection of metric.</li>
    <li>Fix some drill-throughs.</li>
  </ul></li>
</ul>
</dd>
</dl>

Prior to starting the conversion, we need to verify that <nop>MySql is configured correctly for the 6Gb of available memory on the _gratia06_. <b>Note: This is not going to be done at this time.  See Post-Mortem notes.</b>


<!--   ANTICIPATED DOWNTIME --------------- -->
---+ Anticipated downtime
It is expected that this release will require the Gratia services and reporting to be unavailable during the periods specified below:
   * Start: 10/27/2007 (Saturday) 17:00 CDT.%BR%
   * Projected Availability: 10/28/2007 (Sunday) 10:00 CDT.

The changes affecting downtime  for this release are:
   1 Addition of a column in the <nop>JobUsageRecord_Meta table.
   1 The necessity of recreating all the summary tables due to the addition of the new table.

As a side note, it is anticipated that downtime for future releases will be minimal as a new procedure will be used when lengthy database conversion are necessary.  It cannot be down for this release as the necessary hardware is not yet in place.


<!--   COLLECTORS AND DATABASES AFFECTED  --------------- -->
---+ Collectors and Databases Affected

The following Gratia collectors and databases will be converted with this release:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 35 | text, 15 | text, 5 | text, 15 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Schema*|*<nop>MySql port*|*Collector URL*|*Collector host*|*Size (bytes)*|*Size (rows)*|
| fermi_itb | gratia06.fnal.gov:3320 | gratia-fermi.fnal.gov:8881 | gratia08.fnal.gov: |  724K |                  1 |
| fermi_osg | gratia06.fnal.gov:3320 | gratia-fermi.fnal.gov:8880 | gratia08.fnal.gov: |  4.3G |   2,237,912 |
| gratia | gratia06.fnal.gov:3320 | gratia.opensciencegrid.org:8880 | gratia09.fnal.gov |  81G | 26,476,198 |
| gratia_itb | gratia06.fnal.gov:3320 | gratia.opensciencegrid.org:8881 | gratia09.fnal.gov |  15G |   2,688,424 |
| gratia_osg_integration | gratia06.fnal.gov:3320 | gratia.opensciencegrid.org:8885 | gratia09.fnal.gov |  3.1G |       815,371 |
| gratia_qcd  | gratia06.fnal.gov:3320 | gratia-fermi.fnal.gov:8883 | gratia08.fnal.gov: |  2.2G |       804,418 |


The following Gratia collectors and databases will __NOT__  be converted with this release.  These repositories contained specialized reports that have not as yet been upgraded to the new Birt V2..2.1 software. __However__, they will be taken out-of-service while the other databases are being updated.
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 35 | text, 15 | text, 5 |  text, 15 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Schema*|*<nop>MySql port*|*Collector URL*|*Collector host*|*Size (bytes)*|*Size (rows)*|
| gratia_psacct | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8882 | gratia08.fnal.gov: |  6G |   3,790,511 |
| gratia_osg_daily | gratia06.fnal.gov:3320 | gratia.opensciencegrid.org:8884 | gratia09.fnal.gov |  60M |         53,929      |


<!--   CONVERSION STEPS -------------------- -->
---+ Conversion steps
This release involves a database schema change on the <nop>JobUsageRecord_Meta table, it is expected to take a significant amount of time on the production _gratia_ data base.  
In order to minimize the overload that is certain to occur if we were to stop the Gratia collectors from accepting data from the probes, the Gratia collectors will only be taken down for a short period while the database is being backed up.


<!-- ----------------- SHUTDOWN AND DATABASE BACKUP ------------- -->
---++ Shutdown and database backup.
  <ol type="1">
    <li>On _gratia06_ , __comment__ out the _gratia_ user cron jobs.%BR%
            __Status - Done 10/27/2007 08:00__
   </li>

     <li>On _gratia06_, __comment__ out the _mysqlhotcopy_cron_ entry.
<pre class="screen">
43 2 * * * /usr/local/bin/mysqlhotcopy_cron.sh &gt; /var/log/mysqlhotcopy.out 2&gt; &1
</pre>%BR%
            __Status - Done 10/27/2007 08:00__ <p>
 

    </li>

     <li> For __ALL__ of the databases (__including those not be upgraded__) , __stop__ the Gratia update services.%BR%
            - In your browser,  connect to the Gratia administrative services url for each of the databases.%BR%
            - Select the _System / Administration_ menu option in the left menu%BR%
            - Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Stop Update Services_ link.%BR%
            __Status - Started  17:05   Ended  17:17__ 
      </li>

     <li> On __each__ collector/tomcat host, __stop__ the tomcat service for __ALL__ Gratia collectors.
<pre class="screen">
On gratia09:
  service tomcat-gratia  stop
  service tomcat-itb  stop
  service tomcat-osg_daily  stop
  service tomcat-osg_integration stop

On gratia08:
  service tomcat-fermi_itb  stop
  service tomcat-fermi_osg stop
  service tomcat-ps  stop
  service tomcat-qcd stop
</pre>

            __Status - Started  17:55   Ended  17:57__ 


</li>


   <li>Taking a back up the database instance on _gratia06_ (this will include all schema) using part of the _msqlhotcopy_cron_ script.
           This is in the event that the compacting of the <nop>JobUsageRecord_Xml has insufficient space and corrupts the table. Backup is only being done to  _/backup/mysqldb_ on _gratia06_ .
      <pre class="screen">
<b>Instances being upgraded to v0.28:</b>
mysqlhotcopy -p lisp01 --addtodest fermi_itb /backup/mysqldb       Duration - 1 sec
mysqlhotcopy -p lisp01 --addtodest gratia_qcd /backup/mysqldb      Duration -  64 secs
mysqlhotcopy -p lisp01 --addtodest gratia_osg_integration /backup/mysqldb     Duration - 99 secs
mysqlhotcopy -p lisp01 --addtodest fermi_osg /backup/mysqldb    Duration - 132 secs
mysqlhotcopy -p lisp01 --addtodest gratia_itb /backup/mysqldb     Duration -  461 secs
mysqlhotcopy -p lisp01 --addtodest gratia /backup/mysqldb        Duration - 2900 secs (48+ minutes)

<b>Instances NOT being upgraded to v0.28:</b>
mysqlhotcopy -p lisp01 --addtodest gratia_osg_daily /backup/mysqldb    Duration -  6 secs
mysqlhotcopy -p lisp01 --addtodest gratia_psacct /backup/mysqldb     Duration - 179 secs

</pre>

            <b>Status - Started  18:01   Ended -  18:30  for all except the _gratia_ instance.</b>%BR%
            <b>Status of _gratia_ instance - Started    18:30   Ended  19:18  Elapsed - 48+ min) </b><br>
 (Was expecting this to take 80 minutes based on looking at the _mysqlhotcopy_ logs).
%BR% Note: The times reported by _mysqlhotcopy_ seem to understate the actual duration.

<b>Backups complete at 19:20</b>

     </li>





    <li>Compact the gratia schema Xml table.
<pre>optimize table gratia. <nop>JobUsageRecord_Xml ;</pre>

           __Note:__ _<nop>JobUsageRecord_XML_ table size is 53Gb.  On _gratia07_ , started an "_optimize table <nop>JobUsageRecord_Xml_" to determine how long it would take on _gratia06_.  %BR%
            Started -  10/27/2007 08:17  Ended - Never.  There was insufficient space on the disk. Caused the table to become corrupt.  Concern is now that if I do this on production, can I recover.  So am attempting to recover from the _gratia06_ _/backup/mysqldb_ area only the <nop>JobUsageRecord_Xml table. %BR% 
<b>Started scp at 17:01  Ended - 17:46  Elapsed - 45 min</b>%BR%
Status of table on gratia07 is good.  So this is viable.  I will only back up the databases to the local _/backup/myslqdb_ area on _gratia06_.  This analysis was done a little out of the original order and a deviation from how it was planned.


<b>State of the storage when I started optimizing at 17:22</b>
<pre>
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/LG0-LV0    99G   33G   62G  35% /data/mysqldb
/dev/mapper/LG0-LV4   156G   81G   68G  55% /data/mysqldb/gratia
</pre>

<b>State of the storage when optimization completed at 17:47 (Elaspsed: 25+ minutes)</b>
<pre>
/dev/mapper/LG0-LV0    99G   33G   62G  35% /data/mysqldb
/dev/mapper/LG0-LV4   156G   47G  101G  32% /data/mysqldb/gratia
</pre>

After phone conversation with Chris, he advised deleting the duplicate records to speed up the conversion process:
<pre>
delete from DupRecord where error = 'Duplicate';
</pre>
<b>Status:  Done at 21:25 (   64061 records)</b>
</li>


    <li>Back up the database instance on _gratia06_ (this will include all schema) using the _msqlhotcopy_cron_ script.
           With the collector update services stopped on all database schemata, this will in affect be a complete backup.  
            This script copies the backup files that are backed up to <nop>_TiBS_ .
      <pre class="screen">&gt; /usr/local/bin/mysqlhotcopy_cron.sh > /var/log/mysqlhotcopy.out 2>&1</pre>
          <b>Note: Chose not to do this after seeing that recovery of individual tables seemed to work and due to the lack of available space that I could find.  This should be done in the future.</b>

Instead, I backed up only the _gratia_ schema to the _/backup/mysqldb_ directory using:
<pre>
mysqlhotcopy -p lisp01 --addtodest gratia /backup/mysqldb        Duration - 1806 secs (30+ minutes)
</pre>
            <b>Status - Started  19:53   Ended  20:23  Elapsed: 30+ minutes</b>

     </li>


    <li>As an added measure, take a standard tree copy of the _/data/mysqldb_ tree on _gratia06_  .<br>
           <b>Note: Chose not to do this after seeing that recovery of individual tables seemed to work and due to the lack of available space that I could find.  This should be done in the future.</b>
     </li>


    <li>Take a  backup of the _/data/tomcat-*/gratia/data_ directory as a _tar file_.
<pre class="screen">
To be provided by John Weigand (if needed)
</pre></li>
<b>Note: not done, most likely due to battle fatigue.</b>
   </ol>


<!-- ----------------- UPGRADE AND IMPLEMENTATION ------------- -->
---++ Upgrade and implementation
The __upgrades should be single-threaded__ , that is, performed for each database schema on at a time.  
The reason for this is that they are sharing the same <nop>MySql instance and the schema upgrades will be resource intense.

First, build the software release if you have not already done so<br>
   * <b>Status -  Completed on 10/26/07 at 19:22</b><br>
   * Directory - /home/weigand/cdcvs/gratia/target<br>
   * All files (war and tar), including the VDT tar files are in _/home/gratia/gratia-releases/v0.28_ .

We will perform these upgrades based on the size of the individual database schema, in ascending order:
<ol>

  <li>Install the new software on a Gratia tomcat instance:
<pre class="screen">
pswd=xxx
source=/home/weigand/cdcvs/gratia
pgm=/home/weigand/cdcvs/gratia/common/configuration/update-gratia-local 
<b>On gratia09:</b>
$pgm  -d $pswd -S $source  osg_integration
$pgm  -d $pswd -S $source  itb
$pgm  -d $pswd -S $source   gratia

<b>On gratia08:</b>
$pgm  -d $pswd -S $source  fermi_itb
$pgm  -d $pswd -S $source qcd
$pgm  -d $pswd -S $source   fermi_osg
</pre>
</li>

<li>This is optional, but probably a good idea to save off the logs under each tomcat instance and empty the _log_ directory.  It will facilitate catching any errors that may occur (of course there won't be any, but a good idea anyway):
<pre class="screen">
<b>On gratia09:</b>
date=`date '+<nop>%Y<nop>%m<nop>%d'`
cd /data/tomcat-osg_integration/logs
tar zcf  /data/gratia_tomcat_logs_backups/tomcat-osg_integration.$date.tgz     * 
rm -f *

cd     /data/tomcat-itb/logs/
tar zcf /data/gratia_tomcat_logs_backups/tomcat-itb.$date.tgz   *
rm -f *

cd  /data/tomcat-gratia/logs/
tar zcf /data/gratia_tomcat_logs_backups/tomcat-gratia.$date.tgz  *
rm -f *

<b>On gratia08:</b>
date=`date '+<nop>%Y<nop>%m<nop>%d'`
cd        /data/tomcat-fermi_itb/logs/
tar zcf /data/gratia_tomcat_logs_backups/tomcat-fermi_itb.$date.tgz   *
rm -f *

cd  /data/tomcat-qcd/logs/
tar zcf /data/gratia_tomcat_logs_backups/tomcat-qcd.$date.tgz     *
rm -f *

cd     /data/tomcat-fermi_osg/logs/
tar zcf /data/gratia_tomcat_logs_backups/tomcat-fermi_osg.$date.tgz  *
rm -f *

</pre>
</li>

<li>Start the gratia tomcat services:
<pre class="screen">
&gt; service &lt;tomcat service&gt; start
</pre>

When the tomcat service initializes, it will detect that schema changes have been effected and a conversion process will begin.
</li>

<li>When complete (=catalina.out= contains the message, "INFO: Server startup in _xxx_ms") __start__ the Gratia update services for the database schema just upgraded.%BR%
            - In your browser, connect to the Gratia administrative services url for each of the databases.%BR%
            - Select the _System / Administration_ menu option in the left menu%BR%
            - Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Start Update Services_ link.

   <ul>
     <li> As __each__ collector/tomcat host update service is started, monitor the tomcat logs files for any errors.</li>
     <li> Bring up the gratia administrative web interface and verify that the collectors are processing the data.</li>
    </ul>

<li>If satisifed, __stop__ the tomcat service for that tomcat database schema so you can proceed to the next one.<pre class="screen">&gt; service &lt;tomcat service&gt; stop</pre></li>
</ol>

Summary of conversion times:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 10 | text, 15 | text, 15 | text, 6 | text, 25 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Schema*|*Collector*|*Size*|*Size (rows)*|*Wall clock*|*Elapsed*|*Conversion time*|
| fermi_itb | gratia08 |  724K |                  1 |  21:25 - 21:25 |  00:01  | INFO: Server startup in 14786 ms |
| gratia_qcd  | gratia08 |  2.2G |       804,418 |  21:28 - 21:30 |  00:02  | INFO: Server startup in 166889 ms |
| gratia_osg_integration | gratia09 |  3.1G |       815,371 |  21:34 - 21:34 |  00:01  | INFO: Server startup in 14582 ms |
| fermi_osg | gratia08 |  4.3G |   2,237,912 |  21:38 - 21:49  |  00:11  | INFO: Server startup in 634547 ms |
| gratia_itb | gratia09 |  15G |   2,688,424 |  22:12 - 23:26 |  01:14  | INFO: Server startup in 4474992 ms |
| gratia | gratia09 |  47G | 26,476,198 |  00:34 -  |  |  |



<b>Note: the size of the gratia schema at the time of conversion has been reduced from 81Gb to 47Gb prior to conversion due to the optimization of the <nop>JobUsageRecord_Xml table.</b>

<b>Notes on _gratia_itb_  conversion:</b> 
   1 <nop>DupRecord table appears to be causing a problem with gratia_itb conversion.  It has 6.9M rows.  The gratia only has 15K rows and fermi_osg only had 12K.  It has to be the reason it is taking so long compared to fermi_osg.  Deleted these after conversion completed.

<b>Notes on _gratia_  conversion:</b> 
<ol>
<li>
As of 10/28/07 08:40, schema is still being updated.  It is currently in this state:
<pre>
mysql> show processlist;
+--------+--------+-------------------------+------------+---------+-------+----------------------+------------------------------------------------------------+
| Id     | User   | Host                    | db         | Command | Time  | State                | Info                                                       |
+--------+--------+-------------------------+------------+---------+-------+----------------------+------------------------------------------------------------+
| 672862 | gratia | gratia09.fnal.gov:25627 | gratia     | Query   | 28916 | Repair with keycache | alter table JobUsageRecord_Meta add column probeid integer | 
| 672907 | root   | localhost               | gratia     | Query   |     0 | NULL                 | show processlist                                           | 
+--------+--------+-------------------------+------------+---------+-------+----------------------+------------------------------------------------------------+
</pre>
</li>

<li>As of 10/28/07 10:05, it appears it is still successfully doing the conversion.  It is still in the same state as it was at 08:40, but I noticed the database size as grown to 53Gb.  So something must still be happening.
</li>

</ol>


<!-- ----------------- ACTIVATING THE NEW RELEASE ------------- -->
---+ Activating the new release
  <ol type="1">
     <li>On _gratia06_ , __uncomment__   the _mysqlhotcopy_cron_ entry<pre class="screen">43 2 * * * /usr/local/bin/mysqlhotcopy_cron.sh > /var/log/mysqlhotcopy.out 2>&1</pre></li>
     <li> For __ALL__ of the databases ( __including those not be upgraded__ ), __verify__ the Gratia update services are active.%BR%
            - In your browser,  connect to the Gratia administrative services url for each of the databases.%BR%
            - Select the _System / Administration_ menu option in the left menu%BR%
            - Then scroll down to the _Starting/Stopping Database Update Services_ section and and view the status.
      </li>
     <li> On __each__ collector/tomcat host, __start__ the tomcat service for the Gratia collectors.<pre class="screen">On gratia09:
  service tomcat-gratia start
  service tomcat-itb start
  service tomcat-osg_daily start
  service tomcat-osg_integration start

On gratia08:
  service tomcat-fermi_itb  start
  service tomcat-fermi_osg start
  service tomcat-ps start
  service tomcat-qcd start</pre></li>

   <li> As __each__ collector/tomcat host service is started, monitor the tomcat logs files for any errors.</li>

   <li> Bring up the gratia administrative web interface and verify that the collectors are processing the data.</li>

    <li>On _gratia06_, <b>uncomment</b>  the _gratia_ user cron jobs.</li>
</ol>


<!-- ----------------- POST MORTEM  ------------- -->
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.

<ol>
<li>Configuring <nop>MySql to optimally use the 6Gb of memory on _gratia06_.
   <ul><li>Both _gratia06_ and _gratia07_ should be configured the same.</li>
            <li>Both nodes are using a UPS version of MySql which should probably be changed.%BR%
                    The _my.cnf_ on _gratia06_ is actually a symbolic link to the ups area.  </li>
             <li>The init.d services on both hosts should be explicitly setting the _my.cnf_ used so there is no confusion</li>
     </ul>       
</li>


<li>The administrative capability to start/stop the collector update services.%BR%
        One problem with trying to perform a smooth conversion (and also adds an element of risk) is the "feature" that activates the _update services_ on the restart  of _tomcat_ .   This activation occurs after _tomcat_ is started and any database conversions are performed.  It would be nicer is the service remembered it's previous state.  Then after a database conversion, you would be able to validate the database prior to any updates being applied.

</li>


<li>Backups:  mysqlhotcopy is using the /usr/bin/ version and not the UPS version that the database is using.  Need to verify if they are one and the same.  If not, this may not be good.
</li>

<li>Noticed as I was monitoring gratia_itb that I am still having queries come in which can affect conversions... these should be blocked out.
<pre>
| 330688 | reader | osg-test2.unl.edu:36169                   | gratia     | Query   |    1 | Copying to tmp table | SELECT
             R.VOName   ,
          unix_timestamp(EndTime),
             sum(WallDuration)/3 | 
</pre>
</li>

<li>Forgot about Replication and I think it had a big impact on the gratia_itb schema.  Not sure if it is what is causing the java exceptions.<br>
   * stop tomcat at 32:50
   * ran: delete from DupRecord where error = 'Duplicate';
</li>

<li>One thing I forgot to do which I had thought of was to back up the webapps and gratia areas on tomcat which would allow me to restore to the old software easily.  Still can grab them from the 2 tomcats that have not been converted, maybe.  A couple ./gratia files will need to be tweeked. Must do this in future.
</li>

<li><b>Important:</b> From Chris: must remember to <i>stop</i> update services on the administrative menu before taking tomcat down.  Need to add this to my update scripts for VDT when updating from new build.  Should be a  way of doing this in a shutdown type script.
   * Question-like or something to look into: Does the <i>update_gratia_local</i> or the instructions say to do this?
</li>

<li>
<b>Things  to consider:</b> 
   * Move the <i>start/stop</i> update services to the main administrative window showing state of service and links/buttons to start/stop services.  Right now it is too hidden being on the administrative screen and buried in the middle.
   * We need to test another approach to adding/removing columns from huge tables like <nop>JobUsageRecord* tables.  We should be able to test alternatives now on the gratia07 mysql instance since it is smaller.
</li>

<li>There are some Gratia daily reports that kick off around 07:00 each morning that should be stopped  when converting.  Not sure where they come from.  Need to document somewhere.
<li>

<li>
After taking all tomcat services down to start conversion, should re-cycle mysql database to clear everything.  This maybe should be done after each schema conversion.  This is something Chris mentioned last night.
</li>

<li>Maybe found a way to see the progress the conversion is making (this is as of 10/28 10:49):
<pre>
cd /data/mysqldb/gratia
ls -ltr
-rw-------  1 gratia gratia   201636864 Oct 28 00:27 JobUsageRecord_Xml.MYI
-rw-------  1 gratia gratia 21080917896 Oct 28 00:27 JobUsageRecord_Xml.MYD
-rw-rw----  1 gratia gratia  2513827840 Oct 28 00:27 JobUsageRecord_Meta.MYI
-rw-rw----  1 gratia gratia  3781450376 Oct 28 00:27 JobUsageRecord_Meta.MYD
-rw-------  1 gratia gratia  2155423744 Oct 28 00:27 JobUsageRecord.MYI
-rw-------  1 gratia gratia 12936377424 Oct 28 00:27 JobUsageRecord.MYD
-rw-rw----  1 gratia gratia     8260152 Oct 28 00:27 HostDescriptionProbeSummary.MYD
-rw-rw----  1 gratia gratia        9186 Oct 28 00:34 #sql-66c4_a445e.frm
-rw-rw----  1 gratia gratia  3781435492 Oct 28 08:12 #sql-66c4_a445e.MYD
-rw-rw----  1 gratia gratia  1775113216 Oct 28 10:48 #sql-66c4_a445e.MYI
</pre>

This appears to show it is about 3/4 (don't check my math)  through re-building the index.  However, on second thought, it could be close to done at this time and the schema change was to use the probeid instead of the probename to reduce the size of the index.  This is showing it is getting updated as time goes by, but the size is not changing.  The table itself (MYD) appears to have completed at 08:12 and the index is still being played with.
<pre>
-rw-rw----  1 gratia gratia  3781450376 Oct 28 00:27 JobUsageRecord_Meta.MYD
-rw-rw----  1 gratia gratia  3781435492 Oct 28 08:12 #sql-66c4_a445e.MYD

-rw-rw----  1 gratia gratia  2513827840 Oct 28 00:27 JobUsageRecord_Meta.MYI
-rw-rw----  1 gratia gratia  1775113216 Oct 28 10:59 #sql-66c4_a445e.MYI
</pre>

This also explains the apparent growth in the database from 47Gb to 53Gb.  This should drop back down after it completes the re-building.
</li>

<li>Should turn of init.d services on startup and turn back on when down, at least for tomcats on 08 and 09
<pre>
chkconfig tomcat-blah off|on
</pre>

</ol>

%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->
---++!! Major updates
<!--Future editors should add their signatures beneath yours!-->
-- Main.JohnWeigand - 25 Oct 2007

%META:TOPICMOVED{by="JohnWeigand" date="1193405880" from="Accounting.GratiaRelease20071026" to="Accounting.GratiaReleaseV0dot28"}%
