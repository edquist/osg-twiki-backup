%META:TOPICINFO{author="JohnWeigand" date="1213389318" format="1.1" reprev="1.54" version="1.54"}%
%META:TOPICPARENT{name="WebHome"}%
<!--
These are general in nature and should be changed in the this template if the location changes:
   * Set CURRENT_TOMCAT = ==gratia09==
   * Set CURRENT_MYSQL = __gratia06__
   * Set NEW_TOMCAT = ==gratia07==
   * Set NEW_MYSQL = __ gratia07__
   * Set TOMCAT_ALIAS = ==gratia.opensciencegrid.org==
   * Set MYSQL_ALIAS = ==gratia-db01.fnal.gov==
   * Set RELEASE_TAR_REPOSITORY = /afs/fnal.gov/files/expwww/gratia/html/Files

These are specific to a release and require changing:
   * Set RELEASE_DATE = 5/14/08
   * Set CURRENT_RELEASE = v0.34.9
   * Set RELEASE_TAG = v0-34-9
   * Set NEXT_DEVEL_VERSION = v0.35
   * Set BUILD_DIRECTORY = /home/weigand/cdcvs/gratia-%CURRENT_RELEASE%
   * Set GRATIA_RELEASES = /home/gratia/gratia-releases

   * Set EDITTHISTEXT = <div class="twikiSmall"><a href="%TOPIC%">edit this section</a></div>
-->
---+!! Gratia Release v0.34 and %CURRENT_RELEASE% (%RELEASE_DATE% - 5/29/2008)%BR% 

%TOC%

%STARTINCLUDE%
---+ Overview
The main purpose of this release (%CURRENT_RELEASE%) is the DN/FQAN collection improvements and the requirement for authentication for administrating the Collector.

---++ Reporter Improvements (v0.34.7)
   * Report displaying those jobs that have been running more than X days by VO. where X is a week and a month (RR23)
   * Report displaying those sites that show 0 Wall Duration for more than X days - where X is a week and a month. (RR24)
   * Update the osg daily report for Birt 2.2
   * Periodically generated (daily, weekly and/or monthly) csv files that are available for immediate view without need to read db records 
      1. CPUday per day for the last month/30 days for each VO
      2. CPUday per day for the last month/30 days for each site
      3. total  CPUweeks per week for the last 52 weeks (year) 
   * Extend User daily report to include VO name.

---++ Collector Improvements (v0.34.7)
   * Enable authentication in the administration pages
   * Implement looking up the !DN, Role and !VO directly from certificate
   * Update of the record checksum to exclude !DN, Role, !VO"
   * Review and improve the Summary table creation mechanism
   * Update Collector to Collector connection to send more than one record on each connection
   * Add VO to User Summary Table
   * Replace pre-duplicate check by exception handling 

---++ Probe Improvements (v0.34.1)
   * Document on the issues related to full and complete probe black-out detection.
   *  Globus patched to interrogate delegated proxies wherever available for DN and FQAN information.
   * Condor probe updated:
      1 to get information on WS jobs;
      1 to use <nop>ClassAds generated by PER_JOB_HISTORY_DIR (if available) to avoid expensive calls to condor_history. Will fall back on condor_history;
      1 to facilitate "local" probes by means of a constraint option so probes only process certain types of job.
      1 avoid double-counting when information about the same job is received from multiple sources.
   * Package dCache probe as RPM. 
      1 dCache / SRM probes now available (see dCache release notes).
   * Minor improvements to SGE probe from Shreyas Cholia.
   * PBS/LSF probes improved to prevent misreading an incomplete line in the batch manager log.


<!--   -------------------------------------------- -->
---+ Anticipated downtime
It is expected that this release will require the Gratia services and reporting to be unavailable beginning at:
   * Start: %RELEASE_DATE%  hh:mm CST
   * Available: %RELEASE_DATE% hh:mm CST

The changes affecting downtime  for this release are:
   1 Length of time to make a backup of the database to the backup area using =mysqlhotcopy=
   1 Installation and validation on the 6 Gratia schemas



<!-- BUILD DISTRIBUTION -------------------- -->
---+ Build the %CURRENT_RELEASE% for distribution
This release was done in 2 stages and a couple interim stages when problems were detected:
   * v0.34 - for the VDT 1.10.1 distribution (ITB 0.9.0)
   * v0.34.1 - for local distribution.  Changes were required to the local scripts/configurations for the actual conversion at FNAL on gratia08 and gratia09 collectors.

<ol>
<li>Make sure your build area contains all _committed_ changes.
<ul><li>cvs update</li></ul>
</li>

<li>In _gratia/build-scripts/Makefile_ , change the _version_default_ to:
<ul><li>version_default = %NEXT_DEVEL_VERSION%</li>
       <li>commit the change</li>
</ul>
</li>

<li>Tag the release:
<ul><li>cvs tag -R v0-34 gratia</li></ul>
       This tags everything that is in your build directory and presumably tested.
</li>

<li> _Into a *new* area_, export the tagged release:
<ul><li>cvs export -d gratia-v0.34 -r v0-34 gratia</li></ul>
</li>

<li>Build it for the release (this insures that tar files are produced for VDT):
<ul><li>cd gratia-v0.34/build-scripts</li>
<li>source setup-jdk15.sh</li>
<li>make release</li></ul>
</li>

<li>Copy the build/release area to the _gratia_ home directory for posterity:
   * mkdir /home/gratia/gratia-releases/v0.34
   * cp -pr /home/weigand/cdcvs/gratia-v0.34/* /home/gratia/gratia-releases/v0.34/.

<li>Copy the built tar files to the release area:
<ul><li>scp ../target/*_v0.34.tar   flxi07.fnal.gov:%RELEASE_TAR_REPOSITORY%/</li></ul> </li>

<li>Update the version number on the [[InstallationGuideVDT][services release TWiki page]]:</li>
<ul><li>Edit and update the TWiki variable _<nop>ReleaseVersion_.</li></ul>
</ol>
%GREEN%Status: Completed on 5/10/2008 08:00 CDT%ENDCOLOR%

%BLUE%
<!-- ------------------------------------- -->
<hr width="80%" />
<b>Update: 5/11/2008 14:45 CDT:</b>%BR%
Had to modify Makefile to include the voms_servers and voms-server.sh files in the Gratia-Services 
distribution for VDT.  Re-tagged, rebuilt, moved to the flxi07.fnal.gov site.  


<!-- ------------------------------------- -->
<hr width="80%" />
<b>Update: 5/12/2008 14:15 CDT:</b>%BR%
Problems encountered when working with an _empty_ database.  Previous testing was done with an
existing database.  The following programs required modification:
   * collector/gratia-services/net/sf/gratia/services/ListenerThread.java
   * collector/gratia-services/net/sf/gratia/services/NewVOUpdate.java
   * collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
   * common/configuration/hibernate.cfg.xml

Steps 3 through 6 were repeated for these modules.
Note: To tag the individual module (example):
   * cvs tag -F v0-34 collector/gratia-services/net/sf/gratia/services/ListenerThread.java 

Provided a new _configure_gratia_ to VDT.  This was tested in a condor environment as best we could especially for the validation of the condor environment relevant to the PER_JOB_HISTORY_DIR that is critical.

This is the v0.34 used in VDT 1.10.1 (ITB 0.9.0).

<!-- ------------------------------------- -->
<hr width="80%" />
<b>Update: 5/14/2008 15:45 CDT:  Tagged as v0-34-1</b>%BR%
The following modules were modified for changes to local (non-VDT) installations/upgrades.  Tests are currently being run by Chris Green and a gratia instance on _gratia07:/test/data/mysqldb/gratia_ using this software:
   * gratia/build-scripts/Makefile
   * gratia/common/configuration/cleanup_server_lib
   * gratia/common/configuration/configure-collector
   * gratia/common/configuration/update-gratia-local

Steps 3 through 6 were repeated for these modules.
Note: The tagging was done as:
   * cvs tag -R v0-34.1 gratia
and the export done as:
   * cvs export -d gratia-v0.34.1 -r v0-34.1 gratia
and the build done as:
   * cd gratia-v0.34.1/build-scripts
   * source setup-jdk15.sh
   * make release
and the copy to the release area:
   * scp ../target/*_v0.34.1.tar   flxi07.fnal.gov:%RELEASE_TAR_REPOSITORY%/.
   * updated the version number on the [[InstallationGuideVDT][services release TWiki page]]

Also note these changes do not affect the current VDT 1.10.1 release of Gratia v0.34.  The latest version was copied in as in step 6 and 7, but VDT does not need to access this version.

<!-- ------------------------------------- -->
<hr width="80%" />
<b>Update: 5/20/2008 14:42 CDT:  Tagged as v0-34-2 </b>%BR%
<verbatim>
U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
U collector/gratia-services/net/sf/gratia/storage/JobUsageRecord.java
U collector/gratia-services/net/sf/gratia/storage/SummaryUpdater.java
U collector/gratia-services/net/sf/gratia/storage/Utils.java
U common/configuration/collector-pro.dat
U common/configuration/service-configuration.properties
U probe/build/gratia-probe.spec
U probe/common/Gratia.py
U probe/common/GRAM/JobManagerGratia.pm
U probe/condor/condor_meter.pl
U reporting/summary/PSACCTReport.py
</verbatim>

cvs tag -R v0-34-2 gratia%BR%

<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/20/2008 16:46 CDT</b>
<verbatim>
cvs tag -F v0-34-2  collector/gratia-services/net/sf/gratia/services/CollectorService.java
cvs tag -F v0-34-2  collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
cvs tag -F v0-34-2  common/configuration/summary-procedures.sql
</verbatim>

<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/21/2008 11:03 CDT</b>
<verbatim>
Since the osg_daily record does not have a jobid or anything similar, we must keep the 
UserIdentity field in the md5 calculation otherwise we find many false duplicate
U collector/gratia-services/net/sf/gratia/storage/JobUsageRecord.java

Add support for DN comming from cron 
(/DC=gov/DC=fnal/O=Fermilab/OU=Robots/CN=fermigrid0.fnal.gov/CN=cron/CN=Keith Chadwick/CN=UID:chadwick).  
In that case, use the 3rd CN
U collector/gratia-services/net/sf/gratia/storage/JobUsageRecordUpdater.java

Re-add VOName Correction link
U collector/gratia-administration/WebContent/dashboard.jsp

use full md5 for ps accounting too
U common/configuration/collector-pro.dat
</verbatim>

cvs tag -R v0-34-3 gratia%BR%

<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/22/2008 10:45 CDT</b>
Changes:
<verbatim>
Re-add missing Host field lookup needed by psacct
U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
U common/configuration/summary-procedures.sql

actually use the common name we just found
U collector/gratia-services/net/sf/gratia/storage/JobUsageRecordUpdater.java

</verbatim>

cvs tag -R v0-34-4 gratia%BR%


<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/27/2008 13:10 CDT</b>
Changes for gratia_psacct
<verbatim>
report library is in production-osg only
  reporting/gratia-reports/WebContent/reports/production-fermi/gratia-lib1.rptlibrary is no longer in the repository
  reporting/gratia-reports/WebContent/reports/production-osgdaily/gratia-lib1.rptlibrary is no longer in the repository
U reporting/gratia-reports/WebContent/reports/production-osg/gratia-lib1.rptlibrary
</verbatim>

cvs tag -R v0-35-5 gratia%BR%

One more change:
<verbatim>
Give up trying to guess which CN is the "right" one: record them all.
  U collector/gratia-services/net/sf/gratia/storage/JobUsageRecordUpdater.java

cvs tag -F v0-34-5  collector/gratia-services/net/sf/gratia/storage/JobUsageRecordUpdater.java
</verbatim>



<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/28/2008 12:16 CDT</b>
Changes:
<verbatim>
Config change to address "APPARENT DEADLOCK" problem.
  U common/configuration/hibernate.cfg.xml
</verbatim>

cvs tag -R v0-34-6 gratia%BR%

<b>More 5/28/2008 13:00 CDT</b>
<verbatim>
uncomment WeeklyUsageByVO and comment out WeeklyUsageByVORanked (as intended in previsous ci)
  U common/configuration/create_build-stored-procedures-sql

Update DB version to trigger stored procedure reload.
  U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java

Add collector_host to enable SSL operation. Configure ITB to run 5 threads.
Add conversion collector for gratia-fermi on 8884.
   U common/configuration/collector-pro.dat

Need to cope with a previous incomplete upgrade (DB 34 did *not* reload
the triggers like it was supposed to IFF the existing DB version was
already 33).
  U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java

cvs tag -F v0-34-6 common/configuration/create_build-stored-procedures-sql
cvs tag -F v0-34-6 collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
cvs tag -F v0-34-6 common/configuration/collector-pro.dat
</verbatim>


<!-- ------------------------------------- -->
<hr width="80%" />
<b>5/29/2008  10:00 CDT</b>
Changes:
<verbatim>
Summary table rebuild required to correct discrepancies with ResourceType filtering
  U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java

Trigger filter logic from ResourceType should match summary table build.
  U common/configuration/summary-procedures.sql

Expand the list of ResourceType values allowed in the summary table to Batch and RawCPU
  U common/configuration/build-summary-tables.sql 
</verbatim>

cvs tag -R v0-34-7 gratia%BR%



<!-- ------------------------------------- -->
<hr width="80%" />
<b>6/3/2008  17:00 CDT</b>
Changes:
<verbatim>
Remove (after verification) grouping from views and trigger reload of views.
  gratia/common/configuration build-summary-view.sql
  gratia/collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java,

Fix problem with old job data removal.
  gratia/probe/common Gratia.py
  
More statistics when operating in verbose mode.
Only set Grid=Local if we're sure that the ClassAd attribute should have
been added by the JobManager (assuming it passed through same) but was
not (indicating a local job).
Improved logic testing whether we can rely on the check for the
JobManager in the case that the site is running Condor for managed fork
and PBS for standard batch jobs.
  gratia/probe/condor condor_meter.pl

Turn verbose mode on by default and use DebugPrint.py
  gratia/probe/condor condor_meter.cron.sh

Fix behavior upon receiving blank lines (would truncate input).
  gratia/probe/common DebugPrint.py

Incorporate fixes for condor and common packages:
- Correct cleanup of no-longer-useful files in gratia/var/data.
- Improve DebugPrint.py in the case that input contains blank lines.
- Improve logic used in condor probe to decide whether we can use the absence
-  of the GratiaJobOrigin ClassAd attribute to infer that a job is local.
- Condor probe is now verbose but prints to main Gratia log.
- Condor probe only assigns grid=Local to jobs it's really sure are local.
Bump version number to correct local permissions problem with DebugPrint.py.
  gratia/probe/build gratia-probe.spec
</verbatim>

cvs tag -R v0-34-8 gratia%BR%


<!-- ------------------------------------- -->
<hr width="80%" />
<b>6/6/2008  12:00 CDT</b>
Changes:
<verbatim>
Fixed birt "Export Data": shows all columns (penelope)
  gratia/reporting/gratia-reports/WebContent/reports/production-osg/WeeklyUsageByVO-ranked.rptdesign,
  gratia/reporting/gratia-reports/WebContent/reports/production-osg/UsageBySiteByDate-ranked.rptdesign
  gratia/reporting/gratia-reports/WebContent/reports/production-osg/2007WeeklyUsageByVO-ranked.rptdesign

First attempt to correct the parsing of the bundled records sent by replicator.  
Also fix the content of the replication admin page (rowcount).
Clarify debug message list number of input message and records
Change the envelope for bunching to be record type independent.  Fix the test for adding it
U collector/gratia-services/net/sf/gratia/services/ChecksumUpgrader.java
U collector/gratia-services/net/sf/gratia/services/ListenerThread.java
U collector/gratia-services/net/sf/gratia/services/ReplicationDataPump.java
U collector/gratia-services/net/sf/gratia/storage/MetricRecordLoader.java
U collector/gratia-services/net/sf/gratia/storage/UsageRecordLoader.java


Updated to create a gratia-release file in the top level of every deployable
war file and in the gratia directory. The gratia-release file contains, for example:
Gratia release: v0.34.x 
  Build date: Thu Jun  5 14:40:53 CDT 2008 
  Build host: gratia06.fnal.gov 
  Build path: /home/weigand/cdcvs/gratia
  Builder: uid=9789(weigand)
U gratia/build-scripts Makefile


Added the service.admin.DN.0 attribute so manual modification is not
required on each install.  This needs to be changed later to use a
VOMS fermigrid role.
U gratia/common/configuration collector-pro.dat,

Unique index on old md5 column is no longer needed after upgrade is complete.
Update TableStatistics triggers to not attempt to put a null value into a non-null column.
U collector/gratia-services/net/sf/gratia/services/ChecksumUpgrader.java
U collector/gratia-services/net/sf/gratia/storage/DatabaseMaintenance.java
U common/configuration/JobUsage.hbm.xml
U common/configuration/post-install.sh

Fix problem with history replay.
U collector/gratia-services/net/sf/gratia/services/ListenerThread.java

Probe changes not affecting collector:
U probe/build/gratia-probe.spec
U probe/common/GRAM/JobManagerGratia.pm
U probe/glexec/README
U probe/psacct/gratia-psacct

save only the part of bundle that describe the current record
U collector/gratia-services/net/sf/gratia/services/ListenerThread.java
</verbatim>

cvs tag -R %RELEASE_TAG% gratia%BR%

<b>6/10/2008  12:00 CDT</b>
<verbatim>
Changes:

Remove Grid from attributes taken into account for checksum.
New column for GridDescription.
Improve debug messages when handling duplicates.
Correct minor problem in warning message.
U collector/gratia-services/net/sf/gratia/services/ListenerThread.java
U collector/gratia-services/net/sf/gratia/storage/JobUsageRecord.java
U collector/gratia-services/net/sf/gratia/storage/UsageRecordLoader.java
U common/configuration/JobUsage.hbm.xml
</verbatim>

   * cvs tag -F %RELEASE_TAG%  collector/gratia-services/net/sf/gratia/services/ListenerThread.java
   * cvs tag -F %RELEASE_TAG%  collector/gratia-services/net/sf/gratia/storage/JobUsageRecord.java
   * cvs tag -F %RELEASE_TAG%  collector/gratia-services/net/sf/gratia/storage/UsageRecordLoader.java
   * cvs tag -F %RELEASE_TAG%  common/configuration/JobUsage.hbm.xml

<b>6/11/2008  09:00 CDT</b>
<verbatim>
Changes:

Refrain from overwriting the locally-generated ExtraXML attribute with
that received from an external source.
P collector/gratia-services/net/sf/gratia/services/ListenerThread.java
</verbatim>

   * cvs tag -F %RELEASE_TAG%  collector/gratia-services/net/sf/gratia/services/ListenerThread.java

<u>Status of checksum differences</u>%BR%
Philippe and Chris have ascertained that the reason for the difference between the checksums is benign: the <nop>ExtraXML which is used in the checksum calculation is different due to legitimate differences in the make-up of the incoming record. The fact that there are no differences in checksum for anything received since 5/27 on gratiax33 is encouraging.

One change was made to <nop>ListenerThread.java in that previous behavior was to overwrite the <nop>ExtraXML produced from the bits that the receiving server couldn't understand with the incoming <nop>ExtraXML from the replicated (or history) record. This has been changed such that the lately-generated <nop>ExtraXml is used and any previous occupant of that title is ignored. 


Steps 3 through 6 were repeated for these modules.%BR%
Note: 
the export done *as yourself* :
   * cvs export -d gratia-%CURRENT_RELEASE% -r %RELEASE_TAG% gratia
and copy it to the /home/gratia/gratia-releases area (as *gratia* user):
   * cp -pr %BUILD_DIRECTORY% %GRATIA_RELEASES%/.
and build it (as *gratia* user):
   * cd %GRATIA_RELEASES%/gratia-%CURRENT_RELEASE%/build-scripts
   * source setup-jdk15.sh
   * make release
and make a copy of the tarballs in the /home/gratia/gratia-releases directory (as *gratia* user)
   * cp -p %GRATIA_RELEASES%/gratia-%CURRENT_RELEASE%/target/*_%CURRENT_RELEASE%.tar   /home/gratia/gratia-releases/tarballs/.
and the copy to the release distribution area ( *as yourself* ):
   * scp %GRATIA_RELEASES%/gratia-%CURRENT_RELEASE%/target/*_%CURRENT_RELEASE%.tar   flxi07.fnal.gov:%RELEASE_TAR_REPOSITORY%/.
   * updated the version number on the [[InstallationGuideVDT][services release TWiki page]]


%ENDCOLOR%

%GREEN%Status: Finally Completed v0.34.9 on 6/11/2008 09:00 CDT%ENDCOLOR%
 



<!--   -------------------------------------------- -->
---+ Collectors and Databases Affected

The following Gratia collectors and databases will be converted with this release:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 20 | text, 7 | text, 15 | text, 15 | text, 8|"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Schema*|*<nop>MySql port*|*Collector URL*|*Size (bytes)*|*Size (rows)*|*Max dbid*|*Duplicates*|
| fermi_transfer | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8886 |  5.2M |            0 |  0 |  0 |
| gratia_osg_transfer | gratia06.fnal.gov:3320 | gratia09.fnal.org:8886 |  5.2M |            0 |  0 |  0 |
| gratia_osg_daily | gratia06.fnal.gov:3320 | gratia09.fnal.gov:8884 |  89M |  84,437 |   85117 |  0 |
| fermi_itb | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8881 |  558M |  58,082 |  159187 |  92,879 |
| gratia_qcd  | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8883 |  4.3G |  1,210,400 |  1588366 |  0 |
| gratia_osg_integration | gratia06.fnal.gov:3320 | gratia09.fnal.gov:8885 |  8.1G |  1,438,686 |  2561449 |  691,626 |
| gratia_psacct | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8882 |  13G |  7,763,036 |  7792895 |  0 |
| gratia_itb | gratia06.fnal.gov:3320 | gratia09.fnal.gov:8881 |  21G |  8,327,662 |  8958895 |  101,365 |
| fermi_osg | gratia06.fnal.gov:3320 | gratia08.fnal.gov:8880 |  24G |  8,707,956 |  8712636 |  3,652 |
| gratia | gratia06.fnal.gov:3320 | gratia09.fnal.gov:8880 |  117G |  68,418,063 |  70987563 |  2,231,774 |



<!-- ----------------- SHUTDOWN AND DATABASE BACKUP ------------- -->
---+ Shutdown and database backup.
The nightly backups will suffice for this implementation.


<!-- ----------------- UPGRADE AND IMPLEMENTATION ------------- -->
---+ Small DB - Upgrade and implementation
The __upgrades should be single-threaded__ , that is, performed for each database schema one at a time.  

We will perform these upgrades based on the size of the individual database schema, in ascending order.

The _DupRecord_ table (duplicates) should be removed before starting the upgrade.

The general procedure for this upgrade is:
   1 Stop the tomcat service 
      * service TOMCAT_INITD_SERVICE stop
   1 Install the new software on the Gratia tomcat instance:
      * pswd=xxx
      * source=%BUILD_DIRECTORY%
      * pgm=%BUILD_DIRECTORY%/common/configuration/update-gratia-local 
      * $pgm  -d $pswd -S $source -s  DATABASE
   1 Tar the existing log files and save to a backup area.  Then clean the log file directory.
      * date=`date '+<nop>%Y<nop>%m<nop>%d'`
      * cd TOMCAT_LOCATION/logs/
      * tar zcf /data/gratia_tomcat_logs_backups/tomcat-TOMCAT_INITD_SERVICE.$date.tgz *
      * rm -f * 
   1 Effective with this release, there is an administrative login process that, by default, does not allow access to the admin functions.  You will need to update the __TOMCAT_LOCATION/gratia/service-configuration.properties__ file with the following attributes. (Note that if you forget to do this, the administrator can be added at anytime without restarting the tomcat service):
      * service.admin.DN.0=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Philippe G. Canal/UID=pcanal
      * service.admin.DN.1=/DC=org/DC=doegrids/OU=People/CN=Christopher H. Green 851859
      * service.admin.DN.2=/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491
      * service.admin.DN.3=/DC=org/DC=doegrids/OU=People/CN=Steven Timm 74183
      * service.admin.DN.4=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Steven C. Timm/UID=timm
   1 Start the tomcat service 
      * service TOMCAT_INITD_SERVICE start
   %BR%When the tomcat service initializes, it will detect any schema changes have been effected and a conversion process will begin. 
      * _tail_ the log files in _TOMCAT_LOCATION/logs. When the conversion process completes the log will show the following message:
      - "INFO: Server startup in _xxx_ms"
      * In your browser, access the _gratia-administrative_ web service, select the _System / Administration_ menu option in the left menu, Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Stop Update Services_ link
   1 __Wait until the schema upgrade is complete__ , then, _start_ the Gratia _update_ services for the database schema just upgraded.
      * In your browser, connect to the Gratia administrative services url for each of the databases.
      * Select the _System / Administration_ menu option in the left menu
      * Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Start Update Services_ link.
      * As the collector/tomcat host update service is started, monitor the tomcat logs files for any errors.
      * Bring up the gratia _administration_ web interface and verify that the collectors are processing the data on the status page.
      * Bring up the gratia _reporting_ web interface and verify that the reports look reasonable while still _tail'ing_ the log files.
   1 Run the static reports cron as _root_ and verify these reports are generated:
      * 3 pdf files and 3 csv files
   1 If satisifed, you may continue to the next one.

%BLUE%
Did not have to do the stop/start of update service as you those services never got started
%ENDCOLOR%

---++ Status v0.34. through v0.34.7

%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 20 | text, 15 | text, 5 | text, 10 | text, 18 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Database*|*Tomcat node*|*init.d service*|*tomcat location*|*Size*|*Start*|*Complete*|
| fermi_transfer |  gratia08.fnal.gov:8886 | tomcat-fermi_transfer | /data/tomcat-fermi_transfer |  5.2M | 5/18 09:00 | 5/22 13:05 v0.34.4 |
| fermi_itb |  gratia08.fnal.gov:8881 | tomcat-fermi_itb | /data/tomcat-fermi_itb |  558M | 5/18 10:10 | 5/22 12:54 v0.34.4 |
| gratia_qcd  |  gratia08.fnal.gov:8883 | tomcat-qcd | /data/tomcat-qcd |  4.3G | 5/18 11:20 | 5/22 12:59 v0.34.4 |
| gratia_psacct |  gratia08.fnal.gov:8882 | tomcat-ps | /data/tomcat-ps |  13G | 5/18 11:58 | 5/27 16:00 v0.34.5 |
| fermi_osg |  gratia08.fnal.gov:8880 | fermi_osg | /data/tomcat-fermi_osg |  24G | 5/19 07:51 | 5/22 12:44 v0.34.4 |


%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 20 | text, 15 | text, 5 | text, 10 | text, 18 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Database*|*Tomcat node*|*init.d service*|*tomcat location*|*Size*|*Start*|*Complete*|
| gratia_osg_transfer |  gratia09.fnal.org:8886 | tomcat-osg_transfer | /data/tomcat-osg_transfer |  5.2M | 5/18 13:55 | 5/22 13:12 v0.34.4 |
| gratia_osg_daily |  gratia09.fnal.gov:8884 | tomcat-osg_daily | /data/tomcat-osg_daily |  89M | 5/18 14:28 | 5/22 13:15 v0.34.4 |
| gratia_osg_integration |  gratia09.fnal.gov:8885 | tomcat-osg_integration | /data/tomcat-osg_integration |  8.1G | 5/18 18:20 | 5/22 13:18 v0.34.4 |
| gratia_itb |  gratia09.fnal.gov:8881 | tomcat-itb | /data/tomcat-itb |  21G | 5/18 18:48 | 5/29 10:32 v0.34.7 |


%RED% 
*Issues/Problems:*
   1 No host certificate.  So administratiion functions disabled.  Will have to wait until Monday to request host and http certificates from Steve.
      * %GREEN%Installed host certificate on 5/12%ENDCOLOR%
   1 Static reports not working on fermi_transfer and osg_transfer.  These databases are completely empty (zero records) and this is causing the problem.
   1 The _update_gratia_local_ adds a cron entry for static reports every time it is run.  You have to manually edit the crontab of the old entries.
      * It is also adding it with the ssl port number where in the past it was using the non-ssl port. %GREEN%Resolved on 6/6%ENDCOLOR%
      <verbatim>
42 0 * * * '/data/tomcat-fermi_osg/gratia/staticReports.py'
                 '/data/tomcat-fermi_osg' 
                 'http://gratia-fermi.fnal.gov:<b>8880</b>/gratia-reporting/' 
<b>One below added new:</b>
##42 0 * * * '/data/tomcat-fermi_osg/gratia/staticReports.py' 
                 '/data/tomcat-fermi_osg' 
                 'https://gratia-fermi.fnal.gov:<b>8845</b>/gratia-reporting/' 
</verbatim>
   1 osg_daily catalina.out logs
      * May 18, 2008 2:33:26 PM org.eclipse.birt.chart.script.ScriptHandler register%BR%WARNING: An invalid Chart Java event handler class [class net.sf.gratia.reporting.GratiaBirtReportEH] has been detected and ignored.
      * Reporting menu is entirely different.  
      * should the static reports even be run for these
   1 gratia_psacct
      * Appears to be hung in the *post-install.sh summary* . Killed tomcat around 5/19 07:47 with an explicit kill and restart.  It is hung again in the same step.
   1 fermi_osg
      * Encountered this error with *index12* :
      <verbatim>
FINE: Command: Error: alter table ProbeDetails_Meta add unique index index12(md5) : 
com.mysql.jdbc.exceptions.MySQLIntegrityConstraintViolationException: 
     Duplicate entry 'E6A2B4DC7D3C910EF4D584CAEB6F10FA' for key 2
      </verbatim> 
      * This are the *alter table..* statements in the gratia-0.log
      <verbatim>
380:FINE: Executing: alter table MetricRecord_Meta add unique index index12(md5)
382:FINE: Command: OK: alter table MetricRecord_Meta add unique index index12(md5)
388:FINE: Executing: alter table ProbeDetails_Meta add unique index index12(md5)
390:FINE: Command: Error: alter table ProbeDetails_Meta add unique index index12(md5) :
    com.mysql.jdbc.exceptions.MySQLIntegrityConstraintViolationException: 
    Duplicate entry 'E6A2B4DC7D3C910EF4D584CAEB6F10FA' for key 2
470:FINE: Executing: alter table ProbeDetails_Meta add unique index index12(md5)
472:FINE: Command: Error: alter table ProbeDetails_Meta add unique index index12(md5) :
    com.mysql.jdbc.exceptions.MySQLIntegrityConstraintViolationException: 
    Duplicate entry 'E6A2B4DC7D3C910EF4D584CAEB6F10FA' for key 2
590:FINE: Executing: alter table ProbeDetails_Meta add unique index index12(md5)
592:FINE: Command: Error: alter table ProbeDetails_Meta add unique index index12(md5) :
    com.mysql.jdbc.exceptions.MySQLIntegrityConstraintViolationException: 
    Duplicate entry 'E6A2B4DC7D3C910EF4D584CAEB6F10FA' for key 2
618:FINE: Executing: alter table JobUsageRecord_Meta add index index17(md5v2
      </verbatim>
   1 osg_daily
      * 5/20 08:00 noticed java exceptions
      <verbatim>
FINEST: fixDuplicatesOnce: resolving 3 duplicates 
with checksum C7E72574A5A891270BF2F6ECA1127A10
May 20, 2008 9:12:45 AM net.sf.gratia.util.Logging debug
FINEST: fixDuplicatesOnce: deleting record 54646 in favor of record 54704
May 20, 2008 9:12:45 AM net.sf.gratia.util.Logging warning
WARNING: fixDuplicatesOnce: caught exception resolving duplicates 
with checksum C7E72574A5A891270BF2F6ECA1127A10
com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: 
PROCEDURE gratia_osg_daily.del_JUR_from_summary does not exist
      </verbatim>
      * Philippe reports that the checksum calculation is wrong for this instance and is looking into it
      * stopped the instance 5/20 09:12
   1 gratia_itb
      * hibernate got in a deadlock state.  Recycle on 5/20 09:08.  Waiting to see if it occurs again.
      <verbatim>
2008-05-19 08:22:50,075 com.mchange.v2.async.ThreadPoolAsynchronousRunner [WARN]:
 com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@e39068 
-- APPARENT DEADLOCK!!! Creating emergency threads for unassigned pending tasks!
2008-05-19 08:22:50,091 com.mchange.v2.async.ThreadPoolAsynchronousRunner [WARN]:
 com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@e39068 
-- APPARENT DEADLOCK!!! Complete Status:
        Managed Threads: 3
        Active Threads: 3
        Active Tasks:

2008-05-20 10:16:50,019 com.mchange.v2.async.ThreadPoolAsynchronousRunner [WARN]:
 com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@d7f3b9 
-- APPARENT DEADLOCK!!! Creating emergency threads for unassigned pending tasks!
2008-05-20 10:16:50,048 com.mchange.v2.async.ThreadPoolAsynchronousRunner [WARN]:
 com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@d7f3b9 
-- APPARENT DEADLOCK!!! Complete Status:
        Managed Threads: 3
        Active Threads: 3
        Active Tasks:
      </verbatim>
%ENDCOLOR%


%BLUE%
*Notes*
   1 The tmp directory being used by <nop>MySql on gratia06 is defined on the command line and not by the my.cnf value
   <verbatim>
gratia    6415 14500 42 Apr25 ?        10-01:17:45 
          /fnal/ups/prd/mysql/v5_0_22/Linux-2-4/libexec/mysqld 
         --basedir=/fnal/ups/prd/mysql/v5_0_22/Linux-2-4 
         --datadir=/data/mysqldb/ --user=gratia 
         --pid-file=/data/mysqldb//gratia06.fnal.gov.pid --skip-locking 
         --port=3320 --socket=/data/mysqldb/mysql.sock 
         --tmpdir=/data/mysqltmp/tmp
   </verbatim>
   1 *gratia_psacct* 
      * Had to perform an <nop>InnoDB conversion in the beginning.  (Start 12:59 Completed: 13:09,  elapsed 01:11)
      * Then it had to create the new table for the <nop>JobUsageRecord_Meta table column. (Start 13:11 Complete:14:26 elapsed 01:15)
      * Adding the index (Start 15:21 ).  No clue when it ended
      * At 15:46 running gratia/build-summary-tables.sql
   1 *fermi_osg*
      * Creating the <nop>JobUsageRecord_Meta table column and index (Start 5/19 09:32  Completed: 5/19 11:57 Elapsed: 02:25)
      * Summary table  creation (Start: 11:57  Completed:   Elapsed: )
   1 Philippe changed the creation of the summary tables ( _build-summary-tables.sql_ ) 
      * This was to eliminate glexec records from the tables due to a large number of bad records in the main database when glexec first fired up.
      * Ran from gratia-vm02:/data/tomcat-greenc_test area against the gratia07:/test/data/mysqldb/gratia database
      * Started 5/19 14:40  Completed 5/20 04:52  Elapsed: 12:12
      * Reduced time by 4 hours



*Certificates on gratia08 and graita09*
   1 Installed pacman
      * mkdir /usr/local/pacman
      * cd    /usr/local/pacman
      * wget http://physics.bu.edu/pacman/sample_cache/tarballs/pacman-3.24.tar.gz
      * tar zxf pacman-3.24.tar.gz 
      * cd pacman-3.24
      * source setup.sh
   1 Installed VDT CA-Certifcates
      * mdkir /etc/grid-securtiy
      * mkdir /usr/local/vdt-certificates.1101
      * cd    /usr/local/vdt-certificates.1101
      * pacman -get  VDT:CA-Certificates
      * (answer yes to all questions and "r (root)" when asked where you want to install them.  This puts them directly in /etc/grid-security.
   1 Enable the VDT services (crons in his release) to maintain the certificates and crls
      * source /usr/local/vdt-certificates.1101/setup.sh
      * vdt-control --on
%ENDCOLOR%

---++ Status v0.34.9
Since changes have been made to the checksum calculation and these instances have already been upgraded for the new md5 (md5v2) value, the upgrade to v0.34.9 will require the following.  This is so a new checksum (md5v2) value is calculated.

The general procedure for this upgrade is:
   1 Stop the tomcat service 
      * service TOMCAT_INITD_SERVICE stop
   1 Tar the existing log files and save to a backup area.  Then clean the log file directory.
      * date=`date '+<nop>%Y<nop>%m<nop>%d'`
      * cd TOMCAT_LOCATION/logs/
      * tar zcf /data/gratia_tomcat_logs_backups/tomcat-TOMCAT_INITD_SERVICE.$date.tgz *
      * rm -f * 
   1 In !MySql client ( *as root* ), we need to force the recalculation of the new checksum values:
      * show index from !JobUsageRecord_Meta;
      * alter table !JobUsageRecord_Meta drop index index17; %BR%  (index17 should be the md5v2 unique index. If not, drop the one that is.)
      * update !JobUsageRecord_Meta set md5v2 = null;
      * delete from !SystemProplist where car = 'gratia.database.disableChecksumUpgrade';
   1 Install the new software on the Gratia tomcat instance:
      * pswd=xxx
      * source=%GRATIA_RELEASES%/gratia-%CURRENT_RELEASE%
      * pgm=%GRATIA_RELEASES%/gratia-%CURRENT_RELEASE%/common/configuration/update-gratia-local 
      * $pgm  -d $pswd -S $source -s  DATABASE
   1 Effective with this release, there is an administrative login process that, by default, does not allow access to the admin functions.  You will need to update the __TOMCAT_LOCATION/gratia/service-configuration.properties__ file with the following attributes. (Note that if you forget to do this, the administrator can be added at anytime without restarting the tomcat service):
      * service.admin.DN.0=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Philippe G. Canal/UID=pcanal
      * service.admin.DN.1=/DC=org/DC=doegrids/OU=People/CN=Christopher H. Green 851859
      * service.admin.DN.2=/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491
      * service.admin.DN.3=/DC=org/DC=doegrids/OU=People/CN=Steven Timm 74183
      * service.admin.DN.4=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Steven C. Timm/UID=timm
   1 AND change these attributes as well to either *gratia-fermi.fnal.gov* (gratia08) or *gratia.opensciencegrid.org* (gratia09).  *Note* : the port number should be correct, but double-check it.
      * service.open.connection=http://localhost:8884
      * service.secure.connection=https://localhost:8860 
   1 Start the tomcat service 
      * service TOMCAT_INITD_SERVICE start
   %BR%When the tomcat service initializes, it will detect any schema changes have been effected and a conversion process will begin. 
      * _tail_ the log files in _TOMCAT_LOCATION/logs. When the conversion process completes the log will show the following message:
      - "INFO: Server startup in _xxx_ms"
      * In your browser, access the _gratia-administrative_ web service, select the _System / Administration_ menu option in the left menu, Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Stop Update Services_ link
   1 __Wait until the schema upgrade is complete__ , then, _start_ the Gratia _update_ services for the database schema just upgraded.
      * In your browser, connect to the Gratia administrative services url for each of the databases.
      * Select the _System / Administration_ menu option in the left menu
      * Then scroll down to the _Starting/Stopping Database Update Services_ section and select the _Start Update Services_ link.
      * As the collector/tomcat host update service is started, monitor the tomcat logs files for any errors.
      * Bring up the gratia _administration_ web interface and verify that the collectors are processing the data on the status page.
      * Bring up the gratia _reporting_ web interface and verify that the reports look reasonable while still _tail'ing_ the log files.
   1 Run the static reports cron as _root_ and verify these reports are generated:
      * 3 pdf files and 3 csv files
   1 If satisifed, you may continue to the next one.



%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 20 | text, 15 | text, 5 | text, 5 | text, 10 | text, 18 | text, 15 | text, 15 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Database*|*Tomcat node*|*init.d service*|*tomcat location*|*Size*|*Rows*|*Start*|*Complete*|*Alter table*|*checksum*|
| fermi_transfer |  gratia08.fnal.gov:8886 | tomcat-fermi_transfer | /data/tomcat-fermi_transfer |  5.2M | 0 | 6/12 11:20 | 6/12 11:39 |  n/a |  n/a |
| fermi_itb |  gratia08.fnal.gov:8881 | tomcat-fermi_itb | /data/tomcat-fermi_itb |  513M | 59K | 6/12 11:40 | 6/12 12:02 |  n/a |  00:11 |
| gratia_qcd  |  gratia08.fnal.gov:8883 | tomcat-qcd | /data/tomcat-qcd |  4.6G | 1.2M | 6/12 12:33 | 6/12 17:24 |  00:08  |  29:00 |
| gratia_psacct |  gratia08.fnal.gov:8882 | tomcat-ps | /data/tomcat-ps |  19G | 8.5M | 6/13 09:54 | 6/13  |  04:23  null 00:31 |  |
| fermi_osg |  gratia08.fnal.gov:8880 | fermi_osg | /data/tomcat-fermi_osg | 26G |  | 6/13  | 6/13  |  |  |


%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}%
%EDITTABLE{  header="|*Schema*|*URL*|*Description*|" format="| text, 20 | text, 20 | text, 20 | text, 15 | text, 5 |text, 5 | text, 10 | text, 18 | text, 15 | text, 15 |"  changerows="on" quietsave="on" editbutton="Edit table" }%
|*Database*|*Tomcat node*|*init.d service*|*tomcat location*|*Size*|*Rows*|*Start*|*Complete*|*Alter table*|*checksum*|
| gratia_osg_transfer |  gratia09.fnal.org:8886 | tomcat-osg_transfer | /data/tomcat-osg_transfer |  5.2M | | 6/11  | 6/11  | | |
| gratia_osg_daily |  gratia09.fnal.gov:8884 | tomcat-osg_daily | /data/tomcat-osg_daily |  155M | | 6/11  | 6/11  | | |
| gratia_osg_integration |  gratia09.fnal.gov:8885 | tomcat-osg_integration | /data/tomcat-osg_integration |  7.1G | | 6/11  | 6/11  | | |
| gratia_itb |  gratia09.fnal.gov:8881 | tomcat-itb | /data/tomcat-itb |  23G | | 6/11  | 6/11  | | |


%GREEN%
Checksum calculations:
   * tomcat-qcd   Started 06/12 12:46   Complete:6/12 17:24  Elapsed: 04:38
   <verbatim>
Jun 12, 2008 5:19:13 PM net.sf.gratia.util.Logging info
INFO: ChecksumUpgrader: main duplicate resolution phase complete in 1 iterations.
INFO: ChecksumUpgrader: resolved duplicates for a total of 65 duplicated checksums.
FINEST: fixDuplicatesOnce: starting duplicate resolution cycle
FINEST: fixDuplicatesOnce: this cycle detected 0 duplicated checksums
INFO: ChecksumUpgrader: make index on md5v2 unique (could take some time)

Jun 12, 2008 5:21:56 PM net.sf.gratia.util.Logging log
FINE: Executing: alter table JobUsageRecord_Meta drop index index12
Jun 12, 2008 5:24:01 PM net.sf.gratia.util.Logging log
FINE: Command: OK: alter table JobUsageRecord_Meta drop index index12
INFO: ChecksumUpgrader: reactivating listener threads
FINE: ListenerThread: ListenerThread: 0:Duplicate Check: true
   </verbatim>
%ENDCOLOR%


<!-- ----------------- POST MORTEM  ------------- -->
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.
<ol>
</ol>

%INCLUDE{"GratiaDbUpgradeV0dot34dot7"}%


%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->
---++!! Major updates
<!--Future editors should add their signatures beneath yours!-->
-- Main.JohnWeigand - 15 May 2008
