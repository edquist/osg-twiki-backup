%META:TOPICINFO{author="JohnWeigand" date="1213460559" format="1.1" reprev="1.14" version="1.14"}%
%META:TOPICPARENT{name="GratiaReleaseV0dot34"}%
---+!! Main gratia DB - Upgrade  to v0.34.9%BR% 
%TOC%

%STARTINCLUDE%
<!--
   * Set EDITTHISTEXT = <div class="twikiSmall"><a href="%TOPIC%">edit this section</a></div>
-->

---+ Main gratia DB - Upgrade  to v0.34.9
%EDITTHISTEXT%

The main Gratia database ( *gratia* ) served by the *gratia09:/data/tomcat-gratia* instance will require a very long 
conversion time. So to maximize availability we will:
   1 create a current copy of the *gratia06/gratia* database on *gratia07*
   1 using a temporary Gratia collector on *gratia08:/data/tomcat-conversion* 
      * this collector will be upgraded to v0.34.9 allowing the new summary table and md5v2 checksums to be calculated.
      * we will then replicate from *gratia06* to *gratia07* to "catch up" the database before we switch the production *gratia09:/data/tomcat-gratia* over to using it.
   1 switch the current *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database
   1 perform the upgrade of the *gratia06/gratia* database to v0.34.9 using the Gratia collector on *gratia08:/data/tomcat-conversion* .
   1 While the checksum upgrades are occurring, we will replicate from *gratia07* to *gratia06* so the "catch up" time to make it current is reduced.
   1 when the upgrade is complete (inclusive of the conversion to the new md5 key), we will switch the *gratia09:/data/tomcat-gratia* instance back to using the *gratia06/gratia* database

During the upgrade period, this will be the tomcat/database environement for the *gratia* database:
%TABLE%
| *Tomcat instance* | *Database instance* |
| gratia09:/data/tomcat-gratia port 8880 | gratia07:/data/mysqldb/gratia port 3320 |
| gratia08:/data/tomcat-conversion port 8884 | gratia06:/data/mysqldb/gratia port 3320 |

The above is a high level summary of the process.  The details are contained in subsequent sections.

---++ Create a copy of the current database on *gratia07*
On 5/28, the *gratia07:/data/mysqldb* was re-partitioned into a single 200Gb area to provide the required space for the *gratia* database.

---+++ Create the *gratia07:gratia* database using the ZRM backup
This is in process now (5/30) and a finalized procedure will be included when it completes.

%GREEN%Done - 6/3/08 08:00 CDT%ENDCOLOR% %BR%

---+++ Configure *gratia08:/data/tomcat-conversion* to use the *gratia07:gratia* database
On *gratia08*, edit the */data/tomcat-conversion/gratia/service-configuration.properties* file for this resulting attribute:
   * service.mysql.url=jdbc:mysql://gratia07.fnal.gov:3320/gratia

%GREEN%Done - 6/3/08 12:37 CDT (John Weigand)%ENDCOLOR% %BR%
%GREEN%Re-confirmed - 6/11/08 10:30 CDT (John Weigand)%ENDCOLOR%

---+++ Upgrade  *gratia08:/data/tomcat-conversion* to *v0-34-9*
On *gratia08* :
   1 Stop the tomcat service
      * service tomcat-conversion stop 
      * %GREEN%Done - 6/11/08 14:48 CDT%ENDCOLOR% 

On *gratia07* :
   1 Disable the checksum upgrade on *gratia07/gratia* database.  In a !MySql client:
      * update !SystemProplist set cdr=1 where car='gratia.database.disableChecksumUpgrade';
      * %GREEN%Done - 6/11/08 14:51 CDT%ENDCOLOR% 

Back on *gratia08* :
   1 Perform the upgrade to v0.34.9
      * pswd=xxx
      * source=/home/gratia/gratia-releases/gratia-v0.34.9
      * pgm=/home/gratia/gratia-releases/gratia-v0.34.9/common/configuration/update-gratia-local 
      * $pgm  -d $pswd -S $source -s  conversion
   1 Effective with this release, there is an administrative login process that, by default, does not allow access to the admin functions.  You will need to update the __TOMCAT_LOCATION/gratia/service-configuration.properties__ file with the following attributes. You will also need to change 2 connection attributes, as well. (Note that if you forget to do this, the administrator can be added at anytime without restarting the tomcat service):
      * service.admin.DN.0=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Philippe G. Canal/UID=pcanal
      * service.admin.DN.1=/DC=org/DC=doegrids/OU=People/CN=Christopher H. Green 851859
      * service.admin.DN.2=/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491
      * service.admin.DN.3=/DC=org/DC=doegrids/OU=People/CN=Steven Timm 74183
      * service.admin.DN.4=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Steven C. Timm/UID=timm
      * 
      * service.open.connection=http://gratia-fermi.fnal.gov:8884
      * service.secure.connection=https://gratia-fermi.fnal.gov:8860
      * *IMPORTANT IMPORTANT... THIS MUST BE CHANGED*
      * %RED%service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia%ENDCOLOR%
   1 Start the tomcat service
      * service tomcat-conversion start

%GREEN%Done - 6/11/08 14:55 CDT%ENDCOLOR% %BR%
%RED%First shot was against the production database.  I did not realize the _collector-pro.dat_ file set the mysql to gratia_db01:3320/gratia.  Chris bailed me out and confirmed that no damage had occured.  Chris changed the _collector-pro.dat_ for the _conversion_ instance to garbage data to avoid this on future releases forcing manually changing the file.  I will add this to the upgrade
instructions for the *gratia09:tomcat-gratia* . %ENDCOLOR% %BR%
%GREEN%Re-Done - 6/11/08 15:21 CDT%ENDCOLOR% 

%BLUE%First thing it does is add the _GridDescription_ to the _JobUsageRecord_Meta_ table.%BR%
In !MySql client ( _show processlist;_ ):
| Id   | User   | Host                    | db     | Command | Time  | State             | Info                                                                    |
| 2786 | gratia | gratia08.fnal.gov:11262 | gratia | Query   | 57891 | copy to tmp table | alter table !JobUsageRecord_Meta add column !GridDescription varchar(255) | 
Started: 6/11 15:20 CDT%BR%
Status as of 6/12 07:27 CDT 
   * -rw-rw----  1 gratia gratia  23G Jun  5 09:08 !JobUsageRecord_Meta.ibd
   * -rw-rw----  1 gratia gratia  14K Jun 11 15:20 #sql-3275_ae2.frm
   * -rw-rw----  1 gratia gratia  16G Jun 12 07:27 #sql-3275_ae2.ibd
%ENDCOLOR%

%GREEN%
!GridDescription add to !JobUsageRecord_Meta table
   * Started - 6/11 15:21 CDT   Ended - 6/12 22:30 CDT    Elapsed - 31:09
md5v2 and index add to !JobUsageRecord_Meta 
   * Started - 6/12 23:26 CDT   Ended - 6/14 10:07  CDT    Elapsed -34:41
!MasterSummary table creation (started tomcat-conversion service)
   * Started - 6/14 10:45 CDT   Ended - 6/12   CDT    Elapsed
   <verbatim>
Jun 14, 2008 10:55:21 AM net.sf.gratia.util.Logging log
FINE: Executing: /data/tomcat-conversion/gratia/post-install.sh summary
ABOVE ONE WAS AGAINST THE gratia06 database as the post-install.sh script needed to be changed too

Jun 14, 2008 11:21:24 AM net.sf.gratia.util.Logging log
FINE: Executing: /data/tomcat-conversion/gratia/post-install.sh summary


   </verbatim>
%ENDCOLOR%

*WAIT* until the summary tables have been created.  Then you can proceed to the next step of "catching up" the *gratia07* database.

---+++ Bring the *gratia07* database up to current
Since the *gratia07* database is based on a nightly backup and production has been collecting data from the CE nodes, it needs to be brought up to current by replicating from *gratia06* to *gratia07* using the  administrative services *Replication* menu.
   * on http://gratia.opensciencegrid.org:8880/gratia-administration
   * both *Job Usage* and *Metric* replication services should be activated to replicate to:%BR%
     - <nop>http://gratia-fermi.fnal.gov:8884 
   * The starting *dbid* should be determined (*once only* after the backup is restored) using the mysql client on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

%GREEN% *Partially done* - 6/3/08 13:45 CDT (John Weigand)
<pre>
select max(dbid) from JobUsageRecord_Meta; 
+-----------+
| max(dbid) |
+-----------+
|  75242300 | 
+-----------+

select max(dbid) from MetricRecord_Meta; 
+-----------+
| max(dbid) |
+-----------+
|      1506 | 
+-----------+

<b>On gratia06:</b>
update Replication set dbid=75242300 
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=52;

update Replication set dbid=1506 
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=54;

</pre>

Starting point (approx 6/3/08 14:00 CDT)
| gratia06 max(dbid) | 75,851,594 |
| gratia07 max(dbid) | 75,242,300 |
| catch up | 609,294 |

Catch up complete around 6/3/08 23:00 CDT
| hour UTC | updates |
| 080604 12 |     7715 | 
| 080604 11 |     8224 | 
| 080604 10 |     8435 | 
| 080604 09 |     6962 | 
| 080604 08 |     7869 | 
| 080604 07 |     8005 | 
| 080604 06 |     7753 | 
| 080604 05 |     6874 | 
| 080604 04 |     7944 | 
| 080604 03 |     9629 | 
| 080604 02 |    14588 | 
| 080604 01 |    92283 | 
| 080604 00 |    89794 | 
| 080603 23 |    87849 | 
| 080603 22 |    70699 | 
| 080603 21 |    89476 | 
| 080603 20 |    89201 | 
| 080603 19 |    32873 | 

%ENDCOLOR%

*Wait* until the *gratia07* is caught up to the *gratia06* database.  You can monitor this by:
   * on *gratia06* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%
   * on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

When they are close, stop the *administrative update service* on
   *  http://gratia.opensciencegrid.org:8880/gratia-administration

The replication services should still be running to ensure we "catch up" the last of the updates on *gratia06* to *gratia07*.  The replication activity shall be complete when both replication data pumps (as shown by =gratia-0.log= on gratia08) have been through at least one check-duplicate cycle without uploading records.

Then, turn off both the *Job Usage* and *Metric* replication services on:
   * http://gratia.opensciencegrid.org:8880/gratia-administration for <nop>http://gratia-fermi.fnal.gov:8884. 
   * Do *not* delete the entries at this time.%BR%

Take note of max(dbid) in =JobUsageRecord_Meta= and =MetricRecord_Meta= on *gratia07*, as these numbers will be used to set up replication from *gratia07* back to *gratia06* while checksums are being upgraded on *gratia06*.

---+++ Update the Replication services on *gratia06* to *gratia07*
On *gratia06*, if there are *Job Usage* and *Metric* replication services active, we will need to create entries in the *gratia07* instance as this will shortly become our production database.

When creating these entries, we will need to insure that we start with the last record replicated from *gratia06*, which may not have the same dbid on gratia06 as it did on gratia07.

Instructions for doing this will be provided later if this needs to be performed for this release.

This should be done *before* we switch the *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database.

The entries should be added and *only* a test that the entry is correct made at this time.  
We will set the *starting dbid*  later and activate the entry after we switch the tomcat collectors.

---++ Stop *gratia09:/data/tomcat-gratia* and *gratia08:/data/tomcat-conversion*
Stop each collector:
   1 on *gratia08* 
      * service tomcat-conversion stop
   1 on *gratia09* 
      * service tomcat-gratia stop

---++ Upgrade the current *gratia09:/data/tomcat-gratia* to *v0.34.9* 
On gratia09:
   1 Perform the upgrade to v0.34.9
      * pswd=xxx
      * source=/home/gratia/gratia-releases/gratia-0.34.9
      * pgm=/home/gratia/gratia-releases/gratia-0.34.9/common/configuration/update-gratia-local 
      * $pgm  -d $pswd -S $source -s  gratia
   1 Effective with this release, there is an administrative login process that, by default, does not allow access to the admin functions.  You will need to update the __TOMCAT_LOCATION/gratia/service-configuration.properties__ file with the following attributes. You will also need to change 2 connection attributes, as well. (Note that if you forget to do this, the administrator can be added at anytime without restarting the tomcat service):
      * service.admin.DN.0=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Philippe G. Canal/UID=pcanal
      * service.admin.DN.1=/DC=org/DC=doegrids/OU=People/CN=Christopher H. Green 851859
      * service.admin.DN.2=/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491
      * service.admin.DN.3=/DC=org/DC=doegrids/OU=People/CN=Steven Timm 74183
      * service.admin.DN.4=/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Steven C. Timm/UID=timm
      * 
      * service.open.connection=http://gratia-fermi.fnal.gov:8884
      * service.secure.connection=https://gratia-fermi.fnal.gov:8860
      * *IMPORTANT IMPORTANT... THIS MUST BE CHANGED*
      * %RED%service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia%ENDCOLOR%

---++ Switch *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database
Update the *service-configuration.properties* file to change the database that the production instance points to:
   1 In *gratia09:/data/tomcat-gratia/gratia/service-configuration.properties
      * service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
   1 Start the *gratia09* production collector *only* (it will be using the *gratia07* database
      * service tomcat-gratia start

At this point we have production gratia reporting and services back on-line and available.  Verify there are no problems by tailing the log files in *gratia09:/data/tomcat-gratia/logs* .   

If all is well, we can proceed to the v0.34.9 upgrade of the *gratia06:gratia* database.

---++ Backups
We will need to take backups on the *gratia07* database for the *gratia* database.

And also *stop* the backups on *gratia06* for the *gratia* database.

---++ Crons
This section can be delayed depending on the time of day.

---+++ Cron processes during upgrade
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process can remain on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter:
   <ul><li>GratiaHost  <b>gratia07</b>.fnal.gov</li></ul>


<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

Change in /home/gratia/gratia-summary/PSACCTReport.py the 
<verbatim>gMySQLConnectString = " -h gratia-db01.fnal.gov -u reader --port=3320 --password=reader "</verbatim>


<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to exclude the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *does not* require changing as it references the gratia09 gratia collector which will be pointing to the gratia07 database.

<li>gratia07 - user root<br>
A database backup cron entry needs to be *added* to this cron.  The appropriate zrm configuration files have already been added.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script needs to be copied over from *gratia06* to *gratia07* and modified to only backup the *gratia* database.  There should be no copies made to other machines.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces
Notify Brian to effect the changes necessary for his processes to use the *gratia07:3320* database

---++ Switch the current *gratia08:/data/tomcat-conversion* to use the *gratia06/gratia* database
Update *gratia08:/data/tomcat-conversion/gratia/service-configuration.properties* to point to the *gratia06/gratia* database:
   * service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia

On *gratia08* ,start the newly-upgraded tomcat-conversion collector
   * service tomcat-conversion start

---++ Start replication from production collector to upgrade collector.
After initial schema upgrades on *gratia06/gratia* are complete and *gratia08:/data/tomcat-conversion/gratia* is listening for new records, activate the replication of !JobUsage and Metric records. The starting *dbid* entries should be filled in based on max(dbid) for each =Meta= table on *gratia07*.

On *gratia07* :
   * select max(dbid) from <nop>JobUsageRecord_Meta; 
   * select max(dbid) from <nop>MetricRecord_Meta; 

<pre>
update Replication set dbid=?????
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=??;
update Replication set dbid=??????
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=??;
</pre>

---++ Monitor upgrade process on *gratia06/gratia and stop replication 
Towards the end of the upgrade of *gratia06/gratia*, updates to the DB will be stopped pending a final duplicate resolution phase and conversion of the index on =md5v2= to unique. 

Between this happening and the listener queue hitting its threshold, replication from *gratia09:/data/tomcat-gratia* to *gratia08:/data/tomcat-conversion/gratia* should be stopped.

---++ Watch for upgrade completion and complete replication

When the upgrade of *gratia06/gratia* is complete, 
   * the index =index17(md5v2)= on !JobUsageRecord_Meta will have its =non_unique= attribute set to 0    
   * and, *gratia09:/data/tomcat-gratia/logs/gratia-0.log* will indicate that the listeners have been reactivated. 

At this time:
   1 Restart replication from *gratia09:/data/tomcat-gratia* to *gratia08:/data/tomcat-conversion*.
   1 Stop DB updates on *gratia09:/data/tomcat-gratia*.
   1 Wait until replication is complete.


---++ Switch the current *gratia09:/data/tomcat-gratia* to use the *gratia06/gratia* database
To switch-over the tomcat collectors:
   1 Stop each collector
      * on *gratia08* %BR%
         - service tomcat-conversion stop
      * on *gratia09* %BR%
         - service tomcat-gratia stop
   1 Update the *service-configuration.properties* file to change the databases they point to
      * on *gratia08:/data/tomcat-conversion/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
      * on *gratia09:/data/tomcat-gratia/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia
   1 If necessary, change dbid in =Replication= table entries on gratia06 to match correct max(dbid) values for =Meta= tables on *gratia06*
   1 Start the *gratia09* production collector *only* (it will be using the *gratia06* database
      * on *gratia09* %BR%
         - service tomcat-gratia start

At this point production gratia reporting and services area back on-line and available and  using the *gratia06* database  

Verify there are no problems by tailing the log files in *gratia09:/data/tomcat-gratia/logs* .   


---++ Backups
We will need to  *start* the backups on *gratia06* for the *gratia* database.

And *deactivate* the backups on the *gratia07* database for the gratia database.

---++ Crons
---+++ Cron processes
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process remained on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter to point it back to the *gratia06* database.
   <ul><li>GratiaHost  <b>gratia-db01</b>.fnal.gov</li></ul>

<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to now include the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *did not* require changing as it references the gratia09 gratia collector which will now be pointed back  to the gratia06 database.

<li>gratia07 - user root<br>
This database backup cron entry needs to be *removed* from this cron.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script can be removed as well.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces
Notify Brian to switch back to using *gratia-db01:3320* database.


<!-- ----------------- POST MORTEM  ------------- -->
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.

<ol>

</ol>


%STOPINCLUDE%


-- Main.JohnWeigand - 30 May 2008
