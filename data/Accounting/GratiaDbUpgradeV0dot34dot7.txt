%META:TOPICINFO{author="ChrisGreen" date="1212183076" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="GratiaReleaseV0dot34"}%
---+!! Main gratia DB - Upgrade  to v0.34.8%BR% 
%TOC%

%STARTINCLUDE%
<!--
   * Set EDITTHISTEXT = <div class="twikiSmall"><a href="%TOPIC%">edit this section</a></div>
-->

---+ Main gratia DB - Upgrade  to v0.34.8
%EDITTHISTEXT%

The main Gratia database ( *gratia* ) served by the *gratia09:/data/tomcat-gratia* instance will require a very long 
conversion time. So to maximize availability we will:
   1 create a current copy of the *gratia06/gratia* database on *gratia07*
   1 switch the current *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database
   1 perform the upgrade of the *gratia06/gratia* database to v0.34.8 using a temporary Gratia collector on *gratia08:/data/tomcat-conversion* .
   1 when the upgrade is complete (inclusive of the conversion to the new md5 key), we will switch the *gratia09:/data/tomcat-gratia* instance back to using the *gratia06/gratia* database

During the upgrade period, this will be the tomcat/database environement for the *gratia* database:
%TABLE%
| *Tomcat instance* | *Database instance* |
| gratia09:/data/tomcat-gratia port 8880 | gratia07:/data/mysqldb/gratia port 3320 |
| gratia08:/data/tomcat-conversion port 8884 | gratia06:/data/mysqldb/gratia port 3320 |

The above is a high level summary of the process.  The details are contained in subsequent sections.

---++ Create a copy of the current database on *gratia07*
On 5/28, the *gratia07:/data/mysqldb* was re-partitioned into a single 200Gb area to provide the required space for the *gratia* database.

---+++ Create the *gratia07:gratia* database using the ZRM backup
This is in process now (5/30) and a finalized procedure will be included when it completes.

---+++ Configure *gratia08:/data/tomcat-conversion* to use the *gratia07:gratia* database
On *gratia08*, edit the */data/tomcat-conversion/gratia/service-configuration.properties* file for this resulting attribute:
   * service.mysql.url=jdbc:mysql://gratia07.fnal.gov:3320/gratia

*VERY IMPORTANT* - This *must* be a v0.32 version of Gratia.

Before activating this tomcat instance, check that the item =gratia.database.version= in the =SystemProplist= table on the *gratia06/gratia* database matches the variable =gratiaDatabaseVersion= in =DatabaseMaintenance.java= in the source for the code you have installed in *gratia08:/data/tomcat-conversion*. If this is *not* the case, you must install a version of the code that *does* match.

Start this tomcat instance:
   * service tomcat-conversion start

---+++ Bring the *gratia07* database up to current
Since the *gratia07* database is based on a nightly backup and production has been collecting data from the CE nodes, it needs to be brought up to current by replicating from *gratia06* to *gratia07* using the  administrative services *Replication* menu.
   * on http://gratia.opensciencegrid.org:8880/gratia-administration
   * both *Job Usage* and *Metric* replication services should be activated to replicate to:%BR%
     - <nop>http://gratia-fermi.fnal.gov:8884 
   * The starting *dbid* should be determined using the mysql client on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

*Wait* until the *gratia07* is caught up to the *gratia06* database.  You can monitor this by:
   * on *gratia06* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%
   * on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

When they are close, stop the *administrative update service* on
   *  http://gratia.opensciencegrid.org:8880/gratia-administration

The replication services should still be running to ensure we "catch up" the last of the updates on *gratia06* to *gratia07*.  So repeat the verification process for the *dbid* on each database until the *dbid* values are equal in both databases. Then turn off both the *Job Usage* and *Metric* replication services on:
   * http://gratia.opensciencegrid.org:8880/gratia-administration for <nop>http://gratia-fermi.fnal.gov:8884. Do *not* delete the entries at this time.

---+++ Update the Replication services on *gratia06* to *gratia07*
On *gratia06*, if there are *Job Usage* and *Metric* replication services active, we will need to create entries in the *gratia07* instance as this will shortly become our production database.

When creating this entries, we will need to insure that we start with the last *dbid* replicated from *gratia06*.

Instructions for doing this will be provided later if this needs to be performed for this release.

This should be done *before* we switch the *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database.

The entries should be added and *only* a test that the entry is correct made at this time.  We will set the * starting dbid* 
 later and activate he entry after we switch the tomcat collectors.

---++ Switch the current *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database
To switch-over the tomcat collectors:
   1 Stop each collector
      * on *gratia08* %BR%
         - service tomcat-conversion stop
      * on *gratia09* %BR%
         - service tomcat-gratia stop
   1 Update the *service-configuration.properties* file to change the database that the production instance points to:
      * on *gratia09:/data/tomcat-gratia/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
   1 Start the *gratia09* production collector *only* (it will be using the *gratia07* database
      * on *gratia09* %BR%
         - service tomcat-gratia start

At this point we have production gratia reporting and services back on-line and available.  Verify there are no problems by tailing the 
log files in *gratia09:/data/tomcat-gratia/logs* .   If all is well, we can proceed to the v0.34.8 upgrade of the *gratia06:gratia* database.

---+++ Cron processes during upgrade
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process can remain on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter:
   <ul><li>GratiaHost  <b>gratia07</b>.fnal.gov</li></ul>


<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

Change in /home/gratia/gratia-summary/PSACCTReport.py the 
<verbatim>gMySQLConnectString = " -h gratia-db01.fnal.gov -u reader --port=3320 --password=reader "</verbatim>


<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to exclude the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *does not* require changing as it references the gratia09 gratia collector which will be pointing to the gratia07 database.

<li>gratia07 - user root<br>
A database backup cron entry needs to be *added* to this cron.  The appropriate zrm configuration files have already been added.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script needs to be copied over from *gratia06* to *gratia07* and modified to only backup the *gratia* database.  There should be no copies made to other machines.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces



---++ Upgrade  the *gratia06/gratia* database to v0.34.8 

---+++ Upgrade the tomcat-conversion instance to v0.34.8

Use =update-gratia-local= to upgrade the tomcat-conversion instance to v0.34.8.

---+++ Switch the current *gratia08:/data/tomcat-conversion* to use the *gratia06/gratia* database

      * on *gratia08:/data/tomcat-conversion/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia

---+++ Start the upgrade of the *gratia06/gratia database*

Start the newly-upgraded tomcat-conversion collector
      * on *gratia08* %BR%
         - service tomcat-conversion start

---+++ Start replication from production collector to upgrade collector.

After initial schema upgrades on *gratia06/gratia* are complete and *gratia08:/data/tomcat-conversion/gratia* is listening for new records, activate the replication of !JobUsage and Metric records. The starting DBID entries should be correct based on the previous replication activity, but this must be verified.

---+++ Monitor upgrade process and stop replication 

Note that replication *from* v0.32 is not robust against the replicatee being unavailable, so we need to be careful.

Towards the end of the upgrade of *gratia06/gratia*, updates to the DB will be stopped pending a final duplicate resolution phase and conversion of the index on =md5v2= to unique. Between this happening and the listener queue hitting its threshold, replication from *gratia09:/data/tomcat-gratia* to *gratia08:/data/tomcat-conversion/gratia* must be stopped.

---+++ Watch for upgrade completion and complete replication

When the upgrade of *gratia06/gratia* is complete, the index =index17(md5v2)= on !JobUsageRecord_Meta will have its =non_unique= attribute set to 0 and *gratia09:/data/tomcat-gratia/logs/gratia-0.log* will indicate that the listeners have been reactivated. At this time:

   1 Restart replication from *gratia09:/data/tomcat-gratia* to *gratia09:/data/tomcat-conversion*.
   1 Stop DB updates on *gratia09:/data/tomcat-gratia*.
   1 Wait until replication is complete.

---++ Switch the current *gratia09:/data/tomcat-gratia* to use the *gratia06/gratia* database
To switch-over the tomcat collectors:
   1 Stop each collector
      * on *gratia08* %BR%
         - service tomcat-conversion stop
      * on *gratia09* %BR%
         - service tomcat-gratia stop
   1 Update the *service-configuration.properties* file to change the databases they point to
      * on *gratia08:/data/tomcat-conversion/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
      * on *gratia09:/data/tomcat-gratia/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia
   1 Start the *gratia09* production collector *only* (it will be using the *gratia06* database
      * on *gratia09* %BR%
         - service tomcat-gratia start

At this point production gratia reporting and services area back on-line and available and  using the *gratia06* database  Verify there are no problems by tailing the 
log files in *gratia09:/data/tomcat-gratia/logs* .   


---+++ Cron processes
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process remained on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter to point it back to the *gratia06* database.
   <ul><li>GratiaHost  <b>gratia-db01</b>.fnal.gov</li></ul>

<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to now include the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *did not* require changing as it references the gratia09 gratia collector which will now be pointed back  to the gratia06 database.

<li>gratia07 - user root<br>
This database backup cron entry needs to be *removed* from this cron.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script can be removed as well.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces



<!-- ----------------- POST MORTEM  ------------- -->
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.

<ol>

</ol>


%STOPINCLUDE%


-- Main.JohnWeigand - 30 May 2008
