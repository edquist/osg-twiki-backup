%META:TOPICINFO{author="ChrisGreen" date="1213199369" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="GratiaReleaseV0dot34"}%
---+!! Main gratia DB - Upgrade  to v0.34.9%BR% 
%TOC%

%STARTINCLUDE%
<!--
   * Set EDITTHISTEXT = <div class="twikiSmall"><a href="%TOPIC%">edit this section</a></div>
-->

---+ Main gratia DB - Upgrade  to v0.34.9
%EDITTHISTEXT%

The main Gratia database ( *gratia* ) served by the *gratia09:/data/tomcat-gratia* instance will require a very long 
conversion time. So to maximize availability we will:
   1 create a current copy of the *gratia06/gratia* database on *gratia07*
   1 switch the current *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database
   1 perform the upgrade of the *gratia06/gratia* database to v0.34.9 using a temporary Gratia collector on *gratia08:/data/tomcat-conversion* .
   1 when the upgrade is complete (inclusive of the conversion to the new md5 key), we will switch the *gratia09:/data/tomcat-gratia* instance back to using the *gratia06/gratia* database

During the upgrade period, this will be the tomcat/database environement for the *gratia* database:
%TABLE%
| *Tomcat instance* | *Database instance* |
| gratia09:/data/tomcat-gratia port 8880 | gratia07:/data/mysqldb/gratia port 3320 |
| gratia08:/data/tomcat-conversion port 8884 | gratia06:/data/mysqldb/gratia port 3320 |

The above is a high level summary of the process.  The details are contained in subsequent sections.

---++ Create a copy of the current database on *gratia07*
On 5/28, the *gratia07:/data/mysqldb* was re-partitioned into a single 200Gb area to provide the required space for the *gratia* database.

---+++ Create the *gratia07:gratia* database using the ZRM backup
This is in process now (5/30) and a finalized procedure will be included when it completes.

---+++ Configure *gratia08:/data/tomcat-conversion* to use the *gratia07:gratia* database
On *gratia08*, edit the */data/tomcat-conversion/gratia/service-configuration.properties* file for this resulting attribute:
   * service.mysql.url=jdbc:mysql://gratia07.fnal.gov:3320/gratia

%GREEN%Done - 6/3/08 12:37 CDT (John Weigand)%ENDCOLOR%

---+++ Upgrade _but do not start_ *gratia08:/data/tomcat-conversion* to *v0-34-9*

---+++ Prepare *gratia08:/data/tomcat-conversion* for activation

   * Add or update the property =gratia.database.disableChecksumUpgrade= to have =cdr= = *1*.

---+++ Start *gratia08:/data/tomcat-conversion*
   * service tomcat-conversion start

---+++ Bring the *gratia07* database up to current
Since the *gratia07* database is based on a nightly backup and production has been collecting data from the CE nodes, it needs to be brought up to current by replicating from *gratia06* to *gratia07* using the  administrative services *Replication* menu.
   * on http://gratia.opensciencegrid.org:8880/gratia-administration
   * both *Job Usage* and *Metric* replication services should be activated to replicate to:%BR%
     - <nop>http://gratia-fermi.fnal.gov:8884 
   * The starting *dbid* should be determined (*once only* after the backup is restored) using the mysql client on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

%GREEN% *Partially done* - 6/3/08 13:45 CDT (John Weigand)
<pre>
select max(dbid) from JobUsageRecord_Meta; 
+-----------+
| max(dbid) |
+-----------+
|  75242300 | 
+-----------+

select max(dbid) from MetricRecord_Meta; 
+-----------+
| max(dbid) |
+-----------+
|      1506 | 
+-----------+

<b>On gratia06:</b>
update Replication set dbid=75242300 
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=52;

update Replication set dbid=1506 
where openconnection='http://gratia-fermi.fnal.gov:8884' and replicationid=54;

</pre>

Starting point (approx 6/3/08 14:00 CDT)
| gratia06 max(dbid) | 75,851,594 |
| gratia07 max(dbid) | 75,242,300 |
| catch up | 609,294 |

Catch up complete around 6/3/08 23:00 CDT
| hour UTC | updates |
| 080604 12 |     7715 | 
| 080604 11 |     8224 | 
| 080604 10 |     8435 | 
| 080604 09 |     6962 | 
| 080604 08 |     7869 | 
| 080604 07 |     8005 | 
| 080604 06 |     7753 | 
| 080604 05 |     6874 | 
| 080604 04 |     7944 | 
| 080604 03 |     9629 | 
| 080604 02 |    14588 | 
| 080604 01 |    92283 | 
| 080604 00 |    89794 | 
| 080603 23 |    87849 | 
| 080603 22 |    70699 | 
| 080603 21 |    89476 | 
| 080603 20 |    89201 | 
| 080603 19 |    32873 | 

%ENDCOLOR%

*Wait* until the *gratia07* is caught up to the *gratia06* database.  You can monitor this by:
   * on *gratia06* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%
   * on *gratia07* %BR%
      - select max(dbid) from <nop>JobUsageRecord_Meta; %BR%
      - select max(dbid) from <nop>MetricRecord_Meta; %BR%

When they are close, stop the *administrative update service* on
   *  http://gratia.opensciencegrid.org:8880/gratia-administration

The replication services should still be running to ensure we "catch up" the last of the updates on *gratia06* to *gratia07*.  The replication activity shall be complete when both replication data pumps (as shown by =gratia-0.log= on gratia08) have been through at least one check-duplicate cycle without uploading records.

Then, turn off both the *Job Usage* and *Metric* replication services on:
   * http://gratia.opensciencegrid.org:8880/gratia-administration for <nop>http://gratia-fermi.fnal.gov:8884. 
   * Do *not* delete the entries at this time.%BR%

Take note of max(dbid) in =JobUsageRecord_Meta= and =MetricRecord_Meta= on *gratia07*, as these numbers will be used to set up replication from *gratia07* back to *gratia06* while checksums are being upgraded on *gratia06*.

---+++ Update the Replication services on *gratia06* to *gratia07*
On *gratia06*, if there are *Job Usage* and *Metric* replication services active, we will need to create entries in the *gratia07* instance as this will shortly become our production database.

When creating these entries, we will need to insure that we start with the last record replicated from *gratia06*, which may not have the same dbid on gratia06 as it did on gratia07.

Instructions for doing this will be provided later if this needs to be performed for this release.

This should be done *before* we switch the *gratia09:/data/tomcat-gratia* to use the *gratia07/gratia* database.

The entries should be added and *only* a test that the entry is correct made at this time.  We will set the *starting dbid* 
 later and activate the entry after we switch the tomcat collectors.


---++ Upgrade the current *gratia09:/data/tomcat-gratia* to *v0.34.9* and switch it to use the *gratia07/gratia* database
To upgrade and/or switch-over the tomcat collectors:
   1 Stop each collector
      * on *gratia08* %BR%
         - service tomcat-conversion stop
      * on *gratia09* %BR%
         - service tomcat-gratia stop
   1 Upgrade *gratia09:/data/tomcat-gratia* to *v0.34.9*
   1 Update the *service-configuration.properties* file to change the database that the production instance points to:
      * on *gratia09:/data/tomcat-gratia/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
   1 Start the *gratia09* production collector *only* (it will be using the *gratia07* database
      * on *gratia09* %BR%
         - service tomcat-gratia start

At this point we have production gratia reporting and services back on-line and available.  Verify there are no problems by tailing the 
log files in *gratia09:/data/tomcat-gratia/logs* .   If all is well, we can proceed to the v0.34.9 upgrade of the *gratia06:gratia* database.

---+++ Cron processes during upgrade
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process can remain on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter:
   <ul><li>GratiaHost  <b>gratia07</b>.fnal.gov</li></ul>


<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

Change in /home/gratia/gratia-summary/PSACCTReport.py the 
<verbatim>gMySQLConnectString = " -h gratia-db01.fnal.gov -u reader --port=3320 --password=reader "</verbatim>


<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to exclude the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *does not* require changing as it references the gratia09 gratia collector which will be pointing to the gratia07 database.

<li>gratia07 - user root<br>
A database backup cron entry needs to be *added* to this cron.  The appropriate zrm configuration files have already been added.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script needs to be copied over from *gratia06* to *gratia07* and modified to only backup the *gratia* database.  There should be no copies made to other machines.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces


---++ Upgrade  the *gratia06/gratia* database to v0.34.9 

---+++ Upgrade the tomcat-conversion instance to v0.34.9

Use =update-gratia-local= to upgrade the tomcat-conversion instance to v0.34.9.

---+++ Switch the current *gratia08:/data/tomcat-conversion* to use the *gratia06/gratia* database

      * on *gratia08:/data/tomcat-conversion/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia

---+++ Start the upgrade of the *gratia06/gratia database*

Start the newly-upgraded tomcat-conversion collector
      * on *gratia08* %BR%
         - service tomcat-conversion start

---+++ Start replication from production collector to upgrade collector.

After initial schema upgrades on *gratia06/gratia* are complete and *gratia08:/data/tomcat-conversion/gratia* is listening for new records, activate the replication of !JobUsage and Metric records. The starting DBID entries should be filled in based on max(dbid) for each =Meta= table on *gratia07*.

---+++ Monitor upgrade process and stop replication 

Towards the end of the upgrade of *gratia06/gratia*, updates to the DB will be stopped pending a final duplicate resolution phase and conversion of the index on =md5v2= to unique. Between this happening and the listener queue hitting its threshold, replication from *gratia09:/data/tomcat-gratia* to *gratia08:/data/tomcat-conversion/gratia* should be stopped.

---+++ Watch for upgrade completion and complete replication

When the upgrade of *gratia06/gratia* is complete, the index =index17(md5v2)= on !JobUsageRecord_Meta will have its =non_unique= attribute set to 0 and *gratia09:/data/tomcat-gratia/logs/gratia-0.log* will indicate that the listeners have been reactivated. At this time:

   1 Restart replication from *gratia09:/data/tomcat-gratia* to *gratia09:/data/tomcat-conversion*.
   1 Stop DB updates on *gratia09:/data/tomcat-gratia*.
   1 Wait until replication is complete.

---++ Switch the current *gratia09:/data/tomcat-gratia* to use the *gratia06/gratia* database
To switch-over the tomcat collectors:
   1 Stop each collector
      * on *gratia08* %BR%
         - service tomcat-conversion stop
      * on *gratia09* %BR%
         - service tomcat-gratia stop
   1 Update the *service-configuration.properties* file to change the databases they point to
      * on *gratia08:/data/tomcat-conversion/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia07</b>.fnal.gov:3320/gratia
      * on *gratia09:/data/tomcat-gratia/gratia* %BR%
         - service.mysql.url=jdbc:mysql://<b>gratia06</b>.fnal.gov:3320/gratia
   1 If necessary, change dbid in =Replication= table entries on gratia06 to match correct max(dbid) values for =Meta= tables on *gratia06*
   1 Start the *gratia09* production collector *only* (it will be using the *gratia06* database
      * on *gratia09* %BR%
         - service tomcat-gratia start

At this point production gratia reporting and services area back on-line and available and  using the *gratia06* database  Verify there are no problems by tailing the 
log files in *gratia09:/data/tomcat-gratia/logs* .   


---+++ Cron processes
There are several processes (cron) that will need to be copied/modified on  _gratia06_ and _gratia09_  for reports and interfaces.
<ol>
<li>gratia06 - user gratia
  <ul><li>APEL-WLCG interface 
<pre>
 dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update
</pre>
       This cron process remained on *gratia06*.  The only modification required is to the */home/gratia/interfaces/apel-lcg/lcg-db.conf* file for the following parameter to point it back to the *gratia06* database.
   <ul><li>GratiaHost  <b>gratia-db01</b>.fnal.gov</li></ul>

<li>OSG daily reports 
<verbatim>
/home/gratia/gratia-summary/gratiaSum.cron.sh
/home/gratia/gratia-summary/daily_mail_cron.sh
/home/gratia/gratia-summary/range_mail_cron.sh --weekly
/home/gratia/gratia-summary/range_mail_cron.sh --monthly
/home/gratia/gratia-summary/rungratia.sh --production 
</verbatim>
        <li>CMS weekly reports
<verbatim>/home/gratia/gratia-summary/cms_mail_cron.sh --weekly</verbatim>
   </ul>

<li>gratia06 - user root<br>
Database backups on *gratia06* will need to be changed to now include the *gratia* database.  This includes the purging of log files by the zrm process.
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>

<li>gratia09 - user root<br>
The static reports cron on gratia09 *did not* require changing as it references the gratia09 gratia collector which will now be pointed back  to the gratia06 database.

<li>gratia07 - user root<br>
This database backup cron entry needs to be *removed* from this cron.  
<pre>
/usr/local/bin/gratia_backup_cron.sh 
/usr/bin/mysql-zrm --action purge
</pre>
The <b>/usr/local/bin/gratia_backup_cron.sh</b> script can be removed as well.
</ol>

---+++ Nebraska (Brian Bockelman) interfaces



<!-- ----------------- POST MORTEM  ------------- -->
---+ Post-mortem
At this time, this will appear to be random notes.  After the conversion, they may be organized.

<ol>

</ol>


%STOPINCLUDE%


-- Main.JohnWeigand - 30 May 2008
