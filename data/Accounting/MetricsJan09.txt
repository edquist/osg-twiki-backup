%META:TOPICINFO{author="RobGardner" date="1168644142" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="Metrics"}%
---+ Metrics Meeting, January 9, 2007
---++ Attending
   * Phillipe, Chris, Ruth, Rob, Chander, John

---++ Background

Notes from Ruth:

Meeting to discuss high level milestone to define and agree to the Operations Metrics for the first year of OSG which is due now (1st Jan actually) and which comes from the Facility Coordinator.

Need to document a list of metrics, how to measure them, and how we will determine success.

Included a list of some metrics in the Project Plan
http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=514 but did not include how we will determine success.

how should we get there from here without more than a week or two delay?




Meeting at Fermilab to review Following notes need to be synthesized.
d18 5
---++ Notes from Rob
d17 9
   * Discussion of the "happiness metric" suggested in various places.  Chander had a definition, =Happiness = results/expectations=
   * One of the main issues is setting user expectations - Chris has been thinking about this in his role of OSG user support. Is there a reasonable set of expectations at present? And what are the expectations?
      *  Simply, I want to submit jobs and get the results back.
      * Can I run on your site?  (lots of time wasted on things like this.)
      * An example of a worker-node not able to write into $APP, without a unique RSL.
   * We discussed the limitation of advertized OSG site attributes (even when those are available).  Lots of additional site-specific information seems neccessary at the moment.  There seems to be a need for a free-format  information service that collects such information.
   * Need to survey of happiness ratio, but its probably early to do this.
   * Going forward, we need to determine where to set expecations, given the quality of the infrastructure and effort available.
   * Ruth: and site performance should not exclude metrics for data movement and storage.
   * Currently a framework is needed that would be based on: the OSG information service (whats available), Gratia probes and reporters (what happened).  
      * *What clearly is missing is error information.*
   * Note that Gratia also can collect VO probe information.
   * VO dashboards - collecting that information: CMS and ATLAS both have dashboards. 
  
January 9, 2007  - Metrics Meeting at Fermilab

Chander, Phillipe, Chris Green, John Urish

Chander: Happiness = results/expectations

Chris is doing user support.

Is there a reasonable set of expectations?

And what are the expectations?

Can I run on your site?

Eg of WN cannot write into $APP

Unique RSL was required.
d178 3
Need of a free-format  info system that advertise site pe

Survey of happiness factor.

Where to set expecations?

Need to keep in mind the data movement and storage.

Boundary of the VO.

Need to make it easier.

Framework is based on :  information service (whats available), Gratia probes and reporters (what happened).  What is missing is error information.

Gratia also can collect VO probe information.

CMS error information collected by dashboard &#8211; more general 

Global job identifier.
d29 3
Errors discussion:
   * Need of a Global job identifier - this has been under discussion for a long time - what is the status?
   * Globus errors, eg - would like to collect site/VO statistics
   * Phillip is concentrating on batch scheduler errors.
   * Globus errors
   * Phillip concentrating on the batch scheduler
   * Each service should have its own probe.
   * There is a job model in Gratia &#8211; it knows the state it when completed.


Most important list of metrics:
   * How many CPUs are available?
   * Use Cores; get from info system.
   * Does Monalisa measure this? Ans seems yes.
   * Separate policy for the raw measurement.


Length of jobs profile.

Condor free slots.  PBS free nodes.

d44 3
   * Availability of processing and storage resources:
      * Availability is measured by the OSG site functional tests and validations.  
      * Definition of availability is GREEN in VORS.  By VO.
      * VO based validation tests will give VO specific availability measurements.
Availability is measured by the OSG site functional tests and validations. 
(third place)


Definition of availability is GREEN in VORS.  By VO.


VO based validation tests will give VO specific availability measurements.

d49 3
Aggregation of information.
   * There are tools that work above databases.
   * Storage: availability from CEMon (need to wait for glue schema 1.3; there is a final draft; Lawrence will be working on a probe.) .  
   * Usage: there will be a sensor for dCache I/O, and an API for this. 

There are tools that work above databases.


Storage: availability from cemon (need to wait for glue schema 1.3; there is a final draft; Lawrence will be working on a probe.) .  Usage &#8211; there will be one for dcache I/O.  there is an API for this. 



&#8226;       Use of processing and storage resources:
Resource usage is measured by the OSG accounting infrastructure and measures parameters such as wall clock time on a CPU, MBytes  moved to and from storage resources, MBytes of storage used. OSG resource usage is measured for accesses through grid interfaces &#8211; whether that access results in use of a local or remote resource.

&#8226;       System throughput:
Number of jobs submitted, number of jobs executed, number of files accessed,  and amount of data moved per day per user group.
---++ Notes from Philip
&#8226;       Measures of shared use:
Fraction of jobs executed and data stored on sites that are not &#8220;owned&#8221; or provide assured access to the group submitting the jobs.

&#8226;       System latencies:
The time a job spends in the wait queue, the wait  time for a request for transfer from  storage or between resources per day per user group.



&#8226;       Efficiency and Effectiveness:
Fraction of available CPU cycles that are not utilized. Error and rertry statistics for grid use. These statistics must be carefully gathered and understood to allow separation of errors due to the OSG and/or site infrastructures and the user middleware and applications.

Must come from the VOs.




1)	Goals of operational metrics.
2)	Document of metrics definitions.
3)	John is coming up to speed.  Accounting is being called on to get more effort.
4)	

d139 1
---++ Notes from Philippe
Metrics:

Should we have a user survey?

Philippe: Yes, in particular to avoid having a big (unknown)
difference between the user perception and the providers 
perceptions.

Chris: The 'satisfaction' of the user seems to be strongly
correctly to initial impression (aka how hard it was to get
the user job to actually run on the site).

Rob: One issue is un-inform expectation / misunderstanding
of the nature of the OSG.  Or even expectations we can not
possibly meet.

Chris/Rob: So one of the issue is a lack of a good information
system in OSG.

Rob: The gap in expectation is one important reason why I am
concerned about the touchy-feely type of metrics

Philippe: Indeed we need to use the 'user survey' only as one
of the many metrics (and possibly it would not be upfront in
the presentation).

Chander: We need to have a clarification of what the expectation
should be.

All: There is no way (expect trying to submit a job) to know the
'feature' available or not on the site.

Rob: I am hoping that this time around, the OSG will have a real
Information Service (as opposed to the current mesh of tools).
We will have the GIP, (BDII and CEMON) (following the GLUE schema).
We have some VO that do not want to use the OSG information service
and will keep the information they need to match job in their own
database.  Matching all this different type of information is hard.

Rob: It might be usefull to have some set of example application
than can be used to check what can or can not be done.

All: We all agree that before we can do a survey we need to have
gathered the other technical metrics so we can constract them with
the survey the result.

Ruth: When talking about number of jobs running we also must talk
about the amount of data being transferred.

Philippe: Eventually we need to account for the usage of all OSG
service (including Portal and Globus).

Ruth: Can we have metric on the number of Cores available to the OSG.

Rob: Maybe it is the same as the number of job slot visible to the
OSG (or how many concurrent job can the best match VO run as the same
time on the site assuming nothing else is running).

Ruth: I thinking Monalisa is measuring exactly that.






-- Main.RobGardner - 11 Jan 2007

%META:FILEATTACHMENT{name="OSG-Metrics-V1.doc" attachment="OSG-Metrics-V1.doc" attr="" comment="Miron's metrics vision V1" date="1169571973" path="OSG-Metrics-V1.doc" size="29184" stream="OSG-Metrics-V1.doc" user="Main.RobGardner" version="1"}%
%META:FILEATTACHMENT{name="gridpp16_metrics.ppt" attachment="gridpp16_metrics.ppt" attr="" comment="Metrics talk from GridPP" date="1169573203" path="gridpp16_metrics.ppt" size="1452032" stream="gridpp16_metrics.ppt" user="Main.RobGardner" version="1"}%
