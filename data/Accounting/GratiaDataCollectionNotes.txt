%META:TOPICINFO{author="ChrisGreen" date="1228166601" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! Gratia Data Collection Notes

---++ Introduction

This attempts to put all in one place a description of the algorithms followed when collecting and processing job level Gratia data. Where appropriate, common and probe-specific behaviors will be discussed, including any historical behaviors that still may be of relevance.

---++ Overview of Gratia data collection.

Speaking very generally, a Gratia probe will:
   1. Collect information from one or more sources;
   1. Prepare a record for upload to the probe by using the Gratia python API;
   1. Contact a Gratia data collection and reporting service, upload some meta-information and then the job level data.

The different types of data sent to Gratia collectors include:

   1. Local and OSG-originated job level information on batch jobs executed by a variety of LRMS (Local Resource Management Systems) including Condor, PBS (including Torque), LSF, SGE;
   1. Summary level information from a variety of resources (PANDA is a prime example) for _ad hoc_ comparison with job-level data.
   1. Summarized process-level information.
   1. Metric information from RSV tests.
   1. Pilot level information from the glExec pilot/glide-in system.
   1. Storage allocation information from OSG Storage Elements (SEs).
   1. Transfer-level information from dCache and GRIDFtp.

At the collector these data are received, stored in a DB and summarized as appropriate. Reports are available either via the BIRT interface, or as text-based emails.

---++ Anatomy of a record upload.

<ol><li>The probe will register its relevant version information with the Gratia infrastructure using any or all of the following python routines:
    <dl><dt> <tt>Gratia.registerReporter(<em>name</em>, <em>version string</em>)</tt> </dt><dd> <tt>Gratia.registerReporter("glexec.py", "v1.0.2")</tt> </dd>
        <dt> <tt>Gratia.registerReporterLibrary(<em>name</em>, <em>version string</em>)</tt> </dt><dd> <tt>Gratia.registerReporterLibrary("glexec_extraSubs.py", "v1.1")</tt> </dd>
        <dt> <tt>Gratia.registerService(<em>name</em>, <em>version string</em>)</tt> </dt><dd> <tt>Gratia.registerService("Condor", "v7.0.3")</tt> </dd>
    </dl>
    A useful utility routine here is <tt>Gratia.extractRevision("$Revision: %")</tt> </li>
    <li>Initialize the Gratia system and perform a handshake with the collector: <tt>Gratia.initialize(<em>[config-file]</em>)</tt> </li>
    <li>Attempt to send any records written to file but not uploaded successfully.</li>
    <li>For each job record to upload:
    <ul><li>Define the job level record with information gleaned from the primary information source (eg LRMS log).</li>
        <li>Give the send instruction for the record (<tt>record.Send()</tt>).</li>
        <li>Pre-process the data prior to sending by:
        <ul><li>Checking the !MeterName, !SiteName and Grid attributes and adding them to the record as appropriate.</li>
            <li id="check_vo">Checking and obtaining the best possible values for !VOName and !ReportableVOName according to an established order of precedence. Please see the specific [[#Methods_used_to_ascertain_the_us][notes on !VOName and !ReportableVOName]] below.</li>
        <li>Suppress records as appropriate according to configuration settings and data checks ([[#record_suppression][see below]]).</li>
        <li>Create a file backup of the record.</li>
        <li>Send the record and delete the file backup upon successful completion.</li>
        </ul></li>
    </ul></li>
    <li>Remove old log files and old (unusable) data files.</li>
    <li>Produce a summary of records sent and failed.</li>
    <li>Disconnect and exit.</li>
</ol>

---++ Behavior common to all probes.

<ul>
    <li> _Any_ probe reading data from elsewhere (DB, log files, etc) must take care to track progress and avoid sending multiple records describing the same event.</li>
    <li>The second and subsequent records received by the collector describing a given event are not written to the DB in the usual manner but are stored in the !DupRecord table along with a reference to the original record. These are not considered further by the collector except to be cleaned up according to the configured housekeeping schedule. A revised description of an event therefore must be incorporated in the DB manually.</li>
    <li>Records not sent to the collector successfully will be stored on disk and sent when connection is re-established.</li>
    <li>Log-scraping probes (eg PBS / LSF, SGE) must take special care to track progress and avoid rewinding and re-sending information about old jobs. In addition, one must take care to avoid problems associated with the possibility of reading the log at the same time as new information is being added (possibly at high rate). This problem is especially acute when the log is mirrored in some way from its primary location (rsync or NFS).</li>
    <li id="record_suppression">Several !ProbeConfig attributes control whether and how records are suppressed without being uploaded to the collector:
    <dl><dt><tt>SuppressUnknownVORecords</tt></dt>
        <dd>Suppress records with a !VOName of "Unknown."</dd>
        <dt><tt>SuppressNoDNRecords</tt></dt>
        <dd>Suppress records with no DN.</dd>
        <dt><tt>SuppressGridLocalRecords</tt></dt>
        <dd>Suppress records with a Grid attribute of "Local."</dd>
    </dl></li>
</ul>

---++ Behavior common to LRMS probes.

[ LRMS -> Local Resource Management System a.k.a batch manager ]

---+++ Methods used to ascertain the user details associated with a job.

The user details attributes of an LRMS Gratia record are:
   * !LocalUserId;
   * !GlobalUserName;
   * DN;
   * !VOName; _and_
   * !ReportableVOName

In particular, !VOName and !ReportableVOName correspond respectively to _either_:
   * FQAN, VO Name (or sub-VO name as appropriate, eg Minos); _or_
   * voi, !VOc as specified in =$VDT_LOCATION/monitoring/osg-user-vo-map.txt=.

On a modern system (OSG 1.0+ for Condor probe, OSG 1.0/VDT 1.10.1n+ for other probes), the DN, FQAN and VO name are obtained via a Gratia-specific hook in the job manager Perl code which runs voms-proxy-info on the delegated proxy (if available). The proxy is available if the jobs was submitted via:
   * !CondorG;
   * =globus-job-run= (or =globusrun=); _or_
   * =globusrun-ws= with explicit credential delegation.

Whether FQAN / !VOName are available in this manner depends on whether the submitter used a voms-proxy (yes) or vanilla grid-proxy (no).

The information gleaned in this way is stored in a, "certinfo" file in =$VDT_LOCATION/gratia/var/data=.

If the DN is not available via this method, it is obtainable from the !ClassAd for Condor jobs or via reverse grid map file look-up for the SGE probe.

On older OSG installations or if a delegated proxy is not available, the !LocalUserId is used to look up the voi & !VOc. 

To summarize: following algorithm is used by the =Gratia.py= infrastructure at the [[#check_vo][relevant point]] to decide what gets sent to the collector:
   1. !VOName / !ReportableVOName as provided by the specific probe if !VOName is the FQAN;
   1. Credentials from a certinfo file if one can be associated with the job record;
   1. !VOName / !ReportableVOName if provided by the specific probe; _or_
   1. voi & !VOc from a reverse grid map file.

At the collector level every unique combination of !VOName / !ReportableVOName is put into a table. This combination is used to look up the "true" VO name for reporting purposes. If this combination has not been seen before the default translation of the combination is the !VOName unless the !VOName is an FQAN, in which case the default translation is the !ReportableVOName. This translation may be changed on the collector using the administration GUI.

---++ Specific probe behavior and other notes.

---++ Advice to OSG VOs wishing to get as complete an accounting of their activities as possible.

---+++ Things you can do yourself.

---+++ Things to ask of the sites where you run jobs.


-- Main.ChrisGreen - 01 Dec 2008
