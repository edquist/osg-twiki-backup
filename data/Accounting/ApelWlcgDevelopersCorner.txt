%META:TOPICINFO{author="JohnWeigand" date="1365446927" format="1.1" reprev="1.10" version="1.10"}%
%META:TOPICPARENT{name="GratiaInterfacesApelLcg"}%
<!--
   * Set LCG_CONF        = [[#lcg_conf][lcg.conf]]
   * Set LCG_DB_CONF = [[#lcg_db_conf][lcg-db.conf]]
   * Set CRONTAB          = [[#lcg_conf][lcg.conf]]
   * Set CRONTAB           = [[#lcg_reportablesites][lcg-reportableSites]]
   * Set CRONTAB          = [[#lcg_reportablevos][lcg-reportableVOs]]
   * Set CRONTAB           = [[#crontab][crontab entry]]
   * Set SSM_CFG          = [[#ssm_cfg][ssm.cfg]]
   * Set SSM_LOG_CFG = [[#ssm_log_cfg][ssm.log.cfg]]
   * Set SCRIPT_LOCATION  = _/usr/share/gratia-apel_
   * Set CONFIG_LOCATION = _/etc/gratia-apel_
   * Set CRONTAB_LOCATION = _/etc/cron.d_
   * Set SERVICES_LOCATION = _/etc/init.d_
-->

---+!! APEL/WLCG Interface Developers Corner

%TOC%

%STARTINCLUDE%


---+ Developers Corner
This document  describes the scripts and configuration files used by this interface.

File Locations:
   * [[#scripts][scripts]]: %SCRIPT_LOCATION%
   * [[#Configuration_files][configuration files]]: %CONFIG_LOCATION%
   * [[#Crontab][crontab files]]: %CRONTAB_LOCATION%
   * [[#initd_services][initd files]]: %SERVICES_LOCATION%

<a name="script_location"/>
---+ Scripts
All the executable scripts in this section can be found in the %SCRIPT_LOCATION%  directory.

---++ LCG.py
Usage:
<blockquote><pre>
  LCG.py(lcg.sh)  --conf=config_file --date=month [--update] [--no-email]

     --conf - specifies the main configuration file to be used
              which would normally be the lcg.conf file

     --date - specifies the monthly time period to be updated:
              Valid values:
                current  - the current month
                previous - the previous month
                YYYY/MM  - any year and month

              The 'current' and 'previous' values are to facillitate running
              this as a cron script.  Generally, we will run a cron
              entry for the 'previous' month for n days into the current
              month in order to insure all reporting has been completed.

     The following 2 options are to facillitate testing and to avoid
     accidental running and sending of the SSM message to APEL and are
     therefore considered optional:

     --update - this option says to go ahead and update the APEL/WLCG database.
                If this option is NOT specified, then everything is executed
                EXCEPT the actual sending of the SSM message to APEL.
                The message file will be created.

     --no-email - this option says to turnoff the sending of email
                notifications on failures and successful completions.
</pre></blockquote>

This is the main interface program performing the following functions: 

 1. Retrieves the accounting data from the Gratia database for selected OSG sites and VOs for a month. 
   * The __resource_groups__ are defined in a file identified by the 'SiteFilterFile' attribute of the %LCG_CONF%  file. 
   * VOs are defined in a file identified by the 'VOFilterFile' attribute of the %LCG_CONF%  file.
With the change implemented on 6/19/07 providing for individual normalization factor by site, instead of a single query being used to retrieve the gratia data, individual queries by site are performed.  In order to reduce the verbage in the log file, only the first query is output to the log file.  The site and normalization factor used is displayed for the other site queries.

2. Formats the SSM message sent to APEL to update  APEL/WLCG database. The %LCG_DB_CONF% file defines  For testing purposes a clone table called OSG_TABLE_TEST can be set instead. The SQL DML statements for the last run are placed in a directory defined by the 'LogSqlDir' attribute of the lcg.conf file. The file name is YYYY-MM.sql and contains only the DML from the last update run. 

 3. Updates the APEL/CLG database. In order to handle site and VO name changes, the SQL DML is set up to DELETE all data for the month/year and then INSERT the new data. This is a simple approach to handling these kinds of changes and works as of 4/25/2007.  It may be this will not hold up over the long run and will have to be re-visited.

Additional functionality:

  1. The script has been design to do everything EXCEPT update the APEL database unless the --update option is used.  This prevents accidental running of the script especially when testing.

  2. Log files are created in the directory specified by the 'LogSqlDir' attribute of the lcg.conf file. The format of the log files is YYYY-MM.log.

  3. A file containing the latest SQL DML statements used to update the APEL database is created in the same directory as the log files. The format of the sql file is YYYY-MM.sql

  4. The script will, after completing the APEL database update, create  xml and html files of the data contained in the APEL database table. This was added on 4/17/08 to provide visibility into the data being sent.  No functionality is contained in this script to use these files.  This just makes them available.






---++ !DownTimes.py
Class used to query !MyOSG for planned downtimes for a site/resource.

The LCG.py module does a check for __resource groups__ that have not reported any data to Gratia for each day in the month and will report this in a warning email so corrective action can be initiated.  In order to avoid a "false" reporting of this in the event this was a planned/scheduled shutdown, this module retrieves all downtime data from !MyOSG using the following criteria:
<blockquote><pre>
Information to display: Downtime Information
Show Past Downtime for: All
         The reason for requesting "All" is that it is based on End Time
         in which case the "past.." ones will not show resource groups
         currently down.
Resource Groups to display: All Resource Groups
For Resource Group: Grid type OSG
For Resource: Provides following Services - Grid Service/CE
Active Status: active
</pre></blockquote>


[[http://myosg.grid.iu.edu/rgdowntime/?datasource=downtime&summary_attrs_showgipstatus=on&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&summary_attrs_showenv=on&summary_attrs_showcontact=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=90&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active=on&active_value=1&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgdowntime/xml?datasource=downtime&summary_attrs_showgipstatus=on&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&summary_attrs_showenv=on&summary_attrs_showcontact=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=90&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active=on&active_value=1&disable_value=1][MyOSG XML Query]]

Methods used:
<blockquote><pre>
 def site_is_shutdown(self,site,date,service):
    """ For a site/date(YYYY-MM-DD)/service determine if this is a
        planned shutdown.
        Returns:  Boolean
    """
</pre></blockquote>

---++ !InactiveResources.py
Class used to query !MyOSG for sites/resources that have been marked as <I>inactive</i>.

As an alternative to updating !MyOSG for planned downtime, an admin can also mark a __resource__ as *inactive*.  So when the LCG.py is checking for a __resource group__ not reporting to Gratia, it must also check for those marked inactive using the following criteria:
<blockquote><pre>
Information to display: Resource Group Summary
For Resource: Show services
Resource Groups to display: All resource groups
For Resource Group: Grid Type - OSG
For Resource: Provides the following services - Grid Services / CE
</pre></blockquote>

[[http://myosg.grid.iu.edu/rgsummary/?datasource=summary&summary_attrs_showservice=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=all&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active_value=0&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgsummary/xml?datasource=summary&summary_attrs_showservice=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=all&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active_value=0&disable_value=1][MyOSG XML Query]]

Methods used:
<blockquote><pre>
  def resource_is_inactive(self,resource):
    """ For a resource/date(YYYY-MM-DD)/service determine if this is a
        planned shutdown.
        Returns:  Boolean
    """
</pre></blockquote>

---++ !InteropAccounting.py
Class used to query !MyOSG for __resource groups__ with __resources__ having  the !WLCGInformation !InteropAccounting flag set to True indicating that this __resource group__ should be interfaced to APEL/WLCG.

Since some access was required to query the !MyOSG data, it seemed it would be nice to have an easy to use command line view of the for trouble shooting purpose.  So this process can be executed from the command line with the following usage.
<blockquote><pre>
./InteropAccounting.py action
    Actions:
    --show
        Displays MyOsg resource WLCG InteropAccounting and AccountingName data
        for all resouce groups with at least 1 InteropAccounting set to True.
    --is-interfaced=resource_group
        Displays the WLCG InteropAccounting option and AccountingName for the
        resource group specified.
    --interfaced-resource-groups
        Using the interfacedResourceGroups() API, returns a sorted list of the
        resource groups with the InteropAccounting set to True
    --resources=resource_group
        Using the interfacedResources(resource_group) API, returns a sorted list
        of the resources for a specified resource group with the
        Interopaccounting set to True.
    --is-registered=resource_group
        Using the isMyOsgResourceGroup(resource_group) API, returns True if
        the resource group specified is registered in MyOsg
</pre></blockquote>

The !MyOSG criteria used is:
<blockquote><pre>
 For Resource: Show WLCG Informatinon
                         Show services
                         Show FQDN / Aliases
For Resource Group: Grid Type - OSG
</pre></blockquote>

[[http://myosg.grid.iu.edu/rgsummary/?datasource=summary&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&start_type=7daysago&start_date=03%2F20%2F2009&end_type=now&end_date=03%2F27%2F2009&all_resources=on&facility_10009=on&site_10026=on&gridtype=on&gridtype_1=on&service_1=on&service_5=on&service_2=on&service_3=on&service_central_value=0&service_hidden_value=0&active_value=1&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgsummary/xml?datasource=summary&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&start_type=7daysago&start_date=03%2F20%2F2009&end_type=now&end_date=03%2F27%2F2009&all_resources=on&facility_10009=on&site_10026=on&gridtype=on&gridtype_1=on&service_1=on&service_5=on&service_2=on&service_3=on&service_central_value=0&service_hidden_value=0&active_value=1&disable_value=1][MyOSG XML Query]]


Methods used:
<blockquote><pre>
  def isRegistered(self,resource_grp):
    """ Returns True if the resource group is defined in MyOsg. """

  def interfacedResources(self,resource_group):
    """
       Returns a python list of MyOsg resources for the resource group specified
        with the InteropAccounting flag set to True.
    """

  def interfacedResourceGroups(self):
    """
       Returns a python list of MyOsg resource groups with the InteropAccounting
       flag set to True.
    """

  def WLCGAcountingName(self,resource_grp):
    """ Returns the WLCGInformation Accounting Name for a resource group.
        If not interfaced to WLCG, then returns the None value.
        Since the WLCGInformation is at the resource level and there may be
        multiple resources for a resource group, the 1st resource that is
        interfaced will be used and hopefully it is correct.
    """
</pre></blockquote>


---++ !Rebus.py
This class retrieves the latest WLCG Rebus topology csv file and provides various methods for viewing/using the data by executing:
   * wget http://wlcg-rebus.cern.ch/apps/topology/all/csv
It is important to understand that in the WLCG Rebus topology __site__ is equivalent to !MyOsg __resource_group__.

The rationale for using this class in the interface is to insure that the OSG and Rebus are in sync in terms of 
which  OSG __resource_groups__ should be reported.  If a WLCG site (OSG __resource_group__ ) registers with WLCG and 
OSG does not indicate it should be interface (and visa versa), then we to take action.

Additionally, this module can be executed from the command line to more easily view the topology.
<blockquote><pre>
Usage: %(program)s action [-help]

  Provides visibility into the WLCG Rebus topology for use in the
  Gratia/APEL/WLCG interface.     

  Actions:
    --show all | accountingnames | sites
        Displays the Rebus topology for the criteria specificed 
    --is-registered SITE
        Shows information for a site registered in WLCG REBUS topology
    --is-available
        Shows status of query against Rebus url.
</pre></blockquote>

Methods used:
<blockquote><pre>
  def isRegistered(self,site):
    """ Returns Trues if a resource group/site is registered in the WCLG."""

  def accountingName(self,site):
    """ Returns the WLCG REBUS Federation Accounting Name for a 
        registered resource group/site.
        If not registered, it will return an empty string.
    """
</pre></blockquote>

---++ !NormalizationFactors.py
This module is currently __NOT__ used in the APEL/WLCG interface.  It could be at some point in the future, however.  At this point in time (Aug 2011), there does not exist a reliable means of determining this value agreeable to all.  It was written to aid in the viewing of the currently
used values maintained in the __lcg-reportableSites__ file with options to compare that against a simple algorithm normalizing using the weighted average of the individual cluster values for a __resource_group__ based on GIP data.  The command line options for viewing this data are:
<blockquote><pre>
Usage:  %(program)s  action [--help] [--debug] [--site=<resource group>]

  If --site is specified, the actions will only display data for that
  resource group (aka site).

  Actions:
   --show-current
        Displays the currently reportable resource groups and NFs
   --show-gip
        Displays the NFs for resource groups caluculated from GIP
        subcluster data.
   --compare
        Displays all resource groups NFs compared against the currently
        reportable  resource group's NF
   --show-subcluster-data
        Displays the NFs for resource groups caluculated and the details of the
        GIP subcluster data used to calculate the NF.
   --show-benchmark
        Displays the SI2K value for all processor models
</pre></blockquote>


---++ lcg.sh
This shell script is used to execute the LCG.py process.  The primary purpose of this shell script is to get access to a !MySql client. The !MySql being used comes from Fermi UPS.  It assumes UPS is available via /fnal/ups/etc/setups.sh. There is no command line argument to override this.  All other command line arguments are passed directly to the LCG.py script which collects the data from the gratia database and updates the APEL database. <br /> <br />This script will log error messages to stdout. So it is important you do not redirect stdout/err in the cron entry.  This will insure that someone gets an email notification if a failure occurs. <br /> <br />It is also important that you 'cd' to the lcg.sh parent directory if you specify relative paths for the various files in the lcg.conf file.

---++ find-late-updates.sh
Shell script used to show sites/resource that have updated Gratia for the previous month during the current month.  This output of this is visible in the table column <i>late updates</i> here: http://gratia-osg-prod.opensciencegrid.org/gratia-data/interfaces/apel-lcg/

---++ create-apel-index.sh
Shell script that creates the index.html for: http://gratia-osg-prod.opensciencegrid.org/gratia-data/interfaces/apel-lcg/ <br />This is based on certain files in the log directory specified by the <i>LogSqlDir</i> attribute in the <i>lcg.conf</i> file.

---+ Configuration files
All configuration files can be found in the %CONFIG_LOCATION% directory.

<a name="lcg_conf"/>
---++ Configuration (lcg.conf)
The _lcg.conf_ is the main configuration file used by the interface.


%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
%EDITTABLE{   format="| text, 25 | textarea, 4x50 |"  changerows="on" quietsave="on" editbutton="Edit table" }% 
| *File* | *Description* |
| !GratiaCollector | Data directory for Gratia collector: <br />   * /data/tomcat-gratia/webapps/gratia-data/interfaces/apel-lcg <br />The dat/xml/html files are made accessible via this collector. If you do not want the files copied to a collector, then use the keyword 'DO_NOT_SEND'. <br /> |
| [[#SiteFilterFile_lcg_reportableSit][SiteFilterFile]] | File with list of __resource_groups__ to be reported and their normalization factor. |
| [[#SiteFilterHistory_lcg_reportable][SiteFilterHistory]] | History directory for keeping previous periods (months)!SiteFilterFile's (<NAME>.YYYYMM) <br /> |
| [[#VOFilterFile_lcg_reportableVOs][VOFilterFile]] | File with list of VOs to be reported |
| [[#DBConfFile_lcg_db_conf][DBConfFile]] | Configuration file for databases and SSM configurations |
| !LogSqlDir | Directory for log files, dat/xml/html and SSM update and delete files. |
| !MissingDataDays | Number of days where a __resource_group__ has no data reported to Gratia for the month.  If more than this number of days, a warning/advisory email will be generated. |
| !FromEmail | Email address of the sender.  Since this is a cron run process, this is dependent on how email is set up locally. |
| !ToEmail | List of comma separated email addresses.  Email notifications are sent to this list for all executions of this interface for both success and failure. |

<a name="lcg_reportablesites"/>
---++ !SiteFilterFile (lcg-reportableSites)
This file identifies the set of sites/resources reportable to the APEL-LCG database and the normalization factor to be used in the gratia query.
   * token 1 - The __resource_group__  being reported to APEL.
   * token 2 - The normalization value to be used. 
These tokens are whitespace separated.   Comments inidcated with a line starting with a # sign. Empty lines are permitted.

Example:
<blockquote><pre>
##--- CMS Tier 1 -----
USCMS-FNAL-WC1     10264
##--- CMS Tier 2 -----
CIT_CMS_T2         12944
GLOW                9632
  :
####################################
#--- ATLAS Tier 2 -----
BNL-ATLAS          12372
#--- ATLAS Tier 2 -----
AGLT2               8500
HU_ATLAS_Tier2      8872
   :
#--- ALICE Tier 2 ----
NERSC-PDSF         13920
LC-glcc            15680

</pre></blockquote>
<b>Note: In the future, the need for this configuration file should be eliminated.  It would be replaced by a query of !MyOSG</b>


---+++ !SiteFilterHistory (lcg-reportableSites.history files)
This directory (lcg-reportableSites.history) contains the lcg-reportableSites files for each month (format: lcg-reportableSites/lcg-reportableSites.YYYYMM.  

Since the normalization factor changes over time, the only means of recreating past months data is to make this data time sensitive.  The interface program is designed to update the file for the current month every time it is run.  This provides the history for the latest normalization factor used.

When re-running a "past" (not current), the interface uses the time-stamped file for the respective month.  When updates are made, the file should be commited into CVS.

<a name="lcg_reportablevos"/>
---++ !VOFilterFile (lcg-reportableVOs)
This file identifies the VO data reported for each reportable site/resource.  
Example:
<blockquote><pre>
cms
uscms
atlas
usatlas
alice
</pre></blockquote>

<a name="lcg_db_conf"/>
---++ !DBConfFile (lcg-db.conf)
Identifies access information for the Gratia and APEL databases.

%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
%EDITTABLE{   format="| text, 25 | textarea, 2x50 |"  changerows="on" quietsave="on" editbutton="Edit table" }% 
| *File* | *Description* |
| !GratiaHost | Gratia database host |
| !GratiaPort | Gratia database port |
| !GratiaDB | Gratia database (schema) |
| | |
| SSMHome | The HOME directory for the SSM interface software. |
| SSMConfig | The SSM configuration file |
| SSMupdates | The base name of the SSM message file containing updates.  Format: YYYY-MM. [SSMupdate] |
| SSMdeletes | The base name of the SSM message file containing the same records as the SSMupdates file but with all values zeroed out..  Format: YYYY-MM. [SSMdeletes] |



<a name="crontab"/>
---+ Crontab
The crontab file can be found in %CRONTAB_LOCATION%.

As mentioned somewhere way back in this document, the general practice has been run this interface from the 1st of the current month thru the 8th of the next month (e.g. for November, it will run nightly from 11/1 thru 12/8) to accommodate sites/resources that may have had reporting problems and are in "catch-up" mode.  So 2 cron entries are required:
<blockquote><pre>
##########################################################################
# Gratia-to-APEL (WLCG) transfer crontab entry
#
# Note: When scheduling the time of these scripts, you should first
#       check with the root cron's database backup to insure there
#       is not a conflict.
#       This script also must be run on the same host as the
#       tomcat-gratia collector as it copies files into its
#       ./webapps/gratia-data/interfaces/apel-lcg directory.
#-------------------------------------------------------------------------
#
# Previous month's transfers
# For just the 1st n days of the month to insure all have reported
#
15 0 1-8 * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=previous --update;
                     ./create-apel-index.sh /data/tomcat-gratia
#
#-------------------------------------------------------------------------
# Current month's transfers.
# Always daily.
#
15 1 * * *   dir=/home/gratia/interfaces/apel-lcg; cd $dir; ./lcg.sh --config=lcg.conf --date=current --update;
                ./find-late-updates.sh /data/tomcat-gratia; ./create-apel-index.sh /data/tomcat-gratia
#
##########################################################################
</pre></blockquote>

---+ Output Data Files


---+ SSM
Effective June 2012, accounting data sent to the EGI/WLCG APEL accounting system using the Secure Stomp Messanger (SSM).

The Secure Stomp Messenger (SSM) is designed to give a reliable message transfer mechanism using the STOMP protocol.  Messages are encrypted 
during transit, and are sent sequentially, the next message being sent only when the previous one has been acknowledged.
The SSM is written in python.  It is designed and packaged for SL5. 

Additional information about APEL can be found here:
   * [[https://wiki.egi.eu/wiki/APEL][APEL - General Description]]
   * [[https://wiki.egi.eu/wiki/APEL/Server][APEL/Server]]
   * [[https://wiki.egi.eu/wiki/APEL/SSM][APEL/SSM interface]]


---++ SSM Installation
These sections provide a little more detail obtained from the [[https://wiki.egi.eu/wiki/APEL/SSMInstallation][APELs SSM Installation twiki]] especially related to the configuration.

---+++ Prerequistes
The SSM protocal uses SSI authentication and therefore requires a set of CA certificates and a service certificate.  
   * Instructions for installing the CA certificates: [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCertAuth][OSG - Installing Certificate Authorities Certificates and related RPMs twiki]].
   * Instructions for obtaining a service certificate: [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/GetHostServiceCertificates][OSG - How to get host service certificates]]

This is a sample install of the CA certificates:
<blockquote><pre>
&gt; rpm -Uvh http://repo.grid.iu.edu/osg-el5-release-latest.rpm  # OSG repo
&gt; rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm   # EPEL repo
&gt; yum  -y  install yum-priorities
&gt; yum  -y  install osg-ca-certs
&gt; yum  -y  install fetch-crl

## Insure the cron will be activated
&gt; chkconfig fetch-crl-cron on
&gt; /sbin/service fetch-crl-cron start

## etch-crl must have run once for the certificates to be verified successfully
&gt; /sbin/service fetch-crl-boot start    # this will take a little while
</pre></blockquote>

In addition, there are several python libraries required:
<blockquote><pre>
&gt; yum  -y  install stomppy   
&gt; yum  -y  install python-daemon
&gt; yum  -y  install python-ldap
</pre></blockquote>

---+++ Install SSM
The installation from RPM is:
<blockquote><pre>
&gt; rpm -iv /cloud/login/weigand/osg-rpm-install/SSM/etc/ssm-0.11-1.noarch.rpm
</pre></blockquote>

The RPM carries out a number of steps to run the SSM in a specific way.
   * It installs the core files in __/opt/apel/ssm__
   * It creates the messages directory __/var/opt/apel/messages__
   * It creates the log directory __/var/log/apel__
   * It creates the pidfile directory __/var/run/apel__



---++ SSM Configuration files
There are 2 configuration files in __/opt/apel/ssm/conf__ that require some modifications:
   * ssm.cfg
   * ssm.log.cfg

These sections define the specific changes made for this interface.%BR%
More detail regarding other options can be found in the [[https://wiki.egi.eu/wiki/APEL/SSMConfiguration][APEL/SSM Configuration twiki]]

---+++ ssm.cfg
The table below indicates the changes necessary for test and production:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
| Test            | [broker]        | broker-network: TEST-NWOB%BR%username: gratia |
|                   | [certificates]  | certificate: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem%BR%key: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem |
|                   | [producer]     |  /C=UK/O=eScience/OU=CLRC/L=RAL/CN=raptest.esc.rl.ac.uk/emailAddress=sct-certificates@stfc.ac.uk |
| Production | [broker]         | broker-network: PROD%BR%username: gratia |
|                   | [certificates]  | certificate: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem%BR%key: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem |
|                   | [producer]     | consumer-dn: /C=UK/O=eScience/OU=CLRC/L=RAL/CN=rap.esc.rl.ac.uk |


---+++ ssm.log.cfg
The only change required in the interface logging configuration is to hard code the directory path to the log directory and file:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
| [handler]        | args=('/var/log/apel/ssm.log', 'a') |

---++ Running the SSM
The SSM needs to be run once without sending any messages in order to create the necessary
directory structure in __/var/opt/apel/messages__:
<blockquote><pre>
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 accept
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 ack
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 incoming
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:37 outgoing
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 reject
</pre></blockquote>

To run the SSM the first time (but don't do this until you've completed the configuration for test or production):
<blockquote><pre>
&gt; export SSM_HOME=/opt/apel/ssm
&gt; $SSM_HOME/bin/run-ssm
</pre></blockquote>

Then, to send your messages:
   * Write all the messages to the /opt/apel/ssm/messages/outgoing directory
   * export SSM_HOME=/opt/apel/ssm
   * $SSM_HOME/bin/run-ssm


---++ Log file (/var/log/apel/ssm.log)
The APEL/SSM log file for a successful run will look like this:
<blockquote><pre>
2012-07-23 13:37:02,301 - SSM - INFO - =======================================================
2012-07-23 13:37:02,301 - SSM - INFO - Starting the SSM...
2012-07-23 13:37:02,322 - SSM - INFO - Running the SSM once only.
2012-07-23 13:37:02,322 - SSM - INFO - BDII URL: ldap://lcg-bdii.cern.ch:2170
2012-07-23 13:37:02,322 - SSM - INFO - BDII broker network: TEST-NWOB
2012-07-23 13:37:03,526 - SSM - DEBUG - Found broker in BDII: test-msg01.afroditi.hellasgrid.gr:6162
2012-07-23 13:37:03,527 - SSM - DEBUG - Found broker in BDII: test-msg02.afroditi.hellasgrid.gr:6162
2012-07-23 13:37:03,527 - SSM - INFO - Connecting using SSL using key /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem and cert /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem.
2012-07-23 13:37:04,657 - stomp.py - INFO - Established connection to host test-msg01.afroditi.hellasgrid.gr, port 6162
2012-07-23 13:37:04,657 - SSM - INFO - Connecting: ('test-msg01.afroditi.hellasgrid.gr', 6162)
2012-07-23 13:37:04,658 - SSM - INFO - The SSM will not run as a consumer.
2012-07-23 13:37:04,658 - SSM - INFO - The SSM will run as a producer.
2012-07-23 13:37:04,658 - SSM - DEBUG - I will be a producer, my ack queue is: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:04,834 - SSM - INFO - Connected
2012-07-23 13:37:04,858 - SSM - INFO - The SSM started successfully.
2012-07-23 13:37:04,859 - SSM - INFO - No certificate, requesting
2012-07-23 13:37:04,859 - SSM - DEBUG - Sending noid
2012-07-23 13:37:05,296 - SSM - DEBUG - Broker received noid
2012-07-23 13:37:05,432 - SSM - DEBUG - Receiving message from: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:05,432 - SSM - INFO - Certificate received
2012-07-23 13:37:05,440 - SSM - DEBUG - /C=UK/O=eScience/OU=CLRC/L=RAL/CN=raptest.esc.rl.ac.uk/emailAddress=sct-certificates@stfc.ac.uk 
2012-07-23 13:37:05,459 - SSM - DEBUG - Got certificate
2012-07-23 13:37:05,461 - SSM - DEBUG - Hash: f0223f08fc939d83beeaa410eebbe42e
2012-07-23 13:37:05,461 - SSM - DEBUG - Raw length: 319581
2012-07-23 13:37:05,473 - SSM - DEBUG - Encoded length: 56429
2012-07-23 13:37:05,474 - SSM - DEBUG - Signing
2012-07-23 13:37:05,487 - SSM - DEBUG - Encrypting signed message of length 60170
2012-07-23 13:37:05,499 - SSM - DEBUG - Encrypted length: 82357
2012-07-23 13:37:05,499 - SSM - DEBUG - Sending 2012-07.ssm-updates.txt
2012-07-23 13:37:06,587 - SSM - DEBUG - Broker received 2012-07.ssm-updates.txt
2012-07-23 13:37:07,237 - SSM - DEBUG - Receiving message from: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:07,237 - SSM - DEBUG - Received ack for f0223f08fc939d83beeaa410eebbe42e
2012-07-23 13:37:07,260 - SSM - DEBUG - Message 2012-07.ssm-updates.txt acknowledged by consumer
2012-07-23 13:37:07,260 - SSM - INFO - All outgoing messages have been processed.
2012-07-23 13:37:07,439 - SSM - INFO - SSM connection ended.
2012-07-23 13:37:07,440 - SSM - INFO - The SSM has shut down.
2012-07-23 13:37:07,440 - SSM - INFO - =======================================================
</pre></blockquote>


%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->
---++!! Major updates
<!--Future editors should add their signatures beneath yours!-->
-- Main.JohnWeigand - 08 Feb 2010: Split this out as a separate twiki
