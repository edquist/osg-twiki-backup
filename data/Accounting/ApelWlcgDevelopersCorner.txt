%META:TOPICINFO{author="JohnWeigand" date="1374002903" format="1.1" reprev="1.15" version="1.15"}%
%META:TOPICPARENT{name="GratiaInterfacesApelLcg"}%
<!--
   * Set LCG_CONF        = [[#lcg_conf][lcg.conf]]
   * Set LCG_DB_CONF = [[#lcg_db_conf][lcg-db.conf]]
   * Set CRONTAB          = [[#lcg_conf][lcg.conf]]
   * Set REPORTABLE_SITES = [[#SiteFilterFile_lcg_reportableSit][SiteFilterFile]]
   * Set REPORTABLE_VOS = [[#VOFilterFile_lcg_reportableVOs][VOFilterFile]]
   * Set INTEROP_ACCOUNTING = [[#InteropAccounting_py][InteropAccounting class]]
   * Set SSM_CFG          = [[#ssm_cfg][ssm.cfg]]
   * Set SSM_LOG_CFG = [[#ssm_log_cfg][ssm.log.cfg]]
   * Set SCRIPT_LOCATION  = _/usr/share/gratia-apel_
   * Set CONFIG_LOCATION = _/etc/gratia-apel_
   * Set CRONTAB_LOCATION = _/etc/cron.d_
   * Set SERVICES_LOCATION = _/etc/init.d_
-->


---+!! APEL/WLCG Interface Developers Corner

%TOC%

%STARTINCLUDE%


---+ Developers Corner
This document  describes the scripts and configuration files used by this interface.

File Locations:
   * [[#scripts][scripts]]: %SCRIPT_LOCATION%
   * [[#Configuration_files][configuration files]]: %CONFIG_LOCATION%
   * [[#Crontab][crontab files]]: %CRONTAB_LOCATION%
   * [[#initd_services][initd files]]: %SERVICES_LOCATION%

<a name="script_location"/>
---+ Scripts
All the executable scripts in this section can be found in the %SCRIPT_LOCATION%  directory.

---++ LCG.py
---+++ Basic functionality
This is the main interface program performing the following basic functions to collect the accounting data and send it to APEL:

 1. This is a very critical step.  The very first thing that has to be done is to zero all the updates from the previous successful run.  The reason for this is that Gratia has a couple of "correction" type filters built in, such as, a VO Name correction and probe/site relationship,  From one day to the next, adjustments can be made in these areas affecting the data being sent up.   Therefore, what was reported yesterday, may not be the same as what would be reported today if any of these adjustments occur.  Since the only method of maintaining the APEL data is via updates, i.e., incremental ones.   So, this module creates a "delete" file for each set of daily updates made.  This must be processed first.  If this update fails, the module is terminated.   %BR%%BR% Prior migrating to the SSM protocol,  direct access to the APEL database was available and we were able to a SQL delete by resource group which was much simpler.

 2. Retrieves the accounting data from the Gratia database for selected OSG sites and VOs for a month.
   * Access to the Gratia database is defined in the %LCG_DB_CONF% file,
   * The __resource__ __groups__ (and Normalization Factors) are defined in a file identified by the %REPORTABLE_SITES% attribute of the %LCG_CONF%  file.
   * VOs are defined in a file identified by the %REPORTABLE_VOS% attribute of the %LCG_CONF%  file.
   * For each __resource__ __group__ in the %REPORTABLE_SITES% attribute and for the VOs defined in the %REPORTABLE_VOS% atttibute, a query is made in Gratia for each of the __resources__  with WLCG Information !InteropAccounting flag set to True (using the %INTEROP_ACCOUNTING%) %BR% 
%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show query using AGLT2 as an example."}%<blockquote><pre>
SELECT "AGLT2"                  as Site,
   VOName                          as "Group",
   min(UNIX_TIMESTAMP(EndTime))    as EarliestEndTime,
   max(UNIX_TIMESTAMP(EndTime))    as LatestEndTime,
   "07"                     as Month,
   "2013"                      as Year,
   IF(DistinguishedName NOT IN ("", "Unknown"),IF(INSTR(DistinguishedName,":/") &gt; 0,LEFT(DistinguishedName,INSTR(DistinguishedName,":/")-1), DistinguishedName),CommonName) as GlobalUserName,
   Round(Sum(WallDuration)/3600)                        as WallDuration,
   Round(Sum(CpuUserDuration+CpuSystemDuration)/3600)   as CpuDuration,
   Round((Sum(WallDuration)/3600) * 8.72 )            as NormalisedWallDuration,
   Round((Sum(CpuUserDuration+CpuSystemDuration)/3600) * 8.72) as NormalisedCpuDuration,
   Sum(NJobs) as NumberOfJobs
from
     Site,
     Probe,
     VOProbeSummary Main
where
      Site.SiteName in ("AGLT2","AGLT2_CE_2","AGLT2_SL6")
  and Site.siteid = Probe.siteid
  and Probe.ProbeName  = Main.ProbeName
  and Main.VOName in ( "alice","usatlas","atlas","uscms","cms" )
  and  "2013-07-01 00:00:00" &lt;= Main.EndTime and Main.EndTime &lt; "2013-08-01 00:00:00"
  and Main.ResourceType = "Batch"
group by Site,
         VOName,
         Month,
         Year,
         GlobalUserName
</pre></blockquote>
%ENDTWISTY% %BR%
   * In order to reduce the verbage in the log file, only the first query is output to the log file.  The __resources__ and normalization factor used is displayed for the other __resource__ __group__ queries.

3. For each __resource__ __group__ then, an SSM update message is created for the current monthly values. as defined by the _UpdatesDir_ and _UpdateFileName_ attributes of %LCG_CONF%.   A matching update file designed to remove the updates (as noted above in #1) is also created for processing first in the next iteration of this interface using the _DeleteFileName_ attribute of %LCG_CONF%.

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show !UpdatesDir Files"}%
<blockquote><pre>
<b><u>/var/lib/gratia-apel/apel-updates</b></u>
-rw-rw-r-- 1 root root 180732 Jul  8 00:15 2013-06.ssm-deletes.txt
-rw-rw-r-- 1 root root 189072 Jul  8 00:15 2013-06.ssm-updates.txt
-rw-rw-r-- 1 root root 155920 Jul 16 01:15 2013-07.ssm-deletes.txt
-rw-rw-r-- 1 root root 162184 Jul 16 01:15 2013-07.ssm-updates.txt

<b><u>2013-07.ssm-updates.txt</b></u>
APEL-summary-job-message: v0.2
Site: AGLT2
Group: atlas
EarliestEndTime: 1372654800
LatestEndTime: 1373605200
Month: 07
Year: 2013
GlobalUserName: /DC=com/..... CN=somebody1
CpuDuration: 0
NormalisedWallDuration: 116
NormalisedCpuDuration: 2
NumberOfJobs: 559
%%
Site: AGLT2
Group: atlas
EarliestEndTime: 1372654800
LatestEndTime: 1373950800
Month: 07
Year: 2013
GlobalUserName: /DC=com/..... CN=somebody2
WallDuration: 1
CpuDuration: 0
NormalisedWallDuration: 6
NormalisedCpuDuration: 0
NumberOfJobs: 4115
%%

<b><u>2013-07.ssm-deletes.txt</b></u>
APEL-summary-job-message: v0.2
Site: AGLT2
Group: atlas
EarliestEndTime: 1372654800
LatestEndTime: 1373605200
Month: 07
Year: 2013
GlobalUserName: /DC=com/..... CN=somebody1
WallDuration: 0
CpuDuration: 0
NormalisedWallDuration: 0
NormalisedCpuDuration: 0
NumberOfJobs: 0
%%
Site: AGLT2
Group: atlas
EarliestEndTime: 1372654800
LatestEndTime: 1373950800
Month: 07
Year: 2013
GlobalUserName: /DC=com/..... CN=somebody2
WallDuration: 0
CpuDuration: 0
NormalisedWallDuration: 0
NormalisedCpuDuration: 0
NumberOfJobs: 0
%%
</pre></blockquote>
%ENDTWISTY%


---+++ Additional functionality
In addition, these tasks are performed as a means of proactively validating Gratia, OIM and APEL/EGI/WLCG data.

1. Log files are created in the directory specified by the _LogDir_  attribute of the %LCG_CONF%. The format of the log files is YYYY-MM.log.  These show enough detail to determine any problems that might occur.

2. Since there is no critical RSV probe for individual __resources__ (aka Gratia site) not reporting to Gratia, the script runs a separate query for each __resource__ of the __resource groups__ using the same %INTEROP_ACCOUNTING% as the main query.  This query ignores VO and is looking for days in the month when no data has been reported to Gratia.  
  

  4. The script will, after completing the APEL database update, create  xml and html files of the data contained in the APEL database table. This was added on 4/17/08 to provide visibility into the data being sent.  No functionality is contained in this script to use these files.  This just makes them available.

---+++ Usage
The module has been design to do everything EXCEPT update the APEL database unless the --update option is used.  This prevents accidental running of the script and is especially useful when testing changes.

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show usage"}%
<blockquote><pre>
  LCG.py   --conf=config_file --date=month [--update] [--no-email]

     --conf - specifies the main configuration file to be used
              which would normally be the lcg.conf file

     --date - specifies the monthly time period to be updated:
              Valid values:
                current  - the current month
                previous - the previous month
                YYYY/MM  - any year and month

              The 'current' and 'previous' values are to facillitate running
              this as a cron script.  Generally, we will run a cron
              entry for the 'previous' month for n days into the current
              month in order to insure all reporting has been completed.

     The following 2 options are to facilitate testing and to avoid
     accidental running and sending of the SSM message to APEL and are
     therefore considered optional:

     --update - this option says to go ahead and update the APEL/WLCG database.
                If this option is NOT specified, then everything is executed
                EXCEPT the actual sending of the SSM message to APEL.
                The message file will be created.

     --no-email - this option says to turn off the sending of email
                notifications on failures and successful completions.
</pre></blockquote>
%ENDTWISTY%





---++ !DownTimes.py
Class used to query !MyOSG for planned downtimes for a site/resource.

The LCG.py module does a check for __resource groups__ that have not reported any data to Gratia for each day in the month and will report this in a warning email so corrective action can be initiated.  In order to avoid a "false" reporting of this in the event this was a planned/scheduled shutdown, this module retrieves all downtime data from !MyOSG using the following criteria:
<blockquote><pre>
Information to display: Downtime Information
Show Past Downtime for: All
         The reason for requesting "All" is that it is based on End Time
         in which case the "past.." ones will not show resource groups
         currently down.
Resource Groups to display: All Resource Groups
For Resource Group: Grid type OSG
For Resource: Provides following Services - Grid Service/CE
Active Status: active
</pre></blockquote>


[[http://myosg.grid.iu.edu/rgdowntime/?datasource=downtime&summary_attrs_showgipstatus=on&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&summary_attrs_showenv=on&summary_attrs_showcontact=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=90&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active=on&active_value=1&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgdowntime/xml?datasource=downtime&summary_attrs_showgipstatus=on&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&summary_attrs_showenv=on&summary_attrs_showcontact=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=90&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active=on&active_value=1&disable_value=1][MyOSG XML Query]]

Methods used:
<blockquote><pre>
 def site_is_shutdown(self,site,date,service):
    """ For a site/date(YYYY-MM-DD)/service determine if this is a
        planned shutdown.
        Returns:  Boolean
    """
</pre></blockquote>

---++ !InactiveResources.py
Class used to query !MyOSG for sites/resources that have been marked as <I>inactive</i>.

As an alternative to updating !MyOSG for planned downtime, an admin can also mark a __resource__ as *inactive*.  So when the LCG.py is checking for a __resource group__ not reporting to Gratia, it must also check for those marked inactive using the following criteria:
<blockquote><pre>
Information to display: Resource Group Summary
For Resource: Show services
Resource Groups to display: All resource groups
For Resource Group: Grid Type - OSG
For Resource: Provides the following services - Grid Services / CE
</pre></blockquote>

[[http://myosg.grid.iu.edu/rgsummary/?datasource=summary&summary_attrs_showservice=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=all&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active_value=0&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgsummary/xml?datasource=summary&summary_attrs_showservice=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=all&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&bdiitree_type=total_jobs&bdii_object=service&bdii_server=is-osg&start_type=7daysago&start_date=08%2F03%2F2011&end_type=now&end_date=08%2F03%2F2011&all_resources=on&gridtype=on&gridtype_1=on&service=on&service_1=on&service_central_value=0&service_hidden_value=0&active_value=0&disable_value=1][MyOSG XML Query]]

Methods used:
<blockquote><pre>
  def resource_is_inactive(self,resource):
    """ For a resource/date(YYYY-MM-DD)/service determine if this is a
        planned shutdown.
        Returns:  Boolean
    """
</pre></blockquote>

---++ !InteropAccounting.py
Class used to query !MyOSG for __resource groups__ with __resources__ having  the !WLCGInformation !InteropAccounting flag set to True indicating that this __resource group__ should be interfaced to APEL/WLCG.

Since some access was required to query the !MyOSG data, it seemed it would be nice to have an easy to use command line view of the for trouble shooting purpose.  So this process can be executed from the command line with the following usage.

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show usage"}%
<blockquote><pre>
./InteropAccounting.py action
    Actions:
    --show
        Displays MyOsg resource WLCG InteropAccounting and AccountingName data
        for all resouce groups with at least 1 InteropAccounting set to True.
    --is-interfaced=resource_group
        Displays the WLCG InteropAccounting option and AccountingName for the
        resource group specified.
    --interfaced-resource-groups
        Using the interfacedResourceGroups() API, returns a sorted list of the
        resource groups with the InteropAccounting set to True
    --resources=resource_group
        Using the interfacedResources(resource_group) API, returns a sorted list
        of the resources for a specified resource group with the
        Interopaccounting set to True.
    --is-registered=resource_group
        Using the isMyOsgResourceGroup(resource_group) API, returns True if
        the resource group specified is registered in MyOsg
</pre></blockquote>

The !MyOSG criteria used is:
<blockquote><pre>
 For Resource: Show WLCG Informatinon
                         Show services
                         Show FQDN / Aliases
For Resource Group: Grid Type - OSG
</pre></blockquote>

[[http://myosg.grid.iu.edu/rgsummary/?datasource=summary&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&start_type=7daysago&start_date=03%2F20%2F2009&end_type=now&end_date=03%2F27%2F2009&all_resources=on&facility_10009=on&site_10026=on&gridtype=on&gridtype_1=on&service_1=on&service_5=on&service_2=on&service_3=on&service_central_value=0&service_hidden_value=0&active_value=1&disable_value=1][MyOSG Query]]
 - 
[[http://myosg.grid.iu.edu/rgsummary/xml?datasource=summary&summary_attrs_showwlcg=on&summary_attrs_showservice=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=&account_type=cumulative_hours&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&start_type=7daysago&start_date=03%2F20%2F2009&end_type=now&end_date=03%2F27%2F2009&all_resources=on&facility_10009=on&site_10026=on&gridtype=on&gridtype_1=on&service_1=on&service_5=on&service_2=on&service_3=on&service_central_value=0&service_hidden_value=0&active_value=1&disable_value=1][MyOSG XML Query]]


Methods used:
<blockquote><pre>
  def isRegistered(self,resource_grp):
    """ Returns True if the resource group is defined in MyOsg. """

  def interfacedResources(self,resource_group):
    """
       Returns a python list of MyOsg resources for the resource group specified
        with the InteropAccounting flag set to True.
    """

  def interfacedResourceGroups(self):
    """
       Returns a python list of MyOsg resource groups with the InteropAccounting
       flag set to True.
    """

  def WLCGAcountingName(self,resource_grp):
    """ Returns the WLCGInformation Accounting Name for a resource group.
        If not interfaced to WLCG, then returns the None value.
        Since the WLCGInformation is at the resource level and there may be
        multiple resources for a resource group, the 1st resource that is
        interfaced will be used and hopefully it is correct.
    """
</pre></blockquote>
%ENDTWISTY%


---++ !Rebus.py
This class retrieves the latest WLCG Rebus topology csv file and provides various methods for viewing/using the data by executing:
   * wget http://wlcg-rebus.cern.ch/apps/topology/all/csv
It is important to understand that in the WLCG Rebus topology __site__ is equivalent to !MyOsg __resource_group__.

The rationale for using this class in the interface is to insure that the OSG and Rebus are in sync in terms of 
which  OSG __resource_groups__ should be reported.  If a WLCG site (OSG __resource_group__ ) registers with WLCG and 
OSG does not indicate it should be interface (and visa versa), then we to take action.

Additionally, this module can be executed from the command line to more easily view the topology.%BR%
%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show usage"}%
<blockquote><pre>
Usage: %(program)s action [-help]

  Provides visibility into the WLCG Rebus topology for use in the
  Gratia/APEL/WLCG interface.     

  Actions:
    --show all | accountingnames | sites
        Displays the Rebus topology for the criteria specificed 
    --is-registered SITE
        Shows information for a site registered in WLCG REBUS topology
    --is-available
        Shows status of query against Rebus url.
</pre></blockquote>

Methods used:
<blockquote><pre>
  def isRegistered(self,site):
    """ Returns Trues if a resource group/site is registered in the WCLG."""

  def accountingName(self,site):
    """ Returns the WLCG REBUS Federation Accounting Name for a 
        registered resource group/site.
        If not registered, it will return an empty string.
    """
</pre></blockquote>
%ENDTWISTY%

---++ !NormalizationFactors.py
This module is currently __NOT__ used in the APEL/WLCG interface.  It could be at some point in the future, however.  At this point in time (Aug 2011), there does not exist a reliable means of determining this value agreeable to all.  It was written to aid in the viewing of the currently
used values maintained in the __lcg-reportableSites__ file with options to compare that against a simple algorithm normalizing using the weighted average of the individual cluster values for a __resource_group__ based on GIP data. 

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show usage"}%
<blockquote><pre>
Usage:  %(program)s  action [--help] [--debug] [--site=<resource group>]

  If --site is specified, the actions will only display data for that
  resource group (aka site).

  Actions:
   --show-current
        Displays the currently reportable resource groups and NFs
   --show-gip
        Displays the NFs for resource groups caluculated from GIP
        subcluster data.
   --compare
        Displays all resource groups NFs compared against the currently
        reportable  resource group's NF
   --show-subcluster-data
        Displays the NFs for resource groups caluculated and the details of the
        GIP subcluster data used to calculate the NF.
   --show-benchmark
        Displays the SI2K value for all processor models
</pre></blockquote>
%ENDTWISTY%


---++ find-late-updates.sh
Shell script used to show sites/resource that have updated Gratia for the previous month during the current month.  This output of this is visible in the table column <i>late updates</i> here: http://gratia-osg-prod.opensciencegrid.org/gratia-data/interfaces/apel-lcg/

---++ create-apel-index.sh
Shell script that creates the index.html for: http://gratia-osg-prod.opensciencegrid.org/gratia-data/interfaces/apel-lcg/ <br />This is based on certain files in the log directory specified by the <i>LogSqlDir</i> attribute in the <i>lcg.conf</i> file.

---+ Configuration files
All configuration files can be found in the %CONFIG_LOCATION% directory.

<a name="lcg_conf"/>
---++ Configuration (lcg.conf)
The _lcg.conf_ is the main configuration file used by the interface.


%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
%EDITTABLE{   format="| text, 45 | textarea, 4x90 |"  changerows="on" quietsave="on" editbutton="Edit table" }% 
| *File* | *Description* |
| !WebLocation | Data directory for access by gratiaweb url. The xml and html files are made accessible. <br />If you do not want the files copied to a collector, then use the keyword 'DO_NOT_SEND'. <br />copied to a collector, then use the keyword 'DO_NOT_SEND'. <br /> |
| !LogDir | Directory for the log files.  Format YYYY-MM.log |
| !TmpDataDir | Directory for temporary files |
| !UpdatesDir | Directory for files sent to APEL |
| !WebappsDir | Directory for files accessible by the !WebLocation |
| !UpdateFileName | Base name of files sent to SSM with updates.  Format YYYY-MM.UpdateFileName.txt |
| !DeleteFileName | Base name of files sent to SSM with deletes.  Format YYYY-MM.DeleteFileName.txt |
| !SSMFile | SSM executable |
| !SSMConfig | SSM configuration file |
| | |
| [[#SiteFilterFile_lcg_reportableSit][SiteFilterFile]] | File with list of __resource_groups__ to be reported and their normalization factor. |
| [[#SiteFilterHistory_lcg_reportable][SiteFilterHistory]] | History directory for keeping previous period's !SiteFilterFile (lcg-reportableSites.YYYYMM) <br /> |
| [[#VOFilterFile_lcg_reportableVOs][VOFilterFile]] | File with list of VOs to be reported. |
| [[#DBConfFile_lcg_db_conf][DBConfFile]] | Configuration file for database access. |
| | |
| !MissingDataDays | Number of days where a __resource__ has no data reported to Gratia for the month.  If more than this number of days, a warning/advisory email will be generated if there is no scheduled maintenance (in OIM) for those days or the __resource__ has been marked as inactive in OIM. |
| !FromEmail | Email address of the sender.  Since this is a cron run process, this is dependent on how email is set up locally. |
| !ToEmail | List of comma separated email addresses.  Email notifications are sent to this list for all executions of this interface for both success and failure. |
| !CcMail | List of comma separated email address.  It is recommended that the goc be specified here.  Specify NONE if no cc. |

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example lcg.conf"}%
<blockquote><pre>
## WebLocation      DO_NOT_SEND
WebLocation       /var/www/html/gratia-apel
LogDir            /var/log/gratia-apel
TmpDataDir        /var/lib/gratia-apel/tmp
UpdatesDir        /var/lib/gratia-apel/apel-updates
WebappsDir        /var/lib/gratia-apel/webapps
UpdateFileName    ssm-updates
DeleteFileName    ssm-deletes

SSMFile           /usr/share/gratia-apel/ssm/ssm_master.py
SSMConfig         /etc/gratia-apel/ssm/ssm.cfg

SiteFilterFile    /etc/gratia-apel/lcg-reportableSites
SiteFilterHistory /etc/gratia-apel/lcg-reportableSites.history
VOFilterFile      /etc/gratia-apel/lcg-reportableVOs
DBConfFile        /etc/gratia-apel/lcg-db.conf

MissingDataDays   2

FromEmail  whomever@somewhere.edi
ToEmail    whomever@somewhere.edi,whomever2@somewhere.edi
CcEmail    goc@somewhere.org
</pre></blockquote>
%ENDTWISTY%

<a name="lcg_reportablesites"/>
---++ !SiteFilterFile (lcg-reportableSites)
This file identifies the set of sites/resources reportable to the APEL-LCG database and the normalization factor to be used in the gratia query.
   * token 1 - The __resource_group__  being reported to APEL.
   * token 2 - The normalization value to be used. 
These tokens are whitespace separated.   Comments inidcated with a line starting with a # sign. Empty lines are permitted.

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example lcg-reportableSites"}%
<blockquote><pre>
##--- CMS Tier 1 -----
USCMS-FNAL-WC1     10264
##--- CMS Tier 2 -----
CIT_CMS_T2         12944
GLOW                9632
  :
####################################
#--- ATLAS Tier 2 -----
BNL-ATLAS          12372
#--- ATLAS Tier 2 -----
AGLT2               8500
HU_ATLAS_Tier2      8872
   :
#--- ALICE Tier 2 ----
NERSC-PDSF         13920
LC-glcc            15680
</pre></blockquote>
%ENDTWISTY%



<b>Note: In the future, the need for this configuration file should be eliminated.  It would be replaced by a query of !MyOSG</b>


---+++ !SiteFilterHistory (lcg-reportableSites.history files)
This directory (lcg-reportableSites.history) contains the lcg-reportableSites files for each month (format: lcg-reportableSites/lcg-reportableSites.YYYYMM.  

Since the normalization factor changes over time, the only means of recreating past months data is to make this data time sensitive.  The interface program is designed to update the file for the current month every time it is run.  This provides the history for the latest normalization factor used.

When re-running a "past" (not current), the interface uses the time-stamped file for the respective month.  When updates are made, the file should be commited into CVS.

<a name="lcg_reportablevos"/>
---++ !VOFilterFile (lcg-reportableVOs)
This file identifies the VO data reported for each reportable site/resource.  
Example:
<blockquote><pre>
cms
uscms
atlas
usatlas
alice
</pre></blockquote>

<a name="lcg_db_conf"/>
---++ !DBConfFile (lcg-db.conf)
Identifies access information for the Gratia and APEL databases.

%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
%EDITTABLE{   format="| text, 25 | textarea, 2x50 |"  changerows="on" quietsave="on" editbutton="Edit table" }% 
| *File* | *Description* |
| !GratiaHost | Gratia database host |
| !GratiaPort | Gratia database port |
| !GratiaDB | Gratia database (schema) |
| !GratiaUser | Should be a read-only user. |
| !GratiaPswd | Password for a read-only user. |



<a name="crontab"/>
---+ Crontab
The crontab file can be found in %CRONTAB_LOCATION%.

As mentioned somewhere way back in this document, the general practice has been run this interface from the 1st of the current month thru the 8th of the next month (e.g. for November, it will run nightly from 11/1 thru 12/8) to accommodate sites/resources that may have had reporting problems and are in "catch-up" mode.  So 2 cron entries are required:

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example /etc/cron.d/gratia-apel.cron"}%
<blockquote><pre>
#-------------------------------------------------------------------------
# Gratia/APEL-EGI interface crontab entry
#-------------------------------------------------------------------------
# Previous month's transfers are run for just the 1st n days of the month to 
# insure all sites have reported to Gratia. 
# The n days is dependent on when LCG accounting issues the monthly MOU reports
#
# Note the lock file not existing is success hence the the slightly odd logic
# below.
#
# The lock file can be enabled or disabled via a
#   service   gratia-apel-cron start
#   chkconfig gratia-apel-cron on
#-------------------------------------------------------------------------
15 0 1-8 * *  root   [ ! -f /var/lock/subsys/gratia-apel-cron ] || (/usr/share/gratia-apel/LCG.py --config=/etc/gratia-apel/lcg.conf --date=previous --update)
#
#-------------------------------------------------------------------------
# Current month's transfers - Always daily.
#-------------------------------------------------------------------------
15 1 * * *  root [ ! -f /var/lock/subsys/gratia-apel-cron ] || ( /usr/share/gratia-apel/LCG.py --config=/etc/gratia-apel/lcg.conf --date=current --update && /usr/share/gratia-apel/create-apel-index.sh /etc/gratia-apel/lcg.conf) 
#
#-------------------------------------------------------------------------
</pre></blockquote>
%ENDTWISTY%

---+ Output Data Files


---+ SSM
Effective June 2012, accounting data sent to the EGI/WLCG APEL accounting system using the Secure Stomp Messanger (SSM).

The Secure Stomp Messenger (SSM) is designed to give a reliable message transfer mechanism using the STOMP protocol.  Messages are encrypted 
during transit, and are sent sequentially, the next message being sent only when the previous one has been acknowledged.
The SSM is written in python.  It is designed and packaged for SL5. 

Additional information about APEL can be found here:
   * [[https://wiki.egi.eu/wiki/APEL][APEL - General Description]]
   * [[https://wiki.egi.eu/wiki/APEL/Server][APEL/Server]]
   * [[https://wiki.egi.eu/wiki/APEL/SSM][APEL/SSM interface]]


---++ SSM Installation
The SSM software is distributed as a part of the gratia-apel rpm package.  This is done to insure compatibility between the 2 systems.  If not, independent updates of SSM may not be compatible with the Gratia-APEL interface.

These sections provide a little more detail obtained from the [[https://wiki.egi.eu/wiki/APEL/SSMInstallation][APELs SSM Installation twiki]] related to the configuration.

---+++ Prerequistes
The SSM protocal uses SSI authentication and therefore requires a set of CA certificates and a service certificate.  
   * Instructions for installing the CA certificates: [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCertAuth][OSG - Installing Certificate Authorities Certificates and related RPMs twiki]].
   * Instructions for obtaining a service certificate: [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/GetHostServiceCertificates][OSG - How to get host service certificates]]

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example CA certificates install"}%
This is a sample install of the CA certificates:
<blockquote><pre>
&gt; rpm -Uvh http://repo.grid.iu.edu/osg-el5-release-latest.rpm  # OSG repo
&gt; rpm -Uvh http://dl.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm   # EPEL repo
&gt; yum  -y  install yum-priorities
&gt; yum  -y  install osg-ca-certs
&gt; yum  -y  install fetch-crl

## Insure the cron will be activated
&gt; chkconfig fetch-crl-cron on
&gt; /sbin/service fetch-crl-cron start

## fetch-crl must have run once for the certificates to be verified successfully
&gt; /sbin/service fetch-crl-boot start    # this will take a little while
</pre></blockquote>
%ENDTWISTY%

In addition, there are several python libraries required:
<blockquote><pre>
&gt; yum  -y  install stomppy   
&gt; yum  -y  install python-daemon
&gt; yum  -y  install python-ldap
</pre></blockquote>

---+++ Install SSM
The installation from RPM is:
<blockquote><pre>
&gt; rpm -iv /cloud/login/weigand/osg-rpm-install/SSM/etc/ssm-0.11-1.noarch.rpm
</pre></blockquote>

The RPM carries out a number of steps to run the SSM in a specific way.
   * It installs the core files in __/opt/apel/ssm__
   * It creates the messages directory __/var/opt/apel/messages__
   * It creates the log directory __/var/log/apel__
   * It creates the pidfile directory __/var/run/apel__



---++ SSM Configuration files
There are 2 configuration files in __/opt/apel/ssm/conf__ that require some modifications:
   * ssm.cfg
   * ssm.log.cfg

These sections define the specific changes made for this interface.%BR%
More detail regarding other options can be found in the [[https://wiki.egi.eu/wiki/APEL/SSMConfiguration][APEL/SSM Configuration twiki]]

---+++ ssm.cfg
The table below indicates the changes necessary for test and production:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
| Test            | [broker]        | broker-network: TEST-NWOB%BR%username: gratia |
|                   | [certificates]  | certificate: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem%BR%key: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem |
|                   | [producer]     |  /C=UK/O=eScience/OU=CLRC/L=RAL/CN=raptest.esc.rl.ac.uk/emailAddress=sct-certificates@stfc.ac.uk |
| Production | [broker]         | broker-network: PROD%BR%username: gratia |
|                   | [certificates]  | certificate: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem%BR%key: /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem |
|                   | [producer]     | consumer-dn: /C=UK/O=eScience/OU=CLRC/L=RAL/CN=rap.esc.rl.ac.uk |


---+++ ssm.log.cfg
The only change required in the interface logging configuration is to hard code the directory path to the log directory and file:
%TABLE{ tableborder="1" cellpadding="0" cellspacing="1" headerbg="#99CCCC" databg="#FFFFCC, #FFFFFF"}% 
| [handler]        | args=('/var/log/apel/ssm.log', 'a') |

---++ Running the SSM
The SSM needs to be run once without sending any messages in order to create the necessary
directory structure in __/var/opt/apel/messages__:
<blockquote><pre>
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 accept
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 ack
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 incoming
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:37 outgoing
drwxr-xr-x 2 gratia gratia 4096 Jul 23 13:36 reject
</pre></blockquote>

To run the SSM the first time (but don't do this until you've completed the configuration for test or production):
<blockquote><pre>
&gt; export SSM_HOME=/opt/apel/ssm
&gt; $SSM_HOME/bin/run-ssm
</pre></blockquote>

Then, to send your messages:
   * Write all the messages to the /opt/apel/ssm/messages/outgoing directory
   * export SSM_HOME=/opt/apel/ssm
   * $SSM_HOME/bin/run-ssm


---++ Log file (/var/log/apel/ssm.log)
The APEL/SSM log file for a successful run will look like this:

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example ssm.log"}%
<blockquote><pre>
2012-07-23 13:37:02,301 - SSM - INFO - =======================================================
2012-07-23 13:37:02,301 - SSM - INFO - Starting the SSM...
2012-07-23 13:37:02,322 - SSM - INFO - Running the SSM once only.
2012-07-23 13:37:02,322 - SSM - INFO - BDII URL: ldap://lcg-bdii.cern.ch:2170
2012-07-23 13:37:02,322 - SSM - INFO - BDII broker network: TEST-NWOB
2012-07-23 13:37:03,526 - SSM - DEBUG - Found broker in BDII: test-msg01.afroditi.hellasgrid.gr:6162
2012-07-23 13:37:03,527 - SSM - DEBUG - Found broker in BDII: test-msg02.afroditi.hellasgrid.gr:6162
2012-07-23 13:37:03,527 - SSM - INFO - Connecting using SSL using key /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostkey.pem and cert /etc/grid-security/gratia-osg-prod-reports.opensciencegrid.org-hostcert.pem.
2012-07-23 13:37:04,657 - stomp.py - INFO - Established connection to host test-msg01.afroditi.hellasgrid.gr, port 6162
2012-07-23 13:37:04,657 - SSM - INFO - Connecting: ('test-msg01.afroditi.hellasgrid.gr', 6162)
2012-07-23 13:37:04,658 - SSM - INFO - The SSM will not run as a consumer.
2012-07-23 13:37:04,658 - SSM - INFO - The SSM will run as a producer.
2012-07-23 13:37:04,658 - SSM - DEBUG - I will be a producer, my ack queue is: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:04,834 - SSM - INFO - Connected
2012-07-23 13:37:04,858 - SSM - INFO - The SSM started successfully.
2012-07-23 13:37:04,859 - SSM - INFO - No certificate, requesting
2012-07-23 13:37:04,859 - SSM - DEBUG - Sending noid
2012-07-23 13:37:05,296 - SSM - DEBUG - Broker received noid
2012-07-23 13:37:05,432 - SSM - DEBUG - Receiving message from: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:05,432 - SSM - INFO - Certificate received
2012-07-23 13:37:05,440 - SSM - DEBUG - /C=UK/O=eScience/OU=CLRC/L=RAL/CN=raptest.esc.rl.ac.uk/emailAddress=sct-certificates@stfc.ac.uk 
2012-07-23 13:37:05,459 - SSM - DEBUG - Got certificate
2012-07-23 13:37:05,461 - SSM - DEBUG - Hash: f0223f08fc939d83beeaa410eebbe42e
2012-07-23 13:37:05,461 - SSM - DEBUG - Raw length: 319581
2012-07-23 13:37:05,473 - SSM - DEBUG - Encoded length: 56429
2012-07-23 13:37:05,474 - SSM - DEBUG - Signing
2012-07-23 13:37:05,487 - SSM - DEBUG - Encrypting signed message of length 60170
2012-07-23 13:37:05,499 - SSM - DEBUG - Encrypted length: 82357
2012-07-23 13:37:05,499 - SSM - DEBUG - Sending 2012-07.ssm-updates.txt
2012-07-23 13:37:06,587 - SSM - DEBUG - Broker received 2012-07.ssm-updates.txt
2012-07-23 13:37:07,237 - SSM - DEBUG - Receiving message from: /topic/global.accounting.cpu.client.fermicloud140.fnal.gov.7541
2012-07-23 13:37:07,237 - SSM - DEBUG - Received ack for f0223f08fc939d83beeaa410eebbe42e
2012-07-23 13:37:07,260 - SSM - DEBUG - Message 2012-07.ssm-updates.txt acknowledged by consumer
2012-07-23 13:37:07,260 - SSM - INFO - All outgoing messages have been processed.
2012-07-23 13:37:07,439 - SSM - INFO - SSM connection ended.
2012-07-23 13:37:07,440 - SSM - INFO - The SSM has shut down.
2012-07-23 13:37:07,440 - SSM - INFO - =======================================================
</pre></blockquote>
%ENDTWISTY%


%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->
---++!! Major updates
<!--Future editors should add their signatures beneath yours!-->
-- Main.JohnWeigand - 08 Feb 2010: Split this out as a separate twiki
