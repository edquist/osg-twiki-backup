%META:TOPICINFO{author="DanYocum" date="1285863151" format="1.1" version="1.21"}%
%META:TOPICPARENT{name="WebHome"}%
<!--
These are general in nature and should be changed in the this template if the location changes:

These are specific to a release and require changing:
   * Set RELEASE_DATE = 2010/07/14
   * Set CURRENT_RELEASE = v1.06.17
   * Set RELEASE_TAG = v1-06-17
   * Set NEXT_DEVEL_VERSION = v1.07

-->

---+!! Gratia Release %CURRENT_RELEASE% (%RELEASE_DATE%)%BR% 


%TOC%

%STARTINCLUDE%
---+ Overview

---++Main Features:
   * Maintenance release.
   * See previous patch release [[GratiaReleaseV1dot06dot14][v1.06.14]] for changes since v1.04.

   * Implement cutoff date.
   * Speed up housekeeping deletion (and remove use of temporary table).
   * Additional housekeeping logging.
   * Fix occasional problem with inter-service communication caused by premature garbage collection.

---++ Probe Improvements:

   * Add summarization capability to the dCache probe.
   * Separate the !JobUsageRecord support from the main library which is now !GratiaCore.py

---+++ Details

   * Tweak !ProbeConfig extra items for dCache-transfer probe.
   * Reorganize !GridFtpLogDir config item insertion.
   * Sleep and configurable DB name for dCache-transfer probe.
   * Core updates for HTTP timeout waiting for acknowledgement from collector.
   * Improve connection timeout mesage to avoid confusion.
   * Improvements to xrootd probe.
   * Fix for =staticmethod= (make it work in 2.3).
   * Fix mechanism intended to avoid corruption caused by reading in-progress log.
   * Robustness fix to !GetNodeData function in Gratia.py.
   * Fix for exception in debug statements in Gratia.py.
   * Increase default !BundleSize to 100 in !ProbeConfigTemplate and Gratia.py.
   * Remove unecessary dependencies from dCache-storage probe.
   * !EGEE-provided probe is now version-controlled instead of unpackaged and patched from source.
   * Unused !GridftpTransfer.sh script removed prior to final packaging.
   * Condor probe security fixes from Wisconsin.
   * Add gratia-probe-services as a dependency to the xrootd-storage probe
   * Fix to cron scripts for !WorkingFolder discovery in dCache-storage and gridftp-transfer cron scripts.
   * Remove erroneous ITB tag for services API package.
   * Better use of lock files in *_cron.sh.
   * Fixes and updates to dCache-storage probe, including addition of !README file to package.
   * Added redesigned xrootd-transfer probe
   * Minor updates for the xrootd-storage probe found in testing.
   * Fix to condor probe for standalone (non-VDT) operation.


---++ Collector Improvements:

   * Add a publicly available summary of the replication 

   * In addition to the expiration date, when there is a limited retention of the records,
we also reject the records that are in the future.  For now, the rejection is set to
the same duration as the expiration.  For example if the service.lifetime.JobUsageRecord is
set to 3 months, we reject all !JobUsageRecord that are either set more than 3 months in
the past or more than 3 months in the future.

   * Increase the default batch size for row deletion from 200 to 1000.
   * Change the default fault batch size for _Xml row deletion to 100000.

   * Introduce the 2 customizable parameters:

      * service.lifetimeManagement.BatchSize and service.lifetimeManagement.XmlBatchSize

   * Add an index to quickly find empty !ExtraXml

   * Rework the row deletion from !RecordType_Xml.

      a. We no longer keep the row when the !EndTime is greater than the !ServerDate (and !EndTime was greater than the cutoff date).
      a. We no longer use a temporary table.
      a. Improve the 'limited' delete query by using a dbid range to restrict the searches.
         1. We find the maximum plausible dbid by searching in !RecordType_Meta for the dbid clause to or equal to the limit date
         1. We find the minimum plausible dbid by searching in !RecordType_Xml for the first dbid with !ExtraXml==""
         1.  the delete query where clause is:
   !ExtraXml = "" and !ServerDate < :dateLimit and :mindbid <= X.dbid and X.dbid < :maxdbid

   * On gr8x0 the current (typical rate) are (for a Transfer record database):
      * !JobUsageRecord_Xml 10000 rows per second (so for a typical transaction time of 10s).
      * !JobUsageRecord_Meta  120 rows per second (so for a typical transaction time of  9s).
      * The other deletion are more than 1000 rows per seconds such that the total time is completely dominated by _Meta.

   * Add index !ComputeElement(!UniqueID, !Timestamp)

   * Fix lifetime of RMI proxy object used for inter-service communication.

---+++ Pending explicit request from Stakeholders.
   * Improve response time of Status page.
   * Automatic table optimization (schedulable, abortable, triggerable) [Insert will be disabled during optimization]
   * Automatic sync of !VONameCorrection table and info OIM.

---+++ Notices

   * Web Services *must* be improved to pass the cert info along to the Gratia Accounting Probes.

---+ Anticipated downtime
It is expected that this release will require the Gratia services and reporting to be unavailable beginning at:
   * Start: %RELEASE_DATE%  09:00 CDT (14:00 UTC)
   * Available: %RELEASE_DATE% TBA

The changes affecting downtime for this release are:
   1. Installation and validation on each collector / reporter.
   1. Upgrade of each schema to add an index to !JobUsageRecord_Xml.

During this upgrade, the legacy redirectors dealing with probes sending to old addresses / ports will be decommissioned.

---+ Upgrade procedure

[ Note: all times are CDT ]

<ol>
<li>Pre-load kernels on all VMs and dom0s.
<dl compact>
<dt>%RED%08:40%ENDCOLOR%</dt>
<dd><tt>yum upgrade</tt> in progress.</dd>
<dt>%ORANGE%08:50%ENDCOLOR%</dt>
<dd>Check grub configurations.</dd>
<dt>%GREEN%08:56%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Deactivate, disable, shut down and remove configs from /etc/xen/auto gr12x2 and gr13x2, the HA legacy redirector VMs.</li>
<dl compact>
<dt>%RED%09:00%ENDCOLOR%</dt>
<dd>Remove <tt>gratia12:/etc/xen/auto/gr12x2</tt> and <tt>gratia13:/etc/xen/auto/gr13x2</tt>.</dd>
<dt>%RED%09:12%ENDCOLOR%</dt>
<dd>Shutdown and remove LVS on gr12x2 and gr13x2.</dd>
<dt>%GREEN%09:15%ENDCOLOR%</dt>
<dd>Shutdown gr12x2 and gr13x2.</dd>
</dl>
<li>Fail the reporting-DB IP over to the collector DB.</li>
<dl compact>
<dt>%RED%09:17%ENDCOLOR%</dt>
<dd>Execute <tt>/usr/share/heartbeat/hb_takeover</tt> on gr10x5 and gr12x5.</dd>
<dt>%ORANGE%09:18%ENDCOLOR%</dt>
<dd>Verify IPs have switched and connections have gone on gr11x5 and gr13x5.</dd>
<dt>%GREEN%09:21%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Stop slave on reporting DB. Check no open temporary tables. Stop reporting DB.</li>
<dl compact>
<dt>%RED%09:22%ENDCOLOR%</dt>
<dd>Execute <tt>slave stop;</tt> on gr11x5 and gr13x5.</dd>
<dt>%ORANGE%09:22%ENDCOLOR%</dt>
<dd>Verify no temporary tables open with <tt>SHOW STATUS LIKE '%tab%';</tt>.</dd>
<dt>%GREEN%09:24%ENDCOLOR%</dt>
<dd>Shut down DBs on gr11x5 and gr13x5.</dd>
</dl>
<li>Institute IPtables rules on collector IPs to ignore SYNs on port 80/443.</li>
<dl compact>
<dt>%RED%09:30%ENDCOLOR%</dt>
<dd>Execute <tt>iptables -I INPUT 1 -p tcp --syn --destination-port 80 -j DROP; iptables -I INPUT 2 -p tcp --syn --destination-port 443 -j DROP</tt> on all collectors.</dd>
<dt>%GREEN%09:32%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Wait 15 minutes or until there are no remaining connections to 80/443.</li>
<dl compact>
<dt>%GREEN%09:35%ENDCOLOR%</dt>
<dd>Verified no established connections on collectors.</dd>
</dl>
<li>Fail the reporting services over to the collector VMs (collector services will restart, reporting services will stop).</li>
<dl compact>
<dt>%RED%09:36%ENDCOLOR%</dt>
<dd>Execute <tt>/usr/share/heartbeat/hb_takeover</tt> on all collectors.</dd>
<dt>%RED%09:40%ENDCOLOR%</dt>
<dd>Restart failed heartbeat on gr12x1, gr12x3. Shoot recalcitrant gr13x1 collector inna head.</dd>
<dt>%ORANGE%09:55%ENDCOLOR%</dt>
<dd>Verify correct transfer of operations.</dd>
<dt>%GREEN%09:57%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Temporarily remove SYN-blocking.</li>
<dl compact>
<dt>%GREEN%10:02%ENDCOLOR%</dt>
<dd>Execute <tt>iptables -D INPUT 2; iptables -D INPUT 1</tt> on collectors.</dd>
</dl>
<li>Verify reporting URLs work as expected.</li>
<dl compact>
<dt>%GREEN%10:05%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Reboot reporting dom0, ensuring heartbeat does not cause trouble as it comes back.</li>
<dl compact>
<dt>%RED%10:05%ENDCOLOR%</dt>
<dd>Execute reboot.</dd>
<dt>%ORANGE%10:25%ENDCOLOR%</dt>
<dd>Verify all reporting dom0, VMs have restarted. gr11x5 (Fermi reporting DB) doing disk check.<br/>
    Fermi collector heartbeats failed back to reporting VMs. Possibility of stuck probes, maybe. Will need to check after upgrade is complete.</dd>
</dl>
<dt>%GREEN%11:05%ENDCOLOR%</dt>
<dd>gr11x5 returned.</dd>
<li>Re-institute SYN-blocking on collector IPs 80/443.</li>
<dl compact>
<dt>%GREEN%10:40%ENDCOLOR%</dt>
<dd>Executed <tt>iptables -I INPUT 1 -p tcp --syn --destination-port 80 -j DROP; iptables -I INPUT 2 -p tcp --syn --destination-port 443 -j DROP</tt> on all collectors.</dd>
</dl>
<li>Fail reporting-DB IP back to reporting DB.</li>
<dl compact>
<dt>%RED%10:45%ENDCOLOR%</dt>
<dd>Execute takeover on gr13x5 ONLY (OSG); gr11x5 is still checking disk.</dd>
<dt>%ORANGE%10:46%ENDCOLOR%</dt>
<dd>Verified gr13x5 took over operations properly.</dd>
<dt>%RED%11:10%ENDCOLOR%</dt>
<dd>Execute takeover on gr11x5.</dd>
<dt>%GREEN%11:14%ENDCOLOR%</dt>
<dd>Verified gr11x5 took over operations properly.</dd>
</dl>
<li>Stop slave on reporting DB. This will remain off until collector service is restored.</li>
<dl compact>
<dt>%GREEN%10:50%ENDCOLOR%</dt>
<dd>Slave stopped and verified no temporary tables open on gr13x5.</dd>
<dt>%GREEN%11:13%ENDCOLOR%</dt>
<dd>Slave stopped and verified no temporary tables open on gr11x5.</dd>
</dl>
<li>Upgrade reporting services *without* restarting.</li>
<dl compact>
<dt>%RED%10:50%ENDCOLOR%</dt>
<dd>Execute <tt>gratia-ops/upgrade-osg-reporting v1.06.17</tt> (execution machine irrelevant).</dd>
<dt>%GREEN%10:55%ENDCOLOR%</dt>
<dd>Upgrades complete on gr13xN (OSG).</dd>
<dt>%RED%11:15%ENDCOLOR%</dt>
<dd>Fail reporting on gr11xN back to gr10xN (special step because heartbeat screwed up earlier).</dd>
<dt>%ORANGE%11:20%ENDCOLOR%</dt>
<dd>Complete.</dd>
<dt>%RED%11:25%ENDCOLOR%</dt>
<dd>Execute <tt>gratia-ops/upgrade-fermi-reporting v1.06.17</tt> (execution machine irrelevant).</dd>
<dt>%GREEN%11:29%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Wait 15 minutes or until there are no remaining connections to 80/443.</li>
<dl compact>
<dt>%GREEN%11:00%ENDCOLOR%</dt>
<dd>Verified no established connections on collectors.</dd>
</dl>
<li>Fail reporting services back to the reporting VMs, ensuring new reporting services start correctly.</li>
<dl compact>
<dt>%RED%11:05%ENDCOLOR%</dt>
<dd>Execute takeover on gr13xN (OSG).</dd>
<dt>%GREEN%11:15%ENDCOLOR%</dt>
<dd>Takeover on gr13xN complete.</li>
<dt>%RED%11:30%ENDCOLOR%</dt>
<dd>Execute takeover on gr11xN (Fermi).</dd>
<dt>%GREEN%11:40%ENDCOLOR%</dt>
<dd>Takeover on gr11xN complete.</li>
</dl>
<li>Shut down collectors on collector VMs and upgrade *without* restarting.</li>
<dl compact>
<dt>%RED%11:46%ENDCOLOR%</dt>
<dd>Execute <tt>gratia-ops/upgrade-fermi-collectors v1.06.17</tt> (execution machine irrelevant).</dd>
<dt>%RED%11:54%ENDCOLOR%</dt>
<dd>Execute <tt>gratia-ops/upgrade-osg-collectors v1.06.17</tt> (execution machine irrelevant).</dd>
<dt>%GREEN%11:58%ENDCOLOR%</dt>
<dd>Upgrade complete on OSG collectors.</dd>
<dt>%GREEN%12:04%ENDCOLOR%</dt>
<dd>Upgrade complete on Fermi collectors.</dd>
</dl>
<li>Temporarily remove collectors from rcN.d until after reboot.</li>
<dl compact>
<dt>%GREEN%11:58%ENDCOLOR%</dt>
<dd>Executed =chkconfig tomcat-gratia off= on gr12xN.</dd>
<dt>%GREEN%12:04%ENDCOLOR%</dt>
<dd>Executed =chkconfig tomcat-gratia off= on gr10xN.</dd>
</dl>
<li>Manually stop mysqld on collector DBs (may take a while).</li>
<dl>
<dt>%GREEN%12:01%ENDCOLOR%</dt>
<dd>Complete on gr12x5.</dd>
<dt>%GREEN%12:06%ENDCOLOR%</dt>
<dd>Complete on gr10x5.</dd>
<li>Reboot collector dom0, ensuring heartbeat does not cause problems.</li>
<dl compact>
<dt>%RED%12:02%ENDCOLOR%</dt>
<dd>Reboot initiated on gratia12 (OSG).</dd>
<dt>%RED%12:06%ENDCOLOR%</dt>
<dd>Reboot initiated on gratia10 (OSG).</dd>
<dt>%GREEN%12:12%ENDCOLOR%</dt>
<dd>Complete for gratia12 and VMs.</dd>
<dt>%GREEN%12:14%ENDCOLOR%</dt>
<dd>Complete for gratia10 and VMs.</dd>
</dl>
<li>Ensure collectors are configured to come back in collector-only mode.</li>
<dl compact>
<dt>%GREEN%12:20%ENDCOLOR%</dt>
<dd>Heartbeat and collector status verified as correct for all collectors and reporters. Heartbeat had to be chkconfigged on and restarted on gr12x1 and gr12x3.</dd>
</dl>
<li>Re-enable rcN.d scripts on collector VMs.</li>
<dl compact>
<dt>%GREEN%12:25%ENDCOLOR%</dt>
<dd>Complete.</dd>
</dl>
<li>Start collectors.</li>
<dl compact>
<dt>%RED%12:23%ENDCOLOR%</dt>
<dd>Start <tt>osg-daily</tt> collector first as a mine canary.</dd>
<dt>%GREEN%12:23%ENDCOLOR%</dt>
<dd>Collector <tt>osg-daily</tt> ready for data.</dd>
<dt>%RED%12:26%ENDCOLOR%</dt>
<dd>Start all other collectors on gr10xN and gr12xN.</dd>
<dt>%GREEN%12:28%ENDCOLOR%</dt>
<dd>Collector <tt>fermi-itb</tt> ready for data.</dd>
<dt>%GREEN%12:28%ENDCOLOR%</dt>
<dd>Collector <tt>fermi-qcd</tt> ready for data.</dd>
<dt>%GREEN%12:38%ENDCOLOR%</dt>
<dd>Collector <tt>fermi-psacct</tt> ready for data.</dd>
<dt>%GREEN%12:44%ENDCOLOR%</dt>
<dd>Collector <tt>osg-itb</tt> ready for data.</dd>
<dt>%GREEN%13:36%ENDCOLOR%</dt>
<dd>Collector <tt>fermi-osg</tt> ready for data.</dd>
<dt>%GREEN%15:12%ENDCOLOR%</dt>
<dd>Collector <tt>fermi-transfer</tt> ready for data.</dd>
<dt>%GREEN%18:24%ENDCOLOR%</dt>
<dd>Collector <tt>osg-prod</tt> ready for data.</dd>
<dt>%GREEN%21:51%ENDCOLOR%</dt>
<dd>Collector <tt>osg-transfer</tt> ready for data. Collector had to be restarted after index completion due to a probable timed-out connection.</dd>
</dl>
<li>When most collectors are responding to incoming data, fail reporting-DB IP over to collector DB and start DB replication.</li>
<dl compact>
<dt>%GREEN%15:30%ENDCOLOR%</dt>
<dd>All Fermi schemata finished: Fermi reporting DB IP failed over and slave started.</dd>
<dt>%GREEN%19:08%ENDCOLOR%</dt>
<dd>All OSG schemata finished except <tt>osg-transfer</tt>: OSG reporting DB IP failed over and slave started.</dd>
</dl>
<li>When seconds_behind_master -> 0 fail reporting-DB IP back to reporting DB.</li>
<dl compact>
<dt>%GREEN%19:27%ENDCOLOR%</dt>
<dd>gr11x5 DB has caught up; reporting IP failed back and backups rescheduled.</dd>
<dt>%GREEN%10:13%ENDCOLOR%</dt>
<dd>gr13x5 DB has caught up; reporting IP failed back and backups rescheduled.</dd>
</dl>
</ol>


%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->
---++!! Major updates
<!--Future editors should add their signatures beneath yours!-->

%META:TOPICMOVED{by="ChrisGreen" date="1279056653" from="Accounting.GratiaReleaseV1dot06dot16" to="Accounting.GratiaReleaseV1dot06"}%
