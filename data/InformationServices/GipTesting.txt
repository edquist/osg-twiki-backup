%META:TOPICINFO{author="BrianBockelman" date="1205205302" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="WebHome"}%
---+ Testing the GIP for Integration.

The GIP is an important information services component.  Without correct information out on the OSG, VOs will make poor decisions on where to send jobs or perhaps try and utilize storage space which is not there.

Therefore, it is not enough to test to make sure that <i>any</i> data is output from the GIP - the data must also be correct!

---++ Install the latest release candidate.

Get the latest release candidate from either the VDT or the developers.  Installing should be a matter of untarring the tarball in the =$VDT_LOCATION= directory.  Alternately, if you only want to try a specific piece, you can untar it in any directory and point the =$GIP_LOCATION= variable to the newly created =gip/= directory.

[[GipInstall][Updated documentation here.]]

---++ Running the GIP

The GIP can be run with the following command line invocation:

<verbatim>
$VDT_LOCATION/glite/etc/glite-ce-ce-plugin/glite-ce-info $VDT_LOCATION/lcg/etc/osg-info-generic.conf
</verbatim>

This is exactly the command that CEMon uses to execute the GIP; CEMon takes the stdout from the above command and feeds it to central GOC servers.

The command should output a large amount of LDIF and return with exit code 0.  It should complete in under 60 seconds.

---++ Exporting data to CEMon

The tester should now export the data to the ITB CEMon and ITB BDII instance.

The CEMon developers will provide information on how to do this.

---++ Make sure you have the test programs

If you got the GIP install from the VDT, it does not include the testing programs.  You will need to check them out from SVN:

<verbatim>
svn co svn://t2.unl.edu/brian/gip/trunk gip_trunk
</verbatim>

The GIP test programs are now located in =gip_trunk/test/=.

Before *any* test programs can be run, the GIP_LOCATION variable must be set.  Assuming you just ran the above SVN command, do the following:

<verbatim>
export GIP_LOCATION=$PWD/gip_trunk/gip
</verbatim>

---++ Run the schema check

The schema check was developed on the following OpenLDAP version:

<verbatim>
bbockelm:~ brian$ ldapsearch -V
ldapsearch: @(#) $OpenLDAP: ldapsearch 2.3.27 (Oct  4 2007 23:21:32) $
	(LDAP library: OpenLDAP 20327)
</verbatim>

This is the default OpenLDAP library which ships with Mac OS X.  Modern Fedora distributions should also carry the same version.

If you are running RHEL, you will need the following packages:
   * openldap-servers
   * openldap-clients
The =schema_check.py= script does not actually run an LDAP server, but needs =slapadd=, which is only distributed with the servers RPM.

Run =schema_check.py=:

<verbatim>
./gip_trunk/test/schema_check.py
</verbatim>

By default, this will run a check against all sites in the BDII.  To run against a specific site, just put it as a command argument.  Here is an example running against Nebraska:

<verbatim>
bbockelm:~ brian$ /Users/brian/projects/gip/trunk/test/schema_check.py Nebraska
testSchema_Nebraska (__main__.TestSchema) ... FAIL

======================================================================
FAIL: testSchema_Nebraska (__main__.TestSchema)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/brian/projects/gip/trunk/test/schema_check.py", line 77, in testSchema
    " for site %s.  Output:\n%s" % (self.site, output))
AssertionError: slapadd schema check failed for site Nebraska.  Output:
slapadd: dn="GlueSubClusterUniqueID=thpc computers,GlueClusterUniqueID=red.unl.edu,Mds-Vo-name=local,o=grid" (line=4488): (64) value of naming attribute 'GlueSubClusterUniqueID' is not present in entry
slapadd: dn="GlueSubClusterUniqueID=Dell Nodes,GlueClusterUniqueID=red.unl.edu,Mds-Vo-name=local,o=grid" (line=4584): (64) value of naming attribute 'GlueSubClusterUniqueID' is not present in entry
Entry (GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv2,Mds-Vo-name=local,o=grid,Mds-Vo-name=local,o=grid), attribute 'GlueServiceStatus' cannot have multiple values
slapadd: dn="GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv2,Mds-Vo-name=local,o=grid,Mds-Vo-name=local,o=grid" (line=4704): (19) attribute 'GlueServiceStatus' cannot have multiple values


----------------------------------------------------------------------
Ran 1 test in 3.251s
</verbatim>

Unlike this example, your ITB site must pass the schema check before moving on to the next step.

---++ Run the DN check

The DN check makes sure that the DN for each entry is properly formatted.  It is located in =test/dn_check.py=.  Here is a sample failure:

<verbatim>
rcf-nb-01:gip brian$ python trunk/test/dn_check.py Nebraska
GIP.common:ERROR gip_common:161:  Unable to open GIP attributes: [Errno 2] No such file or directory: '/Users/brian/projects/gip/trunk/monitoring/gip-attributes.conf'
testDnAds_Nebraska (__main__.TestDnAds) ... FAIL

======================================================================
FAIL: testDnAds_Nebraska (__main__.TestDnAds)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "trunk/test/dn_check.py", line 47, in testDnAds
    "o=grid entry in DN %s" % prettyDN(entry.dn))
AssertionError: There is an extra o=grid entry in DN GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv2,o=grid,mds-vo-name=local,mds-vo-name=Nebraska,mds-vo-name=local,o=grid

----------------------------------------------------------------------
Ran 1 test in 0.744s

FAILED (failures=1)
</verbatim>

---++ Run the SRM check

One of the most important uses of the GIP is to advertise SRM endpoints to the WLCG for FTS transfers.  This is a critical test for CMS.

A SRM storage element might advertise support for one or more SRM endpoints; currently supported SRM versions are 1.1 and 2.2.

Running the srm_check.py for the test site is sufficient.  It should print all the SRM endpoints on stdout and will raise an error if any of them is problematic.  Below is the output of a sample session where Nebraska advertises 3 SRM endpoints where 1 is incorrect:

<verbatim>
bbockelm:~ brian$ /Users/brian/projects/gip/trunk/test/srm_check.py Nebraska
testSrmAds_Nebraska (__main__.TestSrmAds) ... GIP.Testing.SRM:DEBUG srm_check:44:  Checking SRM entry:
Entry: ('GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv1', 'mds-vo-name=Nebraska', 'mds-vo-name=local', 'o=grid')
Classes: ('GlueKey', 'GlueSchemaVersion', 'GlueService', 'GlueTop')
Attributes: 
 - SchemaVersionMajor: 1
 - SchemaVersionMinor: 3
 - ServiceName: httpg://srm.unl.edu:8443/srm/managerv1
 - ServiceAccessControlRule: VO:ops
 - ServiceWSDL: http://sdm.lbl.gov/srm-wg/srm.v1.1.wsdl
 - ServiceOwner: OSG
 - ServiceAccessPointURL: httpg://srm.unl.edu:8443/srm/managerv1
 - ServiceSemantics: UNDEFINED
 - ServiceType: SRM
 - ForeignKey: GlueSiteUniqueID=red.unl.edu
 - ServiceUniqueID: httpg://srm.unl.edu:8443/srm/managerv1
 - ServiceVersion: 1.1
 - ServiceURI: httpg://srm.unl.edu:8443/srm/managerv1
 - ServiceStatusInfo: UNKNOWN
 - ServiceStartTime: 1970-01-01T00:00:00Z
 - ServiceStatus: Production
 - ServiceEndpoint: httpg://srm.unl.edu:8443/srm/managerv1

GIP.Testing.SRM:DEBUG srm_check:44:  Checking SRM entry:
Entry: ('GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv2', 'mds-vo-name=Nebraska', 'mds-vo-name=local', 'o=grid')
Classes: ('GlueKey', 'GlueSchemaVersion', 'GlueService', 'GlueTop')
Attributes: 
 - SchemaVersionMajor: 1
 - SchemaVersionMinor: 3
 - ServiceName: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceAccessControlRule: VO:ops
 - ServiceWSDL: http://sdm.lbl.gov/srm-wg/srm.v2.2.wsdl
 - ServiceOwner: OSG
 - ServiceAccessPointURL: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceSemantics: UNDEFINED
 - ServiceType: SRM
 - ForeignKey: GlueSiteUniqueID=red.unl.edu
 - ServiceUniqueID: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceVersion: 2.2
 - ServiceURI: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceStatusInfo: UNKNOWN
 - ServiceStartTime: 1970-01-01T00:00:00Z
 - ServiceStatus: Production
 - ServiceEndpoint: httpg://srm.unl.edu:8443/srm/managerv2

GIP.Testing.SRM:DEBUG srm_check:44:  Checking SRM entry:
Entry: ('GlueServiceUniqueID=httpg://srm.unl.edu:8443/srm/managerv2', 'o=grid', 'mds-vo-name=local', 'mds-vo-name=Nebraska', 'mds-vo-name=local', 'o=grid')
Classes: ('GlueKey', 'GlueSchemaVersion', 'GlueService', 'GlueTop')
Attributes: 
 - SchemaVersionMajor: 1
 - SchemaVersionMinor: 3
 - ServiceName: srm.unl.edu
 - ServiceAccessControlRule: atlas
 - ServiceWSDL: UNDEFINED
 - ServiceOwner: OSG
 - ServiceAccessPointURL: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceSemantics: UNDEFINED
 - ServiceType: srm_v2
 - ForeignKey: GlueSiteUniqueID=
 - ServiceUniqueID: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceVersion: 2
 - ServiceURI: httpg://srm.unl.edu:8443/srm/managerv2
 - ServiceStatusInfo: not tested
 - ServiceStartTime: 1970-01-01T00:00:00Z
 - ServiceStatus: OK
 - ServiceEndpoint: httpg://srm.unl.edu:8443/srm/managerv2

FAIL

======================================================================
FAIL: testSrmAds_Nebraska (__main__.TestSrmAds)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/brian/projects/gip/trunk/test/srm_check.py", line 46, in testSrmAds
    msg="ServiceType must be equal to 'SRM'")
AssertionError: ServiceType must be equal to 'SRM'

----------------------------------------------------------------------
Ran 1 test in 0.921s

FAILED (failures=1)
</verbatim>

In addition to passing the srm_check, you must check to make sure that the endpoints show up in =glite_sd_query=; if you do not have access to a gLite UI, ask one of the CMS sites to test it for you.

*It is ultimately the tester's responsibility to make sure that =glite_sd_query= is run, even if it is run at another site.*

---++ Check the advertised site layout

The site_print script will query the BDII and determine the advertised layout of the site.  To use this script against a particular site, simply invoke:

<verbatim>rcf-nb-01:gip brian$ ./trunk/test/site_print.py Nebraska
GIP.common:ERROR gip_common:161:  Unable to open GIP attributes: [Errno 2] No such file or directory: '/Users/brian/projects/gip/trunk/monitoring/gip-attributes.conf'
testSiteAds_Nebraska (__main__.TestPrintSite) ... 
SITE: Nebraska

	* CE: red.unl.edu:2119/jobmanager-pbs-cms, Close SE: srm.unl.edu
		- VO: cdf
		- VO: cms
		- VO: gpn
		- VO: mis
		- VO: osg
		- VO: GLOW
		- VO: fmri
<snip output/>
	* CE: red.unl.edu:2119/jobmanager-pbs-osg, Close SE: srm.unl.edu
		- VO: cdf
		- VO: cms
		- VO: gpn
		- VO: mis
		- VO: osg
		- VO: GLOW
		- VO: fmri

<snip output/>

	* CE: red.unl.edu:2119/jobmanager-pbs-dzero, Close SE: srm.unl.edu

<snip output/>

		- VO: atlas
		- VO: dzero
		- VO: grase
		- VO: ivdgl
		- VO: engage
		- VO: gridex
		- VO: osgedu
		- VO: nanohub
		- VO: localUsers
	* CE: red.unl.edu:2119/jobmanager-pbs-workq, Close SE: srm.unl.edu
		- VO: cdf
		- VO: cms
		- VO: gpn
		- VO: mis
		- VO: osg
		- VO: GLOW
		- VO: fmri
		- VO: gadu
		- VO: ligo
		- VO: sdss
		- VO: atlas
		- VO: dzero
		- VO: grase
		- VO: ivdgl
		- VO: engage
		- VO: gridex
		- VO: osgedu
		- VO: nanohub
		- VO: localUsers
	* CE: red.unl.edu:2119/jobmanager-pbs-pushpa, Close SE: srm.unl.edu
		- VO: cdf
		- VO: cms
		- VO: gpn

<snip output/>

	* CE: red.unl.edu:2119/jobmanager-pbs-cmsprod, Close SE: srm.unl.edu
		- VO: cdf
		- VO: cms
		- VO: gpn
		- VO: mis

<snip output/>

ok

----------------------------------------------------------------------
Ran 1 test in 0.391s

OK
</verbatim>

The script prints possibly a large amount of output per site.  It prints out each CE and the VO Views associated with that CE.  Additionally, if the CE has a close SE entry, it also prints that out.

Note that the above example has the following problems:

   * VOs were advertised for queues which are restricted (I.e., the CMS queue advertised support for gpn, cdf, mis, etc)
   * The localUsers GUMS entry (for Nebraska's local users) was advertised.  This should not show up anywhere.
   * VOs were advertised for queues they don't belong in (i.e., the CMS VO should not appear in any general-purpose queue).
   * A local queue, "pushpa" was advertised, but it shouldn't be exposed to the grid.

We now list what the setup should look like for each LRMS.

---+++ Condor

---++++ "Classic"

This is for all released and testing versions of the GIP.  One CE per VO should appear, and one associated VO view per CE.

Here is the proper entry for USCSMS-FNAL-WC1-CE:

<verbatim>
SITE: USCMS-FNAL-WC1-CE

	* CE: cmsosgce.fnal.gov:2119/jobmanager-condor-cms, Close SE: cmsosgce.fnal.gov
		- VO: cms
	* CE: cmsosgce.fnal.gov:2119/jobmanager-condor-mis, Close SE: cmsosgce.fnal.gov
		- VO: mis
	* CE: cmsosgce.fnal.gov:2119/jobmanager-condor-ops, Close SE: cmsosgce.fnal.gov
		- VO: ops
</verbatim>

---++++ Proposed

Condor should advertise one CE:

<verbatim>
<hostname>:2119/jobmanager-condor-default
</verbatim>

Each VO should have a VOView entry underneath this CE.  Here is the previous example but using the proposed layout:

<verbatim>
SITE: USCMS-FNAL-WC1-CE

	* CE: cmsosgce.fnal.gov:2119/jobmanager-condor-default, Close SE: cmsosgce.fnal.gov
		- VO: cms
		- VO: mis
		- VO: ops
</verbatim>

---+++ PBS, SGE, and LSF

PBS, SGE, and LSF are queue-based systems.  For each queue, there should be one CE entry (two if WS-GRAM is enabled).  The sites should be able to blacklist any queue which doesn't accept grid jobs, limit queues to certain VOs, and remove specific VOs from certain queues.

Here is an example CE for Nebraska:

<verbatim>
	* CE: red.unl.edu:2119/jobmanager-pbs-cmsprod
		LRMS type: pbs, Version: PBSPro_9.1.0.72982
		Slots used 16, Free 250
		Total batch slots: 444
		Max wall time: 1440
		- VO: cms
			Running 250, Waiting 0
</verbatim>

---+++ General Tests

   * Make sure that the site layout and VO views are correct.
   * Send a long-running job into the queue and make sure that the running job shows up in the correct queue and VO.
   * Ensure that any waiting jobs show up in the correct queue and VO
   * Make sure that the total, used, and free batch slots are correct
   * (PBS, SGE, LSF) Make sure that the max wall time is set correctly.
   * Ensure that the LRMS type and version are set correctly.

---+ Release Management

The release manager needs to coordinate the testing of the GIP.  This page is summarized into the below checklist.  The release manager is responsible to make sure that the checklist is performed at sites containing each batch system type and at least one dCache site.

The release manager needs to complete the following checklist
<verbatim>
Release Manager Checklist.

Release Manager: __________
Tag tested: _______________

Each of the following must be completed:
- Open VDT support ticket with latest tag tarball.
- Wait for VDT to package tarball into the release repo.
- Coordinate the sites to test the GIP.  One of each LRMS types must be tested:
   * Condor site: _____
   * PBS site: ________
   * LSF site: ________
   * SGE site: ________
  Attach their completed checklist to the email to the GIP developers and VDT.  Make sure to note anything which fails.
- Attach the completed checklist of one dCache site running the dynamic dCache plugins.  Site name: _______
- When this checklist is completed, email it to the OSG-GIP mailing list and attach it to the VDT support ticket.
</verbatim>

Questions should be directed to the OSG-GIP mailing list.

---++ Site Release Checklist
<verbatim>
Tester name: ______________
Tag tested: _______________

Please fill out each entry; put N/A if it is not applicable.

* GIP executes without error: ___
* Data is exported to CEMon on ITB: ___
* GIP data passes schema check: ___
* GIP data passes the DN check: ___
* SRM endpoints:
   * A SRM endpoint is advertised: ___
   * Additional SRM endpoints are advertised (i.e., SRM v2.2 in addition to SRM v2.1): ___
   * SRM advertisement passes srm_check: ___
* Site batch system (pick one):  LSF/PBS/SGE/Condor
* Site layout output from site_print is correct: ___
* Site's default SE shows up under "Close SE" from site_print: ___
* Use ce_print to determine the following for one CE:
   * Batch system type and version are correct: ___
   * For non-Condor systems, max_walltime is correct: ___
   * Total, free, and used slots are correct: ___
* Submit jobs to the batch system and ensure that:
   * Running job count is correct for VOView and CE: ___
   * Waiting job count is correct for VOView and CE: ___
  For both of the above, make sure it is truly tested - both entries default to 0.  A true test makes sure that both the running and waiting are nonzero.

If you have a dCache SE, continue:

* dCache configuration is done: ___
* dCache version is correct: ___
* dCache free and total space (in KB) is correct: ___
* SRM server shows up as a GlueService; one entry for 1.1 and another for 2.2: ___
* Each GridFTP door in the system shows up in an AccessProtocol entry: ___
* One SA space shows up per VO: ___
* Space reservations are advertised as a VO view when made (optional): ___
</verbatim>

-- Main.BrianBockelman - 10 Mar 2008