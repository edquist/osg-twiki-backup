%META:TOPICINFO{author="BrianBockelman" date="1266593597" format="1.1" reprev="1.44" version="1.44"}%
%LINKCSS%

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%
%STARTINCLUDE%

---++Introduction

This TWiki page serves as entry point for documentation on how to use the OSG. As the OSG support model assumes the end-user scientists to be supported by large "Virtual Organisations" (VO)
that provide the actual user interfaces and user support, the primary target audience here are
the VO user support teams, rather than the actual scientists as end-users.  However this should also help individual or small group researchers who don't have the wherewithal to form a VO, and who therefore are welcome to join the "OSG" VO.

Compute resources on the OSG are accessed through a protocol named GRAM using a tool called =Condor-G=.  Storage resources are accessed through a protocol named SRM, and a variety of clients are used.  In this guide, we provide examples for accessing storage and compute resources.

---++Getting Access to computing resources via OSG

Getting access to computing resources via the OSG involves the following steps:

   1. Acquire an X.509 certificate from one of the [[http://vdt.cs.wisc.edu/certificate_authorities.html][supported CAs]]. As an example, you might follow [[https://twiki.grid.iu.edu/twiki/bin/view/OSGRA/][these instructions for obtaining a doesciencegrid certificate]].
   1. [[GettingStarted#I_want_to_use_OSG_resources][Register with your VO.]] Each VO maintains a <span class="firstterm">virtual organization membership service</span> (<span class="arconym">VOMS</span>) that often has a form for end-user scientists to register. The VO may [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153][implement policies]] using [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154][groups and roles]] and extended proxies in order to differentiate access rights and priorities for different end-users. Every VO is expected to maintain its own [[VO/WebHome][VO-specific instructions on membership, usage, etc., and general information for users]].
   1. [[ReleaseDocumentation.ClientInstallationGuide][Install the OSG client software]] on your desktop.

----+++A word on Policies for resource usage

The OSG does not allocate resources as none of the resources available via the grid are "owned" by the OSG. The owners &#8212; generally large VOs and IT organisations, including national labs &#8212; fully control usage policies on their resources. 

Many VOs allow you to use resources beyond those owned by the VO. The most common policy for resource utilisation outside your own VO is "opportunistic use". The interpretation of this varies greatly. Some sites allow access to all their "spare" resources to anybody registered with a VO on OSG, others to only a small subset of resources and/or VOs. Some sites interpret "spare" to mean "until a more privileged user arrives", others guarantee a minimum wall clock time for your job once it starts.

The only reliable way to find out policy is to try to run a job.

%WARNING% A cluster is a large error amplifier!

---++ Finding Available Resources

%INCLUDE{ "FindAvailableResource" }%

---++Using the Computing Resources

OSG provides an interface at each compute site, refered to as the _Compute Element (CE)_. In principle, both =globus-job-run= and =Condor-G= can be used to submit jobs to the CE. As a matter of policy, the large-scale use of globus-job-run is banned - use Condor-G to submit your jobs.  You may be banned at some OSG sites if you do not use Condor-G or a similar submission tool.  More info on Condor-G [[http://www.cs.wisc.edu/condor/manual/v7.3/5_3Grid_Universe.html#SECTION00632000000000000000][can be found in the extensive Condor manual]].

There's a "[[http://www.cs.wisc.edu/condor/condorg/goldenrules.html][golden rules]]" document you might find useful for submitting a large number of jobs.

---++++Submitting single job using =Condor-G=

%INCLUDE{"CondorSubmittingSingleJob" }%

---++++Submitting multiple jobs using condor-g

%INCLUDE{ "CondorSubmittingMultipleJobs" }%

---+++Consuming storage
There are a variety of different [[ReleaseDocumentation.LocalStorageConfiguration][storage areas in OSG]]. As a general rule, you should keep the size of the files you transfer using Condor-G to a minimum (preferably less than 10 and definitely less than 100MB per job).  The bulk of what you need for running your jobs should be staged in (e.g. as part of a <span class="acronym">DAG</span>) before the job submission. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAG, after the job completes using SRM.

At present, there is no "one-stop shopping" for getting access to SRM.  One option is to send email to =tg-storage at opensciencegrid.org=. They can help you get started, and coordinate getting access to the SRMs across OSG.

Another alternate is use lcg-info (which is not developed by the OSG, but packaged in the OSG client), which provides an almost-correct view.  Run:

<pre class="screen">
<userinput>lcg-info --list-service --bdii is.grid.iu.edu:2170 --vo gpn --attrs ServiceEndpoint</userinput>
</pre>
<br/>
%TWISTY{
mode="div"
showlink="Show output..."
hidelink="Hide output"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
-bash-3.2$ lcg-info --list-service --bdii is.grid.iu.edu:2170 --vo gpn --attrs ServiceEndpoint
- Service: httpg://red-srm1.unl.edu:8443/srm/v2/server
  - ServiceEndpoint     httpg://red-srm1.unl.edu:8443/srm/v2/server

- Service: httpg://se01.cmsaf.mit.edu:8443/srm/managerv1
  - ServiceEndpoint     httpg://se01.cmsaf.mit.edu:8443/srm/managerv1

- Service: httpg://se01.cmsaf.mit.edu:8443/srm/managerv2
  - ServiceEndpoint     httpg://se01.cmsaf.mit.edu:8443/srm/managerv2

- Service: httpg://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server
  - ServiceEndpoint     httpg://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server

- Service: httpg://srmb.ihepa.ufl.edu:8443/srm/v2/server
  - ServiceEndpoint     httpg://srmb.ihepa.ufl.edu:8443/srm/v2/server
%ENDTWISTY%
<br/>
Replace "gpn" with your VO's name.  These are the endpoints for all the SRM servers which claim support for your VO.  Next, take the hostname (such as =red-srm1.unl.edu=) and perform another query to get the path you should write into:
<pre class="screen">
<userinput>lcg-info --list-se --bdii is.grid.iu.edu:2170 --vo gpn --attrs Path --query 'SE=red-srm1.unl.edu'</userinput>
</pre>
<br/>
%TWISTY{
mode="div"
showlink="Show output..."
hidelink="Hide output"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="screen">
-bash-3.2$ lcg-info --list-se --bdii is.grid.iu.edu:2170 --vo gpn --attrs Path --query 'SE=red-srm1.unl.edu'
- SE: red-srm1.unl.edu
  - Path                /mnt/hadoop/public/
</pre>
%ENDTWISTY%
<br/>

You can create an SRM URL from the endpoint path, and VO name (in this case, =gpn=).  Replace httpg with srm, add a "?SFN=" to the endpoint, and add the VO's name to the path:

<pre>
srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/gpn
</pre>

Again, this will not work for every site; sites where this does not work are due to a failing of the tool, not an error of the site.  OSG is working on a storage discovery tool that automates the last few queries.

We will use the above endpoint for the examples below.  You will need to build your own to proceed.

---++++ Write a file to the remote endpoint
<verbatim>
lcg-cp -b -D srmv2 file:////bin/sh srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/gpn/sh-copy
</verbatim>

Note that recursive copying of a directory does not work.

---++++ Download a file from the remote endpoint
<verbatim>
lcg-cp -b -D srmv2 srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/gpn/sh-copy file:///tmp/sh-copy 
</verbatim>

---++++ SRM copy (SRM to SRM):
<verbatim>
 srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy srm://anotherhost.org/newdir/sh-copy 
</verbatim>

---++++ SRM copy (gsiFTP to SRM):
<verbatim>  
 srmcp gsiftp://ftphost.org//path/file srm://myhost.mydomain.edu/dir1/dir2/file
</verbatim>

---+++ globus-url-copy syntax
<span class="command">globus-url-copy</span> uses similar syntax but can only communicate with gsiftp servers

<verbatim>
globus-url-copy file:////bin/sh gsiftp://myhost.mydomain.edu/dir1/dir2/sh-copy
globus-url-copy gsiftp://myhost.mydomain.edu/dir1/dir2/file file:///$OSG_WN_TMP/local-copy
</verbatim>

---+++Condor Hold (_error_) Codes

Sometimes, jobs that you submit actually run, and sometimes they don't. When they don't, exit error codes may be useful.  If the job goes on hold (status 'H'), the Condor hold reason and hold code may be helpful.  These are visible in the output of <span class=command>condor_q -l <i>jobid</i></span>.  Hold codes are documented in the [[http://www.cs.wisc.edu/condor/manual/v6.8/2_5Submitting_Job.html#2162][Condor manual]].

See [[Troubleshooting.CondorErrors]] for more details.

---++ Troubleshooting Using the OSG
%INCLUDE{ "GridUsersGuideTroubleshootingOSG" }%
GridUsersGuideTroubleshootingOSG
