%META:TOPICINFO{author="FkW" date="1165342385" format="1.1" version="1.18"}%
<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


<!--  * Set ALLOWTOPICCHANGE = Main.DocEditGroup !-->
d19 3
---++Introduction

This TWiki page serves as entry point for documentation on how to use the OSG.
As the OSG support model assumes the end-user scientists to be supported by large "Virtual Organisations" (VO)
that provide the actual user interfaces and user support, the primary target audience here are
the VO user support teams, rather than the actual scientists as end-users.

---++Getting Access to computing resources via OSG

Getting access to computing resources via the OSG involves the following steps:

   * acquire an X509 certificate from one of the <a href="http://vdt.cs.wisc.edu/releases/1.3.10/certificate_authorities.html">supported CAs</a>. As an example, you might follow <a href="http://www.grid.iu.edu/osg-ra/PersonalRequest.php">these instructions</a> for obtaining a doesciencegrid certificate.
   * Register with your VO. Each VO maintains a VOMS, and probably a VOMRS based interface for end-user scientists to register. The VO may <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153">implement policies</a> using <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154">groups and roles</a> and extended proxies in order to differentiate access rights and priorities for different end-users. The OSG does support single PIs without the backing of large VOs via the OSG VO. Every VO is expected to maintain its <a href="http://osg.ivdgl.org/twiki/bin/view/VO">twiki page</a> with VO-specific instructions on membership, usage etc. At the moment, only a few VOs do.
   * Install an <a href="http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/ClientInstallationGuide">OSG client</a> on your desktop.
   1. Acquire an X.509 certificate from one of the [[http://vdt.cs.wisc.edu/releases/1.3.10/certificate_authorities.html][supported CAs]]. As an example, you might follow [[http://www.grid.iu.edu/osg-ra/PersonalRequest.php][these instructions]] for obtaining a doesciencegrid certificate.
   2. [[Documentation.JoinToUseResources][Register with your VO.]] Each VO maintains a <span class="firstterm">virtual organization membership service</span> (<span class="arconym">VOMS</span>) that often has a form for end-user scientists to register. The VO may [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153][implement policies]] using [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154][groups and roles]] and extended proxies in order to differentiate access rights and priorities for different end-users. Every VO is expected to maintain its own [[https://goc.grid.iu.edu/othertools/registration/osgweb/pullvo.php][VO-specific instructions]] on membership, usage, etc. and [[https://goc.grid.iu.edu/othertools/registration/osgweb/pullvo.php][general information for users]].
   3. Install the [[ReleaseDocumentation.ClientInstallationGuide][OSG client software]] on your desktop.
The OSG presently does not have any allocations for use of resources. In fact, none of the resources available via the OSG are "owned" by the OSG. The owners, generally large VOs, and IT organisations, including National Labs, fully control usage policies on their resources. The most common policy for resource utilisation beyond the resources that your VO owns is "opportunistic use". The interpretation of what this means varies greatly. Some sites allow access to all their "spare" resources to anybody registered with a VO on OSG, others only to a small subset of resources and/or VOs. Some sites interpret "spare" to mean "until a more privileged user arrives", others guarantee a minimum wall clock time for your job once it starts.
----+++A word on Policies for resource usage
There is no policy schema defined for OSG at this point. It is thus not possible to query a database to determine site policies. Site policy is presently only available to the extend it is specified on the site policy web page linked in via gridcat. The only reliable way to find out policy is to try to run a job.
The OSG does not allocate resources as none of the resources available via the grid are "owned" by the OSG. The owners &#8212; generally large VOs and IT organisations, including national labs &#8212; fully control usage policies on their resources. 

%WARNING% A cluster is a large error amplifier!
All OSG sites are registered with <a href="http://osg-cat.grid.iu.edu/">GridCat</a>. !GridCat maintains information on every site in a DB, and verifies this information regularly. Sites that fail basic functionality tests are red, sites that pass all basic tests are green, and inactive sites are grey. 
---++Finding the list of available sites
In addition, more detailed information can be obtained via ldap queries to the site. For example, if you want to find out if worker nodes at CIT_CMS_T2 have outgoing connections, you can do
All OSG sites are registered with [[http://osg-cat.grid.iu.edu/][GridCat]]. !GridCat maintains information on every site in a DB, and verifies this information regularly. Sites that fail basic functionality tests are red, sites that pass all basic tests are green, and inactive sites are grey. 
<verbatim>
ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "(GlueHostNetworkAdapterOutboundIP=*)" GlueHostNetworkAdapterOutboundIP
</verbatim>
$ <b>ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "(GlueHostNetworkAdapterOutboundIP=*)" GlueHostNetworkAdapterOutboundIP</b>
This returns 
GlueHostNetworkAdapterOutboundIP: TRUE
<verbatim>
GlueHostNetworkAdapterOutboundIP: TRUE
</verbatim>

To find out whether or not your VO is supported by a specific site, you can check with <a href="http://vors.grid.iu.edu/cgi-bin/index.cgi?region=0&VO=0&res=0&grid=1">VORS </a>. Another way is to submit an ldap query to the site, for example:
If you want to find out whether or not a specific site supports your VO, check  [[http://vors.grid.iu.edu/cgi-bin/index.cgi?region=0&VO=0&res=0&grid=1][VORS]]. 
<verbatim>
ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "GlueCEName=cdf"
</verbatim>
<pre class="screen">
This returns a bunch of entries if VO cdf is indeed supported at CIT_CMS_T2. If it is not, the query returns an empty string.
</pre>

If supported, it will return a series of entires. If not, it returns an empty string.
The OSG 0.4.1 release provides access to compute resources via condor-g and to storage resources via either gridftp or SRM. In the following we provide simple examples for how to use these interfaces to consume resources.
---++Using the Computing Resources

OSG release 0.4.1 provides access to compute resources via =condor-g= and to storage resources via either gridFTP or SRM. In the following we provide simple examples for how to use these interfaces to consume resources.
The OSG 0.4.1 release provide pre-WS GRAM as interface at each compute site. We refer to this interface as the "Compute Element", or CE. In principle, both globus-job-run and condor-g can be used to submit jobs to the CE. However, as a matter of policy we require use of the condor-g gridmonitor for scalability reasons. You should expect to lose your submission privileges at OSG sites if you do not use condor-g and gridmonitor for submission of anything more than a couple of jobs to a site. More info on condor-g can be found <a href="http://www.cs.wisc.edu/condor/manual/v6.6/5_3Condor_G.html">here</a>.
---+++Consuming CPU power
There's a <a href="http://www.cs.wisc.edu/condor/condorg/goldenrules.html">golden rule</a> and a <a href="http://www.cs.wisc.edu/condor/condorg/linux_scalability.html">linux scalability</a> document that you might find useful in order to successfully submit a large number of jobs. In addition, we list some "do's and don't's below.
The OSG 0.4.1 release provides the pre-WS GRAM as the interface at each compute site. We refer to this interface as the _Compute Element_ (CE). In principle, both =globus-job-run= and =condor-g= can be used to submit jobs to the CE. However, as a matter of policy we require use of the condor-g gridmonitor for scalability reasons. You should expect to lose your submission privileges at OSG sites if you do not use condor-g and gridmonitor for submission of anything more than a couple of jobs to a site. ([[http://www.cs.wisc.edu/condor/manual/v6.6/5_3Condor_G.html][More info on condor-g]])

d110 1
<verbatim>
#!/bin/sh
There's a [[http://www.cs.wisc.edu/condor/condorg/goldenrules.html][golden rule]] and a [[http://www.cs.wisc.edu/condor/condorg/linux_scalability.html][linux scalability]] document that you might find useful in order to successfully submit a large number of jobs.
# See what's there before you do anything:
printenv
export startdir=`pwd`
echo $startdir
whoami
---++++Simple Example Script for submission to OSG
# Establish your working environment, including adding srmcp into your path:
source $OSG_GRID/setup.sh
%INCLUDE{ "SampleOsgScript" }%
# See what's there after you've sourced your environment:
printenv
---++++Submitting single job using =condor-g=
# Make sure you switch to a local disk as your wrkdir.
# Unfortunately, not all sites start your job in your own directory, on a local disk.
# You therefore have to take this in your own hands.
export wrkdir=${OSG_WN_TMP}/`date +%a%b%e%H%M%S%Z%Y`
mkdir $wrkdir
cd $wrkdir

# To learn more about use of storage spaces in OSG, and the associated envvar's that point to it, read:
#http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/LocalStorageUse

echo "This is my output" > output.log

#Note: If you wanted condor-g to transfer a file back out that you create in your working
# directory at this point then you would have to first move it back into the startdir !
mv output.log $startdir/.

# don't forget to clean up after yourself
cd $startdir
rm -rf $wrkdir

#sleep a little so you can use condor tools to check that this test job is running
sleep 1000

</verbatim>
%ENDMore%

---++++Submitting one job using condor-g

<verbatim>
echo "universe=globus" > test.cmd
echo "GlobusScheduler=osg-gw-2.t2.ucsd.edu:/jobmanager-condor" >> test.cmd
echo "executable=/bin/env" >> test.cmd
echo "stream_output = False" >> test.cmd
echo "stream_error  = False" >> test.cmd
echo "output = test1.out" >> test.cmd
echo "error  = test1.err" >> test.cmd
echo "log    = test1.log" >> test.cmd
echo "queue" >> test.cmd
</verbatim>

Then submit test.cmd after you have sourced the environment for the OSG client, and have
obtained a proxy using voms-proxy-init to the voms of your VO. This might look something like this:

<verbatim>
source $VDT_LOCATION/setup.sh 
voms-proxy-init --voms cms:/cms/uscms/Role=cmsuser 
condor_submit test.cmd
sleep 300
condor_q
</verbatim>

In most cases, a basic grid proxy certificate generated by <b>grid-proxy-init</b> should be sufficient. <b>voms-proxy-init</b> is only necessary if your VO supports user roles and if the site you submit to can map your jobs to different local accounts depending on these roles. 

Grid middleware has notoriously large latency as it is meant to provide high throughput computing, not interactive computing, or even low latency computing tools. The sleep 300 indicates that it might take several minutes before anything starts, even if the site you are submitting to has plenty of free batch slots.
d144 1

Now let's see how we would submit four jobs at once. Our =test.cmd= file now looks like this:
%INCLUDE{"CondorSubmittingSingleJob" }%
Now we replace test.cmd with the following:
d178 1
<verbatim>
  universe=globus
  GlobusScheduler=osg-gw-2.t2.ucsd.edu:/jobmanager-condor
  executable=/bin/ls
  stream_output = False
  stream_error  = False
---++++Submitting multiple jobs using condor-g
  arguments=/tmp
  output = test1.out
  error  = test1.err
  log    = test1.log
  queue
%INCLUDE{ "CondorSubmittingMultipleJobs" }%
  arguments=/usr
  output = test2.out
  error  = test2.err
  log    = test2.log
  queue 
---+++ Usage of !GlobusRSL
  arguments=/var
  output = test3.out
  error  = test3.err
  log    = test3.log
  queue
---++++ Condor
  arguments=/etc
  output = test4.out
  error  = test4.err
  log    = test4.log
  queue
</verbatim>
| condorsubmit | Allow the client to specify abitrary additional attributes to be included in the Condor submit description file. |
   $ *%RED% NOTE %ENDCOLOR%*: For submission of DAGs and other more complicated examples, see the condor-g documentation.
This would submit 4 jobs. For submission of DAGs and other more complicated examples, see the condor-g documentation.
---++++ PBS
---++++Usage of !GlobusRSL
%INCLUDE{ "GlobusRlsPbsAttributes" }%
The Globus Resource Specification Language (RSL) provides a uniform way to communicate with various batch schedulers. Here is a list of RSL attributes for Condor and PBS from Jaime Frey:
---++++ Example: Submitting Job to Purdue
<verbatim>
Condor:
Attribute: condorsubmit
Description: "Allow the client to specify abitrary additional attributes to
              be included in the Condor submit description file."

PBS:
Attribute: email_address
Description: "Set the email address to receive notifications. See the
              email_on_abort, email_on_execution, and emailontermination attributes."

Attribute: email_on_abort
Description: "Send email to the job submitter (or the address
               specified in the email_address RSL attribute if present) if the job is
              aborted by the scheduler."
Values: yes no

Attribute: email_on_execution
Description: "Send email to the job submitter (or the address
              specified in the email_address RSL attribute if present) when the job
              begins execution."
Values: yes no

Attribute: email_on_termination
Description: "Send email to the job submitter (or the address specified in the
              email_address RSL attribute if present) when the job terminates."
Values: yes no
</verbatim>

For example, to submit a job to the <a href="http://www.purdue.teragrid.org/content/view/11/25/">Purdue site</a>, one might want to use the following syntax in the Condor submit script:
d217 5
<verbatim>
For example, to submit a job to the [[http://www.purdue.teragrid.org/content/view/11/25/][Purdue site]], one might want to use the following syntax in the Condor submit script:
</verbatim>
<pre class="programlisting">
globusrsl = (project=TG-XYZ12345)( condorsubmit = (Requirements 'HasCTSS == TRUE && CanReachInternet == TRUE') )
</pre>
   * Always use gridmonitor
   * Never use streaming of stdout/stderr because it disables the gridmonitor. A number of sites do not allow this anyway. If you need to debug running jobs, consider [[http://jobmon.sourceforge.net/][JobMon]] instead. It is much more scalable, puts less load on the site infrastructure, and is more feature rich.
   * Avoid =jobmanager-fork=.
   * Minimise the number and size of files you transfer in or out of a site using condor-g. Use stage in/out to the OSG storage as described below instead. 
   * Use SRM instead of gridftp whenever you can.
   * Avoid very short- and long-running jobs. Ideal wall clock time for a job is a few hours.
   * Never use streaming of stdout/stderr because it disables the gridmonitor. A number of sites do not allow this anyway. If you need to debug running jobs, consider <a href="http://jobmon.sourceforge.net/">JobMon</a> instead. It is much more scalable, puts less load on the site infrastructure, and is more feature rich.
   * Avoid jobmanager-fork
   * Minimise the number and size of files you transfer in/out of a site using condor-g. Use stage in/out to the OSG storage as described below instead. Use SRM instead of gridftp whenever you can.
   * Avoid very short and very long running jobs. Ideal wall clock time for a job is a few hours.
   * Avoid network mounted disks for IO operations as best as you can. Always remember: a cluster is a large error amplifier! If you have >100 jobs running in parallel even a modest amount of IO to a network mounted disk can overwhelm the server, and can cause a large mess on the site you are running!
   * Avoid a steady stream of job submissions at arund the Hz t couple Hz rate. You are better off submitting, say 10 at once, and then take a 20 seconds break, than space them equally by 2 seconds. We found that the gridmonitor is unable to handle a steady stream of submits at 0.5 Hz, while it deals with the other pattern quite well. (more details on this sort of stuff later)
---++++Golden Rules for power users
If you have any additional questions you run into as you scale up your operations on OSG, please feel free to ask other power users for advice by sending email to osg-users at opensciencegrid.org.
d107 1

%INCLUDE{ "GoldenRulesForPowerUsers" }%

There are a variety of different storage areas in OSG. Their use is documented <a href="http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/LocalStorageUse">here</a>. As a general rule, you should keep the size of the files you transfer using condor-g to an absolute minimum. In fact, ideal is a script no larger than a few tens or hundred lines or so. The bulk of what you need for running your jobs should be staged in, e.g. as part of a DAG, before the job submission via pre-WS GRAM. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAQ, after the job completes using !GridFTP or SRM. If you have a choice, always prefer SRM over !GridFTP because SRM queues up transfers to avoid overloads, distributes the load across multiple gftp servers and retries after failures.
---+++Consuming storage
At present, there is no "one-stop shopping" for getting access to SRM. To try out SRM one a reference platform, send email to tg-storage at opensciencegrid.org . They can help you get started, and coordinate getting access to the SRM's across OSG.
There are a variety of different [[http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/LocalStorageUse][storage areas in OSG]]. As a general rule, you should keep the size of the files you transfer using condor-g to an absolute minimum. In fact, ideal is a script no larger than a few tens or hundred lines or so. The bulk of what you need for running your jobs should be staged in (e.g. as part of a <span class="acronym">DAG</span>) before the job submission via pre-WS GRAM. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAQ, after the job completes using !GridFTP or SRM. If you have a choice, always prefer SRM over !GridFTP because SRM queues up transfers to avoid overloads, distributes the load across multiple gftp servers and retries after failures.
Examples of srmcp usage are available from <b>srmcp -help</b>:
At present, there is no "one-stop shopping" for getting access to SRM. To try out SRM one a reference platform, send email to =tg-storage at opensciencegrid.org=. They can help you get started, and coordinate getting access to the SRMs across OSG.
<verbatim>
Example of srm put:
         srmcp file:////bin/sh srm://myhost.mydomain.edu/dir1/dir2/sh-copy
Example of srm get:
         srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy file:///localdir/sh
Example of srm copy (srm to srm):
         srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy srm://anotherhost.org/newdir/sh-copy
Example of srm copy (gsiftp to srm):
         srmcp gsiftp://ftphost.org//path/file srm://myhost.mydomain.edu/dir1/dir2/file
</verbatim>

<b>globus-url-copy</b> uses similar syntax but can only communicate with gsiftp servers:

---+++ globus-url-copy syntax
<span class="command">globus-url-copy</span> uses similar syntax but can only communicate with gsiftp servers

<verbatim>
globus-url-copy file:////bin/sh gsiftp://myhost.mydomain.edu/dir1/dir2/sh-copy
---+++Condor error codes
</verbatim>
Of course, when you submit jobs, sometimes they run and sometimes they don't. If they don't, exit error codes may be useful. Condor error codes unfortunately are not documented anywhere consistently. Here is a list kindly extracted by Jaime Frey from 6.7 Condor code:
---+++Condor Hold (_error_) Codes
<verbatim>
//There may still be some lingering cases that result in this
//unspecified hold code.  Hopefully they will be eliminated soon.
const int CONDOR_HOLD_CODE_Unspecified = 0;
Sometimes, jobs that you submit actually run, and sometimes they don't. When they don't, exit error codes may be useful. Condor doesn't have error codes _per se_ but does have _hold codes_ that serve the purpose. Unfortunately, the condor teams have not consistently documented them. Here is a list kindly extracted by Jaime Frey from 6.7 Condor code:
//User put the job on hold with condor_hold
const int CONDOR_HOLD_CODE_UserRequest = 1;

//Globus reported an error.  The subcode is the GRAM error number.
const int CONDOR_HOLD_CODE_GlobusGramError = 2;
---++ Troubleshooting
//The periodic hold expression evaluated to true
const int CONDOR_HOLD_CODE_JobPolicy   = 3;
In this section we try to provide some hints about when and to whom you should report errors, and which errors you can solve yourself.
//The credentials for the job (e.g. X509 proxy file) are invalid.
const int CONDOR_HOLD_CODE_CorruptedCredential = 4;
You should report OSG-related errors to your [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support"][VO support center]], who will then triage your problems, and forward your problems to the <span class="firstterm">Grid Operations Center</span> )(<span class="acronym">GOC</span>), if warranted. 
//A job policy expression (such as PeriodicHold) evaluated to UNDEFINED.
const int CONDOR_HOLD_CODE_JobPolicyUndefined   = 5;
As some of you may actually be also the support center people of your VO, here are some pointers.
//The condor_starter failed to start the executable.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_FailedToCreateProcess = 6;
   1. You arrive at the worker node but have no grid proxy, are missing srmcp in your path, or are missing some other client tools that should be part of the OSG WN client installation.
//The standard output file for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenOutput = 7;
   1. If a site doesn't have a functioning LDAP service, then it deserves a ticket.
//The standard input file for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenInput = 8;

//The standard output stream for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenOutputStream = 9;
Unfortunately, OSG rules do not require you to have access to any site in particular. It's up to the site to support you and your VO. Some sites support some VOs without supporting all of their users. 
//The standard input stream for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenInputStream = 10;
At present, the only way to understand the intentions of a site is to query its LDAP and see if your VO shows up. The best way to understand if authentication should work for you personally is to check the VO support matrix, and look for your DN among the DNs supported at the site. 
//An internal Condor protocol error was encountered when transferring
files.
const int CONDOR_HOLD_CODE_InvalidTransferAck = 11;

//The condor_starter failed to download input files.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_DownloadFileError = 12;

//The condor_starter failed to upload output files.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UploadFileError = 13;

//The initial working directory of the job cannot be accessed.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_IwdError = 14;

</verbatim>

---++Some Guidance on what to do when things don't work

In this section we try to provide some hints as to when, and where you should complain, and what errors you need to deal with yourself.

The operations model of OSG is such that all complaints you have should go to your <a href="http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support">VO support center</a>. Your VO support center will then triage your problems, and forward your problems to the GOC (Grid Operations Center) if warranted. 

As some of you may actually be also the support center people of your VO, here are some pointers.

---+++Clear cut reasons for generating a trouble ticket

   * Submission to site X used to work but now no longer works. Check if the site has gone red on gridcat. If it hasn't, complain so that a trouble ticket gets generated for that site.
   * A standard script like the examples on this page works when submitted to jobmanager-fork but fails on the worker node for whatever reason.
   * You arrive at the worker node but have no grid proxy, are missing srmcp in your path, or are missing some other client tools that should be part of the OSG WN client installation.
   * You arrive on the worker node and find OS, network architecture, or some other characteristics that does not agree with the supposed characteristics of the site as advertised via the GIP. By extension, if a site doesn't have a functioning ldap service then it deserves a ticket.

---+++Ambiguous situations that may not warrant a trouble ticket

   * You can't authenticate at Site X. Unfortunately, OSG rules do not require you to have access to any site in particular. It's up to the site to support you and your VO. Some sites support some VOs without supporting all of their users. At present, the only way to understand the intentions of a site is to query its ldap, and see if your VO shows up in there. The best way to understand if authentication should work for you personally is to check the VO support matrix, and look for your DN among the DNs supported at Site X. If you find that an unreasonable number of sites don't let you in you should send the list of sites where you failed to osg-users at opensciencegrid.org and maybe we can help you gain access to more sites.
   * You find that perl, python, wget, etc. are either not installed at all, or not installed with the version you'd expect given the OS version that the sit is supposed to have. Unfortunately, you are out of luck in that case. There is no definition for a minimal Linux distro that would clearly specify what should exist on a worker node. If you need something and find it missing you may have to either give up on that site or install it yourself into $OSG_APP .
d170 1

&nbsp;

%BOTTOMMATTER%
-- Main.LeighGrund - 16 Jun 2005<br>
-- Main.IlyaNarsky - 23 Jun 2006
-- Main.FkW - 06 May 2006<br>
-- Main.FkW - 14 May 2006<br>
-- Main.FkW - 23 May 2006<br>
-- Main.ForrestChristian - 2007 Feb %BR%

