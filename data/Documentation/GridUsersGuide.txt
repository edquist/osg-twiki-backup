%META:TOPICINFO{author="BrianBockelman" date="1266376461" format="1.1" version="1.41"}%
%LINKCSS%

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%
%STARTINCLUDE%

---++Introduction

This TWiki page serves as entry point for documentation on how to use the OSG. As the OSG support model assumes the end-user scientists to be supported by large "Virtual Organisations" (VO)
that provide the actual user interfaces and user support, the primary target audience here are
the VO user support teams, rather than the actual scientists as end-users.  However this should also help individual or small group researchers who don't have the wherewithal to form a VO, and who therefore are welcome to join the "OSG" VO.

---++Getting Access to computing resources via OSG

Getting access to computing resources via the OSG involves the following steps:

   1. Acquire an X.509 certificate from one of the [[http://vdt.cs.wisc.edu/certificate_authorities.html][supported CAs]]. As an example, you might follow [[https://twiki.grid.iu.edu/twiki/bin/view/OSGRA/][these instructions for obtaining a doesciencegrid certificate]].
   1. [[GettingStarted#I_want_to_use_OSG_resources][Register with your VO.]] Each VO maintains a <span class="firstterm">virtual organization membership service</span> (<span class="arconym">VOMS</span>) that often has a form for end-user scientists to register. The VO may [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153][implement policies]] using [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154][groups and roles]] and extended proxies in order to differentiate access rights and priorities for different end-users. Every VO is expected to maintain its own [[VO/WebHome][VO-specific instructions on membership, usage, etc., and general information for users]].
   1. [[ReleaseDocumentation.ClientInstallationGuide][Install the OSG client software]] on your desktop.

----+++A word on Policies for resource usage

The OSG does not allocate resources as none of the resources available via the grid are "owned" by the OSG. The owners &#8212; generally large VOs and IT organisations, including national labs &#8212; fully control usage policies on their resources. 

Many VOs allow you to use resources beyond those owned by the VO. The most common policy for resource utilisation outside your own VO is "opportunistic use". The interpretation of this varies greatly. Some sites allow access to all their "spare" resources to anybody registered with a VO on OSG, others to only a small subset of resources and/or VOs. Some sites interpret "spare" to mean "until a more privileged user arrives", others guarantee a minimum wall clock time for your job once it starts.

The only reliable way to find out policy is to try to run a job.

%WARNING% A cluster is a large error amplifier!

---++ Finding Available Resources

%INCLUDE{ "FindAvailableResource" }%

---++Using the Computing Resources

OSG provides access to compute resources via =Condor-G= and to storage resources via SRM. In the following we provide simple examples for how to use these interfaces to consume resources.

---+++Consuming CPU power

OSG provides an interface at each compute site, refered to as the _Compute Element (CE)_. In principle, both =globus-job-run= and =Condor-G= can be used to submit jobs to the CE. However, as a matter of policy we require use of Condor-G gridmonitor for scalability reasons. You should expect to lose your submission privileges at OSG sites if you do not use condor-g and gridmonitor for submission of anything more than a couple of jobs to a site. (More info on condor-G [[http://www.cs.wisc.edu/condor/manual/v6.6.11/5_3Condor_G.html][6.6.11]] [http://www.cs.wisc.edu/condor/manual/v6.8.6/5_Grid_Computing.html][6.8.6])

There's a [[http://www.cs.wisc.edu/condor/condorg/goldenrules.html][golden rule]] and a [[http://www.cs.wisc.edu/condor/condorg/linux_scalability.html][linux scalability]] document that you might find useful in order to successfully submit a large number of jobs.

---++++Simple Example Script for submission to OSG

%INCLUDE{ "SampleOsgScript" }%

---++++Submitting single job using =condor-g=

%INCLUDE{"CondorSubmittingSingleJob" }%

---++++Submitting multiple jobs using condor-g

%INCLUDE{ "CondorSubmittingMultipleJobs" }%

---+++ Usage of !GlobusRSL

The Globus <span class="firstterm">Resource Specification Language</span> (RSL) provides a uniform way to communicate with various batch schedulers. Here is a list of RSL attributes for Condor and PBS from Jaime Frey:

%RED% This could use more explanation of RSL, condor and PBS and what this lets you find out.%ENDCOLOR%

---++++ Condor
|  *Attribute*  |  *Description*  | 
| condorsubmit | Allow the client to specify abitrary additional attributes to be included in the Condor submit description file. |

---++++ PBS

%INCLUDE{ "GlobusRlsPbsAttributes" }%

<!-- ++++ Example: Submitting Job to Purdue

For example, to submit a job to the [[http://www.purdue.teragrid.org/content/view/11/25/][Purdue site]], one might want to use the following syntax in the Condor submit script:

<pre class="programlisting">
globusrsl = (project=TG-XYZ12345)( condorsubmit = (Requirements 'HasCTSS == TRUE && CanReachInternet == TRUE') )
</pre>

%RED%I don't follow why all of a sudden you're using globusurl instead of condor-submit.%ENDCOLOR% -->


<!-- ++++Golden Rules for power users
%RED%In included section, say what gridmonitor is, or provide link. %BR%
Italics doesn't read as nicely as normal print. %BR%
"use srm instead of gridftp..." for moving files (or whatever it is) %BR%
What's "Hz t couple Hz rate"?  When will details come in for this?

AH ended here 4/10/07 on review
%ENDCOLOR%
%INCLUDE{ "GoldenRulesForPowerUsers" }% -->

---+++Consuming storage
<!-- %RED% UsingLocalStorage page -- linked to from below -- needs work %ENDCOLOR% -->
There are a variety of different [[ReleaseDocumentation.LocalStorageConfiguration][storage areas in OSG]]. As a general rule, you should keep the size of the files you transfer using condor-g to an absolute minimum. In fact, the ideal is a script no larger than a few tens or a hundred lines or so. The bulk of what you need for running your jobs should be staged in (e.g. as part of a <span class="acronym">DAG</span>) before the job submission via pre-WS GRAM. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAG, after the job completes using !GridFTP or SRM. If you have a choice, always prefer SRM over !GridFTP because SRM queues up transfers to avoid overloads, distributes the load across multiple gftp servers and retries after failures.

At present, there is no "one-stop shopping" for getting access to SRM. To try out SRM on a reference platform, send email to =tg-storage at opensciencegrid.org=. They can help you get started, and coordinate getting access to the SRMs across OSG.

Examples of srmcp usage are available from =srmcp -help=. 

Please note that usually the server port is 8443 for SRM, and 2811 for !GridFTP (or gsiFTP). Different ways of specifying SRM server endpoints are:
<verbatim>
srm://myhost.mydomain.edu:8443//dir1/dir2/file
srm://myhost.mydomain.edu:8443//srm/managerv1?SFN=/dir1/dir2/file
srm://myhost.mydomain.edu:8443//srm/managerv2?SFN=/dir1/dir2/file 
</verbatim>

For diagnostic purposes, using "-dbg" option with globus-url-copy, and "-debug=true" with srmcp is useful.

---++++ SRM put
<verbatim>
 srmcp file:////bin/sh srm://myhost.mydomain.edu/dir1/dir2/sh-copy
</verbatim>

---++++ SRM get
<verbatim>
 srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy file:///localdir/sh 
</verbatim>

---++++ SRM copy (SRM to SRM):
<verbatim>
 srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy srm://anotherhost.org/newdir/sh-copy 
</verbatim>

---++++ SRM copy (gsiFTP to SRM):
<verbatim>  
 srmcp gsiftp://ftphost.org//path/file srm://myhost.mydomain.edu/dir1/dir2/file
</verbatim>

---+++ globus-url-copy syntax
<span class="command">globus-url-copy</span> uses similar syntax but can only communicate with gsiftp servers

<verbatim>
globus-url-copy file:////bin/sh gsiftp://myhost.mydomain.edu/dir1/dir2/sh-copy
globus-url-copy gsiftp://myhost.mydomain.edu/dir1/dir2/file file:///$OSG_WN_TMP/local-copy
</verbatim>

---+++Condor Hold (_error_) Codes

Sometimes, jobs that you submit actually run, and sometimes they don't. When they don't, exit error codes may be useful.  If the job goes on hold (status 'H'), the Condor hold reason and hold code may be helpful.  These are visible in the output of <span class=command>condor_q -l <i>jobid</i></span>.  Hold codes are documented in the [[http://www.cs.wisc.edu/condor/manual/v6.8/2_5Submitting_Job.html#2162][Condor manual]].

See [[Troubleshooting.CondorErrors]] for more details.

---++ Troubleshooting Using the OSG
%INCLUDE{ "GridUsersGuideTroubleshootingOSG" }%
GridUsersGuideTroubleshootingOSG


<!--
&nbsp;

%BOTTOMMATTER%
 Main.LeighGrund - 16 Jun 2005<br>
 Main.FkW - 06 May 2006<br>
 Main.FkW - 14 May 2006<br>
 Main.FkW - 23 May 2006<br>
 Main.IlyaNarsky - 23 Jun 2006 <br>
 Main.ForrestChristian - 2007 Feb 
Reviewed by Main.AnneHeavey - 11 Apr 2007%BR%
 Main.AbhishekSinghRana - 25 July 2007%BR%

-->
