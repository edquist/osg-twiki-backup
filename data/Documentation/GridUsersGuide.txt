%META:TOPICINFO{author="BrianBockelman" date="1266462773" format="1.1" version="1.43"}%
%LINKCSS%

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%
%STARTINCLUDE%

---++Introduction

This TWiki page serves as entry point for documentation on how to use the OSG. As the OSG support model assumes the end-user scientists to be supported by large "Virtual Organisations" (VO)
that provide the actual user interfaces and user support, the primary target audience here are
the VO user support teams, rather than the actual scientists as end-users.  However this should also help individual or small group researchers who don't have the wherewithal to form a VO, and who therefore are welcome to join the "OSG" VO.

Compute resources on the OSG are accessed through a protocol named GRAM using a tool called =Condor-G=.  Storage resources are accessed through a protocol named SRM, and a variety of clients are used.  In this guide, we provide examples for accessing storage and compute resources.

---++Getting Access to computing resources via OSG

Getting access to computing resources via the OSG involves the following steps:

   1. Acquire an X.509 certificate from one of the [[http://vdt.cs.wisc.edu/certificate_authorities.html][supported CAs]]. As an example, you might follow [[https://twiki.grid.iu.edu/twiki/bin/view/OSGRA/][these instructions for obtaining a doesciencegrid certificate]].
   1. [[GettingStarted#I_want_to_use_OSG_resources][Register with your VO.]] Each VO maintains a <span class="firstterm">virtual organization membership service</span> (<span class="arconym">VOMS</span>) that often has a form for end-user scientists to register. The VO may [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153][implement policies]] using [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154][groups and roles]] and extended proxies in order to differentiate access rights and priorities for different end-users. Every VO is expected to maintain its own [[VO/WebHome][VO-specific instructions on membership, usage, etc., and general information for users]].
   1. [[ReleaseDocumentation.ClientInstallationGuide][Install the OSG client software]] on your desktop.

----+++A word on Policies for resource usage

The OSG does not allocate resources as none of the resources available via the grid are "owned" by the OSG. The owners &#8212; generally large VOs and IT organisations, including national labs &#8212; fully control usage policies on their resources. 

Many VOs allow you to use resources beyond those owned by the VO. The most common policy for resource utilisation outside your own VO is "opportunistic use". The interpretation of this varies greatly. Some sites allow access to all their "spare" resources to anybody registered with a VO on OSG, others to only a small subset of resources and/or VOs. Some sites interpret "spare" to mean "until a more privileged user arrives", others guarantee a minimum wall clock time for your job once it starts.

The only reliable way to find out policy is to try to run a job.

%WARNING% A cluster is a large error amplifier!

---++ Finding Available Resources

%INCLUDE{ "FindAvailableResource" }%

---++Using the Computing Resources

OSG provides an interface at each compute site, refered to as the _Compute Element (CE)_. In principle, both =globus-job-run= and =Condor-G= can be used to submit jobs to the CE. As a matter of policy, the large-scale use of globus-job-run is banned - use Condor-G to submit your jobs.  You may be banned at some OSG sites if you do not use Condor-G or a similar submission tool.  More info on Condor-G [[http://www.cs.wisc.edu/condor/manual/v7.3/5_3Grid_Universe.html#SECTION00632000000000000000][can be found in the extensive Condor manual]].

There's a "[[http://www.cs.wisc.edu/condor/condorg/goldenrules.html][golden rules]]" document you might find useful for submitting a large number of jobs.

---++++Submitting single job using =Condor-G=

%INCLUDE{"CondorSubmittingSingleJob" }%

---++++Submitting multiple jobs using condor-g

%INCLUDE{ "CondorSubmittingMultipleJobs" }%

---+++ Usage of !GlobusRSL

The Globus <span class="firstterm">Resource Specification Language</span> (RSL) provides a uniform way to communicate with various batch schedulers. Here is a list of RSL attributes for Condor and PBS:

%RED% This could use more explanation of RSL, condor and PBS and what this lets you find out.%ENDCOLOR%

---++++ Condor
|  *Attribute*  |  *Description*  | 
| condorsubmit | Allow the client to specify abitrary additional attributes to be included in the Condor submit description file. |

---++++ PBS

%INCLUDE{ "GlobusRlsPbsAttributes" }%

---+++Consuming storage
There are a variety of different [[ReleaseDocumentation.LocalStorageConfiguration][storage areas in OSG]]. As a general rule, you should keep the size of the files you transfer using condor-g to an absolute minimum. In fact, the ideal is a script no larger than a few tens or a hundred lines or so. The bulk of what you need for running your jobs should be staged in (e.g. as part of a <span class="acronym">DAG</span>) before the job submission via pre-WS GRAM. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAG, after the job completes using !GridFTP or SRM. If you have a choice, always prefer SRM over !GridFTP because SRM queues up transfers to avoid overloads, distributes the load across multiple gftp servers and retries after failures.

At present, there is no "one-stop shopping" for getting access to SRM. To try out SRM on a reference platform, send email to =tg-storage at opensciencegrid.org=. They can help you get started, and coordinate getting access to the SRMs across OSG.

Examples of srmcp usage are available from =srmcp -help=. 

Please note that usually the server port is 8443 for SRM, and 2811 for !GridFTP (or gsiFTP). Different ways of specifying SRM server endpoints are:
<verbatim>
srm://myhost.mydomain.edu:8443//dir1/dir2/file
srm://myhost.mydomain.edu:8443//srm/managerv1?SFN=/dir1/dir2/file
srm://myhost.mydomain.edu:8443//srm/managerv2?SFN=/dir1/dir2/file 
</verbatim>

For diagnostic purposes, using "-dbg" option with globus-url-copy, and "-debug=true" with srmcp is useful.

---++++ SRM put
<verbatim>
 srmcp file:////bin/sh srm://myhost.mydomain.edu/dir1/dir2/sh-copy
</verbatim>

---++++ SRM get
<verbatim>
 srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy file:///localdir/sh 
</verbatim>

---++++ SRM copy (SRM to SRM):
<verbatim>
 srmcp srm://myhost.mydomain.edu/dir1/dir2/sh-copy srm://anotherhost.org/newdir/sh-copy 
</verbatim>

---++++ SRM copy (gsiFTP to SRM):
<verbatim>  
 srmcp gsiftp://ftphost.org//path/file srm://myhost.mydomain.edu/dir1/dir2/file
</verbatim>

---+++ globus-url-copy syntax
<span class="command">globus-url-copy</span> uses similar syntax but can only communicate with gsiftp servers

<verbatim>
globus-url-copy file:////bin/sh gsiftp://myhost.mydomain.edu/dir1/dir2/sh-copy
globus-url-copy gsiftp://myhost.mydomain.edu/dir1/dir2/file file:///$OSG_WN_TMP/local-copy
</verbatim>

---+++Condor Hold (_error_) Codes

Sometimes, jobs that you submit actually run, and sometimes they don't. When they don't, exit error codes may be useful.  If the job goes on hold (status 'H'), the Condor hold reason and hold code may be helpful.  These are visible in the output of <span class=command>condor_q -l <i>jobid</i></span>.  Hold codes are documented in the [[http://www.cs.wisc.edu/condor/manual/v6.8/2_5Submitting_Job.html#2162][Condor manual]].

See [[Troubleshooting.CondorErrors]] for more details.

---++ Troubleshooting Using the OSG
%INCLUDE{ "GridUsersGuideTroubleshootingOSG" }%
GridUsersGuideTroubleshootingOSG
