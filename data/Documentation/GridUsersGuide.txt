%META:TOPICINFO{author="IlyaNarsky" date="1150750437" format="1.0" version="1.10"}%
<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


<!--  * Set ALLOWTOPICCHANGE = Main.DocEditGroup !-->
d19 3
---++Introduction

This TWiki page serves as entry point for documentation on how to use the OSG.
As the OSG support model assumes the end-user scientists to be supported by large "Virtual Organisations" (VO)
that provide the actual user interfaces and user support, the primary target audience here are
the VO user support teams, rather than the actual scientists as end-users.

---++Getting Access to computing resources via OSG

Getting access to computing resources via the OSG involves the following steps:

	* acquire an X509 certificate from one of the <a href="http://vdt.cs.wisc.edu/releases/1.3.10/certificate_authorities.html">supported CAs</a>. As an example, you might follow <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/percertreq.php">these instructions</a> for obtaining a doesciencegrid certificate.
	* Register with your VO. Each VO maintains a VOMS, and probably a VOMRS based interface for end-user scientists to register. The VO may <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153">implement policies</a> using <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154">groups and roles</a> and extended proxies in order to differentiate access rights and priorities for different end-users. The OSG does support single PIs without the backing of large VOs via the OSG VO. Details for how to join the OSG VO may be found <a href="http://blabla.org/">here</a> (url for this still missing because osg is still under construction).
	* Install an <a href="http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/ClientInstallationGuide">OSG client</a> on your desktop.
   1. Acquire an X.509 certificate from one of the [[http://vdt.cs.wisc.edu/releases/1.3.10/certificate_authorities.html][supported CAs]]. As an example, you might follow [[http://www.grid.iu.edu/osg-ra/PersonalRequest.php][these instructions]] for obtaining a doesciencegrid certificate.
   2. [[Documentation.JoinToUseResources][Register with your VO.]] Each VO maintains a <span class="firstterm">virtual organization membership service</span> (<span class="arconym">VOMS</span>) that often has a form for end-user scientists to register. The VO may [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=153][implement policies]] using [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=154][groups and roles]] and extended proxies in order to differentiate access rights and priorities for different end-users. Every VO is expected to maintain its own [[https://goc.grid.iu.edu/othertools/registration/osgweb/pullvo.php][VO-specific instructions]] on membership, usage, etc. and [[https://goc.grid.iu.edu/othertools/registration/osgweb/pullvo.php][general information for users]].
   3. Install the [[ReleaseDocumentation.ClientInstallationGuide][OSG client software]] on your desktop.
The OSG presently does not have any allocations for use of resources. In fact, none of the resources available via the OSG are "owned" by the OSG. The owners, generally large VOs, and IT organisations, including National Labs, fully control usage policies on their resources. The most common policy for resource utilisation beyond the resources that your VO owns is "opportunistic use". The interpretation of what this means varies greatly. Some sites allow access to all their "spare" resources to anybody registered with a VO on OSG, others only to a small subset of resources and/or VOs. Some sites interpret "spare" to mean "until a more privileged user arrives", others guarantee a minimum wall clock time for your job once it starts.
----+++A word on Policies for resource usage
There is no policy schema defined for OSG at this point. It is thus not possible to query a database to determine site policies. Site policy is presently only available to the extend it is specified on the site policy web page linked in via gridcat. The only reliable way to find out policy is to try to run a job.
The OSG does not allocate resources as none of the resources available via the grid are "owned" by the OSG. The owners &#8212; generally large VOs and IT organisations, including national labs &#8212; fully control usage policies on their resources. 

%WARNING% A cluster is a large error amplifier!
All OSG sites are registered with <a href="http://osg-cat.grid.iu.edu/">gridcat</a>. Gridcat maintains information on every site in a DB, and verifies this information regularly. Sites that fail basic functionality tests are red, sites that pass all basic tests are green, and inactive sites are grey. 
---++Finding the list of available sites
In addition, more detailed information, e.g. network architecture (Open, NAT, closed), can be obtained via ldap queries to the site. For example, if you want to find out if worker nodes at CIT_CMS_T2 have outgoing connections, you can query 
All OSG sites are registered with [[http://osg-cat.grid.iu.edu/][GridCat]]. !GridCat maintains information on every site in a DB, and verifies this information regularly. Sites that fail basic functionality tests are red, sites that pass all basic tests are green, and inactive sites are grey. 
<verbatim>
ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "(GlueHostNetworkAdapterOutboundIP=*)" GlueHostNetworkAdapterOutboundIP
</verbatim>
$ <b>ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "(GlueHostNetworkAdapterOutboundIP=*)" GlueHostNetworkAdapterOutboundIP</b>
This returns 
GlueHostNetworkAdapterOutboundIP: TRUE
<verbatim>
GlueHostNetworkAdapterOutboundIP: TRUE
</verbatim>

To find out whether or not your VO is supported by a specific site, you might find <a href="http://osg.ccr.buffalo.edu/statistics/acdc/vo_support_matrix.php">this</a> useful. While this VO support matrix is still under development, your only other way to determine if you are allowed to access a site is to try it out.

You can get the same info from ldap, for example:
If you want to find out whether or not a specific site supports your VO, check  [[http://vors.grid.iu.edu/cgi-bin/index.cgi?region=0&VO=0&res=0&grid=1][VORS]]. 
<verbatim>
ldapsearch -LLL -h cit-gatekeeper.ultralight.org -p 2135 -x -b mds-vo-name=CIT_CMS_T2,o=grid "GlueCEName=cdf"
</verbatim>
<pre class="screen">
tells you if VO cdf is supported at CIT_CMS_T2. If it is not, the query returns an empty string.
</pre>

If supported, it will return a series of entires. If not, it returns an empty string.
The OSG 0.4.1 release provides access to compute resources via condor-g and to storage resources via either gridftp or SRM. In the following we provide simple examples for how to use these interfaces to consume resources.
---++Using the Computing Resources

OSG release 0.4.1 provides access to compute resources via =condor-g= and to storage resources via either gridFTP or SRM. In the following we provide simple examples for how to use these interfaces to consume resources.
The OSG 0.4.1 release provide pre-WS GRAM as interface at each compute site. We refer to this interface as the "Compute Element", or CE. In principle, both globus-job-run and condor-g can be used to submit jobs to the CE. However, as a matter of policy we require use of the condor-g gridmonitor for scalability reasons. You should expect to loose your submission privileges at OSG sites if you do not use condor-g and gridmonitor for submission of anything more than a couple jobs to a site.
---+++Consuming CPU power
There's a <a href="http://www.cs.wisc.edu/condor/condorg/goldenrules.html">golden rule</a> and a <a href="http://www.cs.wisc.edu/condor/condorg/linux_scalability.html">linux scalability</a> document that you might find useful in order to successfully submit a large number of jobs. In addition, we list some "do's and don't's below.
The OSG 0.4.1 release provides the pre-WS GRAM as the interface at each compute site. We refer to this interface as the _Compute Element_ (CE). In principle, both =globus-job-run= and =condor-g= can be used to submit jobs to the CE. However, as a matter of policy we require use of the condor-g gridmonitor for scalability reasons. You should expect to lose your submission privileges at OSG sites if you do not use condor-g and gridmonitor for submission of anything more than a couple of jobs to a site. ([[http://www.cs.wisc.edu/condor/manual/v6.6/5_3Condor_G.html][More info on condor-g]])

d110 1
<verbatim>
#!/bin/sh
There's a [[http://www.cs.wisc.edu/condor/condorg/goldenrules.html][golden rule]] and a [[http://www.cs.wisc.edu/condor/condorg/linux_scalability.html][linux scalability]] document that you might find useful in order to successfully submit a large number of jobs.
# See what's there before you do anything:
printenv
export startdir=`pwd`
echo $startdir
whoami
---++++Simple Example Script for submission to OSG
# Establish your working environment, including adding srmcp into your path:
source $OSG_GRID/setup.sh
%INCLUDE{ "SampleOsgScript" }%
# See what's there after you've sourced your environment:
printenv
---++++Submitting single job using =condor-g=
# Make sure you switch to a local disk as your wrkdir.
# Unfortunately, not all sites start your job in your own directory, on a local disk.
# You therefore have to take this in your own hands.
export wrkdir=${OSG_WN_TMP}/`date +%a%b%e%H%M%S%Z%Y`
mkdir $wrkdir
cd $wrkdir

# To learn more about use of storage spaces in OSG, and the associated envvar's that point to it, read:
#http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/LocalStorageUse

echo "This is my output" > output.log

#Note: If you wanted condor-g to transfer a file back out that you create in your working
# directory at this point then you would have to first move it back into the startdir !
mv output.log $startdir/.

# don't forget to clean up after yourself
cd $startdir
rm -rf $wrkdir

#sleep a little so you can use condor tools to check that this test job is running
sleep 1000

</verbatim>
%ENDMore%

---++++Submitting one job using condor-g

<verbatim>
echo "universe=globus" > test.cmd
echo "GlobusScheduler=osg-gw-2.t2.ucsd.edu:/jobmanager-condor" >> test.cmd
echo "executable=/bin/env" >> test.cmd
echo "stream_output = False" >> test.cmd
echo "stream_error  = False" >> test.cmd
echo "output = test1.out" >> test.cmd
echo "log	 = test1.log" >> test.cmd
echo "log    = test1.log" >> test.cmd
echo "queue" >> test.cmd
</verbatim>

Then submit test.cmd after you have sourced the environment for the OSG client, and have
obtained a proxy using voms-proxy-init to the voms of your VO. This might look something like this:

<verbatim>
source $VDT_LOCATION/setup.sh 
voms-proxy-init --voms cms:/cms/uscms/Role=cmsuser 
condor_submit test.cmd
sleep 300
condor_q
</verbatim>

Grid middleware has notoriously large latency as it is meant to provide high throughput computing, not interactive computing, or even low latency computing tools. The sleep 300 indicates that it might take several minutes before anything starts, even if the site you are submitting to has plenty of free batch slots.
d144 1

Now let's see how we would submit four jobs at once. Our =test.cmd= file now looks like this:
%INCLUDE{"CondorSubmittingSingleJob" }%
Now we replace test.cmd with the following:
d178 1
<verbatim>
  universe=globus
  GlobusScheduler=osg-gw-2.t2.ucsd.edu:/jobmanager-condor
  executable=/bin/ls
  stream_output = False
  stream_error  = False
---++++Submitting multiple jobs using condor-g
  arguments=/tmp
  log	 = test1.log
  error  = test1.err
  log    = test1.log
  queue
%INCLUDE{ "CondorSubmittingMultipleJobs" }%
  arguments=/usr
  log	 = test2.log
  error  = test2.err
  log    = test2.log
  queue 
---+++ Usage of !GlobusRSL
  arguments=/var
  log	 = test3.log
  error  = test3.err
  log    = test3.log
  queue
---++++ Condor
  arguments=/etc
  log	 = test4.log
  error  = test4.err
  log    = test4.log
  queue
</verbatim>
| condorsubmit | Allow the client to specify abitrary additional attributes to be included in the Condor submit description file. |
For example, to submit a job to the [[http://www.purdue.teragrid.org/content/view/11/25/][Purdue site]], one might want to use the following syntax in the Condor submit script:
</verbatim>
	* Always use gridmonitor
	* Never use streaming of stdout/stderr because it disables the gridmonitor. A number of sites do not allow this anyway. If you need to debug running jobs, consider <a href="http://jobmon.sourceforge.net/">JobMon</a> instead. It is much more scalable, puts less load on the site infrastructure, and is more feature rich.
	* Avoid jobmanager-fork
	* Minimise the number and size of files you transfer in/out of a site using condor-g. Use stage in/out to the OSG storage as described below instead. Use SRM instead of gridftp whenever you can.
	* Avoid very short and very long running jobs. Ideal wall clock time for a job is a few hours.
	* Avoid network mounted disks for IO operations as best as you can. Always remember: a cluster is a large error amplifier! If you have >100 jobs running in parallel even a modest amount of IO to a network mounted disk can overwhelm the server, and can cause a large mess on the site you are running!
	* Avoid a steady stream of job submissions at arund the Hz t couple Hz rate. You are better off submitting, say 10 at once, and then take a 20 seconds break, than space them equally by 2 seconds. We found that the gridmonitor is unable to handle a steady stream of submits at 0.5 Hz, while it deals with the other pattern quite well. (more details on this sort of stuff later)
   * Use SRM instead of gridftp whenever you can.
If you have any additional questions you run into as you scale up your operations on OSG, please feel free to ask other power users for advice by sending email to osg-users at opensciencegrid.org .
   * Never use streaming of stdout/stderr because it disables the gridmonitor. A number of sites do not allow this anyway. If you need to debug running jobs, consider <a href="http://jobmon.sourceforge.net/">JobMon</a> instead. It is much more scalable, puts less load on the site infrastructure, and is more feature rich.
   * Avoid jobmanager-fork
   * Minimise the number and size of files you transfer in/out of a site using condor-g. Use stage in/out to the OSG storage as described below instead. Use SRM instead of gridftp whenever you can.
There are a variety of different storage areas in OSG. Their use is documented <a href="http://osg.ivdgl.org/twiki/bin/view/ReleaseDocumentation/LocalStorageUse">here</a>. As a general rule, you should keep the size of the files you transfer using condor-g to an absolute minimum. In fact, ideal is a script no larger than a few tens or hundred lines or so. The bulk of what you need for running your jobs should be staged in, e.g. as part of a DAG, before the job submission via pre-WS GRAM. Similarly, output files, and any significant logfiles should be staged out, e.g. as part of a DAQ, after the job completes using gridftp or, even better, SRM.
   * Avoid network mounted disks for IO operations as best as you can. Always remember: a cluster is a large error amplifier! If you have >100 jobs running in parallel even a modest amount of IO to a network mounted disk can overwhelm the server, and can cause a large mess on the site you are running!
   * Avoid a steady stream of job submissions at arund the Hz t couple Hz rate. You are better off submitting, say 10 at once, and then take a 20 seconds break, than space them equally by 2 seconds. We found that the gridmonitor is unable to handle a steady stream of submits at 0.5 Hz, while it deals with the other pattern quite well. (more details on this sort of stuff later)
---++++Golden Rules for power users
</verbatim>

<b>globus-url-copy</b> uses similar syntax but can only communicate with gsiftp servers:

---+++ globus-url-copy syntax
<span class="command">globus-url-copy</span> uses similar syntax but can only communicate with gsiftp servers

<verbatim>
globus-url-copy file:////bin/sh gsiftp://myhost.mydomain.edu/dir1/dir2/sh-copy
---+++Condor error codes
</verbatim>
Of course, when you submit jobs, sometimes they run and sometimes they don't. If they don't, exit error codes may be useful. Condor error codes unfortunately are not documented anywhere consistently. Here is a list kindly extracted by Jaime Frey from 6.7 Condor code:
---+++Condor Hold (_error_) Codes
<verbatim>
//There may still be some lingering cases that result in this
//unspecified hold code.  Hopefully they will be eliminated soon.
const int CONDOR_HOLD_CODE_JobPolicy	= 3;
Sometimes, jobs that you submit actually run, and sometimes they don't. When they don't, exit error codes may be useful. Condor doesn't have error codes _per se_ but does have _hold codes_ that serve the purpose. Unfortunately, the condor teams have not consistently documented them. Here is a list kindly extracted by Jaime Frey from 6.7 Condor code:
//User put the job on hold with condor_hold
const int CONDOR_HOLD_CODE_UserRequest = 1;

//Globus reported an error.  The subcode is the GRAM error number.
const int CONDOR_HOLD_CODE_JobPolicyUndefined	= 5;
---++ Troubleshooting
//The periodic hold expression evaluated to true
const int CONDOR_HOLD_CODE_JobPolicy   = 3;
In this section we try to provide some hints about when and to whom you should report errors, and which errors you can solve yourself.
//The credentials for the job (e.g. X509 proxy file) are invalid.
const int CONDOR_HOLD_CODE_CorruptedCredential = 4;
You should report OSG-related errors to your [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support"][VO support center]], who will then triage your problems, and forward your problems to the <span class="firstterm">Grid Operations Center</span> )(<span class="acronym">GOC</span>), if warranted. 
//A job policy expression (such as PeriodicHold) evaluated to UNDEFINED.
const int CONDOR_HOLD_CODE_JobPolicyUndefined   = 5;
As some of you may actually be also the support center people of your VO, here are some pointers.
//The condor_starter failed to start the executable.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_FailedToCreateProcess = 6;
   1. You arrive at the worker node but have no grid proxy, are missing srmcp in your path, or are missing some other client tools that should be part of the OSG WN client installation.
//The standard output file for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenOutput = 7;
   1. If a site doesn't have a functioning LDAP service, then it deserves a ticket.
//The standard input file for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenInput = 8;

//The standard output stream for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenOutputStream = 9;
Unfortunately, OSG rules do not require you to have access to any site in particular. It's up to the site to support you and your VO. Some sites support some VOs without supporting all of their users. 
//The standard input stream for the job could not be opened.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UnableToOpenInputStream = 10;
At present, the only way to understand the intentions of a site is to query its LDAP and see if your VO shows up. The best way to understand if authentication should work for you personally is to check the VO support matrix, and look for your DN among the DNs supported at the site. 
//An internal Condor protocol error was encountered when transferring
files.
const int CONDOR_HOLD_CODE_InvalidTransferAck = 11;

//The condor_starter failed to download input files.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_DownloadFileError = 12;

//The condor_starter failed to upload output files.
//The subcode will contain the unix errno.
const int CONDOR_HOLD_CODE_UploadFileError = 13;

//The initial working directory of the job cannot be accessed.
The operations model of OSG is such that all complaints you have should go to your <a href="">VO support center</a> (Missing a list of VO support centers here). Your VO support center will then triage your problems, and forward your problems to the GOC if warranted. 
const int CONDOR_HOLD_CODE_IwdError = 14;
As some of you may actually be also the support center people of your VO, here some pointers.
</verbatim>

---++Some Guidance on what to do when things don't work
	* Submission to site X used to work but now no longer works. Check if the site has gone red on gridcat. If it hasn't, complain so that a trouble ticket gets generated for that site.
	* A standard script like the examples on this page works when submitted to jobmanager-fork but fails on the worker node for whatever reason.
	* You arrive at the worker node but have no grid proxy, are missing srmcp in your path, or are missing some other client tools that should be part of the OSG WN client installation.
	* You arrive on the worker node and find OS, network architecture, or some other characteristics that does not agree with the supposed characteristics of the site as advertised via the GIP.

As some of you may actually be also the support center people of your VO, here are some pointers.

	* You can't authenticate at Site X. Unfortunately, OSG rules do not require you to have access to any site in particular. It's up to the site to support you and your VO. Some sites support some VOs without supporting all of their users. At present, the only way to understand the intentions of a site is to query its ldap, and see if your VO shows up in there. The best way to understand if authentication should work for you personally is to check the VO support matrix, and look for your DN among the DNs supported at Site X. If you find that an unreasonable number of sites don't let you in you should send the list of sites where you failed to osg-users at opensciencegrid.org and maybe we can help you gain access to more sites.
	* You find that perl, python, wget, etc. are either not installed at all, or not installed with the version you'd expect given the OS version that the sit is supposed to have. Unfortunately, you are out of luck in that case. There is no definition for a minimal Linux distro that would clearly specify what should exist on a worker node. If you need something and find it missing you may have to either give up on that site or install it yourself into $OSG_APP .
   * Submission to site X used to work but now no longer works. Check if the site has gone red on gridcat. If it hasn't, complain so that a trouble ticket gets generated for that site.
---++Stuff that's still missing on this page

What do we need to document here?

	* Some info on how to use GlobusRSL, and info on what's needed where and why.

Table with twiki pages for all the VOs who maintain twiki pages on this site.

I think that's pretty much it.

   * A standard script like the examples on this page works when submitted to jobmanager-fork but fails on the worker node for whatever reason.
   * You arrive at the worker node but have no grid proxy, are missing srmcp in your path, or are missing some other client tools that should be part of the OSG WN client installation.
   * You arrive on the worker node and find OS, network architecture, or some other characteristics that does not agree with the supposed characteristics of the site as advertised via the GIP. By extension, if a site doesn't have a functioning ldap service then it deserves a ticket.


   * You can't authenticate at Site X. Unfortunately, OSG rules do not require you to have access to any site in particular. It's up to the site to support you and your VO. Some sites support some VOs without supporting all of their users. At present, the only way to understand the intentions of a site is to query its ldap, and see if your VO shows up in there. The best way to understand if authentication should work for you personally is to check the VO support matrix, and look for your DN among the DNs supported at Site X. If you find that an unreasonable number of sites don't let you in you should send the list of sites where you failed to osg-users at opensciencegrid.org and maybe we can help you gain access to more sites.
   * You find that perl, python, wget, etc. are either not installed at all, or not installed with the version you'd expect given the OS version that the sit is supposed to have. Unfortunately, you are out of luck in that case. There is no definition for a minimal Linux distro that would clearly specify what should exist on a worker node. If you need something and find it missing you may have to either give up on that site or install it yourself into $OSG_APP .
d170 1

&nbsp;

%BOTTOMMATTER%
-- Main.LeighGrund - 16 Jun 2005<br>
-- Main.IlyaNarsky - 23 Jun 2006
-- Main.FkW - 06 May 2006<br>
-- Main.FkW - 14 May 2006<br>
-- Main.FkW - 23 May 2006<br>
-- Main.ForrestChristian - 2007 Feb %BR%

