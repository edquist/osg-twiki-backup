%META:TOPICINFO{author="MatsRynge" date="1265307313" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="WebHome"}%
%LINKCSS%

<!-- This is the default OSG documentation template. Please modify it in -->
<!-- the sections indicated to create your topic.                        --> 

<!-- By default the title is the WikiWord used to create this topic. If  -->
<!-- you want to modify it to something more meaningful, just replace    -->
<!-- %TOPIC% below with i.e "My Topic".                                  -->

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%

%STARTINCLUDE%
%EDITTHIS%
---+ A Comparison of OSG Job Submission/Resource Provisioning Methods
---++Introduction (Dan, John, others)
When providing job submission services to its customers and constituents, VOs have a variety of technology choices it can make depending on the types and quantities of the jobs managed. To help OSG better understand the tradeoffs in technology, and therefore be able to better advise its VOs, this paper is designed to compare and contrast the three main technologies that are used for job routing and distribution on the OSG: the Engage OSG Matchmaker, Glide-in WMS, and PANDA. Since each of these services relies on an underlying information architecture we include a brief section on the Information services that OSG offers.
---++ High Level Technology Comparison (Everyone, 1-2 pages) - To be completed after the sections below
---++ Functionality Comparison Table (Everyone)
---++A Very Brief Overview of OSG Information Sources (Dan, others 1 paragraph each)
---+++BDII
---+++RESS
---+++OIM
---+++GIP
---+++!MyOSG
---+++Static Configuration Files
---++Job Submission Methods (Mats, Igor, Maxim)
In this section we want to provide first a high level architecture for each of the technologies below that includes the design philosophy and then an implementation and usage description that addresses answers to each of the following questions:
   * Job submitter perspectives:
      * What are some typical use cases?
      * How many users/jobs can be supported?
      * To what degree is data movement synchronous or asynchronous both on the submit side as well as when the job completes.
      * How does the technology integrate with data management?
      * What is the overhead on a single job submission?
      * Can the infrastructure support short jobs?
      * What types of job patterns are supported? (Complex workflows?)
      * Is Resource Provisioning supported? If so how?
      * What types of matchmakers are used? Which OSG information sources do they utilize?
      * How elaborate is the matchmaking? (can users specify e.g. hardware type, or s/w packages?)
      * How does the system respond to “bad” CEs, sites, or environments?
   * Administrative perspectives:
      * How does the system scale? (What is important, cpu, memory, network bandwidth)
      * How much hardware is required for a “best practices” deployment?
      * How many hosts are required to be set up and maintained?
      * Can a “reasonable” admin type person set this up on his/her own? (Has this been setup by folks other than the developer?)
      * To what degree can it be setup without requiring root access.
---+++Engage OSG Matchmaker (Mats, ~1-2 pages)
---++++ High Level Architecture and Design Consideration

The OSG MatchMaker was created to give small to medium sized VOs a simple yet powerful submit interface to OSG. It is designed to retrieve VO specific site information from ReSS, verify and maintain the sites by configurable verification/maintenance jobs. The site information is then inserted into a local Condor install so that Condor can match jobs with the resources.  OSGMM then monitors jobs in the system to maintain pending/running counts and a moving window of job success/failure rates for each site. This is used to back off from sites if they break or becomes busy with the resouce owners' jobs. The site status information can be queried and imported by other systems such as Pegasus and Swift.


---++++ Usage Description (how users use the technology & use cases)

OSG MatchMaker only provides/maintains site information in a Condor-G system. In order to run jobs, the user has to write Condor job descriptions which can be non-trivial for novice users. Because of this most users will not be able to write jobs from scratch, but have to base their jobs on some existing job example or rely on Engagement staff to help them get started. The Condor job description contains a requirements line which enables the user to be very specific about where the jobs should end up. Possible requirements include but not limited to OS, architecture, network setup, software and the availability of certain datasets.

OSGMM does not provide a mechanism for data transfers. It is up to the user to do the necessary data transfers in for example a job wrapper script, and most of the time this ends up being simple globus-url-copy commands to retrieve inputs and push back outputs back to the submit host.

Because the OSGMM system ends up using Condor-G to submit and manage the jobs, there is a certain overhead for each job (combination of Condor-G/GRAM/LRM overheads) so short jobs (< 1 hour wall time) are not recommended.

Most users end up with using Condor DAGMan to create a workflow of their jobs. DAGMan also provides some important mechanisms such as job retries and pre/post script for the jobs.



---++++ implementation, Deployment, and Management Description


OSGMM was designed to be easy to deploy and support at the VO/university/lab submit level. It is shipped with VDT and can be installed with the OSG client software stack with just one additional pacman command. OSGMM should be installed as root, but will run as a non-privileged user. OSGMM requires about 1 GB RAM (used to track jobs in the system). One instance will serve all users on that submit host, using Condor's fair-share algorithm to handle jobs from multiple users.

Each VO can have a central OSGMM instance (pulling ReSS for information) to run verification/maintenance jobs and then have other instances of OSGMM  (pulling the VO OSGMM instance for information including site status) running on the submit hosts. OSGMM scales up to about 5,000 jobs per submit host.


---+++Glide-in WMS 

-- Main.ParagMhashilkar - 03 Feb 2010

In Grid computing, the computing resources are distributed over many independent sites with only a thin layer of Grid middleware shared between them. This deployment model is very convenient for computing resource providers. They can continue to operate the local distributed resources according to local preferences and expertise, integrating them easily with other, non-Grid resources. Moreover, the decentral­ized nature of the Grid allows for easy scalability as one just needs to split a site into multiple logical pieces if scalability becomes an issue.

However, the Grid deployment model introduces several problems for the users of the system. The three major problems being -
   * The complexity of job scheduling in the Grid environment
   * The non-uniformity of compute resources
   * The lack of good end-to-end job monitoring.

glideinWMS solves these problems by providing a simple way to access the Grid resources. It implements the pilot factory also called the glidein factory (GF) and uses Condor as a user job WMS.

<img src="%ATTACHURL%/glideinWMS_at_a_glance_medium.png" alt="glideinWMS at a glance"> 

Figure above shows a typical deployment of the glideinWMS system. It consists of two Condor pools,
   * User pool (Green color) used by the users for their job management
   * !GlideinWMS pool (Aqua color) used by the glideinWMS system to manage glideins. 

VO Frontend periodically queries user pool to check the status of user job queues. Based on the job queues, it instructs the glidein factory (GF) to submit glideins to run on Grid resource(s). A glidein is a properly configured Condor startd submitted as a Grid job. glideinWMS extensively uses Condor classad mechanism as a means of communication between its services. Once a glidein starts on a worker node, it will join the user Condor pool, making the obtained Grid-batch slot as a slot in the Condor pool. At this point, a regular Condor job can start there as if it is a dedicated resource. glideinWMS does not provide GUI interface for the users to submit their jobs, instead users use Condor client tools like condor_submit to submit their jobs. The user interfaces with the Condor batch system, he/she is shielded from the problems mentioned above that are introduced by the Grid deployment. Based on the time slot allocated for glidein to run, this glidein created batch slot can run more than one user jobs before relinquishing its claim over the grid resource. Thus, glideinWMS supports resource provisioning. This mechanism is quite effective in running short jobs using glideinWMS. Since a single glidein can run multiple short jobs, the effective wait time/overhead of running several short jobs this way is equivalent to running a single grid job. The resource provisioning feature can also be used to provide a guaranteed time slot for long running jobs. glideinWMS also provides users with the capability of selecting resources based on resource characteristics, like, hardware type, OS, packages installed on the worker node and any custom information that is required for the user jobs to run. glideinWMS can be configured to run pluggable scripts as the glidein bootstraps. This is useful in advertising desired resource information. Thus the combination of glidein’s bootstrapping scripts and pluggable scripts can identify potential problems with the resource before the user job could start on the resource. This makes glideinWMS more resilient to bad CE/resources and the environment on the resource.

The main disadvantage of glideinWMS like any other pilot-WMSes is the increased resource utilization of a WMS. A push WMS only needs to make a site-selection decision and hand the user job to the remote Grid site. A pilot-WMS, instead, must handle pilot jobs, user jobs and the handling and monitoring of the resources provided by the pilot jobs. 

glideinWMS is used by in OSG and LCG by several VOs like CMS, CDF, !DZero, !IceCUBE and GPN and by experimental High Energy physics groups like Minos, Minerva and NOVA. glideinWMS supports several workflows supported by Condor from a simple job submission to complex DAGs. glideinWMS can query the information systems like !ReSS and BDII to get the list of available grid resources. Current version of glideinWMS does not have any support for data management by the glideinWMS system itself. However, this is less of a short coming since most of the VOs have their own data management system which can be easily used by the user jobs run via glideinWMS.

glideinWMS is quite scalable. Although, all the glideinWMS components can be collocated on a single host for smaller use case, we recommend following services per host for larger use case -
glidein factory collocated with WMS Condor Pool
   * User Condor Pool Collector
   * User Schedd
   * VO Frontend

Table below shows the scalability figures for the glideinWMS system tested so far.

|    *Criteria*    |    *Design goal*    |    *Achieved so far*    |
| Total number of user jobs in the queue at a given time | 100k | 200k |
| Number of glideins in the system at any given time | 10k | ~26k |
| Number of running jobs per schedd at any given time | 10k | ~23k |
| Grid sites handled | ~100 | ~100 |

Most of the glideinWMS services do not need to be run with root privileges. HTTPD service on the VO frontend and Glidein factory node is typically installed and run as root user. Also, for security reasons, user Schedd(s) for non portal installation should run as root. Since there are several services to be installed and configured, the deployment model needs to be thought out based on the use case. Also, an administrator installing the services needs to be familiar with Condor Batch System and basic GSI concepts. If the gldiein factory is being installed, knowledge of the Grid (OSG, EGEE, Nordugrid) is also needed.

It should be noted that UCSD is hosting a glidein factory that smaller VO can use to start using the glideinWMS without having any knowledge of the Grid. 

Moreover, the glideinWMS team is actively working to make the installation process as easy as possible.

*glideinWMS Home Page:* [[http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/][glideinWMS Home]]

---+++PANDA
---++++ High Level Architecture and Design Considerations

!PandDA stands for Production and Distributed Analysis. Principal features of its design were initially driven by operational requirements of Atlas experiment at LHC, however most of these are directly applicable to general OSG use. They are:
   * Use of pilot jobs for acquisition of processing resources. Workload jobs are assigned to successfully activated and validated pilots based on !PanDA-managed brokerage criteria. This 'late binding' of workload jobs to processing slots prevents latencies and failure modes in slot acquisition from impacting the jobs, and maximizes the flexibility of job allocation to resources based on the dynamic status of processing facilities and job priorities. The pilot is also a principal 'insulation layer' for Panda, encapsulating the complex heterogeneous environments and interfaces of the grids and facilities on which Panda operates.
   * Simple client interface must allow easy integration with diverse front ends for job submission to !PanDA.
   * System-wide site/queue information database recording static and dynamic information used throughout !PanDA to configure and control system behavior from the 'cloud' (region) level down to the individual queue level. It is used by pilots to configure themselves appropriately for the queue they land on; by Panda brokerage for decisions based on cloud and site attributes and status; and by the pilot scheduler to configure pilot job submission appropriately for the target queue.
   * Coherent and comprehensible system view afforded to users, and to Panda's own job brokerage system, through a system-wide job database that records comprehensive static and dynamic information on all jobs in the system. To users and to Panda itself, the job database appears essentially as a single attribute-rich queue feeding a worldwide processing resource.
   * Easy integration of local resources. Minimum site requirements are a grid computing element or local batch queue to receive pilots, outbound http support, and remote data copy support using grid data movement tools.

Additional considerations are as follows:
   * Single workload management system to handle both ongoing managed production and individual users activity (such as analysis), so as to benefit from a common infrastructure and to allow all participants to leverage common operations support.
   * A coherent, homogeneous processing system layered over diverse and heterogeneous processing resources. This helps insulate production operators and analysis users from the complexity of the underlying processing infrastructure. It also maximizes the amount of code in the system that is independent of the underlying middleware and facilities
   * Security based on standard grid security mechanisms. Authentication and authorization is based on X.509 grid certificates, with the job submitter required to hold a grid proxy and VOMS role that is authorized for Panda usage. User identity (DN) is recorded at the job level and used to track and control usage in Panda's monitoring, accounting and brokerage systems. The user proxy itself can optionally be recorded in !MyProxy for use by pilots processing the user job, when pilot identity switching/logging (via gLExec) is in use.
   * Support for usage regulation at user and group levels based on quota allocations, job priorities, usage history, and user-level rights and restrictions.
   * A comprehensive monitoring system
      * detailed drill-down into job, site and data management information for problem diagnostics
      * Web-based interface for end-users and operators
      * usage and quota accounting
      * performance monitoring of Panda subsystems and the computing facilities being utilized.

Jobs are submitted to !PanDA via a simple client interface by which users define job sets, and their associated data. Job specifications are transmitted to the !PanDA server via secure http (authenticated via a grid certificate proxy), with submission information returned to the client. This client interface has been used to implement a variety of !PanDA front-end systems. The !PanDA server receives work from these and places it into a global job queue, upon which a brokerage module operates to prioritize and assign work on the basis of job type, priority, input data and its locality, available CPU resources and other brokerage criteria.

An independent subsystem manages the delivery of pilot jobs to worker nodes via a number of scheduling systems. A pilot once launched on a worker node contacts the dispatcher and receives an available job appropriate to the site. If no appropriate job is available, the pilot may immediately exit or may pause and ask again later, depending on its configuration (standard behavior is for it to exit). If, however, a job is available, the pilot obtains a payload job description, whose principal component is the URL form which the payload script needs to be downloaded. This mechanism, where payload scripts are hosted a separate Web server which may be, and usually us, separate from the !PanDA system proper. This allows users to exercise necessary level of control over the payloads, and reduce the load on the !PanDA server, while still having standard security mechanisms in place.

An important attribute of this scheme for interactive analysis, where minimal latency from job submission to launch is important, is that the pilot dispatch mechanism bypasses any latencies in the scheduling system for submitting and launching the pilot itself. The pilot job mechanism isolates workload jobs from grid and batch system failure modes (a workload job is assigned only once the pilot successfully launches on a worker node). The pilot also isolates the Panda system proper from grid heterogeneities, which are encapsulated in the pilot, so that at the Panda level the grid(s) used by !PanDA appears homogeneous. Pilots generally carry a generic 'production' grid proxy, with an additional VOMS attribute 'pilot' indicating a pilot job. Optionally, pilots may use glexec to switch their identity on the worker node to that of the job submitter.

The overall !PanDA architecture is shown below<br/>

     <img src="%ATTACHURLPATH%/panda-arch.jpg" alt="panda-arch.jpg" width='687' height='524' />

---++++ Usage Description

To better illustrate how !PanDA is used, let us enumerate its principal components with which the end-user or agent interacts most often:
   * !PanDA server
   * !PanDA monitor
   * Pilot generator (aka pilot scheduler)

The first two components are centrally installed, managed and maintained. Same applies to the pilot scheduler, however in select cases local administrators or advanced users can run this process themselves -- the setup is quite simple.

Assuming that these components are in place an running, a typical usage scenario unfolds as follows:
   * the user submits a job using a command line client
   * job is registered on the server and becomes visible to the user in the monitor
   * pilots submitted to sites defined by the user are submitted and run without user intervention; when they connect to the server, the following automated step takes place
   * !PanDA brokerage mechanism picks a pilot according to pre-defined criteria, and communicates information to the pilot, which is sufficient to obtain the actual workload
   * The pilot downloads and executes the payload; throughout the process the status of the pilot and the job is reflected in the monitor
   * Upon job completion, the standard output, standard error and other attendant files are available to the user to inspect, from Web pages served by the monitor

---++++ implementation, Deployment, and Management Description

!PanDA in its critical part is using industry-standard, well understood and tested components. Crucial component employed in a few elements of the !PanDA architecture is the Apache Web server equipped with _modpython_ and _modgridsite_. !PanDA is using a RDBMS as its backend, and has in fact been successfully deployed using both !MySQL and ORACLE RDBMS, as dictated by deployment requirements. As commented above the server and monitor components, both implemented as Web services, are centrally managed. As such, they are subject to security and access control protocols of the site where they are deployed (same applies to RDBMS). The amount of software that needs to be download and installed by users not employing specific project frameworks such as ATLAS is extremely small. Initial setup for a new group and/or user amounts to adding an entry to the database which is used by both pilot scheduler and the server itself.

---++ Summary and Conclusions (1-2 paragraphs)
Additional options not described in this document: Gridway, Pegasus, Swift



%STOPINCLUDE%

%BOTTOMMATTER%

-- Main.DanFraser - 14 Jan 2010



%META:FILEATTACHMENT{name="glideinWMS_at_a_glance_medium.png" attachment="glideinWMS_at_a_glance_medium.png" attr="" comment="glideinWMS Architecture and use case" date="1265230676" path="glideinWMS_at_a_glance_medium.png" size="46186" stream="glideinWMS_at_a_glance_medium.png" tmpFilename="/usr/tmp/CGItemp17405" user="ParagMhashilkar" version="1"}%
%META:FILEATTACHMENT{name="panda-arch.jpg" attachment="panda-arch.jpg" attr="h" comment="Panda Architecture" date="1265249471" path="panda-arch.jpg" size="283296" stream="panda-arch.jpg" tmpFilename="/usr/tmp/CGItemp8774" user="MaximPotekhin" version="1"}%
