%META:TOPICINFO{author="DanFraser" date="1270243980" format="1.1" reprev="1.31" version="1.31"}%
%META:TOPICPARENT{name="WebHome"}%
%LINKCSS%

<!-- This is the default OSG documentation template. Please modify it in -->
<!-- the sections indicated to create your topic.                        --> 

<!-- By default the title is the WikiWord used to create this topic. If  -->
<!-- you want to modify it to something more meaningful, just replace    -->
<!-- %TOPIC% below with i.e "My Topic".                                  -->

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%

%STARTINCLUDE%
%EDITTHIS%
---+ A Comparison of Job Submission Methods on the Open Science Grid
---++Introduction
Grid computing on the OSG utilizes computational resources that are distributed over many independent sites with only a thin layer of Grid middleware shared between them. This deployment model enables the resource providers to be autonomous, especially with regards to operations. Resource providers operate their local distributed resources according to local preferences and expertise. These resources are often integrated with non-Grid resources at the local institution.

However, the Grid deployment model introduces several problems for the users of the system, three major problems being:
   * The complexity of job scheduling in the Grid environment
   * The non-uniformity of compute resources
   * No easy way to accommodate end-to-end job monitoring

When providing job submission services to its customers and constituents, VOs have a variety of technology choices it can make to solve these issues depending on the types and quantities of the jobs managed. To help OSG better understand the tradeoffs in technology, and therefore be able to better advise its VOs, this paper is designed to compare and contrast the three main technologies that are used for job routing and distribution on the OSG: the Engage OSG Matchmaker, Glide-in WMS, and PANDA. (Additional job submission tools that have been used to a lesser degree on the OSG include Gridway, Pegasus and Swift.) Since each of these services relies on an underlying information architecture we include a brief section on the Information services that OSG offers.



---++A Brief Overview of OSG Information Sources
A reliable information layer including resource names, availabilities, capabilities, loads, and access mechanisms is an essential component of any Grid infrastructure and is crucial to be able to distribute jobs across the resources. To handle this information in a systematic way, a variety of tools have evolved for use on the OSG, these include: The OSG Information Management system (OIM); Generic Information Providers (GIPs);  Berkley Database Information Index (BDII) systems; and the Resource Selection Service (!ReSS) systems. 

Every site and resource that is considered part of the OSG is registered in the OIM system (https://oim.grid.iu.edu/oim/home). This database is maintained at the Grid Operation Center (GOC) and stores basic (mostly static) information about sites and resources such as names, descriptions, and contact information. 

Dynamic information, such as availabilities, capabilities, and loads, is provided by localized GIPs that run at each site and maintain a local information repository. (There can be multiple GIPs running on a single site.) Data from the GIPs is dynamically aggregated via CEMON clients at each site that push data to a centralized CEMON collector running at the GOC. This data is pushed out simultaneously to several different databases: a BDII system running at the GOC, an international BDII system running at CERN; and a !ReSS system running at FNAL. In order to standardize and make sense of this information, GIPs maintain their information according to the GLUE schema -- an outgrowth of a collaborative effort between European and US Grid projects.

Data from the GIPs is dynamically aggregated via CEMON clients at each site that push data to a centralized CEMON collector running at the GOC. This data is pushed out simultaneously to several different databases: a BDII system running at the GOC, an international BDII system running at CERN; and a !ReSS system running at FNAL.

The GOC BDII provides a centralized point of access that job schedulers can query to intelligently distribute jobs across the OSG. Accordingly it is deemed a “critical” service that is maintained in a tightly controlled environment by the GOC. The international top level BDII aggregates data from EGEE together with data on US LHC sites for international and intra grid submissions. This service is managed centrally for the LHC and operated at CERN.

In addition to populating the BDII, a highly available !ReSS system is hosted at FNAL. This stores largely the same information as the BDII but presents the data in the form of a Condor Class-ad -- a mechanism for matching jobs with resources across the OSG.

Whether accessing this data via !ReSS or BDII, grid users can easily access the published information and use it manually or in their automated job management system.

---++Job Submission Methods
This section provides a high level architecture and design philosophy for the Engagement OSG Matchmaker (OSGMM),  glideinWMS and !PanDA job submission and management technologies.

---+++Engage OSG Matchmaker
---++++ High Level Architecture and Design Consideration

The OSG !MatchMaker (OSGMM) is a tool that was created to give small to medium sized VOs a straightforward yet powerful submit interface to OSG. It is designed to retrieve VO specific site information from !ReSS and also to verify and maintain the sites by regularly submitting verification/maintenance jobs to make sure a site can continue receiving jobs. Site information from OSGMM is inserted into a local Condor install so that Condor can match jobs with the resources.  OSGMM then monitors jobs in the system to maintain pending/running counts and a moving window of job success/failure rates for each site. This is used to back off from sites if they break or become busy with the resource owners' jobs. Site status information can be queried and imported by other systems such as Pegasus and Swift.

     <img src="%ATTACHURLPATH%/OSGMM-Overview.png" alt="OSGMM-Overview.png" width='600' height='506' />    


---++++ Usage Description

OSG !MatchMaker provides/maintains site information in a Condor-G system. In order to run jobs, the user must write a Condor job description. If they are not familiar with Condor submission systems, as is the normal case, the Engagement staff will work one on one with users to help. Often these jobs are based on previously written examples that incorporate best practices. The Condor job description contains a requirements line which enables the user to be very specific about where the jobs should end up. Possible requirements include but are not limited to OS, architecture, network setup, software and the availability of certain datasets. Also, job submission scripts need to be tweaked for each site due to different ways that sites are setup. In so doing, users gradually begin to understand and work with the complexities of the grid. 

OSGMM does not specify a mechanism for data transfers. Data transfer management logic for example is often written into a job wrapper script. Most of the time this script is composed of simple globus-url-copy commands to retrieve inputs and push back outputs back to the submit host. This has an important advantage of being able to easily integrate with whatever transfer mechanisms users are most comfortable with at their site including GridFTP, SRM or even plain FTP.

The job submission side is also flexible and most users end up with using Condor DAGMan to create a workflow of their jobs. DAGMan also provides some important mechanisms such as job retries and pre/post script for the jobs. Because the OSGMM system ends up using Condor-G to submit and manage the jobs, there is a certain overhead for each job (combination of Condor-G/GRAM/LRM overheads) so short jobs (< 1 hour wall time) are not recommended.

---++++ Implementation, Deployment, and Management Description

OSGMM was designed to be easy to deploy and support at the VO/university/lab submit level. It is shipped with VDT and can be installed with the OSG client software stack with just one additional pacman command. OSGMM should be installed using the VDT method for a shared install which means root has to do the installing. This method takes care of setting up startup scripts for services and privilege separation as Condor and OSGMM will be configured to run as their own non-privileged users. OSGMM requires about 1 GB RAM (used to track jobs in the system). One instance will serve all users on that submit host, using Condor's fair-share algorithm to handle jobs from multiple users.

Each VO can have a central OSGMM instance (polling !ReSS for information) to run verification/maintenance jobs and then have other instances of OSGMM  (polling the VO OSGMM instance for information including site status) running on the submit hosts. General verification and maintenance jobs ship with OSGMM, but each VO can augment the jobs with their own tests and for example software installations. This provides a solution for the VO to push out software to sites, and then advertise availability of the software back to the system. OSGMM scales up to managing about 5,000 Condor-G jobs per submit host. For VOs that prefer not to host their own job submission system, however, the Engage team hosts a fully functional submit host that VOs can utilize.

---++++Some OSGMM Considerations

The OSGMM system is readily adaptable to submit jobs to all available sites on the OSG since in general there are few site requirements needed to use it (e.g. worker nodes are not required to have external network connections). Jobs are sent to distributed systems and then managed by the remote batch queuing mechanisms in place at each site. 

To the user of the OSGMM system however, the heterogeneity of site configurations (e.g. gateway configurations, environment configurations, ...) can cause job failures until certain site specific configurations are incorporated into the job submission scripts. Hence users of this system sometimes need to know more about the internal workings of the sites than they prefer to. By the time that jobs actually get to the remote site, conditions may have changed and one can experience longer than expected waiting for the remote queue to schedule the job for example. Also, if a site is not configured correctly, even correctly configured jobs can fail with unexpected results that can be difficult to debug. Nevertheless, OSGMM has proven to be a powerful tool for job submission on the OSG and has contributed to the success of many users, especially those that have utilized the Engagement VO. 

"Pilot" based submission methods described below offers an alternative approach to job submission. In general, they take more effort to set up and run but add some important capabilities as will be described below.

---+++Glide-in WMS

---++++High Level Architecture and Design Consideration

The Glidein Workload Management System (glideinWMS) utilizes a centralized pilot based submission system called a pilot or Glidein Factory (GF). A "pilot" job is a special type of job that lands on a distributed resource and then pulls in the users job to start executing it. This approach, often called the PULL model, has a few advantages over the standard, or PUSH model, implemented by the OSG !MatchMaker:
   * The matchmaking of jobs to sites is much easier. Instead of trying to predict where the job will start first, pilots are sent to all the Grid sites that are supposed to be able to run the job. The first pilot that starts gets the job. The remaining glideins will either start another job that can run there (even if they were not submitted for the purpose of serving that job), or terminate within a short period of time. As long as there is a significant amount of jobs in the queue, only a small fraction of pilots will terminate without performing any useful work.
   * The pilot can validate the node before pulling a user job; as a consequence correctly configured users jobs are less likely to fail. Pilot jobs can of course fail to start with the same frequency as OSG !MatchMaker submitted jobs, but these failures are seen only by the Glidein Factory that submitted the pilot job and the user does not "experience" this type of failure. 
   * Due to the centralized nature of the system, a pilot system can handle user priorities uniformly across the Grid. In the OSG !MatchMaker model, each Grid site maintains the priorities of the users, based on the policies and historical usage data specific to that site.
   * Since the pilot runs alongside the user job, it can provide additional services, like pseudo-interactive monitoring.

This approach has of course also disadvantages:
   * It absolutely requires networking between the worker nodes and the pilot infrastructure. The OSG !MatchMaker by itself does not have this requirement.
   * It requires a much heavier investment in hardware, so it must manage all the job information in greater detail.
   * In order to get proper security, the pilot must be able to switch identity once the user job is pulled. This is not part of the original Grid security model; recently a Grid tool, namely gLExec has been created to provide this capability. Currently only a subset of Grid sites support it.
???Is this a requirement for using GlideinWMS???

The glideinWMS system was developed on top of the Condor system. Most of its functionality comes from Condor itself, with just a thin layer on top of it. This approach was chosen to minimize the development and maintenance cost of the product, since Condor already provided a very powerful platform on which to build on. Moreover, this allows users with previous Condor know-how an easy path to the Grid world. Users with existing Condor-ready jobs can almost transparently submit these to the Grid via glideinWMS.

The glideinWMS-specific services are composed of a frontend and a glidein factory (GF). The glidein factory is responsible to know which Grid sites are available and what are the site attributes, and to advertise these information as "entry-point ClassAds" to a dedicated collector (known as the WMS pool). The frontend instead plays the role of a matchmaker, internally matching user jobs to the entry-point ClassAds, and then requesting the needed amount of glideins to the factory. Finally, the factory will submit the pilots, also known as glideins, that will start the Condor daemons responsible for resource handling.

The picture below shows the architecture from a schematic point of view:<br>
<img src="%ATTACHURL%/glideinWMS_at_a_glance_medium.png" alt="glideinWMS at a glance"> 

Finally, GSI, utilizing X509 proxies, was chosen as the security mechanism. All communication between the various processes is authenticated via GSI and can be encrypted if desired.

---++++ Usage Description

From the *end user point of view*, glideinWMS is just a distributed Condor system. As with the OSG MatchMaker, the user must write a Condor job description, although for the vanilla universe, in order to run jobs. Most of the VOs currently using gldieinWMS have portals to insulate the final users from the details of the underlying batch system thereby simplifying the user submission model even further.

Glideins advertise a set of attributes that enable users to match jobs to resources. The matching algorithm can be as complex or as simple as desired; for example, CMS matches just on the site name.

Finally, Condor DAGMan can be used to handle workflows just like with the OSG MatchMaker. Jobs of a few minutes to several hours are supported; a single Condor schedd can easily handle 5-10Hz job turnaround rate. Very long jobs may be a problem due to preemption, like with the OSG MatchMaker, but Condor will restart the failed jobs so users usually don't see any failures.

GlideinWMS does not specify a mechanism for data transfers; i.e. it supports only what Condor supports. For modest input and output sizes, the standard Condor file transfer works fine (and recent versions of Condor support, possibly cached, HTTP transfer as well), but for larger file transfers users are left on their own, although many VOs have their own data handling solutions. 

From the *VO point of view*, glideinWMS requires the installation of a Condor central manager and a submit node, and the installation of the glideinWMS-specific daemons.

The Condor central manager and submit node installation is very close to a standard Condor installation, with just small configuration changes; the glideinWMS installer can fully automate this, if desired.

The glideinWMS-specific services are composed of two parts; a factory and a frontend. The VO must install its own frontend, while the factory can be shared among several VOs. Although larger VOs are encouraged to host their own factory to attain maximum flexibility, UCSD currently hosts a factory that is open (upon request) to smaller OSG VOs. So factory installation will not be discussed here, as it is intended for advanced VO admins.

The frontend configuration entails installing a Web server, creating the proper glidein matching configuration and starting the frontend daemons. The main elements to be configured are the list of Condor daemon DNs, the credential(s) used for glidein submission, the VO specific validation and data gathering scripts, and finally the matchmaking expression. The last one is needed because, as described above, the frontend is the glideinWMS matchmaker, and currently does not have a matching logic powerful enough to do true matchmaking; so help from the VO administrators is needed.

The attributes published for the matchmaking at the Condor level are gathered as a mix of static attributes provided by the factory (usually extracted from the information system,  a set of scripts provided by the factory and a set of scripts provided by the frontend. The reasoning behind this is that the factory can tailor the scripts toward the type of resources served, while the frontend extracts, and publishes, VO specific information. There is no prescribed naming schema for the attributes, although nothing prevents the factory and/or VO administrators to adopt one (like GLUE).

The factory and the frontend also can provide validation scripts, so user jobs never land on failing glide-in jobs; instead, only the glideins fail.

---++++ Implementation, Deployment, and Management Description

The VO Frontend periodically queries the user pool to check the status of user job queues. Based on the job queues, it instructs the glidein factory (GF) to submit glideins to run on Grid resources. A glidein is a properly configured Condor startd submitted as a Grid job. GlideinWMS extensively uses the Condor classad mechanism as a means of communication between its services. Once a glidein starts on a worker node, it will join the user Condor pool, creating the obtained Grid-batch slot as a new slot in the Condor pool. At this point, a regular Condor job can start there and everything "appears" as if it is a dedicated resource. Condor itself handles the interaction with gLExec; it is only a matter of proper configuration.

GlideinWMS does not provide a GUI interface for the users to submit their jobs, instead users use Condor client tools like condor_submit to submit their jobs. As the user interfaces with the Condor batch system, he/she is shielded from the problems mentioned above that are introduced by the Grid deployment. Based on the time slot allocated for glidein to run, this glidein created batch slot can run multiple user jobs before relinquishing its claim over the grid resource. In this mechanism glideinWMS can also be viewed as supporting resource provisioning. This mechanism is quite effective in running short jobs using glideinWMS. Since a single glidein can run multiple short jobs, the effective wait time/overhead of running several short jobs this way is equivalent to running a single grid job. The resource provisioning feature can also be used to provide a guaranteed time slot for long running jobs. GlideinWMS also provides users with the capability of selecting resources based on resource characteristics, like, hardware type, OS, packages installed on the worker node and any custom information that is required for the user jobs to run. glideinWMS can be configured to run pluggable scripts as the glidein bootstraps. This is useful in advertising desired resource information. Thus the combination of glidein’s bootstrapping scripts and pluggable scripts can identify potential problems with the resource before the user job could start on the resource. This increases glideinWMS resiliency to bad CE/resources and the environments on the resources.

GlideinWMS does provide a set of Web-based monitoring tools for both the glidein factory and the VO frontend. This allows the factory and VO frontend administrators to easily monitor the system and discover eventual problems.

Monitoring of the Condor pool can be achieved through standard Condor monitoring tools, varying from command line tools (like condor_q and condor_status), CondorView, and up to commercial tools like the ones developed and sold by Cycle Computing. 

---++++ Some Glide-in WMS Considerations

GlideinWMS is used in OSG and LCG by several VOs like CMS, CDF, !DZero, !IceCUBE and GPN and by experimental High Energy physics groups like Minos, Minerva and NOVA. GlideinWMS supports several workflows supported by Condor from a simple job submission to complex DAGs. During the factory configuration step, glideinWMS can query the information systems like !ReSS and BDII to get the list of available grid resources. A system administrator can either accept all the sites or be more selective about the sites listed. GlideinWMS will populate it's site list based on the selection made by the administrator and information about the sites listed in BDII or !ReSS. The current version of glideinWMS does not have any support for data management by the glideinWMS system itself. Most of the VOs however that use this technology have their own data management system which can be easily used by the user jobs run via glideinWMS.

Although, all the glideinWMS components can be collocated on a single host for smaller use case, it is recommend that several of the services be run on different systems. following services per host for larger use case -
   * glidein factory collocated with WMS Condor Pool
   * User Condor Pool Collector
   * User Schedd
   * VO Frontend

Table below shows the scalability figures for the glideinWMS system tested so far.

|    *Criteria*    |    *Design goal*    |    *Achieved so far*    |
| Total number of user jobs in the queue at a given time | 100k | 200k |
| Number of glideins in the system at any given time | 10k | ~26k |
| Number of running jobs per schedd at any given time | 10k | ~23k |
| Grid sites handled | ~100 | ~100 |

Most of the glideinWMS services do not need to be run with root privileges. HTTPD service on the VO frontend and Glidein factory node is typically installed and run as root user. Also, for security reasons, user Schedd(s) for non portal installation should run as root. Since there are several services to be installed and configured, the deployment model needs to be thought out based on the use case. Also, an administrator installing the services needs to be familiar with Condor Batch System and basic GSI concepts. If the glidein factory is being installed, knowledge of the Grid (OSG, EGEE, Nordugrid) is also needed.

It should be noted that UCSD is hosting a glidein factory that smaller VOs can use to start using the glideinWMS without having any knowledge of the Grid. 

Although currently glideinWMS can be challenging to install, the glideinWMS team is actively working to both improve the documentation and to make the installation process as easy as possible.

*The glideinWMS Home Page:* [[http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/][glideinWMS Home]]

---+++ !PanDA
---++++ High Level Architecture and Design Considerations

The Production and Distributed Analysis system (!PanDA) is a pilot based job submission system. !PanDA uses its pilot generator to submit and manage pilot jobs, which communicate with central service whose function is to perform brokerage and direct work load to sites. Unlike glideinWMS, !PanDA pilots are not coupled to Condor software and are in principle arbitrary code, meant to communicate with the server via HTTPS. Principal features of its design were initially driven by operational requirements of Atlas experiment at LHC, however most of these are directly applicable to general OSG use. Some of the key design criteria were as follows:

   * The ability to use pilot jobs for the acquisition of processing resources and all the attendant advantages that pilots provide. Workload jobs are assigned to successfully activated and validated pilots based on !PanDA-managed brokerage criteria. This 'late binding' of workload jobs to processing slots prevents latencies and failure modes in slot acquisition from impacting the jobs, and maximizes the flexibility of job allocation to resources based on the dynamic status of processing facilities and job priorities. The pilot is also a principal 'insulation layer' for !PanDA, encapsulating the complex heterogeneous environments and interfaces of the grids and facilities on which Panda operates. 
   * The client interface must allow easy integration with diverse front ends for job submission to !PanDA. 
   * A system-wide site/queue information database for recording static and dynamic information used throughout !PanDA to configure and control system behavior from the 'cloud' (region) level down to the individual queue level. It is used by pilots to configure themselves appropriately for the queue they land on; by !PanDA brokerage for decisions based on cloud and site attributes and status; and by the pilot scheduler to configure pilot job submission appropriately for the target queue. 
   * Provide a coherent and comprehensible system view for users, and for !PanDA's own job brokerage system, through a system-wide job database that records comprehensive static and dynamic information on all jobs in the system. To users and to !PanDA itself, the job database appears essentially as a single attribute-rich queue feeding a worldwide processing resource. 
   * Easy integration of local resources. The minimum site requirements are a grid computing element or local batch queue to receive pilots, outbound http support, and remote data copy support using grid data movement tools.

In contrast to glideinWMS that uses a Condor based WMS, !PanDA created its own centralized relational database and brokerage system. The centralized database keeps detailed info about every pilot and every job status (with logs and error messages) and can be queried using standard RDP techniques. !PanDA also has a user interface to that database (the monitor) that facilitates workload management, debugging and day-to-day operations.

Additional considerations are as follows:
   * A single workload management system to handle both ongoing managed production and individual users activity (such as analysis), so as to benefit from a common infrastructure and to allow all participants to leverage common operations support.
   * A coherent, homogeneous processing system layered over diverse and heterogeneous processing resources. This helps insulate production operators and analysis users from the complexity of the underlying processing infrastructure. It also maximizes the amount of code in the system that is independent of the underlying middleware and facilities
   * Security based on standard grid security mechanisms. Authentication and authorization is based on X.509 grid certificates, with the job submitter required to hold a grid proxy and VOMS role that is authorized for !PanDA usage. User identity (DN) is recorded at the job level and used to track and control usage in !PanDA monitoring, accounting and brokerage systems. The user proxy itself can optionally be recorded in !MyProxy for use by pilots processing the user job, when pilot identity switching/logging (via gLExec) is in use.
   * Support for usage regulation at user and group levels based on quota allocations, job priorities, usage history, and user-level rights and restrictions.
   * A comprehensive monitoring system
      * detailed drill-down into job, site and data management information for problem diagnostics
      * Web-based interface for end-users and operators
      * usage and quota accounting
      * performance monitoring of Panda subsystems and the computing facilities being utilized.

---++++Documentation
   * [[http://www.usatlas.bnl.gov/twiki/bin/view/AtlasSoftware/PanDA.html][PANDA page]] for US ATLAS
   * Description of [[http://www.usatlas.bnl.gov/twiki/bin/view/AtlasSoftware/AutoPilot][Pilot Generator]]
   * [[http://www.usatlas.bnl.gov/twiki/bin/view/AtlasSoftware/NonAtlasJobs.html][How to]] submit generic jobs to !PanDA

---++++Usage Description

Jobs are submitted to !PanDA via a command-line client interface by which users define job sets, and their associated data. Job specifications are transmitted to the !PanDA server via secure http (authenticated via a grid certificate proxy), with submission information returned to the client. This client interface has been used to implement a variety of !PanDA front-end systems. The !PanDA server receives work from these and places it into a global job queue, upon which a brokerage module operates to prioritize and assign work on the basis of job type, priority, input data and its locality, available CPU resources and other brokerage criteria.

An independent subsystem (called pilot generator or pilot scheduler) manages the delivery of pilot jobs to worker nodes via a number of scheduling systems. A pilot once launched on a worker node contacts the dispatcher (residing on the central server)  and receives an available job appropriate to the site. If no appropriate job is available, the pilot may immediately exit or may pause and ask again later, depending on its configuration (standard behavior is for it to exit). If, however, a job is available, the pilot obtains a payload job description, whose principal component is the URL form which the payload script needs to be downloaded. This mechanism, where payload scripts are hosted a separate Web server which may be, and usually us, separate from the !PanDA system proper. This allows users to exercise necessary level of control over the payloads, and reduce the load on the !PanDA server, while still having standard security mechanisms in place.

An important attribute of this scheme for interactive analysis, where minimal latency from job submission to launch is important, is that the pilot dispatch mechanism bypasses any latencies in the scheduling system for submitting and launching the pilot itself. The pilot job mechanism isolates workload jobs from grid and batch system failure modes (a workload job is assigned only once the pilot successfully launches on a worker node). The pilot also isolates the Panda system proper from grid heterogeneities, which are encapsulated in the pilot, so that at the Panda level the grid(s) used by !PanDA appears homogeneous. Pilots generally carry a generic 'production' grid proxy, with an additional VOMS attribute 'pilot' indicating a pilot job. Optionally, pilots may use glexec to switch their identity on the worker node to that of the job submitter.

The overall !PanDA architecture is shown below<br/>

     <img src="%ATTACHURLPATH%/panda-arch.jpg" alt="panda-arch.jpg" width='687' height='524' />

To better illustrate how !PanDA is used, let us enumerate its principal components with which the end-user or agent interacts most often:
   * !PanDA server
   * !PanDA monitor
   * Pilot generator (aka pilot scheduler)

The first two components are centrally installed, managed and maintained. The pilot scheduler can be located anywhere and run either centrally, or by VOs (e.g. in case of CHARMM project).

Assuming that these components are in place an running, a typical usage scenario unfolds as follows:
   * the user submits a job using a command line client provided by !PanDA
   * job is registered on the server and becomes visible to the user in the monitor (the Web portal)
   * pilots submitted to sites defined by the user are submitted and run without user intervention
   * !PanDA brokerage mechanism picks a pilot according to pre-defined criteria, and communicates information to the pilot, which is sufficient to obtain the actual workload (an example of brokerage criteria may be the location of particular data on a site)
   * The pilot downloads and executes the payload; throughout the process the status of the pilot and the job is reflected in the monitor
   * Upon job completion, the standard output, standard error and log files become available to the user to inspect on Web pages served by the monitor

---++++ implementation, Deployment, and Management Description

!PanDA in its core parts (server and monitor) is using industry-standard, well understood and tested components such as Apache server software and SSL-based encryption. The Apache Web server is equipped with _modpython_ and _modgridsite_. !PanDA is using a RDBMS as its backend, and has in fact been successfully deployed using both !MySQL and ORACLE RDBMS, as dictated by deployment requirements. As commented above the server and monitor components, both implemented as Web services, are centrally managed. As such, they are subject to security and access control protocols of the site where they are deployed (same applies to RDBMS). The amount of software that needs to be download and installed by users not employing specific project frameworks such as ATLAS is small (two Python scripts).

---++ High Level Functionality Comparison Table 

%TABLE{ tablewidth="700" columnwidths="25%, 25%, 25%, 25%" cellpadding="2" dataalign="left" tablerules="all" tableborder="2" databg="#FFFFFF, #FFFFFF"}%
| *Submission Capabilities and Requirements* | *Engage OSGMM* | *Glide-in WMS* | *PANDA* |
| Job Distribution Mechanism | Condor-G | Pilot Based | Pilot Based ||
| Is the service centrally hosted? | Each VO must install at least one instance of OSGMM | The Pilot Factory is centrally hosted, VOs must each install their own Job submission infrastructure to submit jobs to the Pilot Factory. VOs can also install their own Pilot Factory if desired | !PanDA server, monitor and their databases are centrally hosted. Pilot submission is a light weight process that is hosted by the VO. VOs must each install their instance of Job submission scripts ||
| Difficulty level for new VOs to install and setup the required job submission infrastructure | Easy* -- components are available through the VDT | Moderate/Difficult*, active work on making it easier, new VOs currently require handholding to get started | Moderate*, VOs have a minimal number of frontend services to set up and maintain but handholding from developers is still needed. ||
| Difficulty level for new users to get their applications running | Moderate*, users must embed their jobs in Condor wrappers. Data movement can be tricky to manage; Condor provides some built-in data handling capabilities.  || Easy*, submission is based on a command line infrastructure. Data management can be tricky since it requires management of GridFTP staging.  |
| Is the setup and operation of the system well documented in terms of "thoroughness" and "ease-of-use" for end users (1 to 5 stars, 5 stars being best)? | Thoroughness <literal>***</literal>  Ease-of-use <literal>*****</literal> | Thoroughness <literal>****</literal>  Ease-of-use <literal>***</literal> | Thoroughness <literal>****</literal>  Ease-of-Use <literal>***</literal> |
| Support for complex workflows | Yes, based on Condor DAGMan |Yes, based on Condor DAGMan | Complex workflows must be implemented manually ||
| Support for Job prioritization | Requires manual intervention | Full featured, based on Condor infrastructure | Requires manual intervention ||
| Error handling of ill configured sites, CEs, or environments | Regular test/maintenance jobs are sent to make sure sites are up and running. Retries are automatically enabled to minimize end user error messages  | User jobs don't start unless a Pilot job is already running at the site, errors are stored in logfiles for manual inspection.  ||
| Information systems used to schedule jobs | !ReSS (queried every few minutes) | One time use of BDII/OIM to install sites; localized infrastructures keep track of dynamic data based on pilot submissions ||
| User information systems | Command line tools | Command line tools; limited monitoring capabilities for end users |  Portal Interface Viewer provides a good interactive user view of the entire system||
| VOs that have used or are using this service in production | Engagement, SBGrid, GPN |CMS, DZero, CDF, !IceCUBE, GPN, Minos, Minerva, NOVA |Atlas, CHARMM||
| Site requirements for job submission (other than a CE and basic correct configuration) |  None | Outbound network connections on each worker node if Condor CCB is installed; otherwise requires bi-directional network |  Outgoing  connectivity on each worker node ||
|Typical number of systems that a VO needs to set up in order to manage 5000 simultaneous jobs | One dual core, 8GB system | 1-2 systems, 8GB memory total | One system, very lightweight job submission requirements||
|Is Root typically required for VOs to set up the submit hosts? | Yes | Yes | No ||
*In each case assistance is available for VOs to set up the required services.
---++ Summary and Conclusions

The OSG MatchMaker provides a tried and true methodology for submitting jobs to the Grid that is a direct extension of the familiar model of submitting jobs to a local batch scheduler. As such it is the easiest of the three models for most users to understand and is relatively easy to setup since the package can be readily deployed from the OSG Virtual Data Toolkit. In addition to the job management capabilities similar to a local batch scheduler it contains a high degree of sophistication that tests remote sites and can even "prepare" sites with needed software packages that increase the probability of success for user jobs. It also automatically reroutes jobs to different sites when they fail to start after a specified amount of time. The OSG Matchmaker is a solid technology and a good way for VOs to get started. VOs may also choose to utilize the Engagement VO OSGMM system that is already set up and managed. 

Many of the larger VOs in OSG use a fundamentally different model based on pilot jobs that help ensure user job success since the pilot job is itself already successfully running remotely before starting a user job. Pilot based systems tend to hide failures from the users although the (!GlideinWMS) Pilot Factory or (!PanDA) Server operator still has to manage and debug pilot failures -- there is no free lunch. Since VOs have the opportunity to use these as centralized OSG services however, the easiest way to get started with a Pilot system is to install the client side job submission frameworks. Still these systems are not-trivial and currently require assistance from the software developers to get running and working properly. Fortunately, the developers are more than happy to help new VOs get started with these technologies. Also, at a more fundamental level pilot jobs can be seen to parallel the FTP transfer protocol where a data transfer job begins first with a request to the remote site, and then an acknowledgment before starting the actual data transfer job. The submission of pilot jobs can be compared to the FTP initial request, the pilot then sends an acknowledgment that it is running and ready to receive jobs before the user job is submitted to the pilot. Given the success of the FTP style protocol, it makes one believe that this is a good direction to pursue. 

The !GlideinWMS and !PanDA models are quite different in approach although both use the pilot approach. !GlideinWMS was built around the Condor infrastructure, which has been rigorously time tested in both scalability and reliability for remote systems. Being Condor based means that !GlideinWMS can readily accept sophisticated workflows created around Condor DAGs. Also it has an inherent ability for managing and prioritizing jobs that are being submitted. Despite being somewhat difficult to setup initially, it is the most widely used job submission tool on the OSG. 

The !PanDA model was designed to be a lighter weight system, especially for managing Atlas based workloads. It has some interesting benefits for Atlas users since it is data-aware (for Atlas users only) and can send Atlas jobs directly to the systems that have the necessary data "nearby". Also the centralized database of all !PanDA jobs running on the system can be easily queried to see the status and location of all jobs. (!GlideinWMS can do this as well albeit the monitoring process is more complex). The !PanDA system has been thoroughly tested on Atlas workloads, also CHARMM and is now actively looking for additional VOs to take advantage of this technology.


%STOPINCLUDE%

%BOTTOMMATTER%

-- Main.DanFraser - 14 Jan 2010


%META:FILEATTACHMENT{name="glideinWMS_at_a_glance_medium.png" attachment="glideinWMS_at_a_glance_medium.png" attr="" comment="glideinWMS Architecture and use case" date="1265230676" path="glideinWMS_at_a_glance_medium.png" size="46186" stream="glideinWMS_at_a_glance_medium.png" tmpFilename="/usr/tmp/CGItemp17405" user="ParagMhashilkar" version="1"}%
%META:FILEATTACHMENT{name="panda-arch.jpg" attachment="panda-arch.jpg" attr="h" comment="Panda Architecture" date="1265249471" path="panda-arch.jpg" size="283296" stream="panda-arch.jpg" tmpFilename="/usr/tmp/CGItemp8774" user="MaximPotekhin" version="1"}%
%META:FILEATTACHMENT{name="OSGMM-Overview.png" attachment="OSGMM-Overview.png" attr="" comment="" date="1267805883" path="OSGMM-Overview.png" size="133974" stream="OSGMM-Overview.png" tmpFilename="/usr/tmp/CGItemp16966" user="MatsRynge" version="1"}%
