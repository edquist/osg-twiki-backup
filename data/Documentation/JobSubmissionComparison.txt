%META:TOPICINFO{author="MatsRynge" date="1265953355" format="1.1" reprev="1.15" version="1.15"}%
%META:TOPICPARENT{name="WebHome"}%
%LINKCSS%

<!-- This is the default OSG documentation template. Please modify it in -->
<!-- the sections indicated to create your topic.                        --> 

<!-- By default the title is the WikiWord used to create this topic. If  -->
<!-- you want to modify it to something more meaningful, just replace    -->
<!-- %TOPIC% below with i.e "My Topic".                                  -->

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%

%STARTINCLUDE%
%EDITTHIS%
---+ A Comparison of OSG Job Submission/Resource Provisioning Methods
---++Introduction
In Grid computing, computational resources are distributed over many independent sites with only a thin layer of Grid middleware shared between them. This deployment model is very convenient for computational resource providers. They can continue to operate their local distributed resources according to local preferences and expertise, integrating them easily with other, non-Grid resources. Moreover, the decentral­ized nature of the Grid allows for easy scalability as one just needs to split a site into multiple logical pieces if scalability becomes an issue (e.g. install multiple CEs to the same compute cluster).

However, the Grid deployment model introduces several problems for the users of the system -- three major problems being --
   * The complexity of job scheduling in the Grid environment
   * The non-uniformity of compute resources
   * No easy way to accommodate end-to-end job monitoring

When providing job submission services to its customers and constituents, VOs have a variety of technology choices it can make to solve these issues depending on the types and quantities of the jobs managed. To help OSG better understand the tradeoffs in technology, and therefore be able to better advise its VOs, this paper is designed to compare and contrast the three main technologies that are used for job routing and distribution on the OSG: the Engage OSG Matchmaker, Glide-in WMS, and PANDA. Since each of these services relies on an underlying information architecture we include a brief section on the Information services that OSG offers.

---++ High Level Technology Comparison (Everyone, 1-2 pages) - To be completed after the sections below
---++ Functionality Comparison Table (Everyone)
---++A Very Brief Overview of OSG Information Sources (Dan, others)
A reliable information layer including resource names, availabilities, capabilities, loads, and access mechanisms is an essential component of any Grid infrastructure and is crucial to be able to distribute jobs across the resources. To handle this information in a systematic way, a variety of tools have evolved for use on the OSG, these include: The OSG Information Management system (OIM); Generic Information Providers (GIPs);  Berkley Database Information Index (BDII) systems; and the Resource Selection Service (ReSS) systems. 

Every site and resource that is considered part of the OSG is registered in the OIM system (https://oim.grid.iu.edu/oim/home). This database is maintained at the Grid Operation Center (GOC) and stores basic (mostly static) information about sites and resources such as names, descriptions, and contact information. 

Dynamic information, such as availabilities, capabilities, and loads, is provided by localized GIPs that run at each site and maintain a local information repository. (There can be multiple GIPs running on a single site.) In order to standardize and make sense of this information, GIPs maintain their information according to the GLUE schema -- an outgrowth of a collaborative effort between European and US Grid projects.

Data from the GIPs is dynamically aggregated via CEMON clients at each site that push data to a centralized CEMON collector running at the GOC. This data is pushed out simultaneously to several different databases: a BDII system running at the GOC, an international BDII system running at CERN; and a ReSS system running at FNAL.

The GOC BDII provides a centralized point of access that job schedulers can query to intelligently distribute jobs across the OSG. Accordingly it is deemed a “critical” service that is maintained in a tightly controlled environment by the GOC. The international top level BDII aggregates data from EGEE together with data from the OSG for international and intra grid submissions. It is also a “critical” service that is maintained by CERN.

In addition to populating the BDII, a ReSS system is also populated at FNAL. This stores largely the same information as the BDII but presents the data in the form of a Condor Class-ad -- a mechanism for matching jobs with resources across the OSG.

Another important tool is the MyOSG service that provides an XML based user interface for accessing data stored in the OIM as well as much of the data stored in the BDII.

Whether accessing this data via ReSS, BDII, or MyOSG, grid users can easily access the published information and use it manually or in their automated job management system.

---++Job Submission Methods (Mats, Parag, Maxim)
In this section we want to provide first a high level architecture for each of the technologies below that includes the design philosophy and then an implementation and usage description that addresses answers to each of the following questions:
   * Job submitter perspectives:
      * What are some typical use cases?
      * How many users/jobs can be supported?
      * To what degree is data movement synchronous or asynchronous both on the submit side as well as when the job completes.
      * How does the technology integrate with data management?
      * What is the overhead on a single job submission?
      * Can the infrastructure support short jobs?
      * What types of job patterns are supported? (Complex workflows?)
      * Is Resource Provisioning supported? If so how?
      * What types of matchmakers are used? Which OSG information sources do they utilize?
      * How elaborate is the matchmaking? (can users specify e.g. hardware type, or s/w packages?)
      * How does the system respond to “bad” CEs, sites, or environments?
   * Administrative perspectives:
      * How does the system scale? (What is important, cpu, memory, network bandwidth)
      * How much hardware is required for a “best practices” deployment?
      * How many hosts are required to be set up and maintained?
      * Can a “reasonable” admin type person set this up on his/her own? (Has this been setup by folks other than the developer?)
      * To what degree can it be setup without requiring root access.
---+++Engage OSG Matchmaker (Mats, ~1-2 pages)
---++++ High Level Architecture and Design Consideration

The OSG MatchMaker (OSGMM) is a tool that was created to give small to medium sized VOs a straightforward yet powerful submit interface to OSG. It is designed to retrieve VO specific site information from ReSS and also to verify and maintain the sites by regularly submitting verification/maintenance jobs to make sure a site can continue receiving jobs. Site information from OSGMM is inserted into a local Condor install so that Condor can match jobs with the resources.  OSGMM then monitors jobs in the system to maintain pending/running counts and a moving window of job success/failure rates for each site. This is used to back off from sites if they break or becomes busy with the resource owners' jobs. The site status information can be queried and imported by other systems such as Pegasus and Swift.


---++++ Usage Description (how users use the technology & use cases)

OSG MatchMaker provides/maintains site information in a Condor-G system. In order to run jobs, the user must write a Condor job description. If they are not familiar with Condor submission systems, as is the normal case, the Engagement staff will work one on one with users to help. Often these jobs are based on previously written examples and are based on best practices. The Condor job description contains a requirements line which enables the user to be very specific about where the jobs should end up. Possible requirements include but not limited to OS, architecture, network setup, software and the availability of certain datasets. Also, job submission scripts need to be tweaked for each site due to different ways that sites are setup. In so doing, users gradually begin to understand and work with the complexities of the grid. 

OSGMM does not specify a mechanism for data transfers. It is up to the user to do the necessary data transfers for example for a job wrapper script, and most of the time this ends up being simple globus-url-copy commands to retrieve inputs and push back outputs back to the submit host. This has an advantage of being able to easily integrate with whatever transfer mechanisms users are most comfortable with at their site including GridFTP, SRM or even plain FTP. 

Because the OSGMM system ends up using Condor-G to submit and manage the jobs, there is a certain overhead for each job (combination of Condor-G/GRAM/LRM overheads) so short jobs (< 1 hour wall time) are not recommended. 

Most users end up with using Condor DAGMan to create a workflow of their jobs. DAGMan also provides some important mechanisms such as job retries and pre/post script for the jobs.

---++++ implementation, Deployment, and Management Description
OSGMM was designed to be easy to deploy and support at the VO/university/lab submit level. It is shipped with VDT and can be installed with the OSG client software stack with just one additional pacman command. OSGMM should be installed using the VDT method for a shared install which means root has to do the installing. This method takes care of setting up startup scripts for services and privilege separation as Condor and OSGMM will be configured to run as their own non-privileged users. OSGMM requires about 1 GB RAM (used to track jobs in the system). One instance will serve all users on that submit host, using Condor's fair-share algorithm to handle jobs from multiple users.

Each VO can have a central OSGMM instance (polling ReSS for information) to run verification/maintenance jobs and then have other instances of OSGMM  (polling the VO OSGMM instance for information including site status) running on the submit hosts. General verification and maintenance jobs ships with OSGMM, but each VO can augment the jobs with their own tests and for example software installations. This provides a solution for the VO to push out software to sites, and then advertise availability of the software back to the system. OSGMM scales up to managing about 5,000 Condor-G jobs per submit host. For VOs that prefer not to host their own job submission system, however, the Engage team hosts a fully functional submit host that users can utilize.

---++++Some OSGMM Considerations
In exchange for the relatively lightweight job submission mechanism whereby jobs are sent to distributed systems and managed by the distributed systems themselves, there are some inherent limitations as well. First, by the time that jobs actually get to the remote site, conditions may have changed and there can be longer than expected waiting for the remote queue to schedule the job for example. Also, if a site is not configured correctly, even correctly configured jobs can fail with unexpected results. 

"Pilot" based submission methods described below attempt to help hide some of these problems from end users, but do so by adding a considerable amount of sophistication into their tools. In general, they take more effort to set up and run but add some important capabilities as will be described below.

---+++Glide-in WMS

---++++High Level Architecture and Design Consideration

The Glidein Workload Management System (glideinWMS) utilizes a centralized pilot based submission system called a pilot or Glidein Factory (GF). A "pilot" job is a special type of job that lands on a distributed resource and then pulls in the users job to start executing it. This approach, often called the PULL model, has a few advantages over the standard, or PUSH model, implemented by the OSG MatchMaker:
   * The matchmaking of jobs to sites is much easier. Instead of trying to predict where the job will start first, pilots are sent to all the Grid sites that are supposed to be able to run the job. The first pilot that starts gets the job. The remaining glideins will either start another job that can run there (even if they were not submitted for the purpose of serving that job), or terminate within a short period of time. As long as there is a significant amount of jobs in the queue, only a small fraction of pilots will terminate without performing any useful work.
   * The pilot can validate the node before pulling a user job; as a consequence correctly configured users jobs are less likely to fail. Pilot jobs can of course fail to start with the same frequency as OSG MatchMaker submitted jobs, but these failures are seen only by the Glidein Factory that submitted the pilot job and the user does not "experience" this type of failure. 
   * Due to the centralized nature of the system, a pilot system can handle user priorities uniformly across the Grid. In the OSG MatchMaker model, each Grid site maintains the priorities of the users, based on the policies and historical usage data specific to that site.

This approach has of course also disadvantages:
  * It requires a much heavier investment in hardware, so it must manage all the job information in greater detail.
  * In order to get proper security, the pilot must be able to switch identity once the user job is pulled. This is not part of the original Grid security model, so only recently was a Grid tool, namely gLExec provided for this, and only a subset of Gird sites support it.

The glideinWMS system was developed on top of the Condor system. Most of its functionality comes from Condor condor itself, with just a thin layer on top of it. This approach was chosen to minimize the development and maintenance cost of the product, since Condor already provided a very powerful platform on which to build on. Moreover, this allows users with previous Condor know-how an easy path to the Grid world.

The glideinWMS-specific services are composed of a frontend and a glidein factory (GF). The glidein factory is responsible to know which Grid sites are available and what are they attributes, and to advertize these information as "entry-point ClassAds" to a dedicated collector (known as the WMS pool). The frontend instead plays the role of a matchmaker, internally matching user jobs to the entry-point ClassAds, and then requesting the needed amount of glideins to the factory. Finally, the factory will submit the pilots, also know as glideins, that will start the Condor daemons responsible for resource handling.

The picture below shows the architecture from a schematic point of view:<br>
<img src="%ATTACHURL%/glideinWMS_at_a_glance_medium.png" alt="glideinWMS at a glance"> 

Finally, GSI, i.e. X509 proxies, was chosen as the security mechanism. All communication between the various processes is authenticated via GSI and can be encrypted if desired.

---++++ Usage Description (how users use the technology & use cases)

From the *final user point of view*, glideinWMS is just a distributed Condor system. As with the OSG MatchMaker, the user must write a Condor job description, although for the vanilla universe, in order to run jobs. Most of the VOs currently using gldieinWMS have portals to insulate the final users from the details of the underlaying batch system, so this has been less of a problem for us that it has been for Engage.

The glideins advertise a set of attributes the users can use to match jobs to resources. The matching algorithm can be as complex or as simple as liked; for example, CMS matches just on the site name.

Finally, Condor DAGMan can be used to handle workflows just like with the OSG MatchMaker. Jobs of few minutes to several hours are supported; a single Condor schedd can easily handle 5-10Hz job turnaround rate. Very long jobs may be a problem due to preemption, like with the OSG MatchMaker, but Condor will restart the failed jobs so users usually don't see any failures.

GlideinWMS does not specify a mechanism for data transfers; i.e. it supports only what Condor supports. For modest input and output sizes, the standard Condor file transfer works fine (and recent versions of Condor support, possibly cached, HTTP transfer as well), but for larger file transfers users are left on their own. 
In our experience this was never a major problem, as the VO tend to have their own data handling solutions anyhow.

From the *VO point of view*, glideinWMS requires the installation of a Condor central manager and a submit node, and the installation of the glideinWMS-specific daemons.

The Condor central manager and submit node installation is very close to a standard Condor installation, with just small configuration changes; the glideinWMS installer can fully automate this, if desired.

The glideinWMS-specific services are composed of two parts; a factory and a frontend. The VO must install its own frontend, while the factory can be shared among several VOs. While larger VOs are encouraged to host their own factory to attain maximum flexibility, UCSD currently hosts a factory that is open (upon request) to smaller OSG VOs. So factory installation will not be discussed here, as it is intended for advanced VO admins.

The frontend configuration entails installing a Web server, creating the proper glidein matching configuration and starting the frontend daemons. The main elements to be configured are the list of Condor daemon DNs, the credential(s) used for glidein submission, the VO specific validation and data gathering scripts, and finally the matchmaking expression. The last one is needed because, as described above, the frontend is the glideinWMS matchmaker, and currently does not have a matching logic powerful enough to do true matchmaking; so help from the VO administrators is needed.

The attributes published for the matchmaking at the Condor level are gathered as a mix of static attributes provided by the factory (usually extracted from the information system,  a set of scripts provided by the factory and a set of scripts provided by the frontend. The reasoning behind this is that the factory can tailor the scripts toward the type of resources served, while the frontend extracts, and publishes, VO specific information. There is no prescribed naming schema for the attributes, although nothing prevents the factory and/or VO administrators to adopt one (like GLUE).

The factory and the frontend also can provide validation scripts, so user jobs never land on failing jobs; instead, the just the glideins fail.

---++++ Implementation, Deployment, and Management Description
VO Frontend periodically queries the user pool to check the status of user job queues. Based on the job queues, it instructs the glidein factory (GF) to submit glideins to run on Grid resource(s). A glidein is a properly configured Condor startd submitted as a Grid job. glideinWMS extensively uses Condor classad mechanism as a means of communication between its services. Once a glidein starts on a worker node, it will join the user Condor pool, making the obtained Grid-batch slot as a slot in the Condor pool. At this point, a regular Condor job can start there as if it is a dedicated resource. 

Condor itself handles the interaction with gLExec; it is only a matter of proper configuration.

glideinWMS does not provide GUI interface for the users to submit their jobs, instead users use Condor client tools like condor_submit to submit their jobs. The user interfaces with the Condor batch system, he/she is shielded from the problems mentioned above that are introduced by the Grid deployment. Based on the time slot allocated for glidein to run, this glidein created batch slot can run more than one user jobs before relinquishing its claim over the grid resource. Thus, glideinWMS supports resource provisioning. This mechanism is quite effective in running short jobs using glideinWMS. Since a single glidein can run multiple short jobs, the effective wait time/overhead of running several short jobs this way is equivalent to running a single grid job. The resource provisioning feature can also be used to provide a guaranteed time slot for long running jobs. glideinWMS also provides users with the capability of selecting resources based on resource characteristics, like, hardware type, OS, packages installed on the worker node and any custom information that is required for the user jobs to run. glideinWMS can be configured to run pluggable scripts as the glidein bootstraps. This is useful in advertising desired resource information. Thus the combination of glidein’s bootstrapping scripts and pluggable scripts can identify potential problems with the resource before the user job could start on the resource. This makes glideinWMS more resilient to bad CE/resources and the environment on the resource.

glideinWMS does provide a set of Web-based monitoring tools for both the glidein factory and the VO frontend. This allows the factory and VO frontend administrators to easily monitor the system and discover eventual problems.

Monitoring of the Condor pool can be achieved through standard Condor monitoring tools, varying from command line tools (like condor_q and condor_status), CondorView, and up to commercial tools like the ones developed and sold by Cycle Computing. 

---++++ Some Glide-in WMS Considerations

glideinWMS is used by in OSG and LCG by several VOs like CMS, CDF, !DZero, !IceCUBE and GPN and by experimental High Energy physics groups like Minos, Minerva and NOVA. glideinWMS supports several workflows supported by Condor from a simple job submission to complex DAGs. During the factory configuration step, glideinWMS can query the information systems like !ReSS and BDII to get the list of available grid resources. A system administrator can either accept all the sites or be more selective about the sites listed. glideinWMS will populate it's site list based on the selection made by the administrator and information about the sites listed in BDII or !ReSS. Current version of glideinWMS does not have any support for data management by the glideinWMS system itself. However, this is less of a short coming since most of the VOs have their own data management system which can be easily used by the user jobs run via glideinWMS.

glideinWMS is quite scalable. Although, all the glideinWMS components can be collocated on a single host for smaller use case, we recommend following services per host for larger use case -
   * glidein factory collocated with WMS Condor Pool
   * User Condor Pool Collector
   * User Schedd
   * VO Frontend

Table below shows the scalability figures for the glideinWMS system tested so far.

|    *Criteria*    |    *Design goal*    |    *Achieved so far*    |
| Total number of user jobs in the queue at a given time | 100k | 200k |
| Number of glideins in the system at any given time | 10k | ~26k |
| Number of running jobs per schedd at any given time | 10k | ~23k |
| Grid sites handled | ~100 | ~100 |

Most of the glideinWMS services do not need to be run with root privileges. HTTPD service on the VO frontend and Glidein factory node is typically installed and run as root user. Also, for security reasons, user Schedd(s) for non portal installation should run as root. Since there are several services to be installed and configured, the deployment model needs to be thought out based on the use case. Also, an administrator installing the services needs to be familiar with Condor Batch System and basic GSI concepts. If the gldiein factory is being installed, knowledge of the Grid (OSG, EGEE, Nordugrid) is also needed.

It should be noted that UCSD is hosting a glidein factory that smaller VO can use to start using the glideinWMS without having any knowledge of the Grid. 

Moreover, the glideinWMS team is actively working to make the installation process as easy as possible.

*glideinWMS Home Page:* [[http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/][glideinWMS Home]]

---+++PANDA
---++++ High Level Architecture and Design Considerations

Like glideinWMS, the Production and Distributed Analysis system (!PanDA) is a pilot based job submission system with all the benefits (and overheads) that pilot based systems require. Similar to the glideinWMS grid factory, PanDA uses its !PanDA Server to submit and manage pilot jobs. Principal features of its design were initially driven by operational requirements of Atlas experiment at LHC, however most of these are directly applicable to general OSG use. In contrast to glideinWMS that uses a Condor based WMS, PanDA created its own centralized relational database and brokerage system. The centralized database keeps detailed info about every job status and/or error message and can be queried using standard RDP techniques. PanDA also developed a user interface to that database that facilitates workload management, debugging and day-to-day operations. ???Maxim, Can you say something about designing Panda to distribute jobs to where the data is???

   * System-wide site/queue information database recording static and dynamic information used throughout !PanDA to configure and control system behavior from the 'cloud' (region) level down to the individual queue level. It is used by pilots to configure themselves appropriately for the queue they land on; by Panda brokerage for decisions based on cloud and site attributes and status; and by the pilot scheduler to configure pilot job submission appropriately for the target queue.
   * Coherent and comprehensible system view afforded to users, and to Panda's own job brokerage system, through a system-wide job database that records comprehensive static and dynamic information on all jobs in the system. To users and to Panda itself, the job database appears essentially as a single attribute-rich queue feeding a worldwide processing resource.
   * Easy integration of local resources. Minimum site requirements are a grid computing element or local batch queue to receive pilots, outbound http support, and remote data copy support using grid data movement tools. ??? Igor: How does this differ from OSGMM and glideinWMS???

Additional considerations are as follows: ???This section needs to be shortened???
   * Single workload management system to handle both ongoing managed production and individual users activity (such as analysis), so as to benefit from a common infrastructure and to allow all participants to leverage common operations support.
   * A coherent, homogeneous processing system layered over diverse and heterogeneous processing resources. This helps insulate production operators and analysis users from the complexity of the underlying processing infrastructure. It also maximizes the amount of code in the system that is independent of the underlying middleware and facilities
   * Security based on standard grid security mechanisms. Authentication and authorization is based on X.509 grid certificates, with the job submitter required to hold a grid proxy and VOMS role that is authorized for Panda usage. User identity (DN) is recorded at the job level and used to track and control usage in Panda's monitoring, accounting and brokerage systems. The user proxy itself can optionally be recorded in !MyProxy for use by pilots processing the user job, when pilot identity switching/logging (via gLExec) is in use.
   * Support for usage regulation at user and group levels based on quota allocations, job priorities, usage history, and user-level rights and restrictions.
   * A comprehensive monitoring system
      * detailed drill-down into job, site and data management information for problem diagnostics
      * Web-based interface for end-users and operators
      * usage and quota accounting
      * performance monitoring of Panda subsystems and the computing facilities being utilized.

---++++Usage Description

Jobs are submitted to !PanDA via a simple client interface by which users define job sets, and their associated data. Job specifications are transmitted to the !PanDA server via secure http (authenticated via a grid certificate proxy), with submission information returned to the client. This client interface has been used to implement a variety of !PanDA front-end systems. The !PanDA server receives work from these and places it into a global job queue, upon which a brokerage module operates to prioritize and assign work on the basis of job type, priority, input data and its locality, available CPU resources and other brokerage criteria.

An independent subsystem manages the delivery of pilot jobs to worker nodes via a number of scheduling systems. A pilot once launched on a worker node contacts the dispatcher and receives an available job appropriate to the site. If no appropriate job is available, the pilot may immediately exit or may pause and ask again later, depending on its configuration (standard behavior is for it to exit). If, however, a job is available, the pilot obtains a payload job description, whose principal component is the URL form which the payload script needs to be downloaded. This mechanism, where payload scripts are hosted a separate Web server which may be, and usually us, separate from the !PanDA system proper. This allows users to exercise necessary level of control over the payloads, and reduce the load on the !PanDA server, while still having standard security mechanisms in place.

An important attribute of this scheme for interactive analysis, where minimal latency from job submission to launch is important, is that the pilot dispatch mechanism bypasses any latencies in the scheduling system for submitting and launching the pilot itself. The pilot job mechanism isolates workload jobs from grid and batch system failure modes (a workload job is assigned only once the pilot successfully launches on a worker node). The pilot also isolates the Panda system proper from grid heterogeneities, which are encapsulated in the pilot, so that at the Panda level the grid(s) used by !PanDA appears homogeneous. Pilots generally carry a generic 'production' grid proxy, with an additional VOMS attribute 'pilot' indicating a pilot job. Optionally, pilots may use glexec to switch their identity on the worker node to that of the job submitter.
??? Igor: Since you come after the gldieinWMS section, can you please compare this to what the glideinWMS does???

The overall !PanDA architecture is shown below<br/>

     <img src="%ATTACHURLPATH%/panda-arch.jpg" alt="panda-arch.jpg" width='687' height='524' />

To better illustrate how !PanDA is used, let us enumerate its principal components with which the end-user or agent interacts most often:
   * !PanDA server
   * !PanDA monitor
   * Pilot generator (aka pilot scheduler)

The first two components are centrally installed, managed and maintained. Same applies to the pilot scheduler, however in select cases local administrators or advanced users can run this process themselves. ???Has anyone done this???

Assuming that these components are in place an running, a typical usage scenario unfolds as follows:
   * the user submits a job using a command line client
   * job is registered on the server and becomes visible to the user in the monitor
   * pilots submitted to sites defined by the user are submitted and run without user intervention; when they connect to the server, the following automated step takes place
   * !PanDA brokerage mechanism picks a pilot according to pre-defined criteria, and communicates information to the pilot, which is sufficient to obtain the actual workload
   * The pilot downloads and executes the payload; throughout the process the status of the pilot and the job is reflected in the monitor
   * Upon job completion, the standard output, standard error and other attendant files are available to the user to inspect, from Web pages served by the monitor

---++++ implementation, Deployment, and Management Description

!PanDA in its critical part is using industry-standard, well understood and tested components. Crucial component employed in a few elements of the !PanDA architecture is the Apache Web server equipped with _modpython_ and _modgridsite_. !PanDA is using a RDBMS as its backend, and has in fact been successfully deployed using both !MySQL and ORACLE RDBMS, as dictated by deployment requirements. As commented above the server and monitor components, both implemented as Web services, are centrally managed. As such, they are subject to security and access control protocols of the site where they are deployed (same applies to RDBMS). The amount of software that needs to be download and installed by users not employing specific project frameworks such as ATLAS is extremely small. Initial setup for a new group and/or user amounts to adding an entry to the database which is used by both pilot scheduler and the server itself.

---++++ Some !PanDA Considerations
Currently PanDA is limited to Atlas sites where the PanDA system is enabled, although other interested sites can readily incorporate PanDA if they so desire. ???Who is using panda besides Atlas??? ???If a user is already set up to use a Condor DAG workflow, what do they need to do to make it work with PanDA???

---++ Summary and Conclusions (1-2 paragraphs)
Additional options not described in this document: Gridway, Pegasus, Swift



%STOPINCLUDE%

%BOTTOMMATTER%

-- Main.DanFraser - 14 Jan 2010



%META:FILEATTACHMENT{name="glideinWMS_at_a_glance_medium.png" attachment="glideinWMS_at_a_glance_medium.png" attr="" comment="glideinWMS Architecture and use case" date="1265230676" path="glideinWMS_at_a_glance_medium.png" size="46186" stream="glideinWMS_at_a_glance_medium.png" tmpFilename="/usr/tmp/CGItemp17405" user="ParagMhashilkar" version="1"}%
%META:FILEATTACHMENT{name="panda-arch.jpg" attachment="panda-arch.jpg" attr="h" comment="Panda Architecture" date="1265249471" path="panda-arch.jpg" size="283296" stream="panda-arch.jpg" tmpFilename="/usr/tmp/CGItemp8774" user="MaximPotekhin" version="1"}%
