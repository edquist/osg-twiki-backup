%META:TOPICINFO{author="BrianBockelman" date="1266465343" format="1.1" version="1.11"}%
%META:TOPICPARENT{name="GridUsersGuide"}%
%LINKCSS%
---+ Submitting a Single Job Using Condor-G

%STARTINCLUDE%
%EDITTHIS%

<ol>
  <li>Write a simple script to execute!  We provide one below that will print out a few environment variables, setup the worker node client, and sleep for a bit:
<br/>
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="programlisting">
#!/bin/sh
echo "Hello from `hostname`"
echo "Going to source the file $OSG_GRID/setup.sh"
source $OSG_GRID/setup.sh
echo "Resulting environment:"
printenv
echo "Output of lcg-cp --help (lcg-cp is a commonly used SRM client):"
lcg-cp --help
echo 'Directories in \$OSG_APP'
ls $OSG_APP
date
sleep 120
date
</pre>
%ENDTWISTY%
<br/>
Name this script "mytest.sh" and set it executable with =chmod +x mytest.sh=.
  </li>
  <li>Create a file named =condorg_test.submit= that contains the following (we will cover this line-by-line below):
<br/>
%TWISTY{
mode="div"
showlink="Show condor submit file..."
hidelink="Hide condor submit file"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="programlisting">
Universe        = grid
grid_resource = gt2 red.unl.edu:/jobmanager-condor
Executable      = mytest.sh
Output          = job_test.output
Error           = job_test.error
Log             = job_test.log
queue
</pre>
%ENDTWISTY%
Replace the endpoint =red.unl.edu:/jobmanager-condor= with the endpoint you selected from the [[FindAvailableResource][finding available resources guide]].
   </li>
   <li>Source the environment for the OSG client.
<pre class="screen">
<userinput>source $VDT_LOCATION/setup.sh</userinput>
</pre>
   </li>
   <li>Obtain a proxy from your VOMS:
 <pre class="screen">
<userinput>voms-proxy-init --voms cms:/cms</userinput>
</pre>
Replace =cms:/cms= as appropriate for your VO.
   </li>
   <li>Submit the =condorg_test.submit= job:
<pre class="screen">
$ <userinput>condor_submit condorg_test.submit</userinput>
</pre>
   </li>
   <li>Give yourself some time. Grid middleware can add a latency of about a minute on top of the amount of time your job spends in the batch system queue (if any time).  While you're waiting, read on through the rest of the guide for an explanation of the Condor submit file.
   </li>
   <li>Use =condor_q= to display information about your jobs:
<pre class="screen">
<userinput>condor_q -globus</userinput>
</pre>
You should expect to see the jobs idle for a minute or two, go into the running state for a minute or two, and then end.  You can also follow along in the log file job_test.log which Condor should create in your working directory.

Here is sample output from =condor_q -globus=:
<br/>
%TWISTY{
mode="div"
showlink="Show condor output..."
hidelink="Hide condor output"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre>
-bash-3.2$ condor_q -globus


-- Submitter: hcc-grid.unl.edu : <129.93.229.131:50400> : hcc-grid.unl.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
7709.62  dweitzel      ACTIVE  fork     condor.crc.nd.edu  /opt/osg/osg-120/o
7709.802 dweitzel      ACTIVE  fork     condor.crc.nd.edu  /opt/osg/osg-120/o
7709.816 dweitzel      ACTIVE  fork     tuscany.med.harvar  /opt/osg/osg-120/o
7990.0   dweitzel      PENDING condor   condor.crc.nd.edu   /opt/osg/osg-120/o
8078.0   dweitzel      PENDING condor   gridgk02.racf.bnl.  /opt/osg/osg-120/o
8079.0   dweitzel      PENDING pbs      condor.crc.nd.edu  /opt/osg/osg-120/o
8064.0   dweitzel      PENDING condor   condor.oscer.ou.ed  /opt/osg/osg-120/o
8073.0   osgmm         PENDING sge      antaeus.hpcc.ttu.e  /opt/osg/osg-120/o
</pre>
%ENDTWISTY%
    </li>
</ol>


---++ Condor-G submit files
In this section, we will cover different essential lines in the simple Condor-G submit file used above.
<pre class="programlisting">
universe=grid
grid_resource = gt2 red.unl.edu:/jobmanager-condor
</pre>
Setting the Condor universe to grid enables the Condor-G mode for this job.  In the grid universe, we always must specify what resource to submit to.  The two important parts of this line are "gt2" (referring to the GRAM protocol; you will always use this for OSG sites) plus the GRAM endpoint of the remote host, including the jobmanager name.  Note that we are manually steering this job to a specific cluster - no matchmaking will be done.

Next, we specify what executable to run, where to place the stdout/stderr when the job is done, and where to write the condor log file:
<pre class="programlisting">
Executable      = mytest.sh
Output          = job_test.output
Error           = job_test.error
Log             = job_test.log
</pre>
The output and error are only copied over by Condor-G when the job ends - the output is not streamed back to your submit host as it runs.  The log file will be updated by Condor whenever the job state changes.  The =mytest.sh= script is also copied over to the remote worker node by Condor.

---+ Conclusions

Once this job has completed, congratulations!  You have run your first Condor-G job.  This tutorial covered running a single job, consisting of a single script, no input, and only the stdout/stderr returned.  The job demonstrates using Condor-G, but does "nothing interesting".

We will continue on with more complex examples which include:
   * Submitting many unique jobs from a single submit file
   * Transferring input and output files with Condor-G
   * Passing arguments to the script
   * Running workflows where the jobs have dependencies on each other.
   * Understanding and using the OSG worker node user environment.

%STOPINCLUDE%

For more information, see the [[http://www.cs.wisc.edu/condor/condorg/][Condor-G documentation]].