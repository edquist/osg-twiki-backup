%META:TOPICINFO{author="StevenTimm" date="1286507004" format="1.1" version="1.14"}%
%META:TOPICPARENT{name="GlossaryOfTerms"}%
This topic holds the terms beginning with "C" and will be included in the GlossaryOfTerms topic.

%STARTINCLUDE%
%INCLUDE{ "GlossaryJumpIndex" }%

%EDITTHIS%

   $ CA<a name="DefsCA"></a>: _See_ [[#DefsCA][Certificate Authority]].

   $ CE<a name="DefsCe"></a>: _See_ [[#DefsComputeElement][Compute Element]].

   $ CEMon<a name="DefsCEMon"></a>: [[http://grid.pd.infn.it/cemon/field.php?n=Main.AboutCEMon][CEMon]] service provides a common interface for publishing information about a Compute Element in your network. The framework can be configured with multiple sensors for collecting different kinds of data, such as the Generic Information Provider (GIP). The information can be published in multiple formats, such as LDIF and ClassAds. [[https://is.grid.iu.edu/documentation.html][In OSG it is an important part if the Information Service (IS)]]: the CEMons running on the resources collect information from the GIP and push it to the Resource Selection Service (ReSS) and to the CEMon Consumer at the GOC that in turn publishes the information to the both the OSG BDII and the WLCG Interoperability BDII.

   $ CE-SE binding: Job scheduling often requires both a compute element (CE) to run the job and a Storage Element (SE) to provide for an input or output storage extent. Currently , there are static relationships between individual CEs and SEs set by Site Admins. The CE-SE bind schema (part of GLUE schema) aims at providing the means for publishing such a relationship with eventual per-pair data. At the moment, the published information is limited to the local mount point on the CE pointing to the SE's storage space.

   $ CE storages<a name-"DefsCeStorages"></a>: disk spaces or [[#DefsStorageElement][Storage Elements]] accessible from within a [[#DefsComputeElement][Compute Element]].

   $ CPU Hours<a name-"DefsCPUHours"></a>: The work done by a CPU in one hour of wall clock time; varies depending on processing capabilities.  

#DefsCertificate
   $ Certificate: A public-key certificate is a digitally signed statement from one entity (e.g., a certificate authority), saying that the public key (and some other information) of another entity (e.g., the grid user) has some specific value. The X.509 standard defines what information can go into a certificate, and describes how to write it down (the data format). Read more at ReleaseDocumentation.CertificateWhatIs .

#DefsCA
   $ Certificate Authority: An entity that issues certificates for use by other parties. The OSG recognizes certificates issued by a number of certificate authorities. For more information consult [[http://en.wikipedia.org/wiki/Certificate_authority][Wikipedia]].

   $ Cluster: A networked group of worker nodes (plus head node, if applicable) at a site. In the GLUE schema, a cluster is a container that groups together subclusters, or computer nodes. A cluster may be referenced by more then one computing element (CE).

<!--   $ CMS: Compact Muon Solenoid experiment at the LHC at CERN (http://cmsinfo.cern.ch/Welcome.html/)
-->
#DefsComputeElement
   $ Compute element (CE): Compute element  is a term used in Grids to denote any kind of computing interface, e.g., a job entry or batch system. A compute element consists of one or more similar machines, managed by a single scheduler/job queue, which is set up to accept and run grid jobs. The machines do not need to be identical, but must have the same OS and the same processor architecture.  In OSG, the CE runs the bulk of the OSG software stack.  See [[ReleaseDocumentation/SitePlanning][Site Planning]].

   $ Compute Hours: _See_ [[#DefsCPUHours][CPU Hours]]

#DefsCondor
   $ Condor: Condor is a specialized workload management system for compute-intensive jobs. Like other full-featured batch systems, Condor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. Condor is a grid middleware component developed at UW Madison, packaged and distributed as part of the Virtual Data Toolkit (VDT). Read more at [[http://www.cs.wisc.edu/condor/]] . 

   $ condor-g: Condor-G is the job management part of Condor. (The G is for Grid, which it uses to start the job on the remote machine.) Condor-G lets you submit jobs into a queue, have a log detailing the life cycle of your jobs, manage your input and output files, along with everything else you expect from a job queuing system.   OSG uses mostly the "gt2" (Globus) type of grid jobs but Condor-G can also support many other Grid and cloud protocols.

#DefsCRL
   $ CRL: A <b>C</b>ertificate <b>R</b>evocation <b>L</b>ist is a list of [[Documentation/GlossaryC#DefsCertificate][Certificates]] that have been revoked and should not be relied upon. The list enumerates revoked certificates along with the reason(s) for revocation. The issuer and the issue date are also included. In addition, each list contains a proposed date for its next release. When a potential user attempts to access a server, the server allows or denies access based on the CRL entry for that particular user. For more information consult [[http://en.wikipedia.org/wiki/Revocation_list][Wikipedia]]

<!-- from forrest, enstore
---
   $ cern wrapper<a name="DefsCernWrapper"></a>: A [[#DefsFileFamilyWrapper][file family wrapper]] that accommodates data files up to (10<sup>21</sup> &#8211; 1) bytes. It matches an extension to the ANSI standard, as proposed by CERN, and allows data files written at Fermilab to be readable by CERN, and vice-versa. See file family wrapper. 
   $ collective layer<a name="DefsCollectiveLayer"></a>: fourth of five layers to support grid applications. Includes [[#DefsReplicaCatalog][Replica Catalog]], [[#DefsReplicaSelection][Replica Selection]], [[#DefsRequestPlanning][Request Planning]], [[#DefsRequestExecution][Request Execution]].
   $ command pipelining<a name="DefsCommandPipelining"></a>: a process in which commands are broken into several smaller steps, that may be executed in sequence using different components of the CPU, meanwhile allowing others components of the CPU to start executing the next command in parallel. 
   $ Compute Element<a name="DefsComputeElement"></a>: An element capable of running a user job.
   $ condor<a name="DefsCondor"></a>: a project at the University of Wisconsin-Madison. <sup>2</sup>a specialized workload management system for compute-intensive jobs that extends batch features to allow use of idle CPUs or clusters of CPUs. _See also_ [[http://www.cs.wisc.edu/condor/description.html][Condor website]].
   $ configuration server (CS)<a name="DefsConfigurationServer"></a>: Maintains and distributes the information about Enstore system configuration, such as the location and parameters of each Enstore component and/or server. 
   $ connectivity layer<a name="DefsConnectivity"></a>: second of five layers to support grid applications. Includes communication, authorization, delegation, etc.
   $ control channel (in GridFTP)<a name="DefsControlChannel"></a>: a low bandwidth, encrypted and integrity-protected TCP link over which [[#DefsGridFtp][GridFTP]] commands and responses flow.
   $ cpio_odc wrapper<a name="DefsCpio_odcWrapper"></a>: A [[#DefsFileFamilyWrapper][file family wrapper]] which allows the file to be dumpable via cpio. This wrapper has a file length limit of (8G &#8211; 1) bytes. _See also:_ file family wrapper. 
   $ crc (Cyclic Redundancy Check)<a name="DefsCrc"></a>: Used to verify that data has been stored properly. It is used like a checksum, but is less prone to multiple-bit errors. During a transfer, both sides calculate the crc and compare the values, unless the (two dashes)no-crc option is specified. Enstore uses a zero seed Adler-32 crc. 
   $ cwd<a name="DefsCwd"></a>: current working directory 
   $ cycle stealing<a name="DefsCycleStealing"></a>: harnessing the unused cycles of desktop workstations [[http://www.cs.wisc.edu/condor/myths.html][REF]]
-->
%COMMENT{type="tableappend"}%
%STOPINCLUDE%

-- Main.ForrestChristian %BR% 