%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dJames_20Weichel_2018840" date="1297788914" format="1.1" reprev="1.19" version="1.19"}%
%META:TOPICPARENT{name="GlossaryOfTerms"}%
---+ Glossary M

This topic holds the terms beginning with "M" and will be included in the GlossaryOfTerms topic.

%STARTINCLUDE%
%INCLUDE{ "GlossaryJumpIndex" }%
%EDITTHIS%

   $ Managed Fork:  The [[ReleaseDocumentation/ManagedFork][ManagedFork]] jobmanager is a more scalable way to manage jobs which run on the compute element.  Each fork job that is intended for execution on the compute element is queued in condor and run in the Condor "local universe" on the CE.  This allows logging of what commands were run, and also a mechanism to throttle how many are running at once.

#DefsMatchMaker
   $ Match Maker (MM):  OSGMM is the Open Science Grid Match Maker, a service that sits on top of the OSG client software stack. It obtains site information from [[GlossaryOfTerms#DefsRess][ReSS]] (Resource Selection Service) and uses a feedback system from/to Condor to publish the site information and keep job success history data. OSGMM can be used to schedule compute jobs across all resources available to a particular VO on the OSG, and to subsets of the resources with Condor's requirements expressions. 

   $ Member organization of OSG: An institution, facility or VO that contributes to the development and resource pool of OSG.

   $ Middleware: Middleware is software that connects two or more otherwise separate applications across the Internet or local area networks. More specifically, the term refers to an evolving layer of services that resides between the network and more traditional applications for managing security, access and information exchange. OSG recognizes two levels of middleware: Grid middleware (e.g., VDT, Grid3-gridmap, etc.) and VO- specific application middleware.

#DefsMessagePassing
   $ Message Passing: An approach to parallel computing where data and work is divided across the processors (striving for optimal load balancing) and communication between them is managed (striving for minimal and non-blocking communication) by explicitly calling specific library functions. The _de facto_ standard for message passing is [[#DefsMpi][MPI]].

#DefsMpi
   $ Message Passing Interface (MPI): A standard for message passing between processors.  _See_ http://www.mpi-forum.org/docs/docs.html.

#DefsMultiGridInteroperabilityNow
   $ Multi-Grid Interoperability Now (GIN): An initiative between 9 grid infrastructure projects to promote interoperability.

<!--
#DefsMonALISA
   $ !MonALISA: MONitoring Agents using a Large Integrated Services Architecture. It is a grid middleware component. optional in OSG, that offers a scalable monitoring infrastructure based on intelligent agents implemented through a distributed services architecture. Each service can register itself and then be discovered and used by any other services or clients. All services and clients subscribing to a set of events (state changes) in the system are notified automatically. The framework integrates several existing monitoring tools and procedures to collect. Read more at http://monalisa.cacr.caltech.edu/. 
-->

   $ Monitoring (grid monitoring): Grid monitoring involves collecting, analyzing and displaying information from the distributed production infrastructure in order to determine server status and application progress, and to log performance data of CPUs, networks and storage devices. <!--OSG implements MonALISA as a monitoring infrastructure, GridCat as a high level grid cataloging system, and ACDC to provide near real-time snap shots of critical computational job metrics.-->

#DefsMss
   $ MSS: See [[#DefsMassStorageSystem][Mass Storage System]].

#DefsMyOsg
   $ !MyOSG: !MyOSG is an one-stop location for various OSG information including, but not limited to, monitoring status, availability history, Virtual Organization information, contact information and accounting metrics.  _See it in action:_ http://myosg.grid.iu.edu/about.

#DefsMassStorageSystem
   $ Mass Storage System:  A high-capacity, large-scale data archive (usually tape) that is more intelligent than a normal storage system, and used to hold large amounts of infrequently accessed data. See also [[GlossaryOfTerms#DefsHierarchicalResourceManager][HRM]], [[GlossaryOfTerms#DefsTrm][TRM]].

#DefsMatchmaking
   $ Matchmaking: The process of matching a job to a slot while maintaining site priorities.

<!--
   $ MDS<a name="DefsMds"></a>: _See_ [[#DefsMetacomputingDirectoryService][Metacomputing Directory Service]].
   $ media changer (MC)<a name="DefsMediaChanger"></a>: Physcial device that mounts and dismounts the media into and out of drives according to requests from the Movers. 
   $ memory-mapped I/O: _Also_ mmapped I/O. With this type of I/O, part of the CPU&#8217;s address space is interpreted not as accesses to memory, but as accesses to a device; once you map a file to memory, changes made to the memory map are propagated back to the file. Mmapped I/O strives to avoid memory copies of the data between the application memory space and the kernel memory space (also see direct I/O and POSIX I/O). Mmapped I/O is in the POSIX standard. 
   $ Metacomputing Directory Service (MDS)<a name="DefsMetacomputingDirectoryService"></a>: a GIIS from Globus that consists of a LDAP representation in which directory structures, data representations and APIs are defined.
   $ MIMD: _See_ [[#DefsMimd][Multiple Instruction, Multiple Domain]]
   $ mmapped I/O: _See_ memory-mapped I/O.
   $ monitor server (MS)<a name="DefsMonitorServer"></a>: Available for investigating network-related problems. It attempts to mimic the communication between an encp request, the corresponding library manager, and the mover. 
   $ mover (dCache)<a name="DefsMover"></a>: A process that receives or sends data to the client from the dCache system.
   $ Multiple Domain, Multiple Instruction (MIMD)<a name="DefsMimd"></a>: An approach to parallel computing that combines function and domain decompositions. MIMD can be further subdivided into _Global Memory, Shared Variable_ (shared memory multiprocessors); _Global Memory, Message Passing (GMMP)_; _Distrbuted Memory, Shared Variable (DMSV)_ (distributed shared memory); and _Distributed Memory, Message Passing (DMMP)_. MPI is concentrated on DMMP.  _See also:_ [[#MessagePassing][message passing]] [[#DEJ06][(DEJ06)]] 
   $ Multiple Instruction, Single Data (MISD)<a name="DefsMisd"></a>: functional decomposition of a computing problem where each processor is assigned a different role/responsibility. [[#DEJ06]]
-->
%COMMENT{type="tableappend"}%
%STOPINCLUDE%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = Main.JamesWeichel

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = General

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = Student

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = Main.IgorSfiligoi
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->

Main.JamesWeichel - 11 Feb 2011
