%META:TOPICINFO{author="MarcoMambelli" date="1329172063" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="Documentation/Release3.NavAdminSmallSites"}%
---+!! *OSG Site Topology*
%DOC_STATUS_TABLE%
%TOC%

<!-- conventions used in this document
   * Local UCL_HOST = %URLPARAM{"INPUT_HOST" encode="quote" default="host"}%
   * Local UCL_USER = %URLPARAM{"INPUT_USER" encode="quote" default="user"}%
   * Local UCL_DOMAIN = %URLPARAM{"INPUT_DOMAIN" encode="quote" default="opensciencegrid.org"}%
   * Set TWISTY_OPTS_DETAILED = mode="div" showlink="Show Detailed Output" hidelink="Hide" showimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" hideimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" remember="on" start="hide" 
-->

---+ About this document
This document is aimed mainly to [[Documentation/Release3.NavAdminSmallSites][small sites]] and provides a brief overview of some networking concept and possible topologies for an OSG cluster.

---+ Introduction

Your campus or your department will provide you a network connection that in this document we refer as _extranet_ (everything outside of the Firewall you manage), i.e. the network that connects also to the world outside of your OSG cluster.

Depending on geographical and hardware constraints or on your choices, the nodes of a cluster can be connected using the different topologies described in the following sections.
A default configuration, including subnet IP address, is described as well.

The diagrams below have acronyms referring to possible nodes.
The [[Documentation/Release3.NavAdminSmallSites][small sites navigation document]] presents possible components of a cluster.
%TWISTY{%TWISTY_OPTS_DETAILED% showlink="Click here to show more detailed information on the meaning of the names" }%   
Here is a naming example to simplify documentation and examples.
We give a name to the cluster: =gc= (Grid Cluster). Each node in the cluster is here named according its functionality, has a unique fully qualified domain name (FQDN).  The FQDN includes a local hostname and a parent domain name (here "yourdomain.org"). 
The following is a list of roles (abstract functionalities) that nodes may have within the cluster and corresponding hostnames. This means that the host that is your compute element (if you have a CE) has an FQDN =gc-ce.yourdomain.org=.
A node may have more than one name if it runs more than one service. If a node is covering two or more roles, then all those names will refer to the same node and can be _cnames_, e.g. the Storage Element and the Xrootd redirector may be co-located on the same node, making =gc-se= and =gc-xrdr= different names (aliases) of the same node. If there are no nodes covering a role then there will be no node with that name. 
Here is a list of possible names and intended functionalities:
| *Acronym* | *Short* | *Hostname* | *Description* |
| UI | =gc-ui= | =gc-ui.yourdomain.org= | The user interface machine, can also be referred to as Interactive node 1 (you may have more than one) |
| WN | =gc-wn001= | =gc-wn001.yourdomain.org= | Worker Node 1 (usually you have several) |
|| =gc-wn002= | =gc-wn002.yourdomain.org= | Worker Node 2 ...(at least 2) |
| SN | =gc=sn001= | =gc-sn001.yourdomain.org= | Storage Node 1 ..., sometime co-located with a Worker Node (usually you have several) |
| NFS | =gc-nfs= | =gc-nfs.yourdomain.org= | The NFSv4 server |
| HN | gc-hn | =gc-hn.yourdomain.org= | The queue head-node |
| SE | =gc-se= | =gc-se.yourdomain.org= | The Storage Element |
| CE | =gc-ce= | =gc-ce.yourdomain.org= | The Compute Element |
| XRDR | =gc-xrdr | =gc-xrdr.yourdomain.org= | The xrootd redirector |
| GUMS | =gc-gums= | =gc-gums.yourdomain.org=  | The GUMS server |
| Squid | =gc-proxy= | =gc-proxy.yourdomain.org= | Squid (proxy) server |
| NAT | =gc-net= | =gc1-net.yourdomain.org= | The networking box with Firewall/Router (e.g. NAT server) |

%ENDTWISTY%


---+ Topology with intranet and extranet
The recommended configuration for a OSG cluster includes a second network, _intranet_ (internal LAN or VLAN), connecting only nodes of the cluster. In this topology, visible in Fig. 1, the intranet is normally a "private" network, connected to the extranet using a NAT and proxy server (a Firewall). This provides better security, because several nodes have limited network access, and limits the number of IP addresses used by the OSG cluster, specially for clusters with many nodes(>30). Worker nodes do not require a public IP and this way they can work without it.

   * Fig. 1 - Topology with intranet and extranet: <br />
     <img src="%ATTACHURLPATH%/network1.png" alt="network1.png" width='683' height='172' />    

Most of the time all nodes require outbound network connectivity and this can be provided by a proxy server (e.g. Squid) and/or a NAT (network address translation) server.
Some nodes will require also inbound network connectivity and this can be provided using port forwarding on the NAT server or connecting those nodes also to the extranet. We will choose the latter.
Nodes that can benefit from being on the extranet are:
   * NAT - Necessary if you want to provide outbound connectivity to the nodes only in the intranet
   * Squid - Recommended to improve performance
   * SE - Recommended if you want outside users to access your SE
   * CE - Recommended if you want outside users to access your CE
   * UI - Would allow outside access to your UI
<!--   * NFS server - only to improve performance? something else? Check notes of installation with Rik -->


To simplify the documentation we are choosing here a default configuration for the intranet.

<!--
The default address for the intranet (subnet) used in the small sites documents is 192.168.192.0/18. This defines a subnet with 18 bits fixed for the network addresses and 14 bits available for the hosts. This means that you can have up to 16,191 private IP addresses from 192.168.192.1 to 192.168.255.254.
-->

---+ Simplified network topology
If the previous topology is not possible, either because the nodes of the OSG cluster are scattered across campus, or because the hardware does not allow it, or by choice, then all the nodes of the cluster can reside on the same network (extranet and intranet are the same), like shown in Fig. 2.

   * Fig. 2 - Network topology with a single network: <br />
     <img src="%ATTACHURLPATH%/network2.png" alt="network2.png" width='628' height='140' />    

All the cluster nodes will connect directly to your "public" network. Therefore, you will need to obtain an IP address from your local network administrator for each of the hosts you will include in your OSG cluster. 
In this case the subnet and the IP addresses will be whichever you'll be provided with.

---+ Campus/department network
The extranet is the network provided to you for your cluster by the campus or the department. 
The extranet may be a "public" network, directly connected to the Internet or bridged to it. It may also be that you are provided with an 
extranet which is a private network provided internally to your campus or department (intranet for the campus or department), which is is separated from the Internet by a firewall that limits the access. The two examples are shown in Fig. 3.

   * Fig. 3 - Network connection to the outside: <br />
     <img src="%ATTACHURLPATH%/network-connection.png" alt="network-connection.png" width='336' height='246' />    

If your configuration is the latter you have to make sure (talking to your network administrators) that at least the Storage Element and the Compute Element are visible on the "public" Internet so that they can be available on the Grid.
It is also convenient for interactive nodes to be "public" so that users may log in directly to them and a Web proxy could also be on the "public" network for performance reasons.
Read also the [[Documentation/Release3.FirewallInformation][Release 3 Firewall information document]] for further network requirements.

<!--
Furthermore if the extranet is a subnet in the range 192.168.x.y, and x &gt;= 192, then the default configuration mentioned above will not work (there may be conflicts in the assigned IP). In this case the subnet used for the intranet has to be different from the default, read further the [[IntranetNotes][intranet notes]] to find out which documents are affected.
-->

---+ References
Some networking definitions
   * http://en.wikipedia.org/wiki/Intranet
   * http://en.wikipedia.org/wiki/Extranet
   * A [[http://www.techiwarehouse.com/cms/engine.php?page_id=d9e99072][tutorial explaining networking terms]]
The [[Documentation/Release3.FirewallInformation][Release 3 Firewall information document]]

---+ Comments
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Trash/Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Planning
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during DOC workshop
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  JamesWeichel
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->

%META:FILEATTACHMENT{name="network2.png" attachment="network2.png" attr="" comment="" date="1329170731" path="network2.png" size="22888" stream="network2.png" tmpFilename="/usr/tmp/CGItemp35250" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network-connection.png" attachment="network-connection.png" attr="" comment="" date="1329170750" path="network-connection.png" size="16505" stream="network-connection.png" tmpFilename="/usr/tmp/CGItemp35274" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network1.png" attachment="network1.png" attr="" comment="" date="1329170812" path="network1.png" size="24953" stream="network1.png" tmpFilename="/usr/tmp/CGItemp35180" user="MarcoMambelli" version="1"}%
%META:TOPICMOVED{by="MarcoMambelli" date="1329172063" from="Documentation/Release3.ClusterTopology" to="Documentation.ClusterTopology"}%
