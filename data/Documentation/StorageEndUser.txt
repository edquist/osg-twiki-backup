%META:TOPICINFO{author="TedHesselroth" date="1267053901" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="WebHome"}%
%DOC_STATUS_TABLE%

---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC{depth="2"}% 
https://twiki.grid.iu.edu/twiki/pub/Storage/WebHome/images.jpg

#IntroStorageEndUser
---++ Storage for the End User

---+++Overview

The end user of storage is primarily concerned with data movement. Other aspects, such as operation of the Storage Element, space management, and policies for retention and access are handled by the Virtual Organization or site. If you are already a member of a Virtual Organization, it may be useful to consult the documentation for your VO regarding data movement, as tools and computing models for your research may be available. 

#DataMovementPatterns
---+++Data movement patterns

There are several patterns of data movement that have evolved and can be found in use on the Open Science Grid. In general, these patterns are used in combination, where the access. For details on specific implementations, see the StorageEndUser page. 

   * _Files moved through Automated Data Distribution_: (VDT: RFT, Other: Phedex,FTS) When there are a large number of files to be moved, or data movement is an ongoing process, or a file catalog is employed an extra layer of software to handle data transfers may be used. Such a layer provides features that the simple clients lack. Some features of a data movement services are: taking requests for a set of transfers, handling requests according to file catalog name, subscribing to a published data stream, scheduling and initiating transfers, reporting on the status of transfers, and retrying failed transfers.
   * _Files moved in a staging process_: A staging process may have some of the features of a data distribution service, but is more closely tied to job submission. The data movement request is part of a workflow that may include job execution. As a simple example, the workflow may indicate, "move the data in to the site, if that succeeds, run the analysis on it." File transfers and monitoring are handled by the workflow management system.  DAGMAN+Pegasus, STORK
   * _Files moved during job execution_: Jobs running on worker nodes may also move files within the job context. Clients exist in the software environment for simple data movement and may be invoked as part of a job sequence. This approach has the advantage of simplicity, but may be inefficient when the same file is repeatedly moved by different jobs, or only a fraction of the content of a moved file is actually needed. Whenever data is moved out from a worker node, the network topology needs to be taken into account; when the worker node in on a private network, not all transfer modes will be available. Whole files input and output. Results returned to user. Lightweight -wget, squid. Larger, multichannel -see client tools. firewall
   * _Files read during job execution_: Strictly speaking, this should be referred to as data access rather than data movement.  Rather than moving a whole file, job executables may read read from a file while leaving it in place. Files available from an NFS system as a part of a [[ReleaseDocumentation.LocalStorageConfiguration][Local Storage Configuration] may be read through standard language calls. Most Storage Element implementations also support posix IO, though in some cases their use in complicated by the need for preloaded libraries. The Storage Element namespace must also be mounted on the worker node, which is not always the case. Finally, some Storage Elements do not allow appending data to a file, due to difficulties associated with maintaining replicas and tape-backed copies. The use of file access techniques rather than data movement makes for efficient use of bandwidth, and simplifies job management by making data placement a separate process.


---+++Client tools

Clients for file transfer can be divided into three categories. Documentation for specific implementations may be found below. 

   * Clients for gridftp. Gridftp clients and servers comply with the gsiftp protocol, see the documentation. Included in the OSG software are globus-url-copy and uberftp.
   * Clients for SRM. Storage Resource Manager clients and servers comply with the [[https://sdm.lbl.gov/srm-wg][srm]] specification. Three implementations are included in the OSG software: the Fermliab SRM Clients, the LBL SRM Clients (Storage.SRMv2Client), and glite's lcg-utils.
   * Implementation-specific clients. Storage software implementations often have their own clients which can be used to access the data but do not conform to any external specification. In the OSG software, such clients are dccp for dCache and xrootdutils for xrootd.

SRM and gsiftp rely on the Grid Security Infrastructure for their authorization mechanismsy; therefore proxies created through VOMS servers are required for their clients to operate. 

---+++Discovery

Knowing what sites authorize your VO, what the software environment is, and what the endpoints are, among other information, is a requirement for being able to use storage elements. The OSG runs an information service, the OSG BDII, from which the needed information may be discovered. See the section below on Information Services.


---+++Getting help

Requests for help may be submitted through the [[https://ticket.grid.iu.edu/goc/open][GOC ticketing system]]. Tickets are initially assigned to your Virtual Organization, which then may dispatch or escalate the ticket to another organization.

#ClientTools
---++Client Tools for Storage

---+++Client gridftp Tools

---++++globus-url-copy
[[Documentation/StorageDcacheCopying]]

---+++Client SRM Tools
---++++Fermliab SRM Clients

---+++ Fermi SRM Clients
This set of srmclient commands (developed and maintained at Fermilab), can be used to access/validate a Storage Element.
The location of this package in an ITB CE install (based on VDT 1.10.0) is $VDT_LOCATION/srm-v1-client. Once you source 
the $VDT_LOCATION/setup.s(c)sh file, all these commands should be available in your PATH.

*Note* - Even though the name suggests that Fermi srmclients only support SRM protocol version 1.1, this is not true. The 
Fermi srmclients infact support both SRM Protocol versions 1.1 and 2.2. This name will be changed in VDT 1.10.1

*Note* - If you are interested in getting the latest Fermi srmclient package and can not wait for a new VDT release, you can 
download the rpm from the main [[http://www.dcache.org/downloads/1.8.0/index.shtml][dCache website]]. After you have installed the rpm, by default, the
commands will be available in the /opt/d-cache/srm/bin directory.

[[ReleaseDocumentation.FermiSrmClientCommands]]

[[Storage/SpaceResClientCommands]]

---+++ LBNL SRM Clients

A set of SRM client commands, developed at LBNL as generic SRM v2.2 clients, are available to access any SRM v2.2 based storage components. They have been tested for all current SRM v2.2 implementations such as BeStMan, CASTOR, dCache, DPM, SRM/SRB and StoRM. They are continuously being tested for compatibility and interoperability. This can be installed from the VDT distribution or from the tar file download from [[http://datagrid.lbl.gov/bestman][SDM group at LBNL]].  Sample command line examples for [[Storage.BeStMan][BeStMan]] at NERSC and for dCache at FNAL are available on [[Storage.SRMv2Client][this link]].


[[Storage.SRMv2Client]]


---+++ LCG Utils

LCG Utils is a suite of client tools for data movement written for the LHC Computing Grid. The tools are based on the Grid File Access Library, which is also included. Commands with access SRM servers are conformant to the SRM v2.2 specification. However, some commands require a connection to a BDII-based catalog. File copies and deletions based on the SRM URL alone are possible. Examples are written below.

LCG-Utils in VDT is built starting with version 1.10.0. LCG Utils can be installed via the pacman command

<verbatim>
pacman -get http://vdt.cs.wisc.edu/vdt_1101_cache:LCG-Utils
</verbatim>

Choose the "install locally" option for the CA certificates to avoid conflict with existing GSI infrastructure.

After running ". setup.sh", basic commands can be executed as follows. For a copy command, after creating a user proxy, run
<verbatim>
lcg-cp -v -b -D srmv2 file:/testdata/test1 <SRM URL>
</verbatim>

Here the SRM URL is to an SRM v2.2 web service endpoint. Copies between storage elements are also allowed.

To delete a file, use the VO from the proxy and execute
<verbatim>
lcg-del -b -v -l --vo <VO> -D srmv2 <SRM URL>
</verbatim>

A [[%ATTACHURL%/test_lcg_utils_vdt.sh][lcg-utils test script]] may be used. The script writes to a SRM v2.2 storage element, obtains TURLS for the written files, and deletes the. Change the definitions at the beginning of the script to match your site and VO.

Documentation for LCG-Utils may be found the the [[https://edms.cern.ch/file/454439//LCG-2-UserGuide.html#SECTION00097000000000000000][LCG User Guide]]. Note that the requirement of setting LCG_GFAL_INFOSYS applies only when using a BDII server.


---+++Other Client Tools

---++++dccp

---++++xrootd clients

---++ Comments
| Need GOC address. | Main.TedHesselroth | 10 Feb 2010 - 16:49 |
| typo: &#34; be moved to moved&#34;&#60;br /&#62;&#60;br /&#62;GOC address http://www.grid.iu.edu/ | Main.TanyaLevshina | 10 Feb 2010 - 20:20 |
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = TedHesselroth

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = Storage
   * Local DOC_AREA       = 

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = All

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = Knowledge
   * Local DOC_TYPE       = 
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  TanyaLevshina
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->


-- Main.TedHesselroth - 13 Jan 2010
