%META:TOPICINFO{author="MarcoMambelli" date="1302116853" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="CampusGrids.WebHome"}%
%LINKCSS%

<!-- This is the default OSG documentation template. Please modify it in -->
<!-- the sections indicated to create your topic.                        --> 

<!-- By default the title is the WikiWord used to create this topic. If  -->
<!-- you want to modify it to something more meaningful, just replace    -->
<!-- %TOPIC% below with i.e "My Topic".                                  -->

---+!! Campus Grid Factory Install Guide 
%TOC%

---+ About This Document
This document describes the installation of the Campus Grid Factory.  A Campus Grid could require a Campus Grid Factory if they want to integrate a PBS cluster into a Condor Flocking campus grid.  

---+ How To Get Help?
You can fins support in the !SourceForge project [[http://sourceforge.net/projects/campusfactory/support][support page]]

---+ Requirements
The Campus Grid Factory requires:
   * A supported Local Resource Manager (e.g. PBS or LSF)
   * A submit host for the LRM with RHEL 5 64 bit (x86_64) based Operating System, a public IP and port 9168 (Condor Collector) open.
      * The hardware requirements for the Campus Factory node are very modest: production factories have run on 1GB of ram and 1 Core.  
<!--
   * [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl?state=select_from_mirror_page&version=7.5.6&mirror=UW%20Madison&optional_organization_url=http://][Condor-7.5.6]] (installation and configuration documented below).
   * The [[https://sourceforge.net/projects/campusfactory/files/][Campus Factory]].
-->

---+ Introduction
The Campus Factory is a component of the OSG Campus Grid Infrastructure.
The Campus Factory allows access to a single job queue (PBS or LSF) by creating allowing Condor jobs to flock into the queue as user jobs owned by the user running the Campus Factory.
The Campus Factory includes a Condor installation (running a Collector) and a process submitting user jobs to the local queue.

---+ Installation Procedure

---++ Installing Condor 

You do not have to install condor as root.  This guide will assume installing as non-root.  The user that runs condor will be the same user which the pilots will be submitted as to PBS.
   1. [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl?state=select_from_mirror_page&version=7.5.6&mirror=UW%20Madison&optional_organization_url=http://][Download]]
   1. Uncompress condor.  For example if you are running on RHEL5, SL5, or CentOS5:
   <pre class="screen">
tar xzf condor-7.5.6-x86_64_rhap_5-unstripped.tar.gz</pre>
   1. Create the file =condor.sh= with the following, replacing =CONDOR_LOCATION= with the full path to condor-7.5.6.  It should include the condor-7.5.6.
   <pre class="file">
#!/bin/sh
CONDOR_DIR=CONDOR_LOCATION
export CONDOR_CONFIG=$CONDOR_DIR/etc/condor_config
export PATH=$CONDOR_DIR/bin:$CONDOR_DIR/sbin:$PATH</pre>


---+++ Preparing Condor 
   1. Download the Campus Factory [[https://sourceforge.net/projects/campusfactory/files/][release]].
   1. Copy the file `condor_config.generic` from `CONDOR_LOCATION/etc/examples/condor_config.generic`:
   <pre class="screen">
cp etc/examples/condor_config.generic etc/condor_config</pre>
   1. Edit the required lines in the `condor_config` that you just created.
      * =RELEASE_DIR= to the full path of the condor location.  It should be the same as =CONDOR_LOCATION= mentioned in [[#InstallingCondor][Installing Condor]]
      * =LOCAL_DIR= to a directory where condor will place host dependent files (logs...).  A common value is =$(RELEASE_DIR)/local.$(HOSTNAME)=.  Be sure to create this directory.  =$(HOSTNAME)= is same as the output from =hostname= on the command line.
   <pre class="file">
LOCAL_DIR = $(RELEASE_DIR)/local.$(HOSTNAME) </pre>
     * =LOCAL_CONFIG_FILE= set to =$(RELEASE_DIR)/etc/condor_config.local=
   <pre class="file">
LOCAL_CONFIG_FILE = $(RELEASE_DIR)/etc/condor_config.local</pre>
   1. Copy the `condor_config.local` and `condor_mapfile` from the factory release.
   <pre class="screen">
cp FACTORY_LOCATION/share/condor/condor_config.local CONDOR_LOCATION/etc/condor_config.local
cp FACTORY_LOCATION/share/condor/condor_mapfile CONDOR_LOCATION/etc/condor_mapfile</pre>
   1. Edit the required condor lines in the local condor configuration file (`condor_config.local`):
   $ =CONDOR_HOST=: to the full hostname of the machine that will run condor
   $ =CONDOR_ADMIN=: set to the email address that will receive emails about malfunctioning condor.
   $ =UID_DOMAIN=: set to a unique name for this resource.  For example, for a cluster named Firefly at Nebraska, we would set =UID_DOMAIN= to =firefly.unl.edu=
   $ =FILESYSTEM_DOMAIN=: set to your domain.  This can be the same as =UID_DOMAIN=
   $ =COLLECTOR_NAME=: Name of the resource.  This is the long name, such as =Firefly Cluster=
   $ =FLOCK_FROM=: Hosts that will be allowed to run jobs on this cluster
   $ =FLOCK_TO=: Hosts that can run jobs submitted on this cluster.
   $ =INTERNAL_IPS=: IP addresses (or hostnames) of the worker nodes and any NAT machines that the worker nodes may use to contact the =CONDOR_HOST=.


A sample configuration with the variables filled in is shown in the source [[http://sourceforge.net/apps/trac/campusfactory/browser/campus_factory/tags/CampusFactory-0.3.1/share/condor/condor_config.local][condor_config.local]].

   7. Edit the Condor PBS Blahp configuration.  This file is located in =CONDOR_LOCATION/lib/glite/etc/batch_gahp.config=.
   $ =pbs_binpath=: set to the location of the pbs executables (ie qstat, qsub, ...)

   * =pbs_nochecksubmission!=yes=
   * =pbs_nologaccess!=yes=
   * =blah_shared_directories!=/dev/null=

After editing `batch_gahp.config`, copy the file to the new configuration file location:
<pre class="screen">
cp CONDOR_LOCATION/lib/glite/etc/batch_gahp.config CONDOR_LOCATION/lib/glite/etc/blah.config </pre>
 


Start the condor daemons by first sourcing the condor.sh file you created above:
<pre class="screen">
. CONDOR_LOCATION/condor.sh</pre>

Then starting the `condor_master`:
<pre class="screen">
condor_master</pre>

---++ Installing the factory 
In this section, the location of the factory will be represented by =FACTORY_LOCATION=.

---+++ Configuring
Full documentation on configuration options for the factory are listed on the [wiki:Configuration Configuration page].
In the configuration file =FACTORY_LOCATION/etc/campus_factory.conf=, the values you are required to change are:

   $ =worker_tmp=: The local temp directory on the worker node.  This will be where condor places the intermediate job data and glidein logs.
   $ =logdirectory=: Directory to place the logs for the factory.  They will be named campus_factory.log.

---+++ Starting the Factory
   1. Confirm that the condor executables are in your path:
   <pre class="screen">
condor_q</pre>
   Should output
   <pre class="screen">
-- Schedd: HOSTNAME : <IP_ADDRESS>
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held </pre>
   If =condor_q= failes, be sure to source the condor.sh file you created while [[#Installing_Condor][installing condor]].  If sourcing the =condor.sh= file does not fix the problem, confirm that the =condor_master=, =condor_collector=, and =condor_schedd= daemons are running.
   <pre class="screen">
ps aux | grep condor</pre>
   1. Export the correct python path:
   <pre class="screen">
export PYTHONPATH=$PYTHONPATH:FACTORY_LOCATION/python-lib </pre>
   1. Start the factory:
   <pre class="screen">
cd FACTORY_LOCATION
condor_submit share/factory.job </pre>

---++ Next Steps
The next step is to test the factory.  You can find a guide to running jobs using the factory on [wiki:RunningJobs Running Jobs] page.




-- Main.DerekWeitzel - 05 Apr 2011