%META:TOPICINFO{author="MarcoMambelli" date="1302283836" format="1.1" version="1.10"}%
%META:TOPICPARENT{name="CampusGrids.WebHome"}%
%LINKCSS%

%DOC_STATUS_TABLE%

---+!! Campus Grid Factory Install Guide 
%TOC%

<!-- conventions used in this document
   * Local UCL_CWD = ~/cf/
   * Local UCL_HOST = cf
-->

<!-- Local variables
   * Set CONDORREL = 7.5.6
   * Set CFREL = 0.3.1
   * Set AS_OF_DATE = Apr 6, 2011
-->


---+ About This Document
This document describes the installation of the Campus Factory.  The OSG model for setting up a Campus Grid utilizes a Condor based submit host to submit jobs across multiple clusters with different Job Schedulers.  A Campus Grid Factory must be installed on the head node of each non-Condor based cluster as an integration point into the Campus Grid. 

---+ How To Get Help?
You can find support in the !SourceForge project [[http://sourceforge.net/projects/campusfactory/support][support page]]

---+ Requirements
The Campus Grid Factory requires:
   * A supported (LRM) Local Resource Manager (e.g. PBS)
   * A submit host for the LRM with RHEL 5 64 bit (x86_64) based Operating System, a public IP and port 9618 (Condor Collector) open.
      * The hardware recommendations for the Campus Factory are modest: production factories have run on 1GB of ram and 1 Core.  
<!--
   * [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl?state=select_from_mirror_page&version=7.5.6&mirror=UW%20Madison&optional_organization_url=http://][Condor-%CONDORREL%]] (installation and configuration documented below).
   * The [[https://sourceforge.net/projects/campusfactory/files/][Campus Factory]].
-->

---+ Introduction
The Campus Factory is a component of the OSG Campus Grid Infrastructure.
The Campus Factory allows job submission access to a single job queue (PBS or LSF) by creating allowing Condor jobs to flock into the queue as user jobs owned by the user running the Campus Factory.
The Campus Factory includes a Condor installation (running a Collector) and a process submitting user jobs to the local queue.

---+ Installation Procedure

---++ Installing Condor 

You do not have to install condor as root.  This guide will assume installing as non-root.  The user that runs condor will be the same user which the pilots will be submitted as to PBS.
   1 Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl?state=select_from_mirror_page&version=7.5.6&mirror=UW%20Madison&optional_organization_url=http://][download page]] and choose Condor version %CONDORREL% for RHEL 5 64bit (x86_64).
      *  =condor-%CONDORREL%-x86_64_rhap_5-stripped.tar.gz=  (for RHEL 5 x86_64 OS)
   1 Save the downloaded file to your home directory (you can use =wget= or =curl=).
   1 Create the Condor installation directory: <pre class="screen">
mkdir ~/cf
mkdir ~/cf/condor
</pre>
   1 Create a temporary directory and extract the source from the downloaded file in your home directory<pre class="screen">
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xzf ~/condor-%CONDORREL%-x86_64_rhap_5-stripped.tar.gz 
</pre>
   1 Run the Condor installation script:<pre class="screen">
cd condor-%CONDORREL%-x86_64_rhap_5-stripped
./condor_install --prefix=~/cf/condor </pre>   
<!--
./condor_install --prefix=~/cf/condor --type=manager
-->

The installation script created for you 
   * files to set the environment (=condor.sh/csh=)
   * standard configuration files =~/cr/condor/etc/condor_config= 
   * and a host specific directory =~/cr/condor/local.$HOST= (where $HOST is the short name of your host) containing the machine-specific configuration file and log and spool directories. 


---++ Configuring Condor 
These instructions cover only the configuration of a Campus Factory for a PBS job queue

   1. Download and extract the Campus Factory [[https://sourceforge.net/projects/campusfactory/files/][release]]. E.g.:<pre class="screen">
cd ~/cf/
wget http://sourceforge.net/projects/campusfactory/files/CampusFactory-%CFREL%/CampusFactory-%CFREL%.tar.gz/download
tar xzf CampusFactory-%CFREL%.tar.gz</pre>
   1. Define =CONDOR_LOCATION= as your Condor installation directory and =FACTORY_LOCATION= as your Campus Factory installation directory:<pre class="screen">
export CONDOR_LOCATION=~/cf/condor
export FACTORY_LOCATION=~/cf/CampusFactory-%CFREL%</pre>
   1. Make a local configuration directory:<pre class="screen">
mkdir $CONDOR_LOCATION/etc/config.d</pre>
   1. Edit the required lines in the =condor_config= file (=$CONDOR_LOCATION/etc/condor_config=):
      * =LOCAL_CONFIG_DIR= set to =&lt;CONDOR_LOCATION>/etc/config.d=, where lt;CONDOR_LOCATION> is the content of the variable defined above. Use the absolute path, no variables or symbols like =~=. E.g.:<pre class="file">
LOCAL_CONFIG_DIR = /home/marco/cf/condor/etc/config.d</pre>
   1. Copy the =condor_config.factory= and =condor_mapfile= from the factory release.<pre class="screen">
cp $FACTORY_LOCATION/share/condor/condor_config.local $CONDOR_LOCATION/etc/config.d/condor_config.factory
cp $FACTORY_LOCATION/share/condor/condor_mapfile $CONDOR_LOCATION/etc/condor_mapfile</pre>
   1. Edit the required condor lines in the factory condor configuration file (=condor_config.factory=):
   $ =FLOCK_FROM=: Hosts that will be allowed to run jobs on this cluster
   $ =FLOCK_TO=: Hosts that can run jobs submitted on this cluster.
   $ =INTERNAL_IPS=: IP addresses (or hostnames) of the worker nodes and any NAT machines that the worker nodes may use to contact the =CONDOR_HOST=.
   1. Check the value of these condor lines in the local condor configuration file (=local.$(HOST)/condor_config.local=):
   $ =CONDOR_HOST=: to the full hostname of the machine that will run condor
   $ =CONDOR_ADMIN=: set to the email address that will receive emails about malfunctioning condor.
   $ =UID_DOMAIN=: set to a unique name for this resource.  For example, for a cluster named Firefly at Nebraska, we would set =UID_DOMAIN= to =firefly.unl.edu=
   $ =FILESYSTEM_DOMAIN=: set to your domain.  This can be the same as =UID_DOMAIN=
   $ =COLLECTOR_NAME=: Name of the resource.  This is the long name, such as =Firefly Cluster=
      * A sample configuration with the variables filled in is shown in the source [[http://sourceforge.net/apps/trac/campusfactory/browser/campus_factory/tags/CampusFactory-0.3.1/share/condor/condor_config.local][condor_config.local]].
   1. Edit the Condor PBS Blahp configuration.  This file is located in =$CONDOR_LOCATION/libexec/glite/etc/batch_gahp.config=.
      * Use =which qstat= to find the binary path of the PBS executables (i.e. qstat, qsub, ...), =/usr/bin= in the example
      * Set =pbs_binpath= and these other variables accordingly: <pre class="file">
pbs_binpath=/usr/bin
pbs_nochecksubmission=yes
pbs_nologaccess=yes
blah_shared_directories=/dev/null</pre>
   1. After editing =batch_gahp.config=, copy the file to the new configuration file location:<pre class="screen">
cp $CONDOR_LOCATION/libexec/glite/etc/batch_gahp.config $CONDOR_LOCATION/libexec/glite/etc/blah.config </pre>

---+++ Configuring GAHP for other job managers

---++ Starting condor
Start the Condor daemons by first sourcing the setup file =condor.sh=:<pre class="screen">
source $CONDOR_LOCATION/condor.sh</pre>

Then starting the =condor_master=:<pre class="screen">
condor_master</pre>


---++ Installing the factory 
The Campus Factory is the package downloaded above during the Condor configuration.
=$FACTORY_LOCATION= was defined above as the Campus Factory installation directory.

---++ Configuring the factory
Full documentation on configuration options for the factory are listed on the [wiki:Configuration Configuration page].
In the configuration file =$FACTORY_LOCATION/etc/campus_factory.conf=, the values you are required to change are:

   $ =worker_tmp=: The local temp directory on the worker node.  This will be where condor places the intermediate job data and glidein logs.
   $ =logdirectory=: Directory to place the logs for the factory.  They will be named campus_factory.log.

The job template == needs to be configured correctly:
   1. Set these variables to the correct value:<pre class="file">
GLIDEIN_HOST = example.com
WN_TMP = /tmp
GLIDEIN_Site = Grid site</pre>
   1. Customize the environment for your configuration:<pre class="file">
Environment =   _condor_CONDOR_HOST=$(GLIDEIN_HOST); \
                                _condor_COLLECTOR_HOST=$(GLIDEIN_HOST); \
                                _condor_GLIDEIN_HOST=$(GLIDEIN_HOST); \
                                _condor_CONDOR_ADMIN=dweitzel@ff.unl.edu; \
                                _condor_NUM_CPUS=1; \
                                _condor_UID_DOMAIN=ff.unl.edu; \
                                _condor_FILESYSTEM_DOMAIN=ff.unl.edu; \
                                _condor_MAIL=/bin/mail; \
                                _condor_STARTD_NOCLAIM_SHUTDOWN=1200; \
                                _campusfactory_wntmp=$(WN_TMP); \
                                _condor_GLIDEIN_Site="$(GLIDEIN_Site)"</pre>


---++ Starting the Factory
   1. Confirm that the condor executables are in your path:
   <pre class="screen">
condor_q</pre>
   Should output
   <pre class="screen">
-- Schedd: HOSTNAME : <IP_ADDRESS>
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held </pre>
   If =condor_q= failes, be sure to source the condor.sh file you created while [[#Installing_Condor][installing condor]].  If sourcing the =condor.sh= file does not fix the problem, confirm that the =condor_master=, =condor_collector=, and =condor_schedd= daemons are running.
   <pre class="screen">
ps aux | grep condor</pre>
   1. Export the correct python path:
   <pre class="screen">
export PYTHONPATH=$PYTHONPATH:$FACTORY_LOCATION/python-lib </pre>
   1. Start the factory:
   <pre class="screen">
cd FACTORY_LOCATION
condor_submit share/factory.job </pre>

---+ Service Activation and Deactivation

---+ Validation of Service Operation 

---+ Troubleshooting
---++ File Locations
---++ Known Errors
Here a list of known problems with version 3.1 of the Campus Factory. They will be solved in future releases.

In the Condor Campus Factory configuration file () comment the lines:<pre class="file">
# Location of the PBS_GAHP to be used to submit the glideins.
#GLITE_LOCATION = $(LIB)/glite
#PBS_GAHP       = $(GLITE_LOCATION)/bin/batch_gahp
</pre>
These line are already present in the global Condor configuration and the =GLITE_LOCATION= should be =$(LIBEXEC)/glite=.

---++ How can I resolve this Problem?

---+++ Errors in the Campus Factory log file
If you see an error about the schedd query failing, like:<pre>
2011-04-08 10:36:39,682 - DEBUG - Schedds to query: ['']                                                           
2011-04-08 10:36:39,683 - DEBUG - Running external command: condor_q -name  -const '(GlideinJob =!= true) &&       
(JobStatus == 1)' -format '<glidein owner="%s"/>' 'Owner'                                                          
2011-04-08 10:36:39,708 - ERROR - No valid output received from command: condor_q -name  -const '(GlideinJob       
=!= true) &&  (JobStatus == 1)' -format '<glidein owner="%s"/>' 'Owner'                                            
2011-04-08 10:36:39,708 - ERROR - stderr = Error: unknown host -const                                              
</pre>

Probably there are no glidein jobs running on your system.
   * Check that PSB works correctly (qsub, qstat)
   * Check that GLITE_LOCATION is set correctly. See above for the known bug, control that the files in the glite directory are actually there, else you may have a wrong version of Condor or may need to apply a patch


---+ References
[[http://sourceforge.net/apps/trac/campusfactory][Here]] the reference documentation on !SourceForge.

---+ Next Steps
The next step is to test the factory.  You can find a guide to running jobs using the factory on [[http://sourceforge.net/apps/trac/campusfactory/wiki/RunningJobs][Running Jobs]] page.



---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = User

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DerekWeitzel
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = MarcoMambelli
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->