%META:TOPICINFO{author="TedHesselroth" date="1269364619" format="1.1" reprev="1.12" version="1.12"}%
%META:TOPICPARENT{name="WebHome"}%
%DOC_STATUS_TABLE%

---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
https://twiki.grid.iu.edu/twiki/pub/Storage/WebHome/images.jpg
%TOC{depth="3"}%

*The Storage Architecture*

The first six are typically bundled together and make up what is known as a "Storage Element". See [[#StorageImplementationsTable][Table of Storage implementations  used in the Open Science Grid]], below, for details on specific implementations found in the OSG.

   * _Distributed Storage_:
   * _Namespace_:
   * _Data transfer_:
   * _Replication_:
   * _Resource Management_:
   * _Archiving_:

 The latter four are realized as independent components that tie into the job execution stack as well.

   * _Information Services_:
   *   * _Catalogs_:
   *   * _Monitoring_:
   *   * _Discovery_:
   *   * _Accounting_:


---++Distributed Storage

In grid computing, just as computational power in the form of CPUs may be distributed over many computers, or "worker nodes", storage in the form of hard disks may be distributed over many computers in order to provide a large, unified storage area. Often, the same computers that serve as worker nodes for a Compute Element also hold storage for a Storage Element. There are many implementations of distributed storage, several of which may be found on the Open Science Grid. In general, the emphasis of storage on the OSG is towards high throughput based on scalability, rather than low-latency based on highly performant hardware. For a table comparing the features of some of the above software, please see the [[#StorageImplementationsTable][Storage Implementations Table]]. 

   * *Hadoop.* HDFS, or the Hadoop Distributed File System, is a distributed storage system based on the Hadoop implementation of Google's map-reduce algorithm. A key element of HDFS is robust support for [[#ReplicationLink][Replication]], allowing the use of low-cost hard drives while maintaining reliability. HDFS has a highly scalable architecture, in which the basic unit of storage is a block. OSG provides instructions for installing HDFS using packages contributed by the OSG site at Caltech. Community support through an osg-hadoop mailing list is available for operational issues. For more information, please see the [[Storage.Hadoop][OSG Hadoop twiki page]]. 
   * *xrootd* was designed to provide storage for physics analysis programs based on the software package named "root" and includes an Open Load Balancing Daemon (olbd) by which distributed storage clusters may be composed. [[http://xrootd.slac.stanford.edu/][Developed]] at the Stanford Linear Accelerator Center, xrootd is written in C++ with highly-optimized algorithms to provide fast and deterministically bounded processing times, resulting in low latencies even when a large number of files is present. xrootd is provided by the OSG through VDT packaging, please see the [[StorageSiteAdministrator#LinkToxrootdInstall][storage site administrator page]] for details.
   * *dCache.* A major part of the dCache Storage Element implementation is its distributed storage system, based on components known as "pools". Storage is file-based, and replication is supported, allowing the use of commodity hardware. Access to pools is controlled through a "Pool Manager", which allows logical storage areas to be created and access to be granted based on user identity or role, client IP address, operation (read or write), or transfer protocol. OSG provides packaging and support for dCache. For details, please see the [[StorageSiteAdministrator#LinkTodCacheInstall][storage site administrator page]].   
   * *DPM* Of interest to OSG users because of its deployment on the European grid EGEE, the [[https://twiki.cern.ch/twiki//bin/view/LCG/DpmGeneralDescription][Disk Pool Manager]] is a lightweight solution for managing disk storage. It can be accessed via SRM 1 & 2, and also provides data access through the  !GridFTP, rfio transfer protocols.
   * *Other* distributed file systems include Lustre, ZFS, !ReDDNet, NFS 4.1, and L-Store. Of these, only Luster and ZFS may be found on the Open Science Grid, though their use may increase in the future.

Note that it is not _required_ that a Storage Element use a distributed file system. Storage appliances can provide tens of terabytes of storage in a single unit. The globus implementation of the gridftp data transfer mechanism can serve files from any mounted file system, and may be used in combination with  the Bestman Storage Element for SRM access. For further information, see [[StorageSiteAdministrator][Storage for Site Administrators]].

---++Namespace

All distributed file systems rely on the use of a namespace component which allows the logical name and path of a file to be separated from its physical location. Typically, a database is maintained with the needed "metadata" for each file. Since there is typically just one instance of this component in a distributed file system, it can represent a single point of failure. In dCache, frequent backups of the database In addition, for large systems, a performance bottleneck may occur at the namespace node. 


#DataTransfer
---++Data transfer

While most distributed storage systems have their own access protocols, they do allow for other file server mechanisms. These are of particular use for interoperability, when a client at a remote site may not be using the native protocol of the storage service. The most commonly-used data transfer software for this purpose is gridftp.

   * *gridftp* is used for serving files over the wide area network. Security options include the Grid Security Infrastructure, the authentication framework adopted by the OSG. A major feature of gridftp is the ability to transfer files over multiple data channels, which can increase throughput compared to one channel by a factor of ten. There are two implementations of gridftp: by [[http://www.globus.org][Globus]], and by [[http://dcache.org][dCache]]. The dCache implementation is bundled with SRM-dCache and is not installable as a separate component. For an introduction to gridftp, please see [[Documentation/StorageGridFTP]].


#ReplicationLink
---++Replication

Replication of files is used to mitigate data loss in the case of hard disk failure. In implementations that support replication (see the [[#StorageImplementationsTable][Storage implementations Table]]) replication occurs automatically, with the number of copies being detected by the replication service. When a disk is lost the system automatically creates additional replicas for the affected files and in the interim uses existing replicas for uninterrupted service. In dCache the Replica Manager creates replicas of whole files, among a specified subset of pools. The Hadoop HDFS storage service does replication at the block level, and allows specifying that block replication not occur within a set of storage nodes, such as all those situated in one rack.

An alternative to file or block replication is the use of RAID arrays, typically RAID-5, by which data redundancy is executed at the hardware level. Upon loss of a disk, the vendor-supplied rebuilding process restores the redundancy.


---++Resource Management

[[https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.pdf][SRM]] is a software specification for access to mass storage systems. The specification allows for interoperability among clients and servers of various storage implementations. Any client which satisfies the specification can operate with any server which also does so. The specification supports commonly-used storage operations such as get, put, copy (for moving files from one SRM storage element to another), bring-online (to cause a file to be moved from a tape archive to the disk cache for later transfer), and space reservation.


[[Storage/SpaceReservation]]

[[ReleaseDocumentation.ConfigurationFileStorage]]

[[ReleaseDocumentation.DCacheFAQs]]

[[Storage.SRMTester]]

   * S2

---+++ Bestman

[[BestmanStorageElement]]

---+++ dCache

[[dCacheStorageElement]]


   * Other SRM

      * [[http://castor.web.cern.ch/castor/][CASTOR]] (CERN Advanced STORage manager) is a hierarchical storage management (HSM) system developed at CERN used to store physics production files and user files. 




---++Archiving


---++Information Services

   * *Catalogs* (RLS)
   * *Accounting* (Gratia, dCache Chronicle, Hadoop Chronicle)
   * *Monitoring* (RSV, srmTester)
   * *Discovery* (BDII, Generic Information Provider (GIP), Discovery Tool)

---+++Catalogs

---+++Monitoring

---+++Discovery

   * BDII, Generic Information Provider (GIP), Discovery Tool

Storage.OSGStorageDiscoveryTool


---+++Accounting

[[ReleaseDocumentation.GratiaDcacheProbes]]

[[ReleaseDocumentation.GratiaTransferProbe]]



#StorageImplementationsTable
---++Table of Storage implementations  used in the Open Science Grid

%TABLE{ headerbg="#eeeeee" headercolor="#000000" databg="#ffffff" tableborder="1" columnwidths="120," cellpadding="2" cellspacing="1" dataalign="left" valign="top" sort="off"}%


| *Software*  | *Distributed Storage* | *Resource Management* | *Data Transfer Protocols* | *Replication* | *Archiving* | *Namespace* |
| gridftp |  | | gsiftp | | | | 
| xrootd | XrdOss | olbd | xroot,posix+ | | XrdOss | XrdSfs | 
| Bestman | | Bestman SRM |gsiftp,posix| | | | 
| Bestman-xrootd | xrootd | Bestman SRM Gateway  | gsiftp,xroot,posix+ | | | | 
| Hadoop SE | HDFS| Bestman SRM Gateway | gsiftp| Block Replication | |NameNode/fuse |
| SRM-dCache | dCache | Fermi SRM |gsiftp,dcap,posix+,gsidap,xroot| replica manager | HMS |pnfs, chimera | 

+with preloaded libraries 


     

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = TedHesselroth

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = Storage
   * Local DOC_AREA       = 

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = All

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = Knowledge
   * Local DOC_TYPE       = 
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  TanyaLevshina
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->


-- Main.TedHesselroth - 13 Jan 2010

%META:FILEATTACHMENT{name="bestman-gateway-howitworks.jpg" attachment="bestman-gateway-howitworks.jpg" attr="" comment="Bestman - How it works." date="1266347460" path="bestman-gateway-howitworks.jpg" size="50852" stream="bestman-gateway-howitworks.jpg" tmpFilename="/usr/tmp/CGItemp12893" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="bestman_gateway_arch.jpeg" attachment="bestman_gateway_arch.jpeg" attr="" comment="BeStMan-gateway architecture" date="1266347511" path="bestman_gateway_arch.jpeg" size="9978" stream="bestman_gateway_arch.jpeg" tmpFilename="/usr/tmp/CGItemp13012" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="bestman-gateway-xrootd-howitworks.jpg" attachment="bestman-gateway-xrootd-howitworks.jpg" attr="" comment="!BeStMan Gateway with Xrootd - How it works" date="1266349542" path="bestman-gateway-xrootd-howitworks.jpg" size="52148" stream="bestman-gateway-xrootd-howitworks.jpg" tmpFilename="/usr/tmp/CGItemp25197" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="betsman_gateway_xrootd.jpeg" attachment="betsman_gateway_xrootd.jpeg" attr="" comment="BeStMan-gateway/Xrootd architecture" date="1266349681" path="betsman_gateway_xrootd.jpeg" size="35418" stream="betsman_gateway_xrootd.jpeg" tmpFilename="/usr/tmp/CGItemp25142" user="TedHesselroth" version="1"}%
