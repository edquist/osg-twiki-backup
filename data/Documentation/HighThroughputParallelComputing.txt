%META:TOPICINFO{author="DanFraser" date="1260223062" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="WebHome"}%
%LINKCSS%

<!-- This is the default OSG documentation template. Please modify it in -->
<!-- the sections indicated to create your topic.                        --> 

<!-- By default the title is the WikiWord used to create this topic. If  -->
<!-- you want to modify it to something more meaningful, just replace    -->
<!-- %TOPIC% below with i.e "My Topic".                                  -->

---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%

%STARTINCLUDE%

---++What is HTPC and why is it important?

Parallel jobs, in general, are not very portable, and as a result are difficult to run across multiple heterogeneous sites.  In the OSG we are currently working on minimizing these barriers for an important class of small-way (4- to 64- way)parallel jobs where the parallelism can be executed on a single multi-core system. By avoiding the system parallel libraries, we avoid the majority of the obstacles to making parallel jobs portable across the OSG. These jobs are also easier to build and submit to OSG sites since parallel message handling libraries (e.g. MPI, OpenMP, ...) can be packaged together with the executable.

[[http://www.opensciencegrid.org/OSG_Newsletter_October_2009][Press Release: Bringing High Throughput Capabilities to Ensembles of Parallel Applications]]

Currently, eight-way multicore machines are prevalent in the OSG.  Many local schedulers have mechanisms, exposed via RSL, to schedule a single job to run exclusively on one multi-core machine.  By using shared memory on a single machine as an MPI interconnect, and bringing along with the job all the software infrastructure required to run this "local" MPI job, a user can easily create a portable MPI job that can run on many different OSG sites.  There are several use-cases that benefit from being able to run large numbers of these small MPI jobs.  We call this "High Throughput Parallel Computing", or HTPC. The PIs of this project are:
   * Dan Fraser, Computation Institute, University of Chicago
   * John McGee, Renaissance Computing Institute (RENCI)
   * Miron Livny, University of Wisconsin

---++ HTPC Operations Status

Oklahoma-Sooner was the first external site to begin running HTPC Jobs. [[http://gratia-osg-prod.opensciencegrid.org/gratia-reporting/frameset?__report=%2Fdata%2Fapache-tomcat-5.5.27%2Fwebapps%2Fgratia-reports%2Freports%2FUsageByUser.rptdesign&__format=html&EndDate=2009-11-16&ReportTitle=Usage+by+Site+-+drill-through&ReportSubtitle=&Sites=OU_OSCER_ATLAS&StartDate=2009-11-09&DisplayMetric=WallDuration&__overwrite=true&__locale=en_US&__timezone=GMT&__svg=false&__designer=false&__pageoverflow=0&__masterpage=true&VOs=dosar][Gratia Reports for OU HTPC jobs]]. A special thank you to Henry Neeman, Horst Simon and the rest of the OU team.

Additionally we are working with Purdue-Lepton, Nebraska-Firefly, and Clemson-Palmetto. 

---++Setting up an HTPC job on the OSG

Currently we are testing HTPC jobs with MPI parallelism.

---+++Compiling an HTPC job

The main advantage of HTPC is that most any MPI implementation that supports shared memory can be used.  MPICH2 and OpenMPI have both been tested.  To compile an HTPC job, simply compile as usual on any accessible machine.  This need not be a head node, or even an OSG resource.  However, this machine does need to be an OS and architecture compatible with the OSG sites where the job will run.  Currently, Scientific Linux 4 or a 32 or 64 bit machine is a good universal donor system, using OpenMPI for the MPI implementation.  Another significant benefit is that it is easy to test these jobs locally, without waiting for scheduling delays.  Statically linking the binary can ensure that it doesn't depend on any shared libraries unavailable on the target system.

---+++Submitting an HTPC job

There are two main tricks to submitting an HTPC job.  The first is transfering the mpiexec command along with the job itself.  As the job will be transfered as a "data" file, the main executable in the condor-g file will need to be a wrapper script, which simply sets the executable bit on the MPI job proper, and calls mpiexec.

For example, a wrapper script to run an 8-way HTPC job might look like

<verbatim>
#!/bin/sh

chmod 0755 ./mdrun ./mpiexec

./mpiexec -np 8 ./mdrun some_input_file
</verbatim>

The second trick is requesting that the local scheduler allocate all the cores on a single machine for your job.  In PBS, the way to do this is with the RSL in a Condor-G submit file:

<verbatim>
GlobusRSL = (xcount=8)
</verbatim>


---+++ Batch Scheduler Setup

Enabling multi-core jobs requires some site-specific configuration of the local batch system.

---++++ PBS

PBS should work out of the box by using the "xcount" option supported by the PBS jobmanager.

---++++ Condor

Condor requires whole machines to be configured.  See this web page:    http://condor-wiki.cs.wisc.edu/index.cgi/wiki?p=HowToAllowSomeJobs 

---++++ LSF

LSF requires some changes to the job manager to allow the -x option to be passed to the LSF bsub command.  Add the following lines to the LSF job manager, where option are being parsed:

<verbatim>
	if(defined($description->exclusive())) 
	{
	print JOB "#BSUB -x\n";
	}
</verbatim>





%STOPINCLUDE%

%BOTTOMMATTER%


-- Main.DanFraser - 07 Dec 2009
