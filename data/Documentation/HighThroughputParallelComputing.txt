%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dDerek_20Weitzel_20285345" date="1292533056" format="1.1" version="1.21"}%
%META:TOPICPARENT{name="WebHome"}%
%LINKCSS%

---+!! %SPACEOUT{ "%TOPIC%" }%
%DOC_STATUS_TABLE%
%TOC%

%STARTINCLUDE%

---++What is HTPC and why is it important?

High Throughput Parallel Computing (HTPC) is a computational paradigm for an emerging class of applications where large ensembles (hundreds to thousands) of modestly parallel (4- to ~64- way) jobs are used to solve scientific problems ranging from chemistry, biophysics, weather and flood modeling, to general relativity. Effectively supporting this paradigm requires focus in at least three areas:
   * Solving the parallel job portability problem to enable parallel jobs to easily run on heterogeneous resources
   * Optimizing parallel jobs to effectively utilize modern multi-core technologies
   * Effectively distributing HTPC jobs across suitable distributed resources

[[http://www.opensciencegrid.org/OSG_Newsletter_October_2009][Press Release: Bringing High Throughput Capabilities to Ensembles of Parallel Applications]]
<br>
[[http://www.cs.wisc.edu/condor/CondorWeek2010/condor-presentations/thain-fraser-hptc.pdf][HTPC presentation at Condor Week April 2010]]

Parallel jobs, in general, are not very portable, and as a result are difficult to run across multiple heterogeneous sites.  In the Open Science Grid (OSG) framework we are currently working on minimizing these barriers for an important class of modestly parallel (4- to ~64- way) jobs where the parallelism can be executed on a single multi-core machine. Currently, eight-way multicore machines are prevalent in the OSG.  Most local schedulers have mechanisms, exposed via RSL, to schedule a single job to run exclusively on one multi-core machine.  By using shared memory on a single machine as an MPI interconnect, and bringing along with the job all the software infrastructure required to run this "local" parallel job, a user can easily create a portable parallel job that can run on many different OSG sites. Another important benefit is that this strategy helps to optimize multi-core machine utilization while simplifying the build and submit process thereby making HTPC accessible to a wider class of scientific users. 

---++ The HTPC Team
  
This project is funded by an STCI-NSF award. The Principal Investigators/Institutions involved in this project are:
   * Dan Fraser, Computation Institute, University of Chicago -- Project Lead
   * John !McGee, Renaissance Computing Institute (RENCI)
   * Miron Livny, University of Wisconsin
One of the key technical leaders of this project is Greg Thain from the University of Wisconsin.

The HTPC team also consists of key individuals and system administrators who have enabled or are in the process of enabling HTPC to run at their sites:
   * Oklahoma-Sooner (Henry Neeman, Horst Severini, the OU team) was the first external site to begin running HTPC Jobs. [[http://gratia-osg-prod.opensciencegrid.org/gratia-reporting/frameset?__report=%2Fdata%2Ftomcat-gratia%2Fwebapps%2Fgratia-reports%2Freports%2FUsageByUser.rptdesign&__format=html&EndDate=2010-12-31&ReportTitle=Usage+by+Site+-+drill-through&ReportSubtitle=&Sites=OU_OSCER_ATLAS&StartDate=2009-11-09&DisplayMetric=WallDuration&__overwrite=true&__locale=en_US&__timezone=GMT&__svg=false&__designer=false&__pageoverflow=0&__masterpage=true&VOs=dosar][Gratia Reports for OU HTPC jobs]].
   * Purdue-Lepton (Preston Smith, Fengping Hu)
   * Clemson (Sam Hoover)
   * Nebraska-Firefly (Brian Bockelman)
   * UC San Diego (Frank Wurthwein, Terrence Martin, Igor Sfiligoi)
   * UW GLOW (Dan Bradley)

<!--
[[https://submit.chtc.wisc.edu:8443/gratia-reporting/][Gratia reports of HTPC jobs]]
-->

---++Setting up an HTPC job on the OSG

Currently we are testing HTPC jobs with MPI parallelism so the directions below are primarily directed at MPI users. However, since the parallel libraries are packaged together with the application, it is straightforward to use other libraries such as OpenMP, Linda, or others. 

---+++Compiling an HTPC job

The main advantage of HTPC is that most any MPI implementation that supports shared memory can be used.  MPICH2 and OpenMPI have both been tested.  To compile an HTPC job, simply compile as usual on any accessible machine.  This need not be a head node, or even an OSG resource.  However, this machine does need to be an OS and architecture compatible with the OSG sites where the job will run.  Currently, Scientific Linux 4 or a 32 or 64 bit machine is a good universal donor system, using OpenMPI for the MPI implementation.  Another significant benefit is that it is easy to test these jobs locally, without waiting for scheduling delays.  Statically linking the binary can ensure that it doesn't depend on any shared libraries unavailable on the target system.

---+++Submitting an HTPC job

There are two main tricks to submitting an HTPC job.  The first is transfering the mpiexec command along with the job itself.  As the job will be transfered as a "data" file, the main executable in the condor-g file will need to be a wrapper script, which simply sets the executable bit on the MPI job proper, and calls mpiexec.

For example, a wrapper script to run an 8-way HTPC job might look like

<verbatim>
#!/bin/sh

chmod 0755 ./mdrun ./mpiexec

./mpiexec -np 8 ./mdrun some_input_file
</verbatim>

The second trick is requesting that the local scheduler allocate all the cores on a single machine for your job.  In PBS, the way to do this is with the RSL in a Condor-G submit file:

<verbatim>
GlobusRSL = (xcount=8)
</verbatim>

---+++ Batch Scheduler Setup

Enabling multi-core jobs requires some site-specific configuration of the local batch system.

---++++ PBS

PBS should work out of the box by using the "xcount" option supported by the PBS jobmanager.

---++++ Condor

Condor requires whole machines to be configured.  [[https://condor-wiki.cs.wisc.edu/index.cgi/wiki?p=WholeMachineSlots][Click here for directions]]

---++++ LSF

LSF requires some changes to the job manager to allow the -x option to be passed to the LSF bsub command.  Add the following lines to the LSF job manager, in the section where options are being parsed:

<verbatim>
	if(defined($description->exclusive())) 
	{
	print JOB "#BSUB -x\n";
	}
</verbatim>

---++ HTPC Schema

The following attributes will be added to the Glue schema as a CECapability: 

| *Attribute Name* | *Attribute Type* | *Description* |
| !GlueCECapability | string | htpc |
| HTPCrsl | string |extra rsl needed to enable HTPC jobs|
| !HTPCAccessControlBaseRule | string | ACBR format to specify one or more of <literal>  VO:<VO Name> or  VOMS:<FQAN> </literal> |

Another useful variable for HTPC is the "number of cores per machine" but this can be calculated from:
   * number of cores per machine = !GlueHostArchitectureSMPSize * (!LogicalCPUs / !PhysicalCPUs).

---++ Running Amber9 PMEMD

RENCI's Engagement team is running Amber9's PMEMD, a molecular dynamics tool, in the HTPC model. 

[[http://osglog.wordpress.com/2010/11/04/high-throughput-parallel-molecular-dynamics/][This site]] provides details of the goals, approach and status of the work.

---++ Problems encountered and the resolutions
   * Problem: 
      * Resolution:

---++ HTPC and Glide ins

Changes required to the glide-in factory for htpc:

* need a new site for htpc jobs.  For each htpc-site need to edit the RS" line in the glideinWMS.xml file to look like

entry name="clemson-htpc" enabled="True" gatekeeper="osg-gw.clemson.edu/jobmanager-condor" gridtype="gt2" rsl="(condorsubmit=('+RequiresWholeMachine' TRUE))" schedd_name="submit.chtc.wisc.edu" verbosity="std" work_dir="."

Need the following sites
| *Gatekeeper* | *Site Name* | *RSL* |
| osg-gw.clemson.edu/jobmanager-condor | clemson-htpc | rsl="(condorsubmit=('+RequiresWholeMachine' TRUE))" |
| red.unl.edu/jobmanager-condor | nebraska-red-htpc | rsl="(condorsubmit=('+RequiresWholeMachine' TRUE)(requirements = CAN_RUN_WHOLE_MACHINE =?= true))" |
| gpn-husker.unl.edu/jobmanager-condor | nebraska-husker-htpc | rsl="(condorsubmit=('+RequiresWholeMachine' TRUE)(requirements = CAN_RUN_WHOLE_MACHINE =?= true))" |
| lepton.rcac.purdue.edu/jobmanager-pbs | purdue-htpc | rsl="(jobtype=single)(queue=tg_workq)(xcount=8)(host_xcount=1)(maxWallTime=2800)" |
| osg-gw-2.t2.ucsd.edu/jobmanager-condor | ucsd-htpc | rsl = "(condorsubmit=('+RequiresWholeMachine' TRUE))" |
| grid1.oscer.ou.edu/jobmanager-lsf | ou-htpc | rsl = "(jobtype=single)(exclusive=1)(maxWallTime=2800)" |
| pf-grid.unl.edu/jobmanager-condor | nebraska-prairiefire-htpc | rsl = "(condorsubmit=('+RequiresWholeMachine' TRUE)('Requirements' 'isWholeMachineSlot=?=TRUE && TotalSlots == 9'))" |






---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = DanFraser

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = User

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = Scientist

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Training

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DanFraser
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = BrianBockelman
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %YES%
############################################################################################################
-->