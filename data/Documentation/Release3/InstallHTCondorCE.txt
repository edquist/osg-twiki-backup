%META:TOPICINFO{author="BrianBockelman" date="1409750476" format="1.1" version="1.69"}%
%META:TOPICPARENT{name="Blueprint.WebHome"}%
---+!! Installing the HTCondor CE

%TOC{depth="2"}%

---# About this Document

This document is for System Administrators. It covers the installation of the HTCondor-CE software, which aims to provide an end-to-end gatekeeper technology built entirely out of core HTCondor components.  As a goal, we aim for the HTCondor-CE to be a particular "configuration" of HTCondor, and not include any non-HTCondor daemons.

This document follows the general OSG documentation conventions: %TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to expand document conventions..."}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%
%ENDTWISTY%

---# How to get Help?
To get assistance please use the [[Documentation.HelpProcedure][this page]].

---# Requirements

---## Host and OS
   * A host to install the Compute Element
   * OS is %SUPPORTED_OS%
   * Root access

---## Users

%STARTSECTION{"Users"}%

The following users are needed by HTCondor-CE at all sites
| *User* | *Comment* |
| =condor= | The HTCondor-CE will be run as root, but perform most of its operations as the =condor= user. |
| =gratia= | Runs the Gratia probes to collect accounting data |

The above user will be added to the system automatically when the HTCondor RPM installs.  If your fabric management automatically overwrites users and groups, you will want to create this user beforehand.

%ENDSECTION{"Users"}%

---## Certificates
| *Certificate* | *User that owns certificate* | *Path to certificate* |
| Host certificate | =root= | =/etc/grid-security/hostcert.pem= <br> =/etc/grid-security/hostkey.pem= |

Find instructions to request a host certificate [[Documentation/Release3.GetHostServiceCertificates][here]].

---## Networking

%STARTSECTION{"Firewalls"}%
%INCLUDE{"Documentation/Release3/FirewallInformation" section="FirewallTable" lines="htcondorce,htcondorce_shared"}% 

Allow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.</br>

%ENDSECTION{"Firewalls"}%

---## Additional Requirements
To be part of the OSG Production Grid, your CE must be registered in OIM. To register your resource:
   * Use your user certificate.  Find instructions to request a user certificate [[Documentation.CertificateUserGet][here]].
   * Register in OIM as described in Operations.OIMRegistrationInstructions

---### HTCondor Versions
One goal of the HTCondor-CE is to use the site's condor_* binaries, but run a completely different set of daemons (similar to OSG's condor-cron for RSV).  However, during the development of the CE, several bugs were discovered.  Hence, the HTCondor-CE requires at least version 8.0.3 of HTCondor to be used.

---### NFS-shared Directories

If your site uses the HTCondor batch system, no home directories are necessary.  All data movement is handled via HTCondor file transfer. For all other batch systems, a shared NFS directory is needed to export files to worker nodes.

---# Installation Procedure
%NOTE% We currently distribute the HTCondor-CE only in the OSG 3.2 repository; make sure you have used the appropriate repositories.
%INCLUDE{"Documentation/Release3.YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%
%INCLUDE{"Documentation/Release3.InstallCertAuth" section="OSGBriefCaCerts" TOC_SHIFT="+"}%

---## Installing HTCondor CE and Related Software

A complete CE installation contains the job gateway itself (the HTCondor CE job router) and a variety of other support software (such as a Gratia probe, the Squid proxy server, and OSG Configure). To simplify installation, there are special RPMs that bring along all required packages, based on your batch system. Pick one install command from the table below:

%TABLE{sort="off"}%
| *If your batch system is…* | *Then run the following command…* |
| HTCondor | =yum install osg-ce-condor= |
| PBS | =yum install osg-ce-pbs= |
| LSF | =yum install osg-ce-lsf= |
| SGE | =yum install osg-ce-sge= |

%NOTE% To smooth the transition between GRAM-based CE's and HTCondor CE's, we are currently shipping both CE's in the =osg-ce-*= packages. The GRAM software will eventually be dropped but for now it remains the default. The instructions below will show you how to disable GRAM and enable HTCondor CE.

---# Configuration Instructions

---## Setup osg-configure
   1. =osg-configure= is an automatic configuration tool with many [[Documentation.Release3.IniConfigurationOptions][options]] that is used to setup a CE. You must tell it that you're using an HTCondor-CE rather than a GRAM-based CE by editing =/etc/osg/config.d/10-gateway.ini= by disabling the GRAM gateway and enabling the HTCondor gateway: <pre class="file">
gram_gateway_enabled = False
htcondor_gateway_enabled = True
</pre> %NOTE% If you need to run _both_ HTCondor CE and GRAM (e.g. you need to run SAM tests), you need to enable both gateways:<pre class="file">
gram_gateway_enabled = True
htcondor_gateway_enabled = True
</pre> For more information about the GRAM CE, you can read about that at Documentation.Release3.InstallComputeElement.
   1. Enable your batch system by editing the =enabled= field in =/etc/osg/config.d/20-%RED%&lt;your batch system&gt;%ENDCOLOR%.ini=: <pre class="file">
enabled = True
</pre>

---## Setup authorization

---### Using GUMS for Authorization
%INCLUDE{"Documentation/Release3/InstallComputeElement" section="GumsAuth"}%

%NOTE% Once gsi-authz.conf is in place, your site's HTCondor will attempt to utilize the LCMAPS callouts if enabled in the condor_mapfile.  If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null in the HTCondor configuration.

---### Using edg-mkgridmap for authorization
%INCLUDE{"Documentation/Release3/InstallComputeElement" section="GridmapAuth"}%

If you are not using GUMS, edit the =authorize_only= policy of =/etc/lcmaps.db=.  Comment out the =gumsclient= line and uncomment the =gridmapfile= line.  The resulting policy should read (comments removed)

<pre class="file">
authorize_only:
gridmapfile -> good | bad
</pre>

Specify in this file =/etc/condor-ce/config.d/01-common-auth.conf= where your GRIDMAP is as follows:

<pre class="file">
GRIDMAP = /etc/grid-security/grid-mapfile
</pre>

---## Setup Job Routes
The HTCondor-CE depends on the [[http://research.cs.wisc.edu/htcondor/manual/v7.8/5_5HTCondor_Job.html][HTCondor JobRouter]] to transform an incoming grid job into a batch system job.  This is controlled by a job _route_; default routes are installed in =/etc/condor-ce/config.d/02-ce-*.conf=.  Each route corresponds to a separate job transformation. The built-in routes contain only the minimal base functionality needed for starting jobs at a small site.

For HTCondor sites, =02-ce-condor.conf= assumes that the SPOOL location for the site schedd is in the "normal" location of /var/lib/condor/spool, and the schedd's name is =$(FULL_HOSTNAME)=.  You must customize these if you run a non-RPM version of HTCondor.

Place customizations in =/etc/condor-ce/config.d/99-local.conf= (or a similarly named file which overrides 02-*; files in the directory are evaluated in alphabetical order), not the original =02-ce-*.conf=.

%NOTE% This section covers just the basic customizations; A larger site may want to elaborate on how to better customize configurations and [[Documentation/Release3.HTCondorCERoutes][refer to this document]].

---## Other Customizations

   * The Unix environment variables for the HTCondor-CE daemons is controlled by =/etc/sysconfig/condor-ce=.

   * You can place site HTCondor-CE configuration customizations in =/etc/condor-ce/config.d=; *do not edit files* in this directory installed by the HTCondor-CE, as edits will be lost on upgrade.  Instead, add a new file that overrides HTCondor-CE's settings.  Any filename prefixed with "99-" will override the files from the CE.
   
---# Batch System Configuration
=/etc/blahp= has scripts that allow you to set environment variables and modify the submit script that HTCondor-CE submits to the jobmanager.  However, you should not need to change these files.

---## HTCondor

   * The values of *JOB_ROUTER_SCHEDD2_NAME* and *JOB_ROUTER_SCHEDD2_POOL* may need to be customized.  The pool should be set to the value of the _site HTCondor_ CONDOR_HOST (using hostnames rather than IP addresses).
      * If you can successfully run the following command, you have these variables set correctly: <pre> condor_q -pool %RED%JOB_ROUTER_SCHEDD2_POOL%ENDCOLOR% -name %BLUE%JOB_ROUTER_SCHEDD2_NAME%ENDCOLOR%</pre>

   * HTCondor-CE with HTCondor pool schematic view: <br />
     <img src="%ATTACHURLPATH%/condor-ce-condor-schematics.png" alt="condor-ce-condor-schematics.png" width='700' height='306' />  

%NOTE% The HTCondor-CE installs a *custom configuration of HTCondor*.  If you are running HTCondor as a batch system, this means you will have two sets of HTCondor daemons running.  This is similar to how RSV works. To configure the CE, you need to look at =/etc/condor-ce=, *NOT* =/etc/condor=. Also, checking config files source might help to understand difference better: =condor_ce_config_val -config= or =condor_config_val -config=. Similarly, to list the jobs in queue, you will need to perform =condor_ce_q=, not =condor_q=. 

---## Other Batch Systems

---#### Sharing the Spool directory

Like GRAM, HTCondor-CE requires a shared file system between worker nodes and the CE to transfer files.  Unlike GRAM, it does not write job files into the user's =$HOME= directory.  By default, job files are written into =/var/lib/condor-ce= and hence that directory must appear at that location on all worker nodes.  You can control the exported directory by setting the =SPOOL= configuration variable in =/etc/condor-ce/config.d=. We recommend setting up an NFS server on the CE dedicated to sharing the spool directory with the site's worker nodes instead of using a pre-existing NFS share. %RED%Root squash must be turned off and the directory must be readable/writeable by the condor user for HTCondor CE to function correctly.%ENDCOLOR%

---#### Setting job queue and walltime

The job queue, memory usage, and the walltime can be set by the =default_queue=, =default_maxMemory=, and = maxWallTime= attributes, respectively.  The following job route would set the default queue to 'grid' and memory limit to 2.5GB for a PBS batch system: \

<pre class="file">JOB_ROUTER_ENTRIES = \
   [ \
     GridResource = "pbs"; \
     TargetUniverse = 9; \
     name = "Local_PBS"; \
     set_default_queue = "grid"; \
     set_default_maxMemory = 2500; \
     Requirements = true; \
   ]</pre>

---#### Adding attributes to the each job

Additional attributes can be inserted into the job submit script by editing =/usr/libexec/blahp/%RED%&lt;batch system&gt;%ENDCOLOR%_local_submit_attributes.sh=.  This file is sourced during submit time and anything printed to stdout is appended to the job submit script.  For example, the following will set a default walltime for PBS: \

<pre class="file">#!/bin/sh

# Set walltime according to request; the batch system
# may reject this, of course!
if [ -n "$Walltime" ]; then
  echo "#PBS -l walltime=$Walltime"
else
  echo "#PBS -l walltime=24:00:00"
fi
</pre>

The environment variable =$Walltime= is set if the the attribute =remote_cerequirements= is set in the HTCondor-G job.  That attribute should follow the form:

<pre class="file">remote_cerequirements = foo == X && bar == Y && ...</pre>

to set =foo= to value X and =bar= to Y in the environment of =%RED%&lt;batch system&gt;%ENDCOLOR%_local_submit_attributes.sh=.  So, to set the Walltime to 1 hour with the above =%RED%&lt;batch system&gt;%ENDCOLOR%_local_submit_attributes.sh=, the job would need:

<pre class="file">remote_cerequirements = Walltime == 3600</pre>

---#### Debugging

The interaction between HTCondor-CE and other batch systems can be difficult to debug, especially on the initial install.  A reasonable set of debugging information can be produced by adding the following to the HTCondor-CE configuration: \

<pre class="file">MAX_GRIDMANAGER_LOG = 6h
MAX_NUM_GRIDMANAGER_LOG = 8
GRIDMANAGER_LOG = D_FULLDEBUG
</pre>  

With this, each interaction between HTCondor-CE and blahp/batch system will be logged into =/var/log/condor-ce/GridManagerLog.*=.

---### PBS and SLURM

---#### Submitting jobs to SLURM

With the latest PBS emulation layer, HTCondor-CE can submit to SLURM using the PBS backends.  However, the separate SLURM Gratia probe still needs to be installed.

---### LSF

No testing has been done yet with LSF. However, the interaction with HTCondor-CE is similar to PBS and the PBS customization instructions may help.

---### SGE

No testing has been done yet with SGE. However, the interaction with HTCondor-CE is similar to PBS and the PBS customization instructions may help.

---# Testing with RSV
RSV is an useful testing tool to verify if your CE is running correctly. It is a software installed separately (it can be also on a different host), see InstallRSV.
Anyway to test a CE it needs to be authorized via GUMS or edg-mkgridmap, depending on which one you use. MapServiceCertToRsvUser describes how to map the service or user certificate used for RSV to allow to run the tests.

---# Information systems

OSG sites report information about their site to OSG. This is particularly important for WLCG sites, which need to be present in the BDII information service so the WLCG can run jobs on these sites. However, this is important for all OSG sites because the information is used for site discovery.

---## Generic Information Provider (GIP)

The Generic Information Provider (GIP) is a program that discovers information about your site. It only discovers the information: other software propagates that software to OSG.

Configuration of the GIP happens entirely via the =/etc/osg/config.d/*.ini= files. These are the same files that are used by =osg-configure=.

[[Documentation.Release3.NavTechGIP][More information on configuring the GIP]].

---## OSG Info Services

OSG Info Services is a drop-in replacement for !CEMon in OSG Software 3.2.0.

If you are upgrading an OSG Software 3.1.x install to a 3.2.x install, see the [[Documentation.Release3.OSGReleaseSeries#Migrating_from_CEMon_to_OSG_Info][instructions for migrating configuration]].

Setup is similar to !CEMon: it runs as either the =tomcat= user or the user account specified by the =user= option in the =[GIP]= section of the osg-configure config files and so needs the =httpcert.pem= and =httpkey.pem= files set up to be owned by the specified user.

For osg-info-services to run, you need to have a host or service certificate in =/etc/grid-security/http/httpcert.pem= and =/etc/grid-security/http/httpkey.pem=.  It runs as either the =tomcat= user or the user account specified by the =user= option in the =[GIP]= section of the osg-configure config files and so needs the =httpcert.pem= and =httpkey.pem= files set up to be owned by the specified user.  The simplest thing to do is to copy your host certificate.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir /etc/grid-security/http
%UCL_PROMPT_ROOT% cp /etc/grid-security/hostkey.pem /etc/grid-security/http/httpkey.pem
%UCL_PROMPT_ROOT% cp /etc/grid-security/hostcert.pem /etc/grid-security/http/httpcert.pem
%UCL_PROMPT_ROOT% chown -R tomcat /etc/grid-security/http
%UCL_PROMPT_ROOT% chmod 0400 /etc/grid-security/http/httpkey.pem
</pre>

Note that you want to copy, not move, the hostcerts - Globus still expects them in the original location.

In order to use the =osg-info-services= you'll need to setup the =[Info Services]= section in the configuration files and then you'll need to start the =osg-info-services= service.
(If you are not using OSG Software 3.2.5 yet, the section is still called =[CEMon]=).

You will need a =user-vo-map= file, which you can get from either GUMS or edg-mkgridmap.
You can either start the gums-client-cron service, described below, run =gums-host-cron= (from the =gums-client= package) once, or run =edg-mkgridmap= once.

You will need to have GIP configured to use =osg-info-services=.
[[Documentation.Release3.NavTechGIP][More information on configuring the GIP]].

To enable the service that runs osg-info-services periodically, run the following:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% service osg-info-services start
</pre>

Finally, you'll need to enable the service on boot:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% chkconfig osg-info-services on
</pre>

---# Services

---++ Starting and Enabling Services

   1. %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlStart"}%
   2. Start Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gratia-probes-cron start
</pre>
   3. Start your batch system (choose the appropriate one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor start
%UCL_PROMPT_ROOT% /sbin/service pbs_server start
</pre>
   4. Start HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor-ce start
</pre>

[Optional but recommended:] Enable services so that they start automatically when your system is powered on:

   * %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlEnable"}%
   * Enable HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor-ce on
</pre>
   * Enable batch system (choose one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor on
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server on
</pre>
   * Enable Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig gratia-probes-cron on
</pre>

---++ Stopping and Disabling Services

   1. Stop HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor-ce stop
</pre>
   2. Stop your batch system (choose the appropriate one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor stop
%UCL_PROMPT_ROOT% /sbin/service pbs_server stop
</pre>
   3. Stop Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gratia-probes-cron stop
</pre>
   4. (other grid service running on the machine may still use it)
   %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlStop"}%
Stop services from starting when the system is powered on:

   * Disable HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor-ce off
</pre>
   * Disable batch system (choose one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor off
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server off
</pre>
   * Disable Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig gratia-probes-cron off
</pre>
   * (other grid service running on the machine may still use it) %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlDisable"}%
   
---# Troubleshooting

---## Useful configuration and log files 

Configuration Files
| *Service or Process* | *Configuration File* | *Description* |
| condor-ce | =/etc/condor-ce/condor_config= | HTCondor-CE Base Config |
| | =/etc/condor-ce/condor_mapfile= | Authorization file used if not using GUMS |
| | =/etc/condor-ce/config.d/*= | Additional HTCondor-CE Configs <br/> Place personal configurations here, e.g. =99-ce-*.conf= |
| blah | =/etc/blah.config= | Configuration for the BLAH process |

Log files
| *Service or Process* | *Log File* | *Description* |
| condor | =/var/log/condor/*= | HTCondor daemon files |
|| =/var/log/condor/user/*= <br/> =/var/log/condor/Gridmanager.*= | Per-user log files recording individual submissions and interactions with the batch system. |
| condor-ce | =/var/log/condor-ce/*= | HTCondor daemon files |
| lcmaps | =/var/log/messages= | LCMAPS sends authentication and authorization information into syslog; check this if you are having authz difficulties. |

---## Included test utilities

You can use the =condor_ce_run= utility to send test jobs to a remote HTCondor-CE.  For example, to run the =env= binary in the remote batch system, you can do:

<pre class="screen">
condor_ce_run -r %RED%condorce.example.com:9619%ENDCOLOR% env
</pre>

Replacing the %RED%red%ENDCOLOR% text with the hostname of the CE. HTCondor will submit directly to the remote schedd with the =-r= flag;  Without the flag, it will submit to the local schedd using the grid universe.

If =condor_ce_run= fails (see example after the next test tool), then the =condor_ce_trace= tool will assist in verifying the install:

<pre class="screen">
condor_ce_trace %RED%condorce.example.com%ENDCOLOR%
</pre>

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example condor_ce_trace run"}%
<pre class="screen">
%UCL_PROMPT% condor_ce_trace red.unl.edu
Testing HTCondor-CE collector connectivity.
- Successful ping of collector on <129.93.239.129:9619>.

Testing HTCondor-CE schedd connectivity.
- Successful ping of schedd on <129.93.239.129:9620?sock=8556_0571_4>.

Submitting job to schedd <129.93.239.129:9620?sock=8556_0571_4>
- Successful submission; cluster ID 112170
Resulting job ad: 
    [
        BufferSize = 524288; 
        NiceUser = false; 
        CoreSize = -1; 
        CumulativeSlotTime = 0; 
        OnExitHold = false; 
        RequestCpus = 1; 
        Err = "_condor_stderr"; 
        BufferBlockSize = 32768; 
        x509userproxy = "/tmp/x509up_u1221"; 
        TransferOutputRemaps = "_condor_stdout=/home/cse496/bbockelm/projects/condor-ce/.stdout_23011_RD17wL;_condor_stderr=/home/cse496/bbockelm/projects/condor-ce/.stderr_23011_XeO5dg"; 
        ImageSize = 100; 
        CurrentTime = time(); 
        WantCheckpoint = false; 
        CommittedTime = 0; 
        TargetType = "Machine"; 
        WhenToTransferOutput = "ON_EXIT"; 
        Cmd = "/bin/env"; 
        JobUniverse = 5; 
        ExitBySignal = false; 
        HoldReasonCode = 16; 
        Iwd = "/home/cse496/bbockelm/projects/condor-ce"; 
        NumRestarts = 0; 
        CommittedSuspensionTime = 0; 
        Owner = undefined; 
        NumSystemHolds = 0; 
        CumulativeSuspensionTime = 0; 
        RequestDisk = DiskUsage; 
        Requirements = true && TARGET.OPSYS == "LINUX" && TARGET.ARCH == "X86_64" && TARGET.HasFileTransfer && TARGET.Disk >= RequestDisk && TARGET.Memory >= RequestMemory; 
        MinHosts = 1; 
        JobNotification = 0; 
        NumCkpts = 0; 
        LastSuspensionTime = 0; 
        NumJobStarts = 0; 
        WantRemoteSyscalls = false; 
        JobPrio = 0; 
        RootDir = "/"; 
        CurrentHosts = 0; 
        x509UserProxyExpiration = 1367717162; 
        StreamOut = false; 
        WantRemoteIO = true; 
        OnExitRemove = true; 
        DiskUsage = 1; 
        In = "/dev/null"; 
        PeriodicRemove = false; 
        RemoteUserCpu = 0.0; 
        LocalUserCpu = 0.0; 
        LocalSysCpu = 0.0; 
        RemoteSysCpu = 0.0; 
        ClusterId = 112170; 
        Log = "/home/cse496/bbockelm/projects/condor-ce/.log_23011_QRcWCU"; 
        CompletionDate = 0; 
        RemoteWallClockTime = 0.0; 
        LeaveJobInQueue = JobStatus == 4 && ( CompletionDate is UNDDEFINED || CompletionDate == 0 || ( ( time() - CompletionDate ) < 864000 ) ); 
        CondorVersion = "$CondorVersion: 7.9.6 Apr 22 2013 BuildID: RH-7.9.6-0.2.de1f9cc.git.lark.osg.el6 PRE-RELEASE-UWCS $"; 
        MyType = "Job"; 
        StreamErr = false; 
        HoldReason = "Spooling input data files"; 
        PeriodicHold = false; 
        ProcId = 0; 
        Out = "_condor_stdout"; 
        JobStatus = 5; 
        PeriodicRelease = false; 
        RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024); 
        Args = ""; 
        MaxHosts = 1; 
        TotalSuspensions = 0; 
        CommittedSlotTime = 0; 
        x509userproxysubject = "/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman/CN=4221328"; 
        CondorPlatform = "$CondorPlatform: X86_64-ScientificLinux_6.3 $"; 
        ShouldTransferFiles = "YES"; 
        ExitStatus = 0; 
        QDate = 1367674672; 
        EnteredCurrentStatus = 1367674672
    ]
Spooling cluster 112170 files to schedd <129.93.239.129:9620?sock=8556_0571_4>
- Successful spooling
Job status: Held
Job transitioned from Held to Idle
Job transitioned from Idle to Running
Job transitioned from Running to Completed
- Job was successful
</pre>
%ENDTWISTY%

One of known failure modes of the =condor_ce_run= command:

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example =condor_ce_run= with an error related to wrong ownership of SPOOL subdirectories"}%
<pre class="screen">
%UCL_PROMPT% condor_ce_run -r grid39.racf.bnl.gov:9619 /bin/hostname
ERROR: Failed to connect to queue manager grid39.racf.bnl.gov
SECMAN:2007:could not receive post_auth_info.
Traceback (most recent call last):
  File "/usr/bin/condor_ce_run", line 373, in <module>
    sys.exit(main())
  File "/usr/bin/condor_ce_run", line 363, in main
    submit_job(job_info)
  File "/usr/bin/condor_ce_run", line 149, in submit_job
    raise CondorRunException("Could not parse job cluster from " \
__main__.CondorRunException: Could not parse job cluster from condor_submit output
</pre>
%ENDTWISTY%
In such case, please see the section [[Documentation/Release3.InstallHTCondorCE#9_Known_Issues]["Known Issues"]].

---## Included debug utilities

You can use the =condor_ce_q= utility to query jobs in the HTCondor-CE schedd.  Here few examples of =condor_ce_q= alternatives:

<pre class="screen">
condor_ce_q -held %RED%condorce.example.com%ENDCOLOR%
</pre>
or if running more schedds on onehost (=condor_ce_status -schedd=) you can query -held jobs per schedd name:
<pre class="screen">
condor_ce_q -name %RED%schedd_name%ENDCOLOR% -held
</pre>

Replacing the %RED%red%ENDCOLOR% text according to value/hostname/attribute you are interested in.

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example condor_ce_q -held reasons"}%
<pre class="screen">
%UCL_PROMPT% condor_ce_q -pool red-gw2.unl.edu -held
50202.0   osg             3/7  11:23 CE job in status 1 put on hold by SYSTEM_PERIODIC_HOLD due to expired user proxy.
65275.0   glow            3/7  11:21 Job in status 1 put on hold by SYSTEM_PERIODIC_HOLD due to memory usage 2750000. 
</pre>
%ENDTWISTY%

---## Testing by hand

To submit jobs from a remote machine, all you have to do is InstallCondor to set up a submit host. From the test submit host, use this file to submit to the CE:
<pre class="file">
universe = grid
grid_resource = condor %RED%condorce.example.com condorce.example.com:9619%ENDCOLOR%

executable = test.sh
output = test_g.out
error = test_g.err
log = test_g.log

ShouldTransferFiles = YES
WhenToTransferOutput = ON_EXIT

use_x509userproxy = true

queue
</pre>

Replace =condorce.example.com= with the hostname of the HTCondor-CE, submit this file using =condor_submit= from an external host and make sure to create an executable =test.sh=. 

---## Debug Job Routes
Assuming you use more than JOB_ROUTER_DEFAULTS and you have prepared site specific job routes customization [[Documentation/Release3.HTCondorCERoutes][here]].
If =/var/log/condor-ce/JobRouterLog= doesn't containg anything useful to undertand your problem of job routing you may use =debug()= as a function for particular ENTRY in the JOB_ROUTER_ENTRIES.
Example:
<verbatim>
JOB_ROUTER_ENTRIES = \
   [ \
...
   Requirements = debug(target.queue=="analysis.short"); \
...
   ]
</verbatim>
Also, make sure you have set debug mode in the condor-ce config file (where you define ENTRIES e.g.):
<verbatim>
JOB_ROUTER_DEBUG = D_FULLDEBUG
GRIDMANAGER_DEBUG = D_FULLDEBUG
</verbatim>

---## Examining interactions with local jobmanager
To examine the files that are being submitted to the local jobmanager, edit =/etc/blah.config= and add the following line 
<pre class="file">
blah_debug_save_submit_info=DIR_NAME
</pre>
where =DIR_NAME= is replaced by the directory to save the submit files that HTCondor-CE will use to submit jobs to the local jobmanager. Blah will then create a directory for each submission to the local jobmanager with the submit file, and proxy used.   %RED%Note, whitespace is important so do not put any spaces around the = sign.  In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within =DIR_NAME=. %ENDCOLOR%
---# Known Issues

---++!! Unable to handle RSL in HTCondor 8.0.x 

In HTCondor 8.0.x, [[https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3772][a bug]] prevents the =globus_rsl= attribute on the submit side from working correctly.  This will hopefully be fixed in the next point-release of the series.

Until then, we recommend setting the !RequestMemory, !RequestCPUs, and wall time directly in the HTCondor-G submission file or within the job routes.

---++!! Time skews

Like all GSI-based authentication, HTCondor-CE is sensitive to time skews.  Make sure the clock on your CE is synchronized using a utility such as =ntpd=.

*In addition*, at PBS sites, HTCondor itself is sensitive to time skews on the NFS server.  If you see empty stdout / err being returned to the HTCondor-G submit host, verify there is no NFS server  time skew.

---++!! Disable blahp worker node proxy renewal

In =blah.conf=, you should have the following two lines:
<pre>
blah_disable_wn_proxy_renewal=yes
blah_delegate_renewed_proxies=no
</pre>
Neither functionality is used with HTCondor-CE; enabling worker node proxy renewal will actually cause jobs to fail to refresh the proxy in some setups.

---++!! Mismatch between the CE hostname and DNS name

The authorization configuration assumes that the host's hostname and DNS names match; that is, if the host certificate uses the subject =foo.example.com=, then the output of =hostname -f= should be =foo.example.com=.  If this is not true, the site will need to hand-expand the value of $(FULL_HOSTNAME) in the files from =/etc/condor-ce/config.d=.
Contrary, while using external and internal hostname, we've found $(FULL_HOSTNAME) is not respected by HTCondor-CE. Even though FULL_HOSTNAME is defined in all the relevant config files with the hardcoded public hostname and IP, it looks like HTCondor-CE is using its internal FULL_HOSTNAME (given by =hostname -f=) information instead of the superseding config file. We didn't find workaround so far and recommend use same hostname for public and private domain which may not be desirable for some sites. It is hard spot the problem, but here is best "error message" which can indicate an issue - pay attention to failure connect to internal address =128.227.253.206:9619= which supposed to be external:
%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example !MasterLog with an error triggered by internal/external hostname mismatch"}%
<pre class="screen">
%UCL_PROMPT% cat  /var/log/condor-ce/MasterLog
04/22/14 10:55:52 ******************************************************
04/22/14 10:55:52 ** condor_master (CONDOR_MASTER) STARTING UP
04/22/14 10:55:52 ** /usr/sbin/condor_master
04/22/14 10:55:52 ** SubsystemInfo: name=MASTER type=MASTER(2) class=DAEMON(1)
04/22/14 10:55:52 ** Configuration: subsystem:MASTER local:<NONE> class:DAEMON
04/22/14 10:55:52 ** $CondorVersion: 8.0.6 Mar 25 2014 $
04/22/14 10:55:52 ** $CondorPlatform: X86_64-CentOS_6.5 $
04/22/14 10:55:52 ** PID = 10079
04/22/14 10:55:52 ** Log last touched 4/22 10:55:52
04/22/14 10:55:52 ******************************************************
04/22/14 10:55:52 Using config source: /etc/condor-ce/condor_config
04/22/14 10:55:52 Using local config sources:
04/22/14 10:55:52 /usr/share/condor-ce/config.d/01-ce-auth-defaults.conf
04/22/14 10:55:52 /usr/share/condor-ce/config.d/01-ce-router-defaults.conf
04/22/14 10:55:52 /usr/share/condor-ce/config.d/01-common-auth-defaults.conf
04/22/14 10:55:52 /usr/share/condor-ce/config.d/02-ce-pbs-defaults.conf
04/22/14 10:55:52 /usr/share/condor-ce/config.d/03-ce-shared-port-defaults.conf
04/22/14 10:55:52    /etc/condor-ce/config.d/01-ce-auth.conf
04/22/14 10:55:52    /etc/condor-ce/config.d/01-ce-router.conf
04/22/14 10:55:52    /etc/condor-ce/config.d/01-common-auth.conf
04/22/14 10:55:52    /etc/condor-ce/config.d/02-ce-pbs.conf
04/22/14 10:55:52 /etc/condor-ce/config.d/03-ce-shared-port.conf
04/22/14 10:55:52 /usr/share/condor-ce/condor_ce_router_defaults|
04/22/14 10:55:52 Daemon Log is logging: D_ALWAYS D_ERROR
04/22/14 10:55:52 SharedPortEndpoint: waiting for connections to named socket 10079_698c
04/22/14 10:55:52 SharedPortEndpoint: failed to open /var/lock/condor-ce/shared_port_ad: No such file or directory
04/22/14 10:55:52 SharedPortEndpoint: did not successfully find SharedPortServer address. Will retry in 60s.
04/22/14 10:55:52 DaemonCore: private command socket at <128.227.253.206:0?sock=10079_698c>
04/22/14 10:55:52 Master restart (GRACEFUL) is watching /usr/sbin/condor_master (mtime:1395795600)
04/22/14 10:55:52 Started DaemonCore process "/usr/libexec/condor/condor_shared_port", pid and pgroup = 10082
04/22/14 10:55:52 Waiting for /var/lock/condor-ce/shared_port_ad to appear.
04/22/14 10:55:53 Found /var/lock/condor-ce/shared_port_ad.
04/22/14 10:55:53 Collector port not defined, will use default: 9618
04/22/14 10:55:53 Started DaemonCore process "/usr/sbin/condor_collector", pid and pgroup = 10084
04/22/14 10:55:53 Started DaemonCore process "/usr/sbin/condor_schedd", pid and pgroup = 10085
04/22/14 10:55:53 Started DaemonCore process "/usr/libexec/condor/condor_job_router", pid and pgroup = 10087
04/22/14 10:55:57 attempt to connect to <128.227.253.206:9619> failed: Connection refused (connect errno = 111).
04/22/14 10:55:57 ERROR: SECMAN:2004:Failed to create security session to <128.227.253.206:9619> with TCP.|SECMAN:2003:TCP connection to <128.227.253.206:9619> failed.
04/22/14 10:55:57 Failed to start non-blocking update to <128.227.253.206:9619>. 
</pre>
%ENDTWISTY%
 

---++!! Wrong ownership of SPOOL subdirectories
When deploying HTCondor-CE at BNL, it has been noticed that many directories were being created with the wrong ownership (root instead of user condor). Job gets submitted through the Schedd but fails passing submission chain through JobRouter. Here is typical symptom what's being reported in the =/var/log/condor-ce/JobRouter=:

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="example of !JobRouter log file with an error of SPOOL ownership"}%
<pre class="screen">
06/05/14 15:03:28 Attempting to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0' from 104 to 500.500, but the path was unexpectedly owned by 0
06/05/14 15:03:28 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0' from 104 to 500.500
06/05/14 15:03:28 (1.0) Failed to chown /var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0 from 104 to 500.500.
06/05/14 15:03:28 SharedPortClient: sent connection request to schedd at <130.199.185.147:9620> for shared port id 1949_d991_4
06/05/14 15:03:28 Attempting to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0' from 500 to 104.161, but the path was unexpectedly owned by 0
06/05/14 15:03:28 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0' from 500 to 104.161
06/05/14 15:03:28 (1.0) Failed to chown /var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0 from 500 to 104.161.  User may run into permissions problems when fetching sandbox.
06/05/14 15:03:28 JobRouter failure (src=1.0,route=Local_Condor): candidate job could not be claimed by JobRouter: Failed to create/chown source job spool directory to the user.
</pre>
%ENDTWISTY%

%NOTE% The files in HTCondor-CE SPOOL directory should be owned by =condor:condor=.

At the same time checking =/var/log/messages= we noticed that LCMAPS was failing with the following error:
<pre class="screen">
Jun  5 14:35:11 grid39 htondor-ce-llgt[1024]: lcmaps: setup_client_ctx: Error loading certificate chain from file: '/etc/grid-security/hostcert.pem'. OpenSSL reason: no start line.
Jun  5 14:35:11 grid39 htondor-ce-llgt[1024]: lcmaps: SSL_client_connect: Error: can't create SSL handle out of CTX structure
Jun  5 14:35:11 grid39 htondor-ce-llgt[1024]: lcmaps: ssl_io_connect(): Failure in SSL layer setup and connection!
Jun  5 14:35:11 grid39 htondor-ce-llgt[1024]: lcmaps: xacmlqueryscas(): XACML: Interaction failed: TCP
</pre>

Details are tracked down under [[https://jira.opensciencegrid.org/browse/SOFTWARE-1506][JIRA-1506]].

%META:FILEATTACHMENT{name="condor-ce-condor-schematics.png" attachment="condor-ce-condor-schematics.png" attr="" comment="HTCondor-CE with HTCondor pool" date="1399668761" path="condor-ce-condor-schematics.png" size="204812" stream="condor-ce-condor-schematics.png" tmpFilename="/usr/tmp/CGItemp64474" user="MarianZvada" version="4"}%
%META:TOPICMOVED{by="TimCartwright" date="1394124083" from="Documentation.InstallHTCondorCE" to="Documentation/Release3.InstallHTCondorCE"}%
