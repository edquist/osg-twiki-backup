%META:TOPICINFO{author="TimCartwright" date="1394124488" format="1.1" reprev="1.37" version="1.37"}%
%META:TOPICPARENT{name="Blueprint.WebHome"}%
---+!! Installing the HTCondor CE

%TOC{depth="2"}%

---# About this Document

This document is for System Administrators. It covers the installation of the HTCondor-CE software, which aims to provide an end-to-end gatekeeper technology built entirely out of core HTCondor components.  As a goal, we aim for the HTCondor-CE to be a particular "configuration" of HTCondor, and not include any non-HTCondor daemons.

The HTCondor-CE approach is under active investigation; this page provides *developer documentation* for installing and configuration the CE.

This document follows the general OSG documentation conventions: %TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to expand document conventions..."}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%
%ENDTWISTY%

---# How to get Help?
To get assistance please use the [[Documentation.HelpProcedure][this page]].

---# Requirements

---## Host and OS
   * A host to install the Compute Element
   * OS is %SUPPORTED_OS%
   * Root access

---## Users

%STARTSECTION{"Users"}%

The following users are needed by HTCondor-CE at all sites
| *User* | *Comment* |
| =condor= | The HTCondor-CE will be run as root, but perform most of its operations as the =condor= user. |
| =gratia= | Runs the Gratia probes to collect accounting data |

The above user will be added to the system automatically when the HTCondor RPM installs.  If your fabric management automatically overwrites users and groups, you will want to create this user beforehand.

%ENDSECTION{"Users"}%

---## Certificates
| *Certificate* | *User that owns certificate* | *Path to certificate* |
| Host certificate | =root= | =/etc/grid-security/hostcert.pem= <br> =/etc/grid-security/hostkey.pem= |

Find instructions to request a host certificate [[Documentation/Release3.GetHostServiceCertificates][here]].

---## Networking

%STARTSECTION{"Firewalls"}%
%INCLUDE{"Documentation/Release3/FirewallInformation" section="FirewallTable" lines="htcondorce,htcondorce_shared"}% 

Allow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node <br/>

Only ephemeral outgoing ports are necessary.

%ENDSECTION{"Firewalls"}%

---## Additional Requirements
To be part of the OSG Production Grid, your CE must be registered in OIM. To register your resource:
   * Use your user certificate.  Find instructions to request a user certificate [[Documentation.CertificateUserGet][here]].
   * Register in OIM as described in Operations.OIMRegistrationInstructions

---### HTCondor Versions
One goal of the HTCondor-CE is to use the site's condor_* binaries, but run a completely different set of daemons (similar to OSG's condor-cron for RSV).  However, during the development of the CE, several bugs were discovered.  Hence, the HTCondor-CE requires at least version 8.0.3 of HTCondor to be used.

---### NFS-shared Directories

If your site uses the HTCondor batch system, no home directories are necessary.  All data movement is handled via HTCondor file transfer. For all other batch systems, a shared NFS directory is needed to export files to worker nodes.

---# Installation Procedure

%INCLUDE{"Documentation/Release3.YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%
%INCLUDE{"Documentation/Release3.InstallCertAuth" section="OSGBriefCaCerts" TOC_SHIFT="+"}%

---## Install CRLs and authorization
<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install fetch-crl lcmaps lcas-lcmaps-gt4-interface
</pre>

---## Install HTCondor-CE and Gratia
<pre class="rootscreen">
%RED%# For an HTCondor batch system%ENDCOLOR%
%UCL_PROMPT_ROOT% yum install condor
%UCL_PROMPT_ROOT% yum install htcondor-ce-condor gratia-probe-condor
%RED%# For a PBS batch system%ENDCOLOR%
%UCL_PROMPT_ROOT% yum install htcondor-ce-pbs gratia-probe-pbs-lsf
</pre>

%NOTE% In the future, we will use meta-RPMs to put this all into one step
%NOTE% We currently distribute the HTCondor-CE only in the OSG 3.2 repository; make sure you have used the appropriate repositories.

---# Configuration Instructions

---## Setup authorization
If you are using GUMS, you need to configure =/etc/lcmaps.db=, as you would a [[Documentation/Release3.InstallComputeElement#8_1_Using_GUMS_for_Authorization][GRAM-based OSG-CE]].  Remember to uncomment the line in =/etc/grid-security/gsi-authz.conf=.

%NOTE% Once gsi-authz.conf is in place, your site's HTCondor will attempt to utilize the LCMAPS callouts if enabled in the condor_mapfile.  If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null in the HTCondor configuration.

If you are not using GUMS, edit the =authorize_only= policy of =/etc/lcmaps.db=.  Comment out the =gumsclient= line and uncomment the =gridmapfile= line.  The resulting policy should read (comments removed)

<pre class="file">
authorize_only:
gridmapfile -> good | bad
</pre>



---## Setup Job Routes

%NOTE% This section covers just the basic customizations; a site needing more elaborate configurations should refer to [[Documentation/Release3.HTCondorCERoutes][this page]].

The HTCondor-CE depends on the [[http://research.cs.wisc.edu/htcondor/manual/v7.8/5_5HTCondor_Job.html][HTCondor JobRouter]] to transform an incoming grid job into a batch system job.  This is controlled by a job _route_; default routes are installed in =/etc/condor-ce/config.d/02-ce-*.conf=.  Each route corresponds to a separate job transformation. The built-in routes contain only the minimal base functionality needed for starting jobs at a small site.  A larger site may [[Documentation/Release3.HTCondorCERoutes][want to refer to this document]] for hints on how to better customize their site.

For HTCondor sites, =02-ce-condor.conf= assumes that the SPOOL location for the site schedd is in the "normal" location of /var/lib/condor/spool, and the schedd's name is =$(FULL_HOSTNAME)=.  You must customize these if you run a non-RPM version of HTCondor.

Place customizations in =/etc/condor-ce/config.d/99-local.conf= (or a similarly named file which overrides 02-*; files in the directory are evaluated in alphabetical order), not the original =02-ce-*.conf=.

---## Other Customizations

   * The Unix environment variables for the HTCondor-CE daemons is controlled by =/etc/sysconfig/condor-ce=.

   * You can place site HTCondor-CE configuration customizations in =/etc/condor-ce/config.d=; *do not edit files* in this directory installed by the HTCondor-CE, as edits will be lost on upgrade.  Instead, add a new file that overrides HTCondor-CE's settings.  Any filename prefixed with "99-" will override the files from the CE.
   
---# Batch System Configuration
=/etc/blahp= has scripts that allow you to set environment variables and modify the submit script that HTCondor-CE submits to the jobmanager.  However, you should not need to change these files.

---## HTCondor

   * *MANDATORY*: The site HTCondor schedd must have the following value set: <pre>QUEUE_SUPER_USER_MAY_IMPERSONATE = %RED%.*%ENDCOLOR%</pre> in =/etc/condor/config.d= (not in the HTCondor-CE config!).  This allows the !JobRouter to submit jobs from any user.  You may tighten the regular expression to limit which users are allowed to use the CE.
      * As this relaxes a security setting of the site batch system (albeit, one which must be relaxed for the CE to function), this is *not* currently done automatically.
   * The values of *JOB_ROUTER_SCHEDD2_NAME* and *JOB_ROUTER_SCHEDD2_POOL* may need to be customized.  The pool should be set to the value of the _site HTCondor_ CONDOR_HOST.
      * If you can successfully run the following command, you have these variables set correctly: <pre> condor_q -pool %RED%JOB_ROUTER_SCHEDD2_POOL%ENDCOLOR% -name %RED%JOB_ROUTER_SCHEDD2_NAME%ENDCOLOR%</pre>

%NOTE% The HTCondor-CE installs a *custom configuration of HTCondor*.  If you are running HTCondor as a batch system, this means you will have two sets of HTCondor daemons running.  This is similar to how RSV works. To configure the CE, you need to look at =/etc/condor-ce=, *NOT* =/etc/condor=. Similarly, to list the jobs in queue, you will need to perform =condor_ce_q=, not =condor_q=.
---## PBS and SLURM

   * With the latest PBS emulation layer, HTCondor-CE can submit to SLURM using the PBS backends.  However, the separate SLURM Gratia probe still needs to be installed.

   * Like GRAM, HTCondor-CE requires a shared file system between worker nodes and the CE to utilize PBS.  _Unlike GRAM_, it does not write job files into the user's =$HOME= directory.  By default, job files are written into =/var/lib/condor-ce= and hence that directory must appear at that location on all worker nodes.  You can control the exported directory by setting the =SPOOL= configuration variable in =/etc/condor-ce/config.d=.

   * The job queue and the walltime can be set by the =remote_queue= and =RequestMemory= attributes, respectively.  The following job route would set the default queue to 'grid' and memory limit to 2.5GB: <pre>JOB_ROUTER_ENTRIES = \
   [ \
     GridResource = "pbs"; \
     TargetUniverse = 9; \
     name = "Local_PBS"; \
     set_remote_queue = "grid"; \
     set_RequestMemory = 2500; \
     Requirements = true; \
   ]</pre>

   * Additional attributes can be inserted into the job submit script by editing =/usr/libexec/blahp/pbs_local_submit_attributes.sh=.  This file is sourced during submit time and anything printed to stdout is appended to the job submit script.  For example, the following will set a default walltime: <pre>#!/bin/sh

# Set walltime according to request; the batch system
# may reject this, of course!
if [ -n "$Walltime" ]; then
  echo "#PBS -l walltime=$Walltime"
else
  echo "#PBS -l walltime=24:00:00"
fi
</pre> The environment variable =$Walltime= is set if the the attribute =remote_cerequirements= is set in the HTCondor-G job.  That attribute should be of the form <pre>remote_cerequirements = foo == X && bar == Y && ...</pre> to set =foo= to value X and =bar= to Y in the environment of =pbs_local_submit_attributes.sh=.  So, to set the Walltime to 1 hour with the above =pbs_local_submit_attributes.sh=, the job would need <pre>remote_cerequirements = Walltime == 3600</pre>

   * The interaction between HTCondor-CE and PBS can be difficult to debug, especially on the initial install.  A reasonable set of debugging information can be produced by adding the following to the HTCondor-CE configuration <pre>MAX_GRIDMANAGER_LOG = 6h
MAX_NUM_GRIDMANAGER_LOG = 8
GRIDMANAGER_LOG = D_FULLDEBUG
</pre>  With this, each interaction between HTCondor-CE and blahp/PBS will be logged into =/var/log/condor-ce/GridManagerLog.*=.

---## LSF

No testing has been done yet with LSF.  However, the interaction with HTCondor-CE is similar to PBS and the PBS customization instructions may help.

---# Services

---++ Starting and Enabling Services

   1. %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlStart"}%
   
   2. Start Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gratia-probe-cron start
</pre>
   3. Start your batch system (choose the appropriate one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor start
%UCL_PROMPT_ROOT% /sbin/service pbs_server start
</pre>
   4. Start HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor-ce start
</pre>

[Optional but recommended:] Enable services so that they start automatically when your system is powered on:

   * %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlEnable"}%
   * Enable HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor-ce on
</pre>
   * Enable batch system (choose one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor on
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server on
</pre>
   * Enable Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig gratia-probe-cron on
</pre>

---++ Stopping and Disabling Services

   1. Stop HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor-ce stop
</pre>
   2. Stop your batch system (choose the appropriate one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor stop
%UCL_PROMPT_ROOT% /sbin/service pbs_server stop
</pre>
   3. Stop Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gratia-probe-cron stop
</pre>
   4. (other grid service running on the machine may still use it)
   %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlStop"}%
Stop services from starting when the system is powered on:

   * Disable HTCondor-CE: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor-ce off
</pre>
   * Disable batch system (choose one): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor off
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server off
</pre>
   * Disable Gratia: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig gratia-probe-cron off
</pre>
   * (other grid service running on the machine may still use it) %INCLUDE{"Documentation/Release3/InstallCertAuth" section="OSGBriefFetchCrlDisable"}%
   
---# Troubleshooting

---## Useful configuration and log files 

Configuration Files
| *Service or Process* | *Configuration File* | *Description* |
| condor-ce | =/etc/condor-ce/condor_config= | HTCondor-CE Base Config |
| | =/etc/condor-ce/condor_mapfile= | Authorization file used if not using GUMS |
| | =/etc/condor-ce/config.d/*= | Additional HTCondor-CE Configs <br/> Place personal configurations here, e.g. =99-ce-*.conf= |
| blah | =/etc/blah.config= | Configuration for the BLAH process |

Log files
| *Service or Process* | *Log File* | *Description* |
| condor | =/var/log/condor/*= | HTCondor daemon files |
|| =/var/log/condor/user/*= <br/> =/var/log/condor/Gridmanager.*= | Per-user log files recording individual submissions and interactions with the batch system. |
| condor-ce | =/var/log/condor-ce/*= | HTCondor daemon files |
| lcmaps | =/var/log/messages= | LCMAPS sends authentication and authorization information into syslog; check this if you are having authz difficulties. |

---## Included test utilities

You can use the =condor_ce_run= utility to send test jobs to a remote HTCondor-CE.  For example, to run the =env= binary in the remote batch system, you can do:

<pre class="screen">
condor_ce_run -r %RED%condorce.example.com:9619%ENDCOLOR% env
</pre>

Replacing the %RED%red%ENDCOLOR% text with the hostname of the CE. HTCondor will submit directly to the remote schedd with the =-r= flag;  Without the flag, it will submit to the local schedd using the grid universe.

If =condor_ce_run= fails, then the =condor_ce_trace= tool will assist in verifying the install:

<pre class="screen">
condor_ce_trace %RED%condorce.example.com%ENDCOLOR%
</pre>

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example condor_ce_trace run"}%
<pre class="screen">
%UCL_PROMPT% condor_ce_trace red.unl.edu
Testing HTCondor-CE collector connectivity.
- Successful ping of collector on <129.93.239.129:9619>.

Testing HTCondor-CE schedd connectivity.
- Successful ping of schedd on <129.93.239.129:9620?sock=8556_0571_4>.

Submitting job to schedd <129.93.239.129:9620?sock=8556_0571_4>
- Successful submission; cluster ID 112170
Resulting job ad: 
    [
        BufferSize = 524288; 
        NiceUser = false; 
        CoreSize = -1; 
        CumulativeSlotTime = 0; 
        OnExitHold = false; 
        RequestCpus = 1; 
        Err = "_condor_stderr"; 
        BufferBlockSize = 32768; 
        x509userproxy = "/tmp/x509up_u1221"; 
        TransferOutputRemaps = "_condor_stdout=/home/cse496/bbockelm/projects/condor-ce/.stdout_23011_RD17wL;_condor_stderr=/home/cse496/bbockelm/projects/condor-ce/.stderr_23011_XeO5dg"; 
        ImageSize = 100; 
        CurrentTime = time(); 
        WantCheckpoint = false; 
        CommittedTime = 0; 
        TargetType = "Machine"; 
        WhenToTransferOutput = "ON_EXIT"; 
        Cmd = "/bin/env"; 
        JobUniverse = 5; 
        ExitBySignal = false; 
        HoldReasonCode = 16; 
        Iwd = "/home/cse496/bbockelm/projects/condor-ce"; 
        NumRestarts = 0; 
        CommittedSuspensionTime = 0; 
        Owner = undefined; 
        NumSystemHolds = 0; 
        CumulativeSuspensionTime = 0; 
        RequestDisk = DiskUsage; 
        Requirements = true && TARGET.OPSYS == "LINUX" && TARGET.ARCH == "X86_64" && TARGET.HasFileTransfer && TARGET.Disk >= RequestDisk && TARGET.Memory >= RequestMemory; 
        MinHosts = 1; 
        JobNotification = 0; 
        NumCkpts = 0; 
        LastSuspensionTime = 0; 
        NumJobStarts = 0; 
        WantRemoteSyscalls = false; 
        JobPrio = 0; 
        RootDir = "/"; 
        CurrentHosts = 0; 
        x509UserProxyExpiration = 1367717162; 
        StreamOut = false; 
        WantRemoteIO = true; 
        OnExitRemove = true; 
        DiskUsage = 1; 
        In = "/dev/null"; 
        PeriodicRemove = false; 
        RemoteUserCpu = 0.0; 
        LocalUserCpu = 0.0; 
        LocalSysCpu = 0.0; 
        RemoteSysCpu = 0.0; 
        ClusterId = 112170; 
        Log = "/home/cse496/bbockelm/projects/condor-ce/.log_23011_QRcWCU"; 
        CompletionDate = 0; 
        RemoteWallClockTime = 0.0; 
        LeaveJobInQueue = JobStatus == 4 && ( CompletionDate is UNDDEFINED || CompletionDate == 0 || ( ( time() - CompletionDate ) < 864000 ) ); 
        CondorVersion = "$CondorVersion: 7.9.6 Apr 22 2013 BuildID: RH-7.9.6-0.2.de1f9cc.git.lark.osg.el6 PRE-RELEASE-UWCS $"; 
        MyType = "Job"; 
        StreamErr = false; 
        HoldReason = "Spooling input data files"; 
        PeriodicHold = false; 
        ProcId = 0; 
        Out = "_condor_stdout"; 
        JobStatus = 5; 
        PeriodicRelease = false; 
        RequestMemory = ifthenelse(MemoryUsage isnt undefined,MemoryUsage,( ImageSize + 1023 ) / 1024); 
        Args = ""; 
        MaxHosts = 1; 
        TotalSuspensions = 0; 
        CommittedSlotTime = 0; 
        x509userproxysubject = "/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Bockelman/CN=4221328"; 
        CondorPlatform = "$CondorPlatform: X86_64-ScientificLinux_6.3 $"; 
        ShouldTransferFiles = "YES"; 
        ExitStatus = 0; 
        QDate = 1367674672; 
        EnteredCurrentStatus = 1367674672
    ]
Spooling cluster 112170 files to schedd <129.93.239.129:9620?sock=8556_0571_4>
- Successful spooling
Job status: Held
Job transitioned from Held to Idle
Job transitioned from Idle to Running
Job transitioned from Running to Completed
- Job was successful
</pre>
%ENDTWISTY%


---## Testing by hand

From a test submit host, use this file to submit to the CE:
<pre class="file">
universe = grid
grid_resource = condor %RED%condorce.example.com condorce.example.com:9619%ENDCOLOR%

executable = test.sh
output = test_g.out
error = test_g.err
log = test_g.log

ShouldTransferFiles = YES
WhenToTransferOutput = ON_EXIT

use_x509userproxy = true

queue
</pre>

Replace =condorce.example.com= with the hostname of the HTCondor-CE, submit this file using =condor_submit= from an external host and make sure to create an executable =test.sh=. 

---## Examining interactions with local jobmanager
To examine the files that are being submitted to the local jobmanager, edit =/etc/blah.config= and add the following line 
<pre class="file">
blah_debug_save_submit_info=DIR_NAME
</pre>
where =DIR_NAME= is replaced by the directory to save the submit files that HTCondor-CE will use to submit jobs to the local jobmanager. Blah will then create a directory for each submission to the local jobmanager with the submit file, and proxy used.   %RED%Note, whitespace is important so do not put any spaces around the = sign.  In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within =DIR_NAME=. %ENDCOLOR%
---# Known Issues

---++!! Unable to handle RSL in HTCondor 8.0.x 

In HTCondor 8.0.x, [[https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3772][a bug]] prevents the =globus_rsl= attribute on the submit side from working correctly.  This will hopefully be fixed in the next point-release of the series.

Until then, we recommend setting the !RequestMemory, !RequestCPUs, and wall time directly in the HTCondor-G submission file or within the job routes.

---++!! Time skews

Like all GSI-based authentication, HTCondor-CE is sensitive to time skews.  Make sure the clock on your CE is synchronized using a utility such as =ntpd=.

*In addition*, at PBS sites, HTCondor itself is sensitive to time skews on the NFS server.  If you see empty stdout / err being returned to the HTCondor-G submit host, verify there is no NFS server  time skew.

---++!! Disable blahp worker node proxy renewal

In =blah.conf=, you should have the following two lines:
<pre>
blah_disable_wn_proxy_renewal=yes
blah_delegate_renewed_proxies=no
</pre>
Neither functionality is used with HTCondor-CE; enabling worker node proxy renewal will actually cause jobs to fail to refresh the proxy in some setups.

---++!! Mismatch between the CE hostname and DNS name

The authorization configuration assumes that the host's hostname and DNS names match; that is, if the host certificate uses the subject =foo.example.com=, then the output of =hostname -f= should be =foo.example.com=.  If this is not true, the site will need to hand-expand the value of $(FULL_HOSTNAME) in the files from =/etc/condor-ce/config.d=.

%META:TOPICMOVED{by="TimCartwright" date="1394124083" from="Documentation.InstallHTCondorCE" to="Documentation/Release3.InstallHTCondorCE"}%
