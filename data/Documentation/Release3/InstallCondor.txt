%META:TOPICINFO{author="MarcoMambelli" date="1321555538" format="1.1" version="1.2"}%
---+!! Installing Condor for your cluster
%DOC_STATUS_TABLE%
%TOC{depth="3"}%

<!-- Local variables
   * Set CONDORREL = 7.6.4
   * Set AS_OF_DATE = October 27, 2011
-->

---+ About This Document

---++ Introduction
%DISCLAIMER% If you are using Rocks, commonly used by CMS, or you install using Kickstart files like the one provided by ATLAS, then you may not need any of this. The cluster management may install and setup Condor for you. Check your VO documentation first. 

---+ Engineering Considerations

We will use the latest stable release of Condor. As of %AS_OF_DATE%, this is %CONDORREL%.

This installation uses the Condor RPM distribution. It can be downloaded from the Condor site or installed using a RPM or yum repository.
The Condor team set up a yum repository that can be used for this installation.

Condor needs to be installed (using the procedure below) on all nodes of the batch queue, the headnode and the worker nodes, and also on the interactive nodes used to submit Condor jobs.

Certain operations, like the RPM installation and some system configuration, has to be repeated on each node.
Some steps like the configuration of _condor_config_ are performed only once, e.g. on the head node =osg-ce=, others like the customization of the local condor configuration file is different for each node.

Sharing at least the directory hosting the configuration files allows to simplify a bit the configuration by making it easy to make cluster-wide configuration changes.
Anyway this installation is possible also having no shared directories if the customized _condor_config_ file is replicated on all nodes of the queue.


%BR%
*A note about the directory structure:*
This Condor installation was structured to facilitate upgrades to Condor with minimal effort.  Condor RPM follows the [[http://www.pathname.com/fhs/][Filesystem Hierarchy Standard]]. For more information on the directory structure check the [[http://www.cs.wisc.edu/condor/yum/condor_install.html][release notes]]. 
The =/var/lib/condor= directory contains the Condor spool and may be mounted from a different partition as detailed in the section about [[#Mounting_a_separate_partition_fo][isolated spool directory]].
In addition to the files provided by the RPM there is a shared directory to simplify the configuration (=/nfs/condor/condor-etc=). Below you can find how to [[#Installation_without_any_shared][avoid any shared file]].

---+ How to get Help?

To get assistance please use [[HelpProcedure][this page]].

---+ Requirements

   1 A host to install the OSG Client (pristine node). No grid host certificate is required.
   1 OS is %SUPPORTED_OS%.  Currently most of our testing has been done on Scientific Linux 5.
   1 Root access
   1 To test and use the installation a valid [[Documentation.CertificateUserGet][grid user certificate]] is required.

---+ Installation procedure
---++ Preparing the yum install
If you don't have it already, download the YUM repository information provided by the Condor team in =http://www.cs.wisc.edu/condor/yum/repo.d/=, e.g. for RHEL5 (and derived):<pre class="screen">
cd /etc/yum.repos.d
wget http://www.cs.wisc.edu/condor/yum/repo.d/condor-stable-rhel5.repo
</pre>

<!-- The yum file contains something like: <verbatim>
[condor]
name=Condor Repository for RHEL/CentOS/SL $releasever $basearch
failovermethod=priority
baseurl=http://dukpc23.fnal.gov/~benjamin/condor/v7.4.0/RHEL/5/
enabled=1
gpgcheck=0
</verbatim>
-->

---++ Condor Installation and Configuration
*On each node* Start with installing Condor from the repository
<pre  class="screen">
yum install condor
</pre>

---+++ Shared configuration files
On the server exporting =/nfs/condor/condor-etc= (other nodes cannot write if you choose to export the directory with root squash) edit the following configuration files:
   * Create the cluster Condor configuration file =/nfs/condor/condor-etc/condor_config.cluster= with the following content:
      * Copy the following content (also attached in [[%ATTACHURL%/condor_config.cluster][condor_config.cluster]]) changing the values to suite your cluster (yourdomain.org, gc1-ce.yourdomain.org, in %RED%red%ENDCOLOR%). You may find some suggestion in the local configuration file =/etc/condor/condor_config.local=:<pre class="file">
## Condor configuration for OSG T3
## For more detial please see
## http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.$(HOSTNAME)
# The following should be your T3 domain
UID_DOMAIN = %RED%yourdomain.org%ENDCOLOR%
# Human readable name for your Condor pool
COLLECTOR_NAME = "Tier 3 Condor at $(UID_DOMAIN)"
# A shared file system (NFS), e.g. job dir, is assumed if the name is the same
FILESYSTEM_DOMAIN = $(UID_DOMAIN)
ALLOW_WRITE = *.$(UID_DOMAIN)
CONDOR_ADMIN = root@$(FULL_HOSTNAME)
# The following should be the full name of the head node
CONDOR_HOST = %RED%gc1-ce.yourdomain.org%ENDCOLOR%
# Port range should be opened in the firewall (can be different on different machines)
# This 9000-9999 is coherent with the iptables configuration in the T3 documentation 
IN_HIGHPORT = 9999
IN_LOWPORT = 9000
# This is to enforce password authentication
SEC_DAEMON_AUTHENTICATION = required
SEC_DAEMON_AUTHENTICATION_METHODS = password
SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi
SEC_PASSWORD_FILE = /var/lib/condor/condor_credential
ALLOW_DAEMON = condor_pool@*
##  Sets how often the condor_negotiator starts a negotiation cycle 
##  for negotiator and schedd). 
#  It is defined in seconds and defaults to 60 (1 minute), default is 300. 
NEGOTIATOR_INTERVAL = 20
##  Scheduling parameters for the startd
TRUST_UID_DOMAIN = TRUE
# start as available and do not suspend, preempt or kill
START = TRUE
SUSPEND = FALSE
PREEMPT = FALSE
KILL = FALSE
</pre>
      * Make sure that you have the following important line in the file <pre>CONDOR_HOST = gc1-ce</pre>
         * Note: =CONDOR_HOST= can be set with or without the domain name: =gc1-ce= or =gc1-ce.yourdomain.org=
   *  On the NFS server create the files with the host configuration specific for the nodes using the following content. We will create 3 base configuration files: one for the headnode, one for worker nodes, one for the interactive nodes (user interface). (specific for the headnode) copying the following line:
      * For the headnode, =/nfs/condor/condor-etc/condor_config.headnode=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR
</pre>
      * For the worker nodes, =/nfs/condor/condor-etc/condor_config.worker=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, STARTD
</pre>
      * For the interactive nodes, =/nfs/condor/condor-etc/condor_config.interactive=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, SCHEDD
</pre>
   * Then, always on the NFS server, for each node create a link pointing to the template, e.g.:<pre class="screen">
cd /nfs/condor/condor-etc/
ln -s condor_config.headnode condor_config.gc1-ce
ln -s condor_config.interactive condor_config.gc1-ui1
ln -s condor_config.worker condor_config.gc1-c001
ln -s condor_config.worker condor_config.gc1-c002
ln -s condor_config.worker condor_config.gc1-c003
</pre> Each node must have its own =condor_config.&lt;hostname>= file. If some nodes require a special configuration you can copy the template (e.g. =condor_config.worker=) and customize it.

---+++ Remaining node configuration
*On each node* perform these remaining configuration steps.
   * Edit the file =/etc/condor/condor_config=. This is the default configuration that will be invoked when condor is started. We will direct this file to be followed by specific configurations for T3 purposes. Replace:<pre class="file">
##  Where is the machine-specific local config file for each host?
LOCAL_CONFIG_FILE      = $(RELEASE_DIR)/etc/$(HOSTNAME).local
</pre>With<pre class="file">
##  Next configuration to be read is for the T3 cluster setup
LOCAL_CONFIG_FILE       = /nfs/condor/condor-etc/condor_config.cluster
</pre>

   * Remove the default condor_config.local in the /etc/condor directory to avoid possible confusion.<pre class="screen">
rm /etc/condor/condor_config.local
</pre>
   * Set the password that will be used by the Condor system (at the prompt enter the same password for all nodes):<pre class="screen">
condor_store_cred -c add
</pre>
   * Enable automatic startup at boot:<pre class="screen">
chkconfig --level 235 condor on
</pre>

---++ Start and test Condor
Condor is starting automatically during reboots. You can start it manually typing <pre  class="screen">
/etc/init.d/condor start </pre>  (should say ok)

You can check if Condor is running correctly<pre  class="screen">
condor_config_val log   # (should be /var/log/condor/)
cd /var/log/condor/
#check master log file
less MasterLog
# verify the status of the negotiator
condor_status -negotiator</pre>

You can see the resources in your Condor cluster using =condor_status= and submit test jobs with =condor_submit=. 
Check CondorTest for more.

---++ Setup
Condor is installed in the default path, so there is no need of special setup to use it. It will be automatically in the environment of every user.

---++ Upgrades
Only one version of Condor  at the time can be installed via RPM and used. 
To install a different version just remove the old RPM and install the new one following the instructions above.
The configuration files in the shared directory will persist so you can skip that step during updates.

---++ Special needs
The following sections present instructions or suggestion for uncommon configurations

---+++ Changes to the Firewall (iptables)
If you are using a Firewall (e.g. iptables) on all nodes you need to open the ports used by Condor:
   * Edit the =/etc/sysconfig/iptables= file to add these lines ahead of the reject line:<pre class="file">
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p tcp -m tcp --dport 9000:10000 -j ACCEPT  
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p udp -m udp --dport 9000:10000 -j ACCEPT 
</pre> where the _network_address_ is the address of the intranet of the T3 cluster, e.g. 192.168.192.0/18. (Or the extranet if your T3 does not have a separate intranet). You can omit the =-s= option if you have nodes of your Condor cluster (startd, schedd, ...) outside of that network.
   * Restart the firewall:<pre class="screen">
/etc/init.d/iptables restart
</pre>

---+++ Installation without any shared directory
If you choose not to use NFS in your cluster and there is no shared =/nfs/condor/condor-etc/= the section above about shared configuration files is not valid. All the configuration files should be in =/etc/condor=.
   * =condor_config= should be edited to have <pre class="file">LOCAL_CONFIG_FILE = /etc/condor/condor_config.cluster</pre>
   * =condor_config.cluster= should be created as described and replicated on each node in =/etc/condor= and should contain the modified:<pre class="file">LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.$(HOSTNAME)</pre>
   * =condor_config.&lt;hostname>= should be created on each node in =/etc/condor= by copying the proper =condor_config.headnode/worker/interactive= described above and by customizing it for the needs of the node.

Changes to the cluster config or to the configuration of one of the node types require synchronization by replicating the proper files after the change.

---+++ Mounting a separate partition for /var/lib/condor
=/var/lib/condor= is the directory used by Condor for status files and spooling (=/scratch/condor= in the shared installation).
For performance reason it should always be a local disk.
Is is recommended for it to be big in order to accommodate jobs that use a lot of disk space (e.g. ATLAS recommends 20GB for each job slot on the worker nodes) and possibly on a separate partition so that when a job fills up the disk, it will not fill the system disk and bring down the system.
The partition can be mounted on =/var/lib/condor= before installing Condor or at a latter time, e.g.:<pre class="screen">
/etc/init.d/condor stop
cd /var/lib
mv condor condor_old
mkdir condor
mount -t ext3 /dev/&lt;your partition> condor
chown condor:condor condor
mv condor_old/* condor/
rmdir condor_old
/etc/init.d/condor start
</pre>

---+++ Use the old RPMs from Condor
The new RPMs distributed by the condor team are much better than the previous one, so the use of the previous one is not supported. Anyway if you must use the old RPMs you can check the [[https://twiki.grid.iu.edu/bin/view/Tier3/CondorRPMInstall?rev=4][old instructions for RPM installation]] to see the additional steps necessary to complete the installation.



---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during the DOC workshop
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = AlainRoy
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = AlainRoy
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->
