%META:TOPICINFO{author="MarcoMambelli" date="1368208436" format="1.1" version="1.16"}%
---+!! Installing Condor for your cluster
%DOC_STATUS_TABLE%
%TOC{depth="3"}%

<!-- Local variables
   * Set CONDORREL = 7.8.7
   * Set AS_OF_DATE = January 22, 2013
-->

---+ About This Document

This installation document uses the HTCondor RPM from the OSG repository.  
OSG borrowed the RPM from Fedora, (by which we mean that we've copied their source RPM and rebuilt it against the version of Globus which we provide). This version of HTCondor provides most of HTCondor's functionality, except for Standard Universe and HTCondor-G support for CREAM and NorduGrid.

Other RPM installations, e.g. from the Fedora or the HTCondor repositories, are very similar but the instructions may need some adapting.
Installing Condor using a tarball distribution is instead quite different.
Some cluster management tools, like Rocks, may install and setup Condor for you.

See CondorInformation for an overview of the different options for having HTCondor.
 
%NOTE% During 2012 the name of this workflow management system changed form *Condor* to *High Throughput Condor* (or *HTCondor*). In this and other OSG documents you may see the use of both names to refer to the same software.

---+ Engineering Considerations

HTCondor needs to be installed (using the procedure below) on all nodes of the batch queue, the head node (=gc-hn=, HTCondor central manager with Collector and Negotiator) the worker nodes (=gc-cXXX=, with the Startd), and also on the nodes used to submit HTCondor jobs, like interactive nodes or the %LINK_GLOSSARY_CE% (=gc-uiXXX= and =gc-ce=, with the Schedd).

Certain operations, like the RPM installation and some system configuration, has to be repeated on each node.
Some steps like the configuration of _condor_config_ are performed only once, e.g. on the head node =osg-hn=, others like the customization of the local condor configuration file is different for each node.

In the absence of a cluster management system that is automatically replicating files, sharing at least the directory hosting the configuration files allows to simplify a bit the configuration by making it easy to make cluster-wide configuration changes.
Anyway this installation is possible also having no shared directories if the customized _condor_config_ file is replicated on all nodes of the queue.

When using RPMs HTCondor is installed in the default path, so there is no need of special setup to use it. It will be automatically in the environment of every user.

*A note about the directory structure:*
This Condor installation was structured to facilitate upgrades to Condor with minimal effort:  
   * Condor RPM follows the [[http://www.pathname.com/fhs/][Filesystem Hierarchy Standard]]. This is different from the self-contained tarball installation. For more information on the directory structure check the [[http://www.cs.wisc.edu/condor/yum/condor_install.html][release notes]]. 
   * The =/var/lib/condor= directory contains the Condor spool and may be mounted from a different partition as detailed in the section about [[#Mounting_a_separate_partition_fo][isolated spool directory]].
   * The =/etc/condor= directory contains the configuration
   * There may be a shared directory to simplify the configuration (=/nfs/condor/condor-etc=) in addition to the files provided by the RPM. Below you can find how to avoid any shared file (either using a cluster management system or without). Check the [[#HTCondor_configuration][configuration section]].

---+ Requirements
---++ Host and OS
   1 A host to install the Condor head node (Collector and Negotiator).
   1 A host to install the Condor submit node (Schedd).
   1 Hosts to run the Condor jobs (Startd).
   1 OS is %SUPPORTED_OS%.  Currently most of our testing has been done on Scientific Linux 5.
   1 Root access

#NetworkingReq
---++ Networking
%STARTSECTION{"Firewalls"}%
%INCLUDE{"FirewallInformation" section="FirewallTable" lines="condorcollector,condor"}%

=LOWPORT= and =HIGHPORT= are two values set in the %LINK_GLOSSARY_CONDOR% configuration file, e.g. <pre class="file">
LOWPORT = &lt;low port&gt;
HIGHPORT = &lt;high port&gt;</pre>
The %LINK_GLOSSARY_CONDOR% collector port can be changed in the configuration file.
For more information please check the [[http://research.cs.wisc.edu/condor/manual/v7.8/3_7Networking_includes.html][Networking section of the Condor manual]].

%ENDSECTION{"Firewalls"}%

You'll find more client specific details also in the [[#Firewall_Considerations][Firewall section]] of this document.

---+ Installation procedure
%INCLUDE{"YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%

%STARTSECTION{"InstallCondorRPMs"}%
---++ Condor Installation
*On each node* Start with installing Condor from the repository
<pre  class="rootscreen">
%UCL_PROMPT_ROOT% yum install condor
</pre>

---++ HTCondor configuration
The cluster you are installing Condor on most likely includes many nodes. Below we talk present 2 alternative ways to manage the configuration:
   1.  If you have a cluster management system (e.g. [[http://puppetlabs.com/][Puppet]], [[http://cfengine.com/][cfengine]], [[http://www.opscode.com/chef/][Chef]], or some similar tool), you can use that to manage centrally the different files. 
   1. Otherwise you can use a shared file system to avoid copying by hand all the configurations, specially the ones of the many worker nodes
You can use option 1 (avoiding a shared directory) even if you have no cluster management system but the burden of copying all the files and maintaining them in sync will fall onto you and manual operations may be error prone.
Use the option that works better for you.

---+++ Configuration using a cluster/configuration management system
Cluster management systems differ in ways to describe what to do and you'll have to consult the manual of your system for that. This description is generic. All cluster management systems allow to group nodes in classes. For the HTCondor configuration we'll distinguish a head node (generally one), submit nodes and worker nodes, as discussed in the engineering consideration above.

The HTCondor configuration is in =/etc/condor=. The following sections describe the files and configuration changes that should be applied to the different type of nodes and (at the end) to all the nodes.

---++++ HTCondor head node
*On the HTCondor head node* add the =/etc/condor/config.d/local.conf= file with the following content:<pre class="file">
## OSG cluster configuration
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR
</pre>

%NOTE% If your head node is also the gatekeeper of the cluster, then you need to set the daemon list to the union of the two (=DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR, SCHEDD=)
 
---++++ HTCondor submit hosts
*On the submit nodes* (e.g. the gatekeeper of a CE and the interactive nodes), add the =/etc/condor/config.d/local.conf= file with the following content:<pre class="file">
## OSG cluster configuration
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, SCHEDD
</pre>

---++++ HTCondor worker nodes
*On all worker nodes*, add the =/etc/condor/config.d/local.conf= file with the following content:<pre class="file">
## OSG cluster configuration
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, STARTD
</pre>

---++++ Common configuration
*On all nodes* add the =/etc/condor/config.d/cluster.conf= file with the following content:
      * Copy the following content changing the values to suite your cluster (yourdomain.org, gc-hn.yourdomain.org, in %RED%red%ENDCOLOR%). You may find some suggestion in the local configuration file =/etc/condor/condor_config.local=:<pre class="file">
## Condor configuration for OSG Clusters
## For more detial please see
## http://www.cs.wisc.edu/condor/manual/v7.8/3_3Configuration.html
# The following should be your cluster domain. This is an arbitrary string used by Condor, not necessarily matching your IP domain
UID_DOMAIN = %RED%yourdomain.org%ENDCOLOR%
# Human readable name for your Condor pool
COLLECTOR_NAME = "OSG Cluster Condor at $(UID_DOMAIN)"
# A shared file system (NFS), e.g. job dir, is assumed if the name is the same
FILESYSTEM_DOMAIN = $(UID_DOMAIN)
# Here you have to use your network domain, or any comma separated list of hostnames and IP addresses including all your 
# condor hosts. * can be used as wildcard
ALLOW_WRITE = *.%RED%yourdomain.org%ENDCOLOR%
CONDOR_ADMIN = root@$(FULL_HOSTNAME)
# The following should be the full name of the head node (Condor central manager)
CONDOR_HOST = %RED%gc-hn.yourdomain.org%ENDCOLOR%
# Port range should be opened in the firewall (can be different on different machines)
# This 9000-9999 is coherent with the iptables configuration in the Firewall documentation 
IN_HIGHPORT = 9999
IN_LOWPORT = 9000
# This is to enforce password authentication
SEC_DAEMON_AUTHENTICATION = required
SEC_DAEMON_AUTHENTICATION_METHODS = password
SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi
SEC_PASSWORD_FILE = /var/lib/condor/condor_credential
ALLOW_DAEMON = condor_pool@*
##  Sets how often the condor_negotiator starts a negotiation cycle 
##  for negotiator and schedd). 
#  It is defined in seconds and defaults to 60 (1 minute), default is 300. 
NEGOTIATOR_INTERVAL = 20
##  Scheduling parameters for the startd
TRUST_UID_DOMAIN = TRUE
# start as available and do not suspend, preempt or kill
START = TRUE
SUSPEND = FALSE
PREEMPT = FALSE
KILL = FALSE
</pre>
      * Make sure that you have the following important line in the file <pre>CONDOR_HOST = gc-hn</pre>
         * Note: =CONDOR_HOST= can be set with or without the domain name: =gc-ce= or =gc-hn.yourdomain.org=

*On each node* perform also these remaining configuration steps.
   1. Remove the default condor_config.local in the /etc/condor directory to avoid possible confusion.<pre class="rootscreen">
%UCL_PROMPT_ROOT% rm /etc/condor/condor_config.local
</pre>
   1. Set the password that will be used by the Condor system (at the prompt enter the same password for all nodes):<pre class="rootscreen">
%UCL_PROMPT_ROOT% condor_store_cred -c add
</pre>
   1. Start Condor and enable automatic startup as [[#Starting_and_Enabling_Services][illustratrd below]].


---+++ Configuration using a shared file system
---++++ Shared configuration files
The following files are on the shared directory =/nfs/condor/condor-etc/=. To edit them you must be on a node that has write access to it, e.g. the NFS server exporting the directly (other nodes cannot write if you choose to export the directory with root squash). 
If you have no shared file system these files must be copied as [[#Installation_without_any_shared][explained below]].

Edit the following configuration files:
   1. Create the cluster Condor configuration file =/nfs/condor/condor-etc/condor_config.cluster= with the following content:
      * Copy the following content (also attached in [[%ATTACHURL%/condor_config.cluster][condor_config.cluster]]) changing the values to suite your cluster (yourdomain.org, gc-hn.yourdomain.org, in %RED%red%ENDCOLOR%). You may find some suggestion in the local configuration file =/etc/condor/condor_config.local=:<pre class="file">
## Condor configuration for OSG Clusters
## For more detial please see
## http://www.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html
LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.$(HOSTNAME)
# The following should be your cluster domain. This is an arbitrary string used by Condor, not necessarily matching your IP domain
UID_DOMAIN = %RED%yourdomain.org%ENDCOLOR%
# Human readable name for your Condor pool
COLLECTOR_NAME = "OSG Cluster Condor at $(UID_DOMAIN)"
# A shared file system (NFS), e.g. job dir, is assumed if the name is the same
FILESYSTEM_DOMAIN = $(UID_DOMAIN)
# Here you have to use your network domain, or any comma separated list of hostnames and IP addresses including all your 
# condor hosts. * can be used as wildcard
ALLOW_WRITE = *.%RED%yourdomain.org%ENDCOLOR%
CONDOR_ADMIN = root@$(FULL_HOSTNAME)
# The following should be the full name of the head node (Condor central manager)
CONDOR_HOST = %RED%gc-hn.yourdomain.org%ENDCOLOR%
# Port range should be opened in the firewall (can be different on different machines)
# This 9000-9999 is coherent with the iptables configuration in the Firewall documentation 
IN_HIGHPORT = 9999
IN_LOWPORT = 9000
# This is to enforce password authentication
SEC_DAEMON_AUTHENTICATION = required
SEC_DAEMON_AUTHENTICATION_METHODS = password
SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi
SEC_PASSWORD_FILE = /var/lib/condor/condor_credential
ALLOW_DAEMON = condor_pool@*
##  Sets how often the condor_negotiator starts a negotiation cycle 
##  for negotiator and schedd). 
#  It is defined in seconds and defaults to 60 (1 minute), default is 300. 
NEGOTIATOR_INTERVAL = 20
##  Scheduling parameters for the startd
TRUST_UID_DOMAIN = TRUE
# start as available and do not suspend, preempt or kill
START = TRUE
SUSPEND = FALSE
PREEMPT = FALSE
KILL = FALSE
</pre>
      * Make sure that you have the following important line in the file <pre>CONDOR_HOST = gc-hn</pre>
         * Note: =CONDOR_HOST= can be set with or without the domain name: =gc-ce= or =gc-hn.yourdomain.org=
   1.  In the same directory create the files with the host configuration specific for the nodes using the following content. We will create 3 base configuration files: one for the head node, one for worker nodes, one for the submit nodes (interactive nodes abd CE) copying the following line. These control that the correct daemons are started:
      * For the head node (Condor central manager), =/nfs/condor/condor-etc/condor_config.headnode=:<pre class="file">
## OSG cluster configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR
</pre>
      * For the worker nodes, =/nfs/condor/condor-etc/condor_config.worker=:<pre class="file">
## OSG cluster configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, STARTD
</pre>
      * For the submit nodes (e.g. the gatekeeper of a CE and the interactive nodes), =/nfs/condor/condor-etc/condor_config.submit=:<pre class="file">
## OSG cluster configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html
# List of daemons on the node (Condor central manager requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, SCHEDD
</pre>
   * If your head node is also the gatekeeper of the cluster, then you need to set the daemon list to the union of the two (=DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR, SCHEDD=)
   * Then, always on the same direcotry, for each node create a link pointing to the template, e.g.:<pre class="rootscreen">
%UCL_PROMPT_ROOT% cd /nfs/condor/condor-etc/
%UCL_PROMPT_ROOT% ln -s condor_config.headnode condor_config.gc-hn
%UCL_PROMPT_ROOT% ln -s condor_config.submit condor_config.gc-ce
%UCL_PROMPT_ROOT% ln -s condor_config.submit condor_config.gc-ui1
%UCL_PROMPT_ROOT% ln -s condor_config.worker condor_config.gc-c001
%UCL_PROMPT_ROOT% ln -s condor_config.worker condor_config.gc-c002
%UCL_PROMPT_ROOT% ln -s condor_config.worker condor_config.gc-c003
</pre> Each node must have its own =condor_config.&lt;hostname>= file. If some worker nodes require a special configuration you can copy the template (e.g. =condor_config.worker=) and customize it.

---++++ Remaining node configuration
*On each node* perform these remaining configuration steps.
   1. Edit the file =/etc/condor/condor_config=. This is the default configuration that will be invoked when condor is started. We will direct Condor to follow this configuration with the OSG specific configuration (=condor_config.cluster=). Replace:<pre class="file">
##  Where is the machine-specific local config file for each host?
LOCAL_CONFIG_FILE      = $(RELEASE_DIR)/etc/$(HOSTNAME).local
</pre>With<pre class="file">
##  Next configuration to be read is for the OSG cluster setup
LOCAL_CONFIG_FILE       = /nfs/condor/condor-etc/condor_config.cluster
</pre>
   1. Remove the default condor_config.local in the /etc/condor directory to avoid possible confusion.<pre class="rootscreen">
%UCL_PROMPT_ROOT% rm /etc/condor/condor_config.local
</pre>
   1. Set the password that will be used by the Condor system (at the prompt enter the same password for all nodes):<pre class="rootscreen">
%UCL_PROMPT_ROOT% condor_store_cred -c add
</pre>
   1. Start Condor and enable automatic startup as [[#Starting_and_Enabling_Services][illustratrd below]].


---++ Special needs
The following sections present instructions or suggestion for uncommon configurations

---+++ Changes to the Firewall (iptables)
If you are using a Firewall (e.g. iptables) on all nodes you need to open the ports used by Condor:
   * Edit the =/etc/sysconfig/iptables= file to add these lines ahead of the reject line:<pre class="file">
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p tcp -m tcp --dport 9000:10000 -j ACCEPT  
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p udp -m udp --dport 9000:10000 -j ACCEPT 
</pre> where the _network_address_ is the address of the intranet of the OSG cluster, e.g. 192.168.192.0/18. (Or the extranet if your OSG cluster does not have a separate intranet). You can omit the =-s= option if you have nodes of your Condor cluster (startd, schedd, ...) outside of that network.
   * Restart the firewall:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service iptables restart
</pre>

<!--
---+++!! Installation without any shared directory
If you choose not to use NFS in your cluster and there is no shared =/nfs/condor/condor-etc/=, then the section above about shared configuration files is not valid. All the configuration files should be in =/etc/condor=.
   * =condor_config= should be edited to have <pre class="file">LOCAL_CONFIG_FILE = /etc/condor/condor_config.cluster</pre>
   * =condor_config.cluster= should be created as described and replicated on each node in =/etc/condor= and should contain the modified:<pre class="file">LOCAL_CONFIG_FILE = /etc/condor/condor_config.$(HOSTNAME)</pre>
   * =condor_config.&lt;hostname>= should be created on each node in =/etc/condor= by copying the proper =condor_config.headnode/worker/submit= described above and by customizing it for the needs of the node, if needed.

Changes to the cluster config or to the configuration of one of the node types require synchronization by replicating the proper files on the whole cluster after the change.
-->

---+++ Mounting a separate partition for /var/lib/condor
=/var/lib/condor= is the directory used by Condor for status files and spooling, sometime referred as scratch space.
For performance reason it should always be on a local disk.
Is is recommended for it to be big in order to accommodate jobs that use a lot of disk space (e.g. ATLAS recommends 20GB for each job slot on the worker nodes) and possibly on a separate partition so that when a job fills up the disk, it will not fill the system disk and bring down the system.
The partition can be mounted on =/var/lib/condor= before installing Condor or at a latter time, e.g.:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor stop
%UCL_PROMPT_ROOT% cd /var/lib
%UCL_PROMPT_ROOT% mv condor condor_old
%UCL_PROMPT_ROOT% mkdir condor
%UCL_PROMPT_ROOT% mount -t ext3 /dev/&lt;your partition> condor
%UCL_PROMPT_ROOT% chown condor:condor condor
%UCL_PROMPT_ROOT% mv condor_old/* condor/
%UCL_PROMPT_ROOT% rmdir condor_old
%UCL_PROMPT_ROOT% /sbin/service condor start
</pre>


---+ Services
The condor master on each node taked care to start and monitor the correct services as selected in the configuration file.

---++ Starting and Enabling Services
To start the services:
   1. To start Condor you can use the service command, e.g.: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor start
</pre>

You should also enable the appropriate services so that they are automatically started when your system is powered on: 
   * To enable Condor by default on the node: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor on
</pre>

---++ Stopping and Disabling Services
To stop the services:
   1. To stop Condor you can use: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor stop
</pre>

In addition, you can disable services by running the following commands.  However, you don't need to do this normally.
   * Optionally, to disable Condor: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/chkconfig condor off
</pre>

---+ Troubleshooting

#ImportantFiles
---++ Useful configuration and log files

Configuration Files
| *Service or Process* | *Configuration File* | *Description* |
| condor | =/etc/condor/condor_config= | Configuration file |
| | =/etc/condor/condor_config.cluster= | Configuration file |

Log files
| *Service or Process* | *Log File* | *Description* |
| condor | =/var/log/condor/= | All log files |

---++ Test Condor

After starting Condor you can check if it is running correctly<pre  class="screen">
%UCL_PROMPT% condor_config_val log   # (should be /var/log/condor/)
%UCL_PROMPT% cd /var/log/condor/
#check master log file
%UCL_PROMPT% less MasterLog
# verify the status of the negotiator
%UCL_PROMPT% condor_status -negotiator</pre>

You can see the resources in your Condor cluster using =condor_status= and submit test jobs with =condor_submit=. 
Check CondorTest for more.



---+ How to get Help?
To get assistance please use this [[HelpProcedure][Help Procedure]].


---+ References
   * [[http://research.cs.wisc.edu/condor/manual/][Condor manuals]]
   * [[CondorInformation][Options for installing Condor]]
   * The Condor team has a wonderful set of How-To recipes for condor's admins: [[http://nmi.cs.wisc.edu/node/1465]]
Additional configuration:
   * SetupCondorAdvanced
On using Condor:
   * Documentation.CondorSubmittingSingleJob
   * Documentation.CondorSubmittingMultipleJobs
   * Documentation.CondorSubmittingMultipleComplexJobs

---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during the DOC workshop
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = TimCartwright
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->
