%META:TOPICINFO{author="MarcoMambelli" date="1329181267" format="1.1" reprev="1.9" version="1.9"}%
%DOC_STATUS_TABLE%

---+!! Small Sites
%TOC%

---+ About this document
This page collects a series of pointers that could be useful specially for sites that are not part of a bigger infrastructure or do not have already a clear plan on how to build the site.

This page is meant to complement the content of OSG Release 3 documentation, based on RPM packages. For a complete guide based on OSG 1.2 Pacman packages see the the [[Tier3.WebHome][Tier 3 documents]].

Some of the ideas below can be useful to anyone planning a cluster in OSG, including Campus Grids, some apply only when your resources are federated beyond the border of your institution, to the [[WhatIsOSG][OSG Production Grid]].

---+ Concepts
OSG systems (sites) typically provide one or more of the following capabilities:
   * access to local computational resources using a batch queue
   * interactive access to local computational resources
   * storage of large amounts of data using a distributed file system
   * access to external computing resources on the Campus or Production Grid
   * the ability to transfer large datasets to and from the Production Grid
  
A site can also offer computing resources and data to fellow grid users, e.g. on the OSG Production Grid.

In a OSG cluster, there are three classes of cluster nodes:
   1 Batch queue worker nodes that execute jobs submitted via the batch queue. 
   1 Shared Interactive nodes where users can log in directly and run their applications. 
   1 Nodes that serve various roles such as batch queues, file systems, or other middleware components. In some cases one node can host multiple roles while in other cases for a variety of reasons (including security or performance), nodes should be set aside for single purpose uses.

Important components (or roles) may include, depending on your configuration: a shared file system server (e.g. NFSv4), a Batch Queue (e.g. [[InstallCondor][Condor]]), a Distributed File System (e.g. [[XrootdOverview][Xroot]]), and, specially if you are interested in the Production Grid, a [[NavAdminStorage][Storage Element (SE)]], a [[NavAdminCompute][Compute Element (CE)]], and [[NavTechGUMS][GUMS]] <!-- InstallGums -->.

The [[Operations.OIMTermDefinition][OIM Definitions document]] explains the OSG resources as defined in the OSG blueprint.


---+ Requirements and Planning
Here some initial notes:
   * The [[SitePlanning][site planning document]] can help you decide what you want and which hardware you may need
   * This [[Documentation.ClusterTopology][topology document]] can give an idea of possible network configuration and what to consider
   * The [[FirewallInformation][Firewall information document]] will provide information about network requirements (open ports, ...) needed for the OSG services. Additional information about specific network requirements is in the install document of each OSG software
   * HostTimeSetup will explain how to synchronize your hosts, essential for a distributed system like OSG

To start working on the OSG Production Grid you will *need* [[Documentation.CertificateWhatIs][x509 certificates]]:
   * [[CertificateUserGet][A personal grid certificate]]
   * [[GetHostServiceCertificates][Certificates for your hosts and services]]

And once you decide the services in your resources and their names you will have to register them in the OSG catalog, [[https://oim.grid.iu.edu/oim/home][OIM]]:
   * Operations.OIMRegistrationInstructions explains how to register your resources

---+ Installation and Setup
In this new release we are not covering anymore subjects like accounts, SSH and NFS configuration (in red in Figure 1). You can still see the notes in the [[Tier3.WebHome][Tier 3 Web documents]]. 

   * Figure 1 - Stack of the different available modules presented: Hardware, OS setup, and networking is in red, Condor (or other batch scheduler) in yellow, Distributed storage and Production Grid components in blue. Experiment specific software would sit on top and is not in the picture. Lower modules are functional to the modules sitting on top or inside them. Included boxes enhance/modify the container. Dashed boxes are optional (if used affect the components on top of them, but are not required). [[%ATTACHURLPATH%/gc-dependencies-wb-sm.png][Click here]] for an alternative view.<br />
     <img src="%ATTACHURLPATH%/cluster-stack.png" alt="cluster-stack.png" width='459' height='171' />    

For the following steps referring OSG Release 3 software, the OS is %SUPPORTED_OS% (currently most of our testing has been done on Scientific Linux 5) and you need root access, else you'll have to use the Pacman installation documented in Tier3.WebHome.

Each cluster needs a _Local Resource Manager_ (LRM, also called _queue manager_ or _scheduler_) to schedule the jobs on the worker nodes. Common LRMs are Condor, LSF, PBS, SGE; if you have one installed or a preferred one you can use that one. 
If not, you can install Condor as documented in InstallCondor. SetupCondorAdvanced presents some unusual but useful Condor configurations like the use of Condor Startd Cron (sometime also called Hawkeye).

Additional component are documented in the Documentation/Release3.WebHome. These include:
   * The [[NavAdminStorage][Storage Element (SE)]]
   * The [[NavAdminCompute][Compute Element (CE)]]: if you just installed Condor choose  "If you are using Condor"/"Configuring your CE to use Condor" and skip the other batch systems in InstallComputeElement
   * [[NavTechGUMS][GUMS]] 
   * The [[RsvOverview][RSV monitoring]]


---+ Operation
Some recommendation about day to day operation:
   * 

---+ Troubleshooting 
Here is an useful collection of [[TroubleshootingGuide][Troubleshooting documents]]:
   * TroubleshootingGuide
   * CondorErrors
   * GlobusErrors
   * TroubleshootingFaq
   * TroubleshootingComputeElement
   * TestOSGClient
   * TroubleshootRsv
   * SrmTester

---+ Getting help
The first line of support is primarily provided by [[HelpProcedure#VO_Assistance][the VO]] and/or local computing support personnel. 

In addition, OSG provides support via multiple channels including:
   * the [[HelpProcedure#Emergency_Assistance][24x7 GOC Emergency assistance]] for urgent matters
   * the [[SiteCoordination.ChatCalendar][Campfire Web chat]] for direct interactive support 
   * the [[https://ticket.grid.iu.edu/goc/open][GOC ticketing system]] to better track the case
   * its [[http://www.opensciencegrid.org/Consortium_Mailing_Lists][mailing lists]] for community support

Check HelpProcedure for a complete list of what is available, which channel is recommended for specific issues and  and where/how you can get help during the installation or maintenance of your site.

---+ References
   * Tier3.WebHome

---+ Comments
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = General

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Trash/Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Navigation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = JamesWeichel
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
 
-->


%META:FILEATTACHMENT{name="cluster-stack.png" attachment="cluster-stack.png" attr="" comment="" date="1329174687" path="cluster-stack.png" size="41137" stream="cluster-stack.png" tmpFilename="/usr/tmp/CGItemp36149" user="MarcoMambelli" version="1"}%
