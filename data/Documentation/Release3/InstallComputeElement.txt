%META:TOPICINFO{author="SuchandraThapa" date="1331763536" format="1.1" reprev="1.69" version="1.69"}%
%META:TOPICPARENT{name="NavAdminCompute"}%
%DOC_STATUS_TABLE%

---+!! Installing the Compute Element
%TOC{depth="2"}%

---+ About this Document
This document is for VO System Administrators.  Here we describe how to install and configure a Compute Element on your Linux machine.   We also mention the need of a job manager but don't go into details about its installation.

This document follows the general OSG documentation conventions: %TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to expand document conventions..."}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%
%ENDTWISTY%

---+ How to get Help?
To get assistance please use [[HelpProcedure][Help Procedure]].


---+ 1.0 Requirements and Preparation

---++ Host and OS
   * A host to install the Compute Element (Pristine node)
   * OS is %SUPPORTED_OS%. Currently most of our testing has been done on Scientific Linux 5.
   * Root access

---++ Users

---++ Certificates
| *Certificate* | *User that owns certificate* | *Path to certificate* |
| Host certificate | =root= | =/etc/grid-security/hostcert.pem= <br> =/etc/grid-security/hostkey.pem= |

[[GetHostServiceCertificates][Here]] are instructions to request a host certificate.

---++ Networking
%STARTSECTION{"Firewalls"}%
%INCLUDE{"FirewallInformation" section="FirewallTable" lines="gram,portrange,portsource,gridftp"}% 

For =GLOBUS_TCP_PORT_RANGE= is recommend to open 8ports*number of job slots <br/>
Allow inbound and outbound network connection _to all cluster servers, e.g. GUMS and job manager head-node_ <br/>
Inbound and outbound network connection outside of the cluster can be limited _to any clients who may need to submit jobs_
%ENDSECTION{"Firewalls"}%

---++ Additional Requirements
To be part of OSG your CE must be registered in OIM. To register your resource:
   * Use your user certificate.  [[CertificateUserGet][Here]] are instructions to request a user certificate.
   * Register in OIM as described in Operations.OIMRegistrationInstructions
Testing Requirements:
   * To test a CE you need to submit jobs, e.g using your user certificate.  

%INCLUDE{"YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%
%INCLUDE{"InstallCertAuth" section="OSGBriefCaCerts" TOC_SHIFT="+"}%

#InstallBatch
---+ 2.0 Install your batch system

Before you install your CE, you almost certainly need to install your batch system. Installation of your batch system is beyond the scope of the OSG documentation, but there are a few things to note. 

---++ 2.1 If you are using Condor

(More details on this information are in [[CondorInformation][our Condor information]]).

You can install Condor from at least three places:

   1. We provide Condor in the OSG repository. We've borrowed the RPM from Fedora, which provides _most_ of Condor, except for Standard Universe, and Condor-G support for CREAM and !NorduGrid.
   1. You can install Condor from the Condor Team's [[http://research.cs.wisc.edu/condor/yum/][yum repository]]
      $ *Caveat 1*: This only works if you install Condor 7.6 or later. 
      $ *Caveat 2*: You should tell yum not to take Condor from our repository to ensure it will never replace the one you installed. To do this, edit =/etc/yum.repos.d/osg.repo= to add the following line:<pre class="file">exclude=condor empty-condor*</pre>
   1. You can install Condor from a binary tarball (from the Condor Team) or source in an arbitrary location on your CE.

Option 3 requires you to do a bit of upfront effort. The =osg-ce-condor= RPM (documented below) depends on having Condor installed as an RPM and uses RPM's dependency resolution mechanism. If you choose option 3, you need to do two things:

   1. Install a "dummy RPM" called =empty-condor= that will convince RPM that Condor has been installed via RPM, but will not actually provide Condor: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install empty-condor</pre>
   1. After following the instructions and installing =osg-ce-condor=, configure Globus to use your version of Condor. See [[#ConfigCondor][Section 4.2]] below for details.

---++ 2.2 If you are using Torque or PBSPro

You can install Torque from at least two places:

   1. Torque is in the EPEL repository.
   1. You can download Torque from [[http://www.adaptivecomputing.com/resources/downloads/torque/][the software developer]] and install it in an arbitrary location on your CE.

Option 2 works if you do a bit of upfront effort. The =osg-ce-pbs= RPM (documented below) depends on having Torque installed via the EPEL RPM and it uses RPM's dependency resolution mechanism. If you choose option 2, you need to do two things:

   1. Install a "dummy RPM" called =empty-torque= that will convince RPM that Torque has been installed via RPM, but will not actually provide Torque: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install empty-torque</pre>
   1. Configure Globus to use your version of Torque. See [[#ConfigPBS][Section 4.3]] below for details.


---++ 2.3 If you are using !Gridengine

You can install !Gridengine from at least two places:

   1. !Gridengine is in the EPEL repository.
   1. You can download !Gridengine from [[http://www.oracle.com/us/products/tools/oracle-grid-engine-075549.html][Oracle's Gridengine site] and install it in an arbitrary location on your CE.

Option 2 works if you do a bit of upfront effort. The =osg-ce-sge= RPM (documented below) depends on having SGE installed via the EPEL RPM and it uses RPM's dependency resolution mechanism. If you choose option 2, you need to do two things:

   1. Install a "dummy RPM" called =empty-gridengine= that will convince RPM that !Gridengine has been installed via RPM, but will not actually provide !Gridengine: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install empty-gridengine</pre>
   1. Configure Globus to use your version of !Gridengine. See [[#ConfigGridengine][Section 4.4]] for details.

%STARTSECTION{"InstallCeRpms"}%
#InstallCE
---+ 3.0 Install the CE
Install the CE RPM. OSG support only CEs with a single batch system. Do only *ONE* of the following installation lines, depending on your batch system: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install osg-ce-condor 
%UCL_PROMPT_ROOT% yum install osg-ce-pbs 
%UCL_PROMPT_ROOT% yum install osg-ce-lsf 
%UCL_PROMPT_ROOT% yum install osg-ce-sge
%UCL_PROMPT_ROOT% yum install osg-ce</pre>
The last one configures Globus to use the fork job manager.

---++ 3.1 Install Managed Fork
[[Documentation/GlossaryM#DefsManagedFork][ManagedFork]] is recommended for service jobs on the gatekeeper instead of the default fork job manager. !ManagedFork requires Condor and it will bring it in as dependency if it is not already installed. See [[CondorInformation][our Condor information]] for more information on different ways to install Condor.
To install [[Documentation/GlossaryM#DefsManagedFork][ManagedFork]], do the following:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install globus-gram-job-manager-managedfork
</pre>
%ENDSECTION{"InstallCeRpms"}%



#ConfigureCE
---+ 4.0 Configuration Instructions

---++ 4.1 Run =osg-configure=

You will configure your CE with a tool named =osg-configure=. (Previously known as =configure-osg=.) First you edit the files in =/etc/osg/config.d/= to describe your configuration. (In the old Pacman-based VDT, these were in a single =config.ini= file, but they have been separated into separate files.) There are a lot of options in these files, and you should refer [[IniConfigurationOptions][this other document]] for details:
   1. IniConfigurationOptions describes the syntax, usage and the various options you can set in the _.ini_ files in =/etc/osg/config.d/=. This document describes all possible configuration files. Depending on what you installed you may have only some of them. 

Once you have edited the _.ini_ files in =/etc/osg/config.d/=, run =osg-configure= with the =-v= option to check that your configuration is valid without making any changes:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -v
</pre>

If you do not have any errors, run =osg-configure= with the =-c= option to configure your installation:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -c 
</pre>

*If you are updating from a Pacman-based CE*, you can copy your old =config.ini= file to =/etc/osg/config.d/99-old-config.ini=. The options in this file will override the options in the other files. Adjust the configuration variables as needed. 

#ConfigCondor
---++ 4.2 Configuring your CE to use Condor

---+++ 4.2.1 globus-condor.conf
You can configure GRAM's use of Condor by editing =/etc/globus/globus-condor.conf=. 

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example globus-condor.conf"}%
<pre class="file">
# Condor classified ads contain requirements for processing jobs. Among these
# is OpSys, which defines which operating system the job may be run on. Condor
# has a reasonable default for this, the operating system of this machine. If
# you want to use a different default, specify it here and uncomment it.
# condor_os="LINUX"

# Condor classified ads contain requirements for processing jobs. Among these
# is Arch, which defines which computer architecure the job may be run on.
# Condor has a reasonable default for this, the architecture of this machine.
# If you want to use a different default, specifiy it here and uncomment it.
# condor_arch="INTEL"

# Path to the condor_submit executable. This is required to submit condor
# jobs.
condor_submit="/usr/bin/condor_submit"

# Path to the condor_rm executable. This is required to cancel condor jobs.
condor_rm="/usr/bin/condor_rm"

# Value of the CONDOR_CONFIG environment variable. On systems where condor is
# installed in non-default location, this variable helps condor find its
# configuratino files. If you need to set CONDOR_CONFIG to run condor processes
# uncomment the next line at set its value
#condor_config=""

# The GRAM condor module can perform tests on files used by a condor job
# prior to submitting it to condor. These checks include tests on the
# files named by the directory, executable, and stdin
# RSL attributes to ensure they exist and have suitable permissions. These
# checks are done in the condor standard universe always by the GRAM condor
# module, but can also be done for "vanilla" universe jobs if desired,
#check_vanilla_files="no"

# Condor supports parallel universe jobs using mpi site-specific scripts which
# invoke appropriate mpi commands to start the job. If you want to enable
# mpi jobs on condor, you'll need to uncomment the following line and set
# it to the path of your mpi script.
#condor_mpi_script="no"

# Enable Condor file transfer mode by default on the OSG
isNFSLite=1
</pre>
%ENDTWISTY%

Options of general use in this file include:

| *Option* | *Meaning* | *Purpose* |
| =condor_submit= | The full pathname for the =condor_submit= binary. | Edit this if you installed Condor into a different location |
| =condor_rm= | The full pathname for the =condor_rm= binary. | Edit this if you installed Condor into a different location |
| =condor_config= | The full pathname to Condor's =condor_config= file. | Edit this if you installed Condor into a different location |
| =isNFSLite= | 1 enables NFSLite, 0 disables it. | Enable if you want to use Condor's file transfer mechanism, Disable if you want Condor to assume a shared filesystem |

---+++ 4.2.2 Condor accounting groups

Condor accounting groups are a mechanism to provide fairshare on a group basis, as opposed to a Unix user basis.  They are independent of the Unix groups the user may already be in, and are [[http://research.cs.wisc.edu/condor/manual/v7.6/3_4User_Priorities.html#SECTION00447000000000000000][documented in the Condor manual]].  If you are using Condor accounting groups, you can map user jobs from GRAM into Condor accounting groups based on their numeric user id, their DN, or their VOMS attributes.

---++++ 4.2.2.1 =/etc/osg/uid_table.txt=
The uid file is consulted first. It contains line of the form:

<pre class="file">
uid GroupName
</pre>

For example, you might have:
<pre class="file">
uscms02 TestGroup
osg     other.osgedu
</pre>

---++++ 4.2.2.2 =/etc/osg/extattr_table.txt=
The extended attribute file is only consulted if the user is not found in the uid file. It contains lines of the form:

<pre class="file">
SubjectOrAttribute GroupName
</pre>

The _SubjectOrAttribute_ can be a Perl regular expression.

For instance, you might put the cmsprio user (known by a portion of the DN) into one group, anyone with the production role (in the VOMS attribute) into a second group, and everyone else into a third group:

<pre class="file">
cmsprio cms.other.prio
cms\/Role=production cms.prod
.* other
</pre>

Whatever group is chosen, it will be put into the Condor submit file with:

<pre class="file">
+AccountingGroup = "GroupName"
</pre>

#ConfigPBS
---++ 4.3 PBS-specific notes
Osg-configure 1.0.0 and later allow you to set most of the pbs specific gram options in your ini file so you will not need to make changes to the gram configuration files manually.  *Please note that production sites should make changes to the =seg_enabled=, =log_directory=, and =pbs_server= settings in order to make sure your site will be able to handle reasonable job loads.*  See the discussion on the SEG below for more details.

If you are using a version of osg-configure prior to 1.0.0, you can configure GRAM's use of PBS by editing =/etc/globus/globus-pbs.conf=. 

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example globus-pbs.conf"}%
<pre class="file">
# The SEG will parse log messages from the PBS log files located in the
# log_path directory 
log_path="/var/torque/server_logs"

# Some sites run the PBS server on a different node than GRAM is running.
# If so, they might need to set the pbs_default variable to the name of 
# the server so that GRAM will contact it
pbs_default=""

# For the mpi jobtype, the pbs LRM implementation supports both the
# MPI 2-specified mpiexec command and the non-standard mpirun command common
# in older mpi systems. If either of these is path to an executable, it will
# be used to start the job processes (with mpiexec preferred over mpirun). Set
# to "no" to not use mpiexec or mpirun
mpiexec=no
mpirun=no

# The qsub command is used to submit jobs to the pbs server. It is required
# for the PBS LRM to function
qsub="/usr/bin/qsub-torque"
# The qstat command is used to determine when PBS jobs complete. It is 
# required for the PBS LRM to function unless the SEG module is used.
qstat="/usr/bin/qstat-torque"
# The qdel command is used to cancel PBS jobs. It is required for the LRM
# to function.
qdel="/usr/bin/qdel-torque"

# The PBS LRM supports using the PBS_NODEFILE environment variable to
# point to a file containing a list of hosts on which to execcute the job.
# If cluster is set to yes, then the LRM interface will submit a script
# which attempts to use the remote_shell program to start the job on those
# nodes. It will divide the job count by cpu_per_node to determine how many
# processes to start on each node.
cluster="1"
remote_shell="no"
cpu_per_node="1"

# The GRAM pbs implementation supports softenv as a way to set up environment
# variables for jobs via the softenv RSL attribute. For more information about
# softenv, see
#     http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html
softenv_dir=
</pre>
%ENDTWISTY%

| *Option* | *Meaning* | *Purpose* |
| =qsub= | The full pathname for the =qsub= binary. | Edit this if you installed PBS/Torque into a different location |
| =qstat= | The full pathname for the =qstat= binary. | Edit this if you installed PBS/Torque into a different location |
| =qdel= | The full pathname for the =qdel= binary. | Edit this if you installed PBS/Torque into a different location |
| =logpath= | The full pathname for the PBS log files. | Edit this to point to your log files |


Note, the default configuration for osg-configure is not to use the Globus SEG.  The SEG reads your pbs log files directly to monitor the status of jobs and significantly reduces the load on your CE avoiding the need to run qstat to monitor jobs. Without the SEG, GRAM will not be able to handle more than a few hundred jobs as multiple invocations of qstat per second will increase the load on the server to unmanageable levels as the number of jobs increase.  A production site should use the SEG, however this means that you might need to export your log files (via NFS or similar) to your CE and provide the location of these files using the =log_directory= option.  For more information on configuration of PBS, see [[PbsBatchSystemHints][PBS Batch System Hints]] and [[IniConfigurationOptions#PBS][ini options for the pbs jobmanager]].

#ConfigGridengine
---++ 4.4 !Gridengine-specific notes
Osg-configure 1.0.0 and later allow you to set most of the Gridengine specific gram options in your ini file so you will not need to make changes to the gram configuration files manually.   *Please note that production sites should make changes to the =seg_enabled=, =log_directory=, and =pbs_server= settings in order to make sure your site will be able to handle reasonable job loads.* See the discussion on the SEG below for more details.

If you are using a version of osg-configure prior to 1.0.0 or need to change a setting that isn't handled by osg-configure, you can configure GRAM's use of Gridengine by editing  =/etc/globus/globus-sge.conf=. 

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Show an example globus-sge.conf"}%
<pre class="file">
# SGE_ROOT value which points to the local GridEngine installation. If this
# is set to undefined, then it will be determined from the job manager's
# environment, or if not there, from the contents of the SGE_CONFIG file
# below
sge_root=undefined

# SGE_CELL value which points to the SGE Cell to interact with. If this
# is set to undefined, then it will be determined from the job manager's
# environment, or if not there, from the contents of the SGE_CONFIG file
# below
sge_cell=undefined

# This points to a file which contains definitions of the SGE_CELL and SGE_ROOT
# values for this machine. It may either be something like an EPEL
# /etc/sysconfig/gridengine file or the settings.sh file in the SGE
# installation directory
sge_config="/etc/sysconfig/gridengine"

# The Scheduler Event Generator module for SGE requires that the reporting
# file be available for reading. This requires some configuration on the SGE
# side to make it possible to use:
# - SGE must be configured to write information to the reporting file
# - SGE must not load that data inthe ARCo database
# By default, if the Scheduler Event Generator is enabled, it will use
# $SGE_ROOT/$SGE_CELL/common/reporting. To set a specific path, uncomment
# the following line and set the log_path value to the path to the reporting
# file
# log_path="@SGE_REPORTING_FILE@"

# Tools for managing GridEngine jobs:
# - QSUB is used to submit jobs to the GridEngine LRM
# - QSTAT is used to determine job status (unless the scheduler-event-generator
#   interface is used)
# - QDEL is used to cancel jobs
qsub=/usr/bin/qsub
qstat=/usr/bin/qstat
qdel=/usr/bin/qdel
qconf=/usr/bin/qconf

# Programs to run MPI jobs. If SUN_MPRUN is set to anything besides "no", it
# will be used to launch MPI jobs.  Failing that, if MPIRUN is set to
# anything besides "no", it will be used to launch MPI jobs.
sun_mprun=no
mpirun=no

# Parallel environment configuration.
# GridEngine supports different environments to run parallel jobs. There are
# three configuration items which may be used to control how and when these are
# validated.
# - default_pe=ENVIRONMENT
#   If this is set, jobs with no parallel environment defined in the job
#   request, will be submitted using the specified ENVIRONMENT. If this is not
#   set, then parallel jobs will fail if an environment is not present in the
#   RSL
# - available_pes="PE1 PE2..."
#   List of available parallel environments.  If
#   this is not set, the set of parallel environments will be computed by
#   the LRM adapter when it begins execution via the qconf command.
#   If a parallel job is submitted and no parallel environment is
#   specified (either explicitly in RSL or via the default_pe), then the
#   error message will include this list of parallel environments.
# - validate_pes=yes|no
#   If this is set to yes, and the job RSL contains a parallel environment
#   not in the available_pes list, then the LRM interface will reject the job
#   with a message indicating the environment is not supported by GRAM.
#
# default_pe=""
validate_pes=no
# available_pes=""

# Queue configuration
#
# GridEngine supports multiples queues for scheduling jobs. There are
# three configuration items which may be used to control how and when these are
# validated.
# - default_queue=QUEUE
#   If this is set, jobs with no queue defined in the job
#   request will be submitted to the named QUEUE. If this is not
#   set and there is no queue in the job RSL, then GRAM will not set one in
#   the SGE submission script, which may use a site-specific default queue or
#   fail.
# - available_queues="QUEUE1 QUEUE2..."
#   List of available queues. If this is not set, the GRAM SGE adaptor will
#   generate a list of queues when it starts via qconf.
# - validate_queues=yes|no
#   If this is set to yes, then the LRM interface will reject jobs with an
#   error message indicating that the queue is unknown, providing the
#   available_queues values in the error. 
#
# default_queue=""
validate_queues=no
# available_queues=""
</pre>
%ENDTWISTY%

| *Option* | *Meaning* | *Purpose* |
| =sge_config= | The full pathname for the !GridEngine configuration file. | Edit this if you installed !GridEngine into a different location |
| =qsub= | The full pathname for the =qsub= binary. | Edit this if you installed !GridEngineinto a different location |
| =qstat= | The full pathname for the =qstat= binary. | Edit this if you installed !GridEngine into a different location |
| =qdel= | The full pathname for the =qdel= binary. | Edit this if you installed !GridEngine into a different location |
| =qconf= | The full pathname for the =qconf= binary. | Edit this if you installed !GridEngine into a different location |
| =logpath= | The full pathname for the !GridEngine log files. | Edit this to point to your log files |

Note, the default configuration for osg-configure is not to use the Globus SEG.  The SEG reads your Gridengine log files directly to monitor the status of jobs and significantly reduces the load on your CE avoiding the need to run qstat to monitor jobs. Without the SEG, GRAM will not be able to handle more than a few hundred jobs as multiple invocations of qstat per second will increase the load on the server to unmanageable levels as the number of jobs increase.  A production site should use the SEG, however this means that you might need to export your log files (via NFS or similar) to your CE and provide the location of these files using the =log_directory= option.  For more information on configuration of Gridengine, see  [[IniConfigurationOptions#SGE][ini options for the Gridengine jobmanager]].


#ConfigManagedFork
---++ 4.5 !ManagedFork-specific notes
!ManagedFork is configured using =osg-configure=, following [[IniConfigurationOptions#Managed_Fork][these instructions]].

By default the !ManagedFork job manager will behave just like the fork job manager. To adjust the behavior two steps are required:
   1 Edit the Condor Configuration file (see [[InstallCondor#ImportantFiles][the Condor install document]] for the path). Here are two examples:
      * _Allow only 20 local universe jobs to execute concurrently_:<pre class="file">
   START_LOCAL_UNIVERSE = TotalLocalJobsRunning < 20</pre>
      * _Set a hard limit on most jobs, but always let grid monitor jobs run ( *strongly recommended* )_:<pre class="file">
   START_LOCAL_UNIVERSE = TotalLocalJobsRunning < 20 || GridMonitorJob =?= TRUE</pre>
   1 Reconfigure Condor by using =condor_reconfig=:<pre class="rootscreen">
%UCL_PROMPT_ROOT% condor_reconfig</pre>



---+ 5.0 Authorization

You have two options for configuring the set of users that can access your CE: GUMS or edg-mkgridmap. 

#GumsConfig
---++ 5.1. Using GUMS for Authorization

GUMS is a separate service that knows the set of users that can access your site. It is convenient to use GUMS because when you have multiple services at a single site, each of them can use the same GUMS service. (This is particularly true if you use glexec on your worker nodes.) 

As of this writing (December, 2011), our only supported GUMS installation is provided with the old-style Pacman installation. [[InstallGums][Documentation on installing GUMS]].

Historical note: The older Pacman-style VDT used the PRIMA software to communicate with GUMS. The RPM-based installation now uses lcmaps, which is the same software that supports glexec. This means that the way you do configuration has changed.

%NOTE% The following steps are only needed if your are using a version of osg-configure prior to 1.0.0.  osg-configure 1.0.0 and later will automatically configure your =lcmaps.db=, =gums-client.properties= and your =gsi-authz.conf= files for you.

To configure your CE to access gums, you need to make three changes:

   1. Edit =/etc/lcmaps.db= to modify the value passed to argument "--endpoint" to include the hostname of your GUMS server. For example:<pre class="file">
gumsclient = "lcmaps_gums_client.mod"
             "-resourcetype ce"
             "-actiontype execute-now"
             "-capath /etc/grid-security/certificates"
             "-cert   /etc/grid-security/hostcert.pem"
             "-key    /etc/grid-security/hostkey.pem"
             "--cert-owner root"
# Change this URL to your GUMS server
             "--endpoint %RED%https://gums.example.com%ENDCOLOR%:8443/gums/services/GUMSXACMLAuthorizationServicePort"</pre>Please note that this is only a _portion_ of your =lcmaps.db= file. We did not include the whole file here for simplicity.
   1. Edit =/etc/gums/gums-client.properties= and change both gums.location and gums.authz entries to include the hostname your GUMS server. For example:<pre class="file">
gums.location=https://%RED%gums.example.com%ENDCOLOR%:8443/gums/services/GUMSAdmin
gums.authz=https://%RED%gums.example.com%ENDCOLOR%:8443/gums/services/GUMSXACMLAuthorizationServicePort</pre>
   * Uncomment (remove the initial hash mark, #, in =/etc/grid-security/gsi-authz.conf= so that it looks like this:<pre class="file">
globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout</pre>

Run =gums-host-cron= by hand once when you first do an installation:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% gums-host-cron
</pre>

#EdgMkgridmapConfig
---++ 5.2: Using edg-mkgridmap for authorization

[[Documentation.Release3.NavTechEdgMkgridmap][Details of edg-mkgridmap are documented elsewhere]].

=edg-mkgridmap= is a periodic process (run via =cron=) that contacts a list of VOMS servers that you specify. It assembles a ist of users from those servers and creates a _grid-mapfile_ on the CE. This grid-mapfile serves both as a list of authorized users and provides a mapping from user names to local user ids. 

To configure _edg-mkgridmap_, edit =/etc/edg-mkgridmap.conf=. We distribute a default version that lists all known OSG VOs and maps users to shared accounts. A portion of this configuration file looks like:

<pre class="file">
#### GROUP: group URI [lcluser]
#
#-------------------
# USER-VO-MAP mis MIS -- 6 -- Rob Quick (rquick@iupui.edu)    
group vomss://voms.grid.iu.edu:8443/voms/mis mis
#-------------------
# USER-VO-MAP osgedu OSGEDU -- 24 -- Rob Quick (rquick@iupui.edu)    
group vomss://voms.grid.iu.edu:8443/voms/osgedu osgedu
</pre>
To disable access to a VO, simple add a hash mark (#) add the beginning of a line beginning with =group=. For instance, to disable the osgedu group, the final line in the above example would read:
<pre class="file">
%RED%#%ENDCOLOR%group vomss://voms.grid.iu.edu:8443/voms/osgedu osgedu
</pre>

To create an initial mapping you can invoke =edg-mkgridmap=. Then check the result in =/etc/grid-security/grid-mapfile=.

---+ 6.0 Information systems 

OSG sites report information about their site to OSG. This is particularly important for WLCG sites, which need to be present in the BDII information service so the WLCG can run jobs on these sites. However, this is important for all OSG sites because the information is used for site discovery.

---++ 6.1 Generic Information Provider (GIP)

The Generic Information Provider (GIP) is a program that discovers information about your site. It only discovers the information: other software propagates that software to OSG.

Configuration of the GIP happens entirely via the =/etc/osg/config.d/*.ini= files. These are the same files that are used by =osg-configure=. 

[[Documentation.Release3.NavTechGIP][More information on configuring the GIP]].

---++ 6.2 CEMon

CEMon runs the GIP, then pushes its data to OSG. CEMon runs inside of =tomcat=, which runs as the =tomcat= user.

For CEMon to run, you need to have a host or service certificate in =/etc/grid-security/http/httpcert.pem= and =/etc/grid-security/http/httpkey.pem=. These files need to be owned by the =tomcat= user. The simplest thing to do is to copy your host certificate.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir /etc/grid-security/http
%UCL_PROMPT_ROOT% cp /etc/grid-security/hostkey.pem /etc/grid-security/http/httpkey.pem
%UCL_PROMPT_ROOT% cp /etc/grid-security/hostcert.pem /etc/grid-security/http/httpcert.pem
%UCL_PROMPT_ROOT% chown -R tomcat /etc/grid-security/http
%UCL_PROMPT_ROOT% chmod 0400 /etc/grid-security/http/httpkey.pem
</pre>

Note that you want to copy, not move, the hostcerts - Globus still expects them in the original location.  
Make sure =/etc/grid-security/grid-mapfile= exists, even if it is empty: <pre class="rootscreen">%UCL_PROMPT_ROOT% touch /etc/grid-security/grid-mapfile</pre>

%WARNING% There is a minor problem in the GIP installation. For now, run the following commands to ensure the GIP will work properly:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% chown tomcat /var/log/gip
%UCL_PROMPT_ROOT% chown tomcat /var/cache/gip
</pre>

---++ 6.3 OSG Info Services

OSG Info Services is a replacement for CEMon. It has not yet been declared as ready for production use, so we currently recommend CEMon. If you wish to use it, documentation will eventually be present here. 

---+ 7.0 Services

---++ 7.1 Starting and Enabling Services

   1. %INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlStart" TOC_SHIFT="+"}%
   1. Start your batch system (choose the appropriate ones): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor start
%UCL_PROMPT_ROOT% /sbin/service pbs_server on
%UCL_PROMPT_ROOT% /sbin/service gridengine on
</pre>
   1. Depending on your authorization mechanism, choose one of these:
      1. *GUMS:* If you use GUMS, you need to run a client that creates the =user-vo-map= file:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gums-client-cron start
</pre>
      1. *edg-mkgridmap:* If you use edg-mkgridmap to make a grid-mapfile: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service edg-mkgridmap start
</pre>
   1. Start the Globus gatekeeper:  <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service globus-gatekeeper start
</pre>
   1. Start the Globus !GridFTP server:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service globus-gridftp-server start
</pre>
   1. Start Tomcat (used for CEMon)<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service tomcat5 start
</pre>

You should also enable the appropriate services so that they are automatically started when your system is powered on:<br/>
%INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlEnable" TOC_SHIFT="+"}%
<pre class="rootscreen">
# Batch system:
%UCL_PROMPT_ROOT% /sbin/chkconfig condor on
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server on
%UCL_PROMPT_ROOT% /sbin/chkconfig gridengine on

# GUMS
%UCL_PROMPT_ROOT% /sbin/chkconfig gums-client-cron on

# edg-mkgridmap
%UCL_PROMPT_ROOT% /sbin/chkconfig edg-mkgridmap on

# Gatekeeper:
%UCL_PROMPT_ROOT% /sbin/chkconfig globus-gatekeeper on

# GridFTP server:
%UCL_PROMPT_ROOT% /sbin/chkconfig globus-gridftp-server on

# Tomcat: 
%UCL_PROMPT_ROOT% /sbin/chkconfig tomcat5 on
</pre>

---++ 7.2 Stopping and Disabling Services

Run following commands if you need to stop any services.

   1. %INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlStop" TOC_SHIFT="+"}%
   1. Turn off the gatekeeper: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service globus-gatekeeper stop
</pre>
   1. Turn off the Globus !GridFTP Server:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service globus-gridftp-server stop
</pre>
   1. Turn off Tomcat: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service tomcat5 stop 
</pre>
   1. Based on your authorization mechanism:
      1. *GUMS*: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gums-client-cron stop
</pre>
      1. *edg-mkgridmap*: <pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service edg-mkgridmap stop
</pre>
   1. Stop your batch system:<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service condor stop
%UCL_PROMPT_ROOT% /sbin/service torque-server stop
%UCL_PROMPT_ROOT% /sbin/service gridengine stop
</pre>

In addition, you can disable services by running the following commands.  However, you don't need to do this normally.

%INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlDisable"}%

To disable the other services:

<pre class="rootscreen">
# Gatekeeper:
%UCL_PROMPT_ROOT% /sbin/chkconfig globus-gatekeeper off

# GridFTP:
%UCL_PROMPT_ROOT% /sbin/chkconfig globus-gridftp-server off

# Tomcat:
%UCL_PROMPT_ROOT% /sbin/chkconfig tomcat5 off

# GUMS
%UCL_PROMPT_ROOT% /sbin/chkconfig gums-client-cron off

# edg-mkgridmap
%UCL_PROMPT_ROOT% /sbin/chkconfig edg-mkgridmap off

# Batch managers, choose one:
%UCL_PROMPT_ROOT% /sbin/chkconfig condor off 
%UCL_PROMPT_ROOT% /sbin/chkconfig pbs_server off 
%UCL_PROMPT_ROOT% /sbin/chkconfig gridengine off 
</pre>

---+ 8.0 Known problems
---++ 8.1 Address family not supported by protocol
Globus 5.2.0 and 5.2.1 is not using !IPv6 but it is binding to an IPv6 address making IPv6 a requirement. This has been reported in [[http://jira.globus.org/browse/GRAM-309][BUG 309]] and will be fixed.

As a temporary workaround you must support IPv6:
   1. Check that the output of =/sbin/ifconfig= contains a =inet6 addr=, e.g. <pre class="screen">
eth0      Link encap:Ethernet  HWaddr 00:21:9B:89:68:44  
          inet addr:10.1.5.107  Bcast:10.1.255.255  Mask:255.255.0.0
          inet6 addr: fe80::221:9bff:fe89:6844/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:31100499 errors:0 dropped:0 overruns:0 frame:0
          TX packets:18863509 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:4070910220 (3.7 GiB)  TX bytes:2891537947 (2.6 GiB)
          Interrupt:16 Memory:f8000000-f8012800 
</pre>
If there is no inet6 addr, then:
   1. Enable !IPv6. Add to =/etc/sysconfig/network= the line <pre class="file">
NETWORKING_IPV6=yes
</pre> This should be the default unless you disabled IPv6 in Grub or had a line in =/etc/sysconfig/network= stating NETWORKING_IPV6=no (that should be removed if you have it).
   1. Make sure to have the =ipv6= kernel module. Check that =/sbin/lsmod | grep ipv6= is not empty (i.e. the module is installed) or that you can install it with =modprobe=


---+ References
Next steps:
   * ValidatingComputeElement contains few tests to validate your CE
   * TestOSGClient has further commands that you can run against the CE
   * The [[TroubleshootingComputeElement][Compute Element Troubleshooting Guide]] contains some suggestions to fix problems and a list of log files. 

---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AlainRoy

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = User

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = MarcoMambelli 	
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = MarcoMambelli 	
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %NO%
############################################################################################################
-->

%META:TOPICMOVED{by="MarcoMambelli" date="1318875735" from="Documentation/Release3.InstallCE" to="Documentation/Release3.InstallComputeElement"}%
