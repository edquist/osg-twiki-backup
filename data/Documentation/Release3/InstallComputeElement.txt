%META:TOPICINFO{author="AlainRoy" date="1321917096" format="1.1" reprev="1.31" version="1.31"}%
%META:TOPICPARENT{name="InstallBestPractices"}%
%DOC_STATUS_TABLE%

---+!! Installing the Compute Element
%TOC%

---+ Requirements
A RHEL5 (or !CentOS 5, SL5) machine with [[http://fedoraproject.org/wiki/EPEL][EPEL]] repos enabled.

%INCLUDE{"YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%
%INCLUDE{"InstallCertAuth" section="OSGBriefCaCerts" TOC_SHIFT="+"}%

#InstallBatch
---+ Install your batch system

Before you install your CE, you almost certainly need to install your batch system. Installation of your batch system is beyond the scope of the OSG documentation, but there are a few things to note. 

---++ If you are using Condor

You can install Condor from at least three places:

   1. We provide Condor in the OSG repository. We've borrowed the RPM from Fedora, which provides _most_ of Condor, except for Standard Universe, and Condor-G support for CREAM and !NorduGrid.
   1. You can install Condor from the Condor Team's [[http://research.cs.wisc.edu/condor/yum/][yum repository]]
   1. You can install Condor from a binary tarball (from the Condor Team) or source in an arbitrary location on your CE.

Options 1 and 2 should work fine with the caveat that option 2 only works if you are installing Condor 7.6 or later. Option 3 works if you do a bit of upfront effort. The =osg-ce-condor= RPM (documented below) depends on having Condor installed as an RPM and uses RPM's dependency resolution mechanism. If you choose option 3, you need to do two things:

   1. Install a "dummy RPM" called =empty-condor= that will convince RPM that Condor has been installed via RPM, but will not actually provide Condor: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install empty-condor</pre>
   1. Configure Globus to use your version of Condor. See below for details.

---++ If you are using Torque or PBSPro

You can install Torque from at least two places:

   1. Torque is in the EPEL repository.
   1. You can download Torque from [[http://www.adaptivecomputing.com/resources/downloads/torque/][the software developer] and install it in an arbitrary location on your CE.

Option 2 works if you do a bit of upfront effort. The =osg-ce-pbs= RPM (documented below) depends on having Torque installed via the EPEL RPM and it uses RPM's dependency resolution mechanism. If you choose option 2, you need to do two things:

   1. Install a "dummy RPM" called =empty-torque= that will convince RPM that Torque has been installed via RPM, but will not actually provide Torque: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install empty-torque</pre>
   1. Configure Globus to use your version of Torque. See below for details.


---++ If you are using Gridengine

You can install Gridengine from at least two places:

   1. Gridengine is in the EPEL repository.
   1. You can download Gridengine from [[http://www.oracle.com/us/products/tools/oracle-grid-engine-075549.html][Oracle's Gridengine site] and install it in an arbitrary location on your CE.

Option 2 works if you do a bit of upfront effort. The =osg-ce-sge= RPM (documented below) depends on having SGE installed via the EPEL RPM and it uses RPM's dependency resolution mechanism. If you choose option 2, you need to do two things:

   1. Install a "dummy RPM" called =empty-gridengine= that will convince RPM that Gridengine has been installed via RPM, but will not actually provide Gridengine: <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install empty-gridengine</pre>
   1. Configure Globus to use your version of Gridengine. See below for details.

#InstallCE
---+ Install the CE
   1 Install the CE RPM.  Do *ONE* of the following installation lines, depending on your batch system  <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install osg-ce-condor 
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install osg-ce-pbs 
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install osg-ce-lsf 
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install osg-ce-sge
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install osg-ce</pre>
The last one configures Globus to use the fork job manager.
To install managedfork, do the following:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum --enablerepo=osg-testing install globus-gram-job-manager-managedfork
</pre>

#ConfigureCE
---+ Configuration Instructions

---++ Run =osg-configure=

Edit the files in =/etc/osg/config.d=, then run this command to check that your configuration looks good

<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -v
</pre>

Then run this command to configure your CE.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -c 
</pre>

More documentation will go here soon, including hints on how to use your old config.ini file if you like. 

---++ Globus Gatekeeper

Installing the above packages will result in the Globus gatekeeper being installed and enabled as well as the appropriate jobmanager being installed and configured. To turn on the Globus gatekeeper:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service globus-gatekeeper start
Started globus-gatekeeper                                  [  %GREEN%OK%ENDCOLOR%  ]
</pre>

   * globus-gatekeeper init script fails silently if you don't have /etc/grid-security/hostcert.pem / hostkey.pem

---++ Condor-specific notes

   * The Condor jobmanager is configured to not use a shared file system.  There are some rare use cases where this break; if you think this is causing issues, set =isNFSLite=0= in =/etc/globus/globus-condor.conf=.
   * You can control the mapping of jobs to Condor accounting groups using the files =/etc/osg/extattr_table.txt= and =/etc/osg/uid_table.txt=.  See the OSG's documentation on group accounting for more information.

<pre>
RAW TEXT, will be updated soon

/etc/globus/globus-condor.conf 

# Path to the condor_submit executable. This is required to submit condor
# jobs.
condor_submit="/usr/bin/condor_submit"

# Path to the condor_rm executable. This is required to cancel condor jobs.
condor_rm="/usr/bin/condor_rm"

# Value of the CONDOR_CONFIG environment variable. On systems where condor is
# installed in non-default location, this variable helps condor find its
# configuratino files. If you need to set CONDOR_CONFIG to run condor processes
# uncomment the next line at set its value
#condor_config=""
</pre>


---++ PBS-specific notes
   * The location of =qsub=, =qstat=, and =qdel= are configured in the file =/etc/globus/globus-pbs.conf=.  Confirm that the locations are correct for your PBS installation.  They default to the PBS locations in EPEL, for example: =/usr/bin/qstat-torque=.
   * A new feature of Globus is SEG.  In order for SEG to work, the pbs server logs need to be available to the globus-gatekeeper.  If they are not available (for example, the pbs_server is not running on the gatekeeper), you can turn off by removing the option in =/etc/grid-services/jobmanager-pbs=.
      * If the server_logs are available, then their location is configured in =/etc/globus/globus-pbs.conf=.


<pre>
RAW TEXT WILL BE EDITED
/etc/globus/globus-pbs.conf

# The SEG will parse log messages from the PBS log files located in the
# log_path directory 
log_path="/var/torque/server_logs"

# The qsub command is used to submit jobs to the pbs server. It is required
# for the PBS LRM to function
qsub="/usr/bin/qsub-torque"
# The qstat command is used to determine when PBS jobs complete. It is 
# required for the PBS LRM to function unless the SEG module is used.
qstat="/usr/bin/qstat-torque"
# The qdel command is used to cancel PBS jobs. It is required for the LRM
# to function.
qdel="/usr/bin/qdel-torque"

</pre>

---++ Gridengine-specific notes

/etc/globus/globus-sge.conf

<pre>
RAW TEXT WILL EDIT

# This points to a file which contains definitions of the SGE_CELL and SGE_ROOT
# values for this machine. It may either be something like an EPEL
# /etc/sysconfig/gridengine file or the settings.sh file in the SGE
# installation directory
sge_config="/etc/sysconfig/gridengine"

# The Scheduler Event Generator module for SGE requires that the reporting
# file be available for reading. This requires some configuration on the SGE
# side to make it possible to use:
# - SGE must be configured to write information to the reporting file
# - SGE must not load that data inthe ARCo database
# By default, if the Scheduler Event Generator is enabled, it will use
# $SGE_ROOT/$SGE_CELL/common/reporting. To set a specific path, uncomment
# the following line and set the log_path value to the path to the reporting
# file
# log_path="@SGE_REPORTING_FILE@"

# Tools for managing GridEngine jobs:
# - QSUB is used to submit jobs to the GridEngine LRM
# - QSTAT is used to determine job status (unless the scheduler-event-generator
#   interface is used)
# - QDEL is used to cancel jobs
qsub=/usr/bin/qsub
qstat=/usr/bin/qstat
qdel=/usr/bin/qdel
qconf=/usr/bin/qconf

</pre>


---++ Authorization

---+++ Authorization Option #1: GUMS

   * Edit =/etc/lcmaps.db= (modify value passed to argument "--endpoint") and =/etc/gums/gums-client.properties= (change both gums.location and gums.authz entries) to reflect your GUMS server
   * Uncomment the single line in =/etc/grid-security/gsi-authz.conf=

#CeEdgMkgridmap
---+++ Authorization Option #2: edg-mkgridmap

   * Edit =/etc/edg-mkgridmap.conf=
   * <pre class="rootscreen">%UCL_PROMPT_ROOT% /sbin/service edg-mkgridmap start</pre>

edg-mkgridmap is a cron job, but you control whether it runs or not using an init script, as shown above. 

---++ Gratia

   * Edit /etc/gratia/[jobmanger-you-are-using]/ProbeConfig, configuring it like an existing CE.  Make sure you set !EnableProbe="1", otherwise the probe will never run.
   * This will be performed by a near-future configure-osg.

---++ CEMon

Make sure =/etc/grid-security/grid-mapfile= exists, even if it is empty (i.e. =touch /etc/grid-security/grid-mapfile=).

You can either use the httpcert/httpkey or hostcert/hostkey. However, make sure the "tomcat" user has access to whichever cert/key pair you decide to use. 
You can use the hostcert/key as follows

<pre class="rootscreen">
mkdir /etc/grid-security/http
cp /etc/grid-security/hostkey.pem /etc/grid-security/http
cp /etc/grid-security/hostcert.pem /etc/grid-security/http
chown -R tomcat:tomcat /etc/grid-security/http
chmod 0400 /etc/grid-security/http/hostkey.pem
</pre>

Make a directory for the GIP to use:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir /var/gip
%UCL_PROMPT_ROOT% mkdir /var/gip/tmp
</pre>

   * (Deprecated info alert!) gip no longer uses /var/gip/tmp. It stores tmp ldif files under /var/cache/gip (Make sure it's owned by tomcat). gip still uses /var/gip/log to store gip.log

---++ Other

Run =gums-host-cron= by hand once:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% gums-host-cron
</pre>

---+ Start Services

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service fetch-crl-cron start
%UCL_PROMPT_ROOT% /sbin/service fetch-crl-boot start
%UCL_PROMPT_ROOT% /sbin/service condor start # Replace with batch system of your choice
%UCL_PROMPT_ROOT% /sbin/service globus-gatekeeper start
%UCL_PROMPT_ROOT% /sbin/service globus-gridftp-server start
%UCL_PROMPT_ROOT% /sbin/service tomcat5 start  # Tomcat is used for CEMon
%UCL_PROMPT_ROOT% /sbin/service gums-client-cron start
</pre>

Run 'chkconfig <service-name> on' command for any services that you want to start automatically after system reboot.

---+ Stop Services

Run following commands, only if you need to stop any services.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service fetch-crl-cron stop
%UCL_PROMPT_ROOT% /sbin/service fetch-crl-boot stop
%UCL_PROMPT_ROOT% /sbin/service condor stop # Replace with batch system of your choice
%UCL_PROMPT_ROOT% /sbin/service globus-gatekeeper stop
%UCL_PROMPT_ROOT% /sbin/service globus-gridftp-server stop
%UCL_PROMPT_ROOT% /sbin/service tomcat5 stop  # Tomcat is used for CEMon
%UCL_PROMPT_ROOT% /sbin/service gums-client-cron stop
</pre>
---+ References

---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AlainRoy

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = User

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = SuchandraThapa 	
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = SuchandraThapa 	
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->

%META:TOPICMOVED{by="MarcoMambelli" date="1318875735" from="Documentation/Release3.InstallCE" to="Documentation/Release3.InstallComputeElement"}%
