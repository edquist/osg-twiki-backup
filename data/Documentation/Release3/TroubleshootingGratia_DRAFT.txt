%META:TOPICINFO{author="BrianLin" date="1479252014" format="1.1" version="1.3"}%
---+ Troubleshooting Gratia Accounting Guide

%TOC{depth="3"}%

---++ About This Guide

This document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service. See also other documents recommended in the Reference section below.

This document follows the general OSG documentation conventions: %TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to expand document conventions..."}%
%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="CommandLine"}%
%ENDTWISTY%

---+ Common Gratia Problems

---++ Are the Gratia cron jobs running?
You should make sure the Gratia cron jobs are running. The simplest way is with the =service= command:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /sbin/service gratia-probes-cron status
gratia probes cron is enabled.
</pre>

If it is not enabled, enable it as described above.

A future release of Gratia will provide status on each of the individual probes, but right now this only ensures that the basic cron job is running. In the meantime, you can check if the individual Gratia probes are enabled. To do this, look at the =EnableProbe= option in the =ProbeConfig= file, as described above. A quick command to do this is shown here. Note that the Condor and !GridFTP Transfer probes are enabled while the glexec probe is disabled:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cd /etc/gratia
%UCL_PROMPT_ROOT% grep -r EnableProbe *
condor/ProbeConfig:    EnableProbe="1"
glexec/ProbeConfig:    EnableProbe="0"
gridftp-transfer/ProbeConfig:    EnableProbe="1"
</pre>

If you see no log files in =/var/log/gratia= you may have an error in the probe configuration file. Run manually the test for your probe (check =/etc/cron.d/gratia-probe-condor.cron=), e.g. =/usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig=. If there is an error you may get a suggestion on where it is, e.g.:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% /usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig
Parse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4
</pre>
Correct the error and restart gratia.

---++ Have you configured the resource names correctly?
Do the names of your resources match the names in OIM? 

For example, from the SE portion of the =/etc/osg/config.d/30-gip.ini=:

<pre class="file">
;===================================================================
;                             SE
;===================================================================

; For each storage element, add a new SE section.
; Each SE name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each SE section must start with
; the words "SE", and cannot be named "CHANGEME".

; There are two main configuration types; one for dCache, one for BestMan

; Don't forget to change the section name!  One section per SE at the site.
[%RED%YOUR_SE_NAME%ENDCOLOR%]

; The first part of this section shows options which are mandatory for all SEs.
; dCache and BestMan-specific portions are shown afterward.

; Set to False to turn off this SE
enabled = True

; Name of the SE; set to be the same as the OIM registered name
name = %RED%YOUR_SE_NAME%ENDCOLOR%
</pre>

Do those names match the names that you registered with OIM? If not, edit the names, and rerun "osg-configure -c". 

---++ Did the site name change?
Was the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the Gratia operations team to update the name of your site at the Gratia collector. To do this:

   1. Open a ticket at [[https://ticket.grid.iu.edu/goc/submit][the GOC ticket web page]]
   1. Under "Optional Details", select "Software or Service"
   1. After you do, another popup will appear. Select "Gratia (Collector Issue)"
   1. Type a friendly email that asks the Gratia team to change your site name at the collector. Make sure to tell them the old name and the new name. 

---++ Is a site reporting data?

You can see if the OSG Gratia Server is getting data from a site by going to [[http://gratiaweb.grid.iu.edu/gratia/bysite/][GratiaWeb]]:
   
   1. Specify the site name in Facility under Data Filter.
   1. Click "Refine".

---++ Not collecting Condor accounting data

Condor must be configured to put information about each job into a special directory. Gratia will read and remove the files in order to collect the accounting information. 

The configuration variable is called =PER_JOB_HISTORY_DIR=. If you install the OSG RPM for Condor, the Gratia probe will extend its configuration by adding a file to =/etc/condor/config.d=, and will set this variable to =/var/lib/gratia/data=. If you are using a different installation method, you probably need to set the variable yourself. You can check if it's set by using =condor_config_val=, like this:

<pre class="screen">
%UCL_PROMPT% condor_config_val -v PER_JOB_HISTORY_DIR
PER_JOB_HISTORY_DIR: /var/lib/gratia/data
  Defined in '/etc/condor/config.d/99_gratia.conf', line 5.
</pre>

If you set this value, you need to restart condor:

<pre class="screen">
%UCL_PROMPT_ROOT% condor_restart
Sent "Restart" command to local master
</pre>

Unlike many Condor settings, a *condor_reconfig* is not sufficient - you must restart!

---++ Reporting old Condor data

If you accidentally did not set =PER_JOB_HISTORY_DIR= (see above), Gratia will not publish accounting information about jobs. You can have Gratia read the Condor history file and publish data that way.  If you know the time period of the missing data, you should specify a start and end times. This reduces the load on the Gratia collector.  To do so:

<pre class="screen">
%BLUE%Preferred method using start and end times%ENDCOLOR%
%UCL_PROMPT_ROOT% /usr/share/gratia/condor/condor_meter --history --start-time="2014-06-01" --end-time="2014-06-02" --verbose
2014-06-03 10:00:36 CDT Gratia: RUNNING condor_meter MANUALLY using HTCondor history from 2014-06-01 to 2014-06-02
2014-06-03 10:00:36 CDT Gratia: RUNNING: condor_history -l -constraint '((JobCurrentStartDate > 1401598800) && (JobCurrentStartDate < 1401685200))'
2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records submitted: 399
2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records found: 400
2014-06-03 10:00:49 CDT Gratia: RUNNING condor_meter MANUALLY Finished

 %BLUE% or if you need to go back to the beginning of time%ENDCOLOR%
%UCL_PROMPT_ROOT% /usr/share/gratia/condor/condor_meter --history --verbose
2014-06-03 10:06:19 CDT Gratia: RUNNING condor_meter MANUALLY using all HTCondor history
2014-06-03 10:06:19 CDT Gratia: RUNNING: condor_history -l
2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records submitted: 13026
2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records found: 13027
2014-06-03 10:11:38 CDT Gratia: RUNNING condor_meter MANUALLY Finished
</pre>

Not much is printed to the screen, but you can see progress in the Gratia log file:

%TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to show a log file processing the Condor history file..."}%
<pre class="file">
13:35:28 CDT Gratia: Initializing Gratia with /etc/gratia/condor/ProbeConfig
13:35:28 CDT Gratia: Creating a ProbeDetails record 2012-04-04T18:35:28Z
13:35:28 CDT Gratia: ***********************************************************
13:35:28 CDT Gratia: OK - Handshake added to bundle (1/100)
13:35:28 CDT Gratia: ***********************************************************
13:35:28 CDT Gratia: List of backup directories: [u'/var/lib/gratia/tmp']
13:35:28 CDT Gratia: Reprocessing response: OK - Reprocessing 0 record(s) uploaded, 0 bundled, 0 failed
13:35:28 CDT Gratia: After reprocessing: 0 in outbox 0 in staged outbox 0 tar files
13:35:28 CDT Gratia: Creating a UsageRecord 2012-04-04T18:35:28Z
...
13:35:29 CDT Gratia: Processing bundle file: 
13:35:29 CDT Gratia: Processing bundle file: /var/lib/gratia/tmp/gratiafiles/
    subdir.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80/
    outbox/r.18425.condor_fermicloud084.fnal.gov_gratia-osg-itb.opensciencegrid.org_80.gratia.xml__BSuXo18428
...
13:35:29 CDT Gratia: ***********************************************************
13:35:29 CDT Gratia: Removing log files older than 31 days from /var/log/gratia
13:35:29 CDT Gratia: /var/log/gratia uses 0.035% and there is 73% free
13:35:29 CDT Gratia: Removing incomplete data files older than 31 days from /var/lib/gratia/data/
13:35:29 CDT Gratia: /var/lib/gratia/data uses 0% and there is 73% free
13:35:29 CDT Gratia: End of execution summary: new records sent successfully: 37
</pre>
%ENDTWISTY%

__Note__  that Condor rotates history files, so you can only report what Condor has kept. Controlling the Condor history is documented in the Condor manual. In particular, see the options for [[http://research.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html#param:MaxHistoryLog][MAX_HISTORY_LOG]] and [[http://research.cs.wisc.edu/condor/manual/v7.6/3_3Configuration.html#param:MaxHistoryRotations][MAX_HISTORY_ROTATIONS]].

__Also Note__ that it is a very good idea to turn off the _gratia_probes_cron_ service until this is complete.  Remember to turn it back on when finished.

---++ Gratia log files: bad Gratia hostname

This is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in =cd /var/log/gratia/=. There is one log file per day, labeled by the date:

<pre class="file">
%UCL_PROMPT_ROOT% cd /var/log/gratia/
%UCL_PROMPT_ROOT% cat 2012-04-03.log 
...
%RED%You can see that Gratia is using the correct configuration file:%ENDCOLOR%
15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig

%RED%Here Gratia is removing a file from the Condor PER_JOB_HISTORY_DIR and creating a Gratia accounting record for it%ENDCOLOR%
15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z
15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0
15:06:55 CDT Gratia: ***********************************************************
15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/
    subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/
    outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606
15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0

%RED%Later, Gratia failed to connect to the server due to a bad hostname%ENDCOLOR%
15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type "socket.gaierror": (-2, 'Name or service not known')
...
15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted:
15:06:55 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/
    subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/
    outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606
</pre>

---++ Recovering from bad Gratia hostname

If you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data. This can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files. 

When a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as =condor=), the name of the host you're running on, and the name of the Gratia host you're sending the information to. For simplicity, lets call that name _probe-records_, but you'll see what it really looks like below. Within this directory, you'll see some subdirectories:

| * Directory * | *Purpose* |
| /var/lib/gratia/tmp/grataifiles/%RED%probe-records%ENDCOLOR%/outbox | The usual location for the accounting records |
| /var/lib/gratia/tmp/grataifiles/%RED%probe-records%ENDCOLOR%/staged/store | An overflow location when there are problems |

When you recover old records, you need to:
   1. Before trying to transfer any old records, check if they are more than three months old. If they are, the server will not accept them. This is a policy that is enforced strictly by the admins.
   1. Move files from the outbox of the incorrect _probe-records_ directory into the outbox of the correctly named _probe-records_ directory.
   1. Move tarred and compressed files from the staged/store of the incorrect _probe-records_ directory into the staged/store of the correctly named _probe-records_ directory. Then you uncompress them and remove the compressed version.

In the examples below, the hostname for gratia was "accidentally" spelled backwards. Instead of =gratia-osg-itb.opensciencegrid.org=, it was =aitarg-osg-itb.opensciencegrid.org=. 

---+++ Correct the hostname
First you need to fix the hostname. For a CE, you can edit =/etc/osg/config.d/30-gratia.ini= and rerun =osg-configure -c=. In other installations, you have to edit the appropriate =ProbeConfig= file. 

---+++ Run one job
Next, submit a job via Globus to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example:

As a user: <pre class="screen">
%UCL_PROMPT% globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname
</pre>

As root (adjust for your batch system): <pre class="rootscreen">
%UCL_PROMPT_ROOT% /share/gratia/condor/condor_meter 
</pre>

---+++ Restore the individual Gratia records
First, find the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldly name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is noted in red below, but _it will be different on your computer_. 

<pre class="file">
%UCL_PROMPT% less /var/log/gratia/2012-04-06
...
16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted:
16:04:29 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/
    subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/
    outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918
</pre>

(The filename was wrapped for legibility.)

You can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.
%UCL_PROMPT_ROOT% mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%gratia%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.
</pre>

---+++ Restore compressed Gratia records
If this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in "staged/store" instead of "outbox" as above:

<pre class="rootscreen">
%RED%# Find the old files%ENDCOLOR%
%UCL_PROMPT_ROOT% cd /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%aitarg%ENDCOLOR%-osg-itb.opensciencegrid.org_80/staged/store

%RED%# Move them to the correct directory%ENDCOLOR%
%UCL_PROMPT_ROOT% mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_%RED%gratia%ENDCOLOR%-osg-itb.opensciencegrid.org_80/outbox/.
%UCL_PROMPT_ROOT% cd !$

%RED%# For each tz file:%ENDCOLOR%
%UCL_PROMPT_ROOT% tar xf tz.1223.... [name shortened for legibility]
%UCL_PROMPT_ROOT% rm tz.1223....
</pre>

When you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron.

---+ Appendix: Important Gratia files

This document cannot cover all the errors you might experience. If you need to look for more data, you can look at log files for the various services on your CE. 

| *File* | *Purpose* |
| =/var/log/gratia/%RED%DATE%ENDCOLOR%.log= | Log file that records information about processing and uploading of Gratia accounting data |
| =/var/log/gratia/gridftpTransfer.log= | Log file specific to the Gratia !GridFTP probe |
| =/var/lib/gratia/data= | Location for Condor and PBS job data before being processed by Gratia<br>Condor's =PER_JOB_HISTORY_DIR= should be set to this location |
| =/var/lib/gratia/tmp/gratiafiles= | Location for temporary Gratia data as it is being processed, usually empty.<br>If you have files that are more than 30 minutes old in this directory, there may be a problem |
| =/etc/gratia/%RED%PROBE-NAME%ENDCOLOR%/ProbeConfig= | Configuration for Gratia probes, one per probe type</br>Normally you don't need to edit this |

The most common RPMs you will see are:

| *RPM* | *Purpose* |
| =gratia-probe-common= | Code shared between all Graita probes |
| =gratia-probe-gram= | Code needed to make Gratia probes with with GRAM |
| =gratia-probe-condor= | The probe that tracks Condor usage via GRAM |
| =gratia-probe-pbs-lsf= | The probe that tracks PBS and/or LSF usage via GRAM  |
| =gratia-probe-gridftp-transfer= | The probe that tracks transfers done with !GridFTP |
| =gratia-probe-metric= | The probe that reports RSV results. (Not exactly accounting, but it re-uses Gratia) |

---# Some useful procedures and examples
If you have an OSG Compute Element all the pieces should be present and you should report correctly to Gratia. But sometimes you may require special configurations: some of the jobs have no VO association and risk not to be reported to the OSG accounting (Gratia)

%NOTE% Before modifying your local configuration check if those are jobs you should count or if someone else should count them. Avoid double counting and choose the most appropriate place to report the jobs. E.g. If you are receiving jobs form a Compute Element that is forwarding them to you (e.g. via HTCondor flocking), the CE first accepting them should report those jobs. The same applies if you have a gateway accepting jobs and resubmitting them via BOSCO.

---## All your jobs belong to a VO 
If you have no CE or have a mix of Grid submitted and local jobs running on your cluster and you want to report them all as running for a specific VO, then set !VOOverride in the probe configuration.<br>
Further, if you want to keep track also of the original local group, add also !MapGroupToRole.  In the below example, replace the %RED%osg%ENDCOLOR% with a registered VO that you would like to report as.  If you don't have a VO that you are affiliated with, you may use "osg".

<pre class="file">
...
    %RED%MapGroupToRole="1"%ENDCOLOR%
    %RED%VOOverride="OSG"%ENDCOLOR%
...
</pre>

---## You are running a Campus Grid and have no Compute Element 
If you have a HTCondor cluster connected to a [[https://twiki.grid.iu.edu/bin/view/Accounting/ProbeConfigGlideinWMS#Users_without_Certificates][campus grid]] you can use 3 different ways to associate your jobs to a VO. The last 2 reported below, especially the 3rd, allow you to map only some of the jobs, e.g. if you do not want to report some jobs to OSG accounting:
   * If all your users belong to one VO you can use the example above.
   * Otherwise you can use the local Group Names as VO names (you'd have to have control of the unix groups) <pre class="file">
    ...
    SuppressGridLocalRecords="0"
    %RED%MapUnknownToGroup="1"%ENDCOLOR%
    ...
</pre>
   * As 3rd alternative you can use =user-vo-map= as explained below

---## You have some local users that you want to report to OSG accounting
If some users are already associated to a VO, e.g. via one of the mechanisms described in the previous chapter, and some users are not part of any VO and you don't want to report them at all: a flexible mechanism is the use of the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/Edg-mkgridmap#3_0_Configuration][user-vo-map file]]. If you are already an OSG site you may also use GUMS.

=/var/lib/osg/user-vo-map= is a file generated automatically on a CE via [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/Edg-mkgridmap][edg-mkgridmap]] or GUMS. In it there is a list of valid VO names and all users are mapped to a VO. See [[http://vdt.cs.wisc.edu/extras/edg-mkgridmap.conf.html][user-vo-map file]] for more information. Here is an example
<pre class="file">
#User-VO map
# #comment line, format of each regular line line: account VO
# Next 2 lines with VO names, same order, all lowercase, with case (lines starting with #voi, #VOc)
#voi cdf fermilab accelerator argoneut cdms minerva miniboone minos mipp mu2e nova numi patriot theory fermilab-test microboone microboone microboone microboone map gm2 coupp fermilab accelerator argoneut cdms minerva miniboone minos mipp nova numi patriot theory map gm2 argoneut minerva minos nova map gm2 fermilab argoneut minerva minos nova map gm2 fermilab darkside darkside darkside darkside darkside darkside mis star cms cms cms cms cms cms cms cms cms cms cms dzero dzero dzero dosar des glow nanohub geant4 geant4 i2u2 osg atlas atlas atlas atlas atlas atlas osgedu ops des engage engage engage engage ilc nysgrid sbgrid cigi icecube alice gluex gridunesp dayabay hcc belle belle csiu suragrid nees superbvo.org superbvo.org superbvo.org dream lbne lbne lbne lbne lsst uc3 mcdrd lqcd auger glast.org enmr
#VOc CDF FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB FERMILAB MIS STAR CMS CMS CMS CMS CMS CMS CMS CMS CMS CMS CMS DZERO DZERO DZERO DOSAR DES GLOW NANOHUB GEANT4 GEANT4 I2U2 OSG ATLAS ATLAS ATLAS ATLAS ATLAS ATLAS OSGEDU OPS DES Engage Engage Engage Engage ILC NYSGRID SBGrid CIGI IceCube ALICE Gluex GridUNESP DayaBay HCC belle belle CSIU SURAGrid NEES SuperB SuperB SuperB DREAM LBNE LBNE LBNE LBNE LSST UC3 MCDRD LQCD Auger glast.org ENMR
#---- accounts for vo: cdf-fnal ----#
#---- accounts for vo: fermilab ----#
fnalgrid fermilab
#---- accounts for vo: accelerator ----#
#---- accounts for vo: argoneut ----#
...
#---- accounts for vo: osg ----#
osg osg
#---- accounts for vo: newAtlasProd ----#
usatlas1 atlas
#---- accounts for vo: newUsatlasProd ----#
#---- accounts for vo: newUsatlasSoft ----#
usatlas2 atlas
#---- accounts for vo: newAtlasSoftware ----#
#---- accounts for vo: newusatlas ----#
usatlas3 atlas
#---- accounts for vo: newatlas ----#
usatlas4 atlas
...
</pre>

---### In GUMS
You can define [[https://www.racf.bnl.gov/Facility/GUMS/1.2/use_configuration.html][groupToAccountMappings]] and [[https://www.racf.bnl.gov/Facility/GUMS/1.2/use_users.html][add Manual Users Group Members]].

Now GUMS supports also ==local-user-vo-map== like explained below in the =edg-mkgridmap= section. You still have to define the VO via a[[https://www.racf.bnl.gov/Facility/GUMS/1.2/use_configuration.html][groupToAccountMapping]]

---#### Older GUMS
In older versions of GUMS you will have to update or manually patch ==gums-host-cron== to use ==local-user-vo-map== like explained below in the =edg-mkgridmap= section.
<pre class="screen"># diff /usr/bin/gums-host-cron /usr/bin/gums-host-cron.20140913
308,315d307
< # --- Add local user accounts maps
< if [ ! -e $LOCALUSERMAP ]; then
< touch $LOCALUSERMAP
< fi
<
< cat $LOCALUSERMAP >> $INVERSEMAP_NEW.tmp
<
<
336,337d327
< LOCALUSERMAP=$OSG_LOCATION/local-user-vo-map
<
</pre>

---### Using Edg-mkgridmap
If you want to add some local user and map it to an existing VO, then edit =/etc/osg/local-user-vo-map= mapping the users, e.g.:
<pre class="file">
# file /etc/osg/local-user-vo-map
newuser1 voname
newuser2 voname
newuser3 voname2
</pre>

If you want to add some local user and map it to a new VO, then edit =/etc/osg/local-user-vo-map= mapping the users as before, and also add the VO lines to =/etc/edg-mkgridmap.conf=, e.g. if =voname2= used above is new:
<pre class="file">
# file /etc/edg-mkgridmap.conf
# USER-VO-MAP voname2 REPORTABLE_VONAME2 -- 13 -- This is my local VO (admin@mydomain.edu)
</pre>
After USER-VO-MAP there are the VOName and !ReportableVOName for the accounting record and then some arbitrary comments.

%NOTE% All the files above are part of the OSG Compute Element installation but you can still edit by hand a =/var/lib/osg/user-vo-map= with the mappings you are interested in and the Gratia probes will use it even if you have no OSG CE installed.

#ReferenceSection
---++ Reference

   * [[Documentation.Release3/GratiaIntroduction_DRAFT][Gratia Probe Introduction]]
   * [[Documentation.Release3/InstallGratia_DRAFT][Installing and Maintaining Gratia Probes]]
   * [[Documentation.Release3/GratiaReference_DRAFT][Gratia Probe Reference]]
   * [[http://gratia-osg-prod-reports.opensciencegrid.org/gratia-reporting/][Gratia Job Accounting Data]] (Production)
   * [[http://gratia-osg-itb-reports.opensciencegrid.org/gratia-reporting/][Gratia Job Accounting Data]] (ITB)
   * [[http://gratia-osg-transfer-reports.opensciencegrid.org/gratia-reporting/][Gratia Transfer Accounting Data]]
   * You can get daily emails with OSG accounting reports in them. Join the [[Accounting.ContactUs][osg-accounting-info]] mailing list. 
   * [[Accounting.WebHome][The Gratia group's web page]]
   * [[Documentation/Release3.NavTechGratia][Gratia's technical documentation index]]
