%META:TOPICINFO{author="FkW" date="1154680700" format="1.1" reprev="1.39" version="1.39"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! DcacheWorkshop for storage admins (July 17th 2006, FNAL)

%TOC%

---++ Where?
FCC2, i.e. second floor conference room in Feynman computing center @ FNAL.
Or to join virtually, follow these instructions:

If you will be joining by video, we will be using 
the ESnet ECS Ad-hoc bridge.
The number to dial in to connect by video is
88322243 for 88dcache at 348kps.

If you need to dial in by phone only, 
call 510-883-7860 then enter the Ad-hoc number 
88322243 for 88dcache followed by the # sign.

If you need to join by VRVS see instructions at
http://www-staff.es.net/~mikep/adhoc/vrvsecs.htm

'Storm' VRVS room has been reserved.

There's also a web page at the main OSG events calendar:
http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=146

---++ Agenda

---+++ Intro of Attendees
Please feel free to edit this twiki by adding in your site name, and dCache infrastructure deployed today, and planned for the future, say one year from now.

 <b>Abhishek Singh Rana & Frank Wuerthwein - UCSD </b>
   * <b>[[%ATTACHURL%/UCSD-dCache-topology-diagram.jpg][UCSD dCache Topology Diagram]]</b>
   * USCMS T2, dCache/SRM in production usage since early 2004.
   * 80+ worker nodes, 70+ have dCache pools, 0.2 - 1.7 TB per pool.
   * ~42TB currently, phased growth ongoing. 
   * ~11 dCache infrastructure nodes with various hardware configs.
   * 1 Core + 1 PNFS server & Mgr (DB) + 1 Replica Mgr (DB) + 1 DCap + 1 SRM (DB) + 6 GFTP.
   * <u>Only</u> Nodes on WAN (dual-homed): 1 Core + 1 SRM (DB) + 6 GFTP.
   * PNFS mounted <u>only</u> on infrastructure nodes, not mounted on worker nodes.
   * Multiple mover queues on pools - default LAN (dCap) + WAN (SRM & GFTP).
   * Network: 6 GFTP nodes on 10GbE, 2 of which with 2 GFTP doors each. 
   * Version: dcache-1.6.6-5 with April 2006 jar update from FNAL.
   * <nowiki>PostgreSQL 7.4.6, jdk1.5.0_01</nowiki>
   * Implicit Space reservation enabled (mid 2005). 
   * Replica Mgr in production usage (early 2004) - all pools are resilient. 
   * ext3 as filesystem. 
   * gPlazma non-GUMS RBAC mode in production usage (mid 2005).
   * Download transfer milestone - 13 TB/day FNAL->UCSD, 3rd party SRM, by <nowiki>PhEdEx</nowiki>.

 <b>Suresh Singh, Michael Thomas - Caltech</b>
   * USCMS Tier2 site, dCache used in production
   * ~60 worker nodes, all but a few are dCache pool nodes
   * 34 Tb, growing to ~60 later this year
   * 2 dCache infrastructure nodes
   * all workers on public address space
   * replication on (2 copies?)
   * second, smaller 4-node dCache installation used for development with <nowiki>LambdaStation</nowiki>

 <b>Preston Smith - Purdue</b>
   * USCMS Tier-2, dCache used in production
   * Version: dcache-1.6.6-5 with May10 2006 jar update from FNAL.
   * 28 TB currently
   * Pools live on large blocks of RAID storage (6 pool nodes).
   * 3 GridFTP doors
   * All nodes reside on public network 
   * No replication currently.
   * Upcoming expansion will add pools on worker nodes (with replication), in addition to additional RAID storage. 
   * 2 infrastructure nodes (pnfs on one node, srm/dcap/admin on 2nd)

<b>Dan Schrager - Indiana University</b>
   * Experimental dCache installation at http://bandit.uits.indiana.edu:2288/
   * SC4 (not production yet) dCache installation at http://tier2-d1.uchicago.edu:2288/

<b>Yujun Wu - Fermilab</b>
   * CMS T1 site, dCache/SRM used in production
   * Version: dCache 1.7.0
   *110TB of disk on 23 read/write pool nodes with 2 bonded GE interfaces. Each read/write pool node has multiple mover queues (LAN queue and WAN queueu);
   * 8 nodes with about 9 TB of space are dedicated as stage pool node area for restoring the tape-based data to disk;
   * We also have resilient dCache built with 500 worker nodes with a total space of 55 TB;
   * All nodes reside on public network;
   * 10 dCache infrastructure nodes (Core, dCap, SRM, PNFS, Replica Manager, Informaiton system, and Management);
   * PNFS is mounted on all the worker nodes;

<b>Zhengping(Jane) Liu, Yingzi(Iris) Wu --- Brookhaven National Lab</b>
   * USATLAS Tier1 site, dCache used in production
   * 150TB currently, phased growth ongoing
   * 350+ worker nodes (dCache pools)
   * 1 admin + 1 PNFS server + 4 dCap + 1 SRM + 5 GridFTP
   * HPSS as tape backend.
   * Network: 5 GFTP nodes with 2GFTP doors each on 10GbE, 
   * Version: dcache-1.6.6.5
   * ProsgreSQL 8.1.4, jdk 1.5.0_05
   * file system: ext3 in admin and read pool, xfs in write pool
   * Transfer - 7 TB/day 

<b>Dan Bradley - UWMadisonCMS</b>
   * CMS T2 site, dCache used in production
   * 36TB Xserve RAID + 48TB/2 resilient
   * 6 dCache infrastructure nodes: pnfs, core+httpd+admin, SRM, 3 dcap doors (500 logins each), gridftp, replicaManager
   * 18 2TB Xserve RAID pools
   * 48 1TB resilient pool nodes
   * pnfs is only mounted on infrastructure nodes
   * all nodes on public network
   * dCache 1.6.6-5 with jar update May 10/2006 (Jon's jars)
   * PosgreSQL 7.4.6, jdk 1.5.0_07
   * Multiple mover queues on pools - 1000 default LAN (dCap) + 5 WAN (SRM & GFTP).
   * ext3 filesystem on resilient pools, jfs on Xserves (considering change to xfs)
   * Download transfer milestone - 15 TB/day FNAL->Wisconsin, 3rd party SRM, by <nowiki>PhEdEx</nowiki>.

<b>Anand Padmanabhan - UIowa </b>
   * Experimental dCache Installation
   * 2 dCache Infrastructure nodes ( 1 admin+pnfs+pool + 1 pool+gsiftp-door+srm-door)
   * Currently no replication
   * Version: dcache-1.6.6.5, ProsgreSQL 8.1.4, jdk 1.5.0_07
   * http://rtgrid1.its.uiowa.edu:2288/

 <b> Attendees: Please continue with details .. </b>


---+++ Layperson Intro to dCache
   * [[%ATTACHURL%/dcacheIntro-7-17-2006][dCache Introduction]]
     Goal here is to present a layperson's view of how dCache works.

---+++ Walkthrough of dCache installation (moved to afternoon)
[[http://www.atlasgrid.bnl.gov/workshop_dcache/install_note][Installation Instructions]]

---+++ dCache on Rocks
   * [[%ATTACHURL%/dCache-ROCKS.ppt][dCache on Rocks]]

---+++ dCache performance tuning and basics of admin interface
   * [[%ATTACHURL%/ASR-dCache-tuning-p1.ppt][Basics of Tuning]]
   * [[%ATTACHURL%/ASR-dCache-admin-p2.ppt][Basics of Admin Interface]]

---+++ dCache FAQ and discussion
Here comes your input. Ask whatever questions you want to from off the dCache people.

---++++ How to drain a pool?
You can either use the replicaManager or the CopyManager.
The latter is a relatively new feature.

   * Using the replica manager:
http://cmsdcam2.fnal.gov/dcache/resilient/Resilient_dCache_TroubleShooting.html

The replicaManager is repsonsible for managing desired number of replicas. It can be used ot drain a pool as follows:

Use dcache admin interface on the node that has the admin cell.
Then do something like:
<pre>
dcache> set dest replicaManager
dcache> set pool <pool name 1 > drainoff
dcache> set pool <pool name 2 > drainoff
</pre>

You then need to wait, and occasionally check progress:
<pre>
dcache> set dest replicaManager
dcache> ls unique <pool name 1>
dcache> ls unique <pool name 2>
</pre>

This returns the # of files that are not yet replicated, and thus still unique to pool 1 and pool 2. The pnfsid's of the files that are still unique to the pool(s) are listed in the replicaManager logfile.

Note: "ls unique" looks for files only in pools that are in the 
online state. E.g. other pools in drainoff state are not considered when
checking for uniqueness. You can thus put multiple pools into drainoff state at once without worrying about "ls unique" giving misleading answers.
 
If there are no unique files in the pools then you can take the pool out as follows:

<pre>
dcache> set pool <pool name 1> down
dcache> set pool <pool name 2> down
</pre>

If this doesn't work, complain to support@dcache.org .
Please refer in your email to this twiki page and OSG. 

---++++ What do all of the cells and domains do?
When  trying to debug dcache it is often very confusing because there are so many components. Without knowing what all these components do,
and what hardware resources they stress (cpu, memory, disk, etc.) it is hard to even know which logfile to look at when trying to debug something, and how to improve performance.

Action items:
   * provide a suggested hardware deployment scenario, or two, three.
   * provide a (complete) list of components, and a one paragraph description of what they do, and what hardware resources they stress.
   * provide a (complete) list of how each of us deploys dcache.


---++++ What do I do if I loose a file that still is in pnfs?
You need to rm the file from pnfs as well, and then retransfer the file.

---++++ Is there a way to configure dcap to have a timeout?
Yes there is. It's an option that has to be specified with dcap,
(dccp -o). This could also be hardcoded into libdcap.so and thus fixed
as a site characteristic. Ideally, one would want to have this as a serverside rather than a clientside timeout such that the admins deploying the server control it. As of now, a serverside timeout is not possible. Rob K. finding out more about this from DESY.

---++++ PNFS checking
There are various people who have developed scripts for either searching for lost files, or chksumming all files. There are at least three such systems:

   * http://hepuser.ucsd.edu/twiki/bin/view/Main/PnfsChecker
   * http://t2.unl.edu/cms/storage/test_pfns.py/file_view
   * there are scripts in use at fnal.

---++++ Billing
There is detailed information about "billing" in the dCache book:
http://www.dcache.org/manuals/Book/cb-accounting.shtml

This is implemented as a cell in dCache, and we all have this installed by default. Unfortunately, none of us knows what to do with it.

There is the option to have the billing information stored in a DB, and there is a tool available to display this info from the DB.

The fnal version of that tool is at:
http://cmsdcam.fnal.gov:9090/lps/plots/src/plots2.lzx

The source, and deployment instructions will be available here:
https://plone4.fnal.gov/P0/DCache/dcache/webdcache
(Vladimir will update this page to provide the necessary info!)

---++++ Some info that people sent around after the meeting

Email from Dan Bradley to Ilya Kravchenko explaining the "WAN queue" they implemented
to improve WAN Xfer.

<pre>
What I meant by WAN I/O queues is this:

Each dCache pool has a limit on the number of active movers.  This is  in the pool setup file.  Example:

mover set max active 600

If you just configure it like the above line, than this applies to all  protocols, including dcap and gridftp.  
We are using pull-mode SRM copy  to write files to our pools, which are on the public net, so with max  
active movers of 600, this means we are saying it is ok to have 600  simultaneous gridftp transfers to 
each pool node (and each of those  transfers is using 20 streams!)  Of course, we weren't trying to run  
even 600 transfers all at the same time through phedex, but with even  40 simultaneous transfers going 
on, this still resulted in a single  nodes being overloaded, which resulted in transfer errors, timeouts,  etc.

We therefore needed to reconfigure dCache to limit the maximum number  of transfers to the same node 
to a reasonable level.  We also wanted to  improve the load balancing, because we found that with the 
settings we  were using, transfers were not being scheduled evenly across the  available machines.

To limit the maximum number of gridftp transfers to a node, we could  have simply set the max active 
movers to something lower, like 5.   However, in the past, we found we needed a much larger number 
of dcap  movers in order to support applications such as ORCA.  This is  precisely why the dCache folks 
had added the ability to create multiple  I/O queues, and channel different protocols to different queues.  
The  setup we are using is like this (in very abbreviated form):

pool setup file:
mover set max active 1000 -queue=default
mover set max active 5 -queue=WAN

pool.batch:
-io-queues=default,WAN \

srm-cms-dcache.batch:
-io-queue=WAN \

gridftpdoor-cms-dcache.batch:
-io-queue=WAN \

dcapdoor.batch:
-io-queue=default \


So that says to use a queue named "WAN" for gridftp transfers, and to  use a queue named "default" for dcap transfers.

To improve the load balancing problem, we did two things.  One was  simply to add a lot more nodes to the write pools.  
The other was to  set -spacecostfactor to 0 in PoolManager.conf, because during the load  test, it doesn't make sense to 
try to fill up nodes that are less full,  since the files are continually getting removed, creating a permanent  imbalance in spacecostfactor.

--Dan 
</pre>

---+++ New features coming within the next 6 months

---++++ gPLAZMA
   * [[%ATTACHURL%/gPlazma-Presentation.pdf][Deploying gPlazma dCache cell.]]
---++++ SRM v2.2
   * [[%ATTACHURL%/SRMTalk-DCache-Workshop-June17-2006.pdf][SRMTalk-DCache-Workshop-June17-2006.pdf]]: SRM V2.2 Implementation Status
   * [[%ATTACHURL%/SRMTransferExplained.pdf][Various Types of SRM Transfers from Network Point of View]]

Also, take a look at the new srm monitoring tool:
http://esepc21.fnal.gov:8080/srmwatch/

---++++ other dCache features 

---+++ Operations Support Tools
Discussion of things people need, and what rudiments of these exist today.
Ideally, we'd "self-organize", and commit to contributing some tools,
and have others that we ask for from OSG Extensions.

---++++ OSG Registration
CMS Tier2 sites are currently required to register their storage elements with the GOC.  This allows them to appear in the GridCat catalog.  It would be nice if more OSG storage elements were registered with the GOC.  Registration is as simple as filling out a web form:

http://www.opensciencegrid.org/index.php?option=com_wrapper&Itemid=68&elMenu=Grid%20Support

---++++ Alarms
Nagios setup for dCache monitoring (BNL, Wisconsin).
MonALISA alarms (Nebraska).

---++++ Integrity Checking
Work by Nebraska and UCSD already mentioned above under pnfs checking, and here again:
   * http://t2.unl.edu/cms/storage/test_pfns.py/file_view
   * http://hepuser.ucsd.edu/twiki/bin/view/Main/PnfsChecker

---++++ Performance Monitoring
Work by FNAL, DESY, Nebraska, others?

-- Main.FkW - 17 Jul 2006
-- Main.FkW - 04 Aug 2006


   * [[%ATTACHURL%/dcacheIntro-7-17-2006.ppt][dcacheIntro-7-17-2006.ppt]]: Layperson intro to dcache

   * [[%ATTACHURL%/SRMTransferExplained.pdf][SRMTransferExplained.pdf]]: Various Types SRM Transfers from Network Point of View


---++ Action Items

Following are the action items that were compiled from the Workshop.  They are listed in no particular oder, and include who was the
responsible person and what the status of the item is.


| *Action Item* | *Person* | *Status* |
| dCache configuration (hardware, software, etc) on the twiki site | All attendees | In Progress |
| Convert the dCache configuration information into a table | Frank W. & Jorge R. | |
| Add experience using the dcap door with order 1000's of transfers going to 1 door | Ilya | |
| Investigate adding Replica Manager Operational Info to dCache book | Alex K. | In Progress |
| Document deploying dCache on multiple servers | Rob K. & Iris Wu | |
| Document dCache components and info about them | Rob K | Done - https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions |
| Add action items to twiki | Eileen B | In Progress |
| Follow up on implementation of dcap server side timeouts | Rob K | Done - will be in coming dCache release |
| Send list of scripts, tools etc to contribute to Rob K | All attendees | |
| Add info on Brian's pnfs checker on twiki | Frank W | Done - linked in URL to UNL web page in twiki above |
| Add URL of tarball of tools/scripts  | Suresh | |
| Add link to get the dCache Monitoring Package | Vladimir P | Done |
| Send information dCache deployments being monitored by gridcat | Michael T | |

%META:FILEATTACHMENT{name="gPlazma-Presentation.pdf" attr="" autoattached="1" comment="" date="1153163628" path="gPlazma-Presentation.pdf" size="316082" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SRMTransferExplained.pdf" attr="" autoattached="1" comment="" date="1153154636" path="SRMTransferExplained.pdf" size="88832" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="ASR-dCache-tuning-p1.ppt" attr="" autoattached="1" comment="" date="1153445759" path="ASR-dCache-tuning-p1.ppt" size="67072" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="ASR-dCache-admin-p2.ppt" attr="" autoattached="1" comment="" date="1153445683" path="ASR-dCache-admin-p2.ppt" size="119296" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="SRMTalk-DCache-Workshop-June17-2006.pdf" attr="" autoattached="1" comment="" date="1153152912" path="SRMTalk-DCache-Workshop-June17-2006.pdf" size="76357" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006.ppt" attr="" autoattached="1" comment="" date="1153153475" path="dcacheIntro-7-17-2006.ppt" size="411136" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006" attr="" autoattached="1" comment="" date="1153145214" path="dcacheIntro-7-17-2006" size="407040" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dCache-ROCKS.ppt" attr="" autoattached="1" comment="" date="1153154025" path="dCache-ROCKS.ppt" size="443904" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="UCSD-dCache-topology-diagram.jpg" attr="" autoattached="1" comment="" date="1153445738" path="UCSD-dCache-topology-diagram.jpg" size="66668" user="Main.AbhishekSinghRana" version="2"}%
