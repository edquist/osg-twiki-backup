%META:TOPICINFO{author="FkW" date="1153588728" format="1.1" version="1.32"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! DcacheWorkshop for storage admins (July 17th 2006, FNAL)

%TOC%

---++ Where?
FCC2, i.e. second floor conference room in Feynman computing center @ FNAL.
Or to join virtually, follow these instructions:

If you will be joining by video, we will be using 
the ESnet ECS Ad-hoc bridge.
The number to dial in to connect by video is
88322243 for 88dcache at 348kps.

If you need to dial in by phone only, 
call 510-883-7860 then enter the Ad-hoc number 
88322243 for 88dcache followed by the # sign.

If you need to join by VRVS see instructions at
http://www-staff.es.net/~mikep/adhoc/vrvsecs.htm

'Storm' VRVS room has been reserved.

There's also a web page at the main OSG events calendar:
http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=146

---++ Agenda

---+++ Intro of Attendees
Please feel free to edit this twiki by adding in your site name, and dCache infrastructure deployed today, and planned for the future, say one year from now.

 <b>Abhishek Singh Rana & Frank Wuerthwein - UCSD </b>
   * <b>[[%ATTACHURL%/UCSD-dCache-topology-diagram.jpg][UCSD dCache Topology Diagram]]</b>
   * USCMS T2, dCache/SRM in production usage since early 2004.
   * 80+ worker nodes, 70+ have dCache pools, 0.2 - 1.7 TB per pool.
   * ~42TB currently, phased growth ongoing. 
   * ~11 dCache infrastructure nodes with various hardware configs.
   * 1 Core + 1 PNFS server & Mgr (DB) + 1 Replica Mgr (DB) + 1 DCap + 1 SRM (DB) + 6 GFTP.
   * <u>Only</u> Nodes on WAN (dual-homed): 1 Core + 1 SRM (DB) + 6 GFTP.
   * PNFS mounted <u>only</u> on infrastructure nodes, not mounted on worker nodes.
   * Multiple mover queues on pools - default LAN (dCap) + WAN (SRM & GFTP).
   * Network: 6 GFTP nodes on 10GbE, 2 of which with 2 GFTP doors each. 
   * Version: dcache-1.6.6-5 with April 2006 jar update from FNAL.
   * <nowiki>PostgreSQL 7.4.6, jdk1.5.0_01</nowiki>
   * Implicit Space reservation enabled (mid 2005). 
   * Replica Mgr in production usage (early 2004) - all pools are resilient. 
   * ext3 as filesystem. 
   * gPlazma non-GUMS RBAC mode in production usage (mid 2005).
   * Download transfer milestone - 13 TB/day FNAL->UCSD, 3rd party SRM, by <nowiki>PhEdEx</nowiki>.

 <b>Suresh Singh, Michael Thomas - Caltech</b>
   * USCMS Tier2 site, dCache used in production
   * ~60 worker nodes, all but a few are dCache pool nodes
   * 34 Tb, growing to ~60 later this year
   * 2 dCache infrastructure nodes
   * all workers on public address space
   * replication on (2 copies?)
   * second, smaller 4-node dCache installation used for development with <nowiki>LambdaStation</nowiki>

 <b>Preston Smith - Purdue</b>
   * USCMS Tier-2, dCache used in production
   * Version: dcache-1.6.6-5 with May10 2006 jar update from FNAL.
   * 28 TB currently
   * Pools live on large blocks of RAID storage (6 pool nodes).
   * 3 GridFTP doors
   * All nodes reside on public network 
   * No replication currently.
   * Upcoming expansion will add pools on worker nodes (with replication), in addition to additional RAID storage. 
   * 2 infrastructure nodes (pnfs on one node, srm/dcap/admin on 2nd)

<b>Dan Schrager - Indiana University</b>
   * Experimental dCache installation at http://bandit.uits.indiana.edu:2288/
   * SC4 (not production yet) dCache installation at http://tier2-d1.uchicago.edu:2288/

<b>Yujun Wu - Fermilab</b>
   * CMS T1 site, dCache/SRM used in production
   * Version: dCache 1.7.0
   *110TB of disk on 23 read/write pool nodes with 2 bonded GE interfaces. Each read/write pool node has multiple mover queues (LAN queue and WAN queueu);
   * 8 nodes with about 9 TB of space are dedicated as stage pool node area for restoring the tape-based data to disk;
   * We also have resilient dCache built with 500 worker nodes with a total space of 55 TB;
   * All nodes reside on public network;
   * 10 dCache infrastructure nodes (Core, dCap, SRM, PNFS, Replica Manager, Informaiton system, and Management);
   * PNFS is mounted on all the worker nodes;

<b>Zhengping(Jane) Liu, Yingzi(Iris) Wu --- Brookhaven National Lab</b>
   * USATLAS Tier1 site, dCache used in production
   * 150TB currently, phased growth ongoing
   * 350+ worker nodes (dCache pools)
   * 1 admin + 1 PNFS server + 4 dCap + 1 SRM + 5 GridFTP
   * HPSS as tape backend.
   * Network: 5 GFTP nodes with 2GFTP doors each on 10GbE, 
   * Version: dcache-1.6.6.5
   * ProsgreSQL 8.1.4, jdk 1.5.0_05
   * file system: ext3 in admin and read pool, xfs in write pool
   * Transfer - 7 TB/day 

 <b> Attendees: Please continue with details .. </b>


---+++ Layperson Intro to dCache
   * [[%ATTACHURL%/dcacheIntro-7-17-2006][dCache Introduction]]
     Goal here is to present a layperson's view of how dCache works.

---+++ Walkthrough of dCache installation (moved to afternoon)
[[http://www.atlasgrid.bnl.gov/workshop_dcache/install_note][Installation Instructions]]

---+++ dCache on Rocks
   * [[%ATTACHURL%/dCache-ROCKS.ppt][dCache on Rocks]]

---+++ dCache performance tuning and basics of admin interface
   * [[%ATTACHURL%/ASR-dCache-tuning-p1.ppt][Basics of Tuning]]
   * [[%ATTACHURL%/ASR-dCache-admin-p2.ppt][Basics of Admin Interface]]

---+++ dCache FAQ and discussion
Here comes your input. Ask whatever questions you want to from off the dCache people.

---++++ How to drain a pool?
You can either use the replicaManager or the CopyManager.
The latter is a relatively new feature.

   * Using the replica manager:
http://cmsdcam2.fnal.gov/dcache/resilient/Resilient_dCache_TroubleShooting.html

The replicaManager is repsonsible for managing desired number of replicas. It can be used ot drain a pool as follows:

Use dcache admin interface on the node that has the admin cell.
Then do something like:
<pre>
dcache> set dest replicaManager
dcache> set pool <pool name 1 > drainoff
dcache> set pool <pool name 2 > drainoff
</pre>

You then need to wait, and occasionally check progress:
<pre>
dcache> set dest replicaManager
dcache> ls unique <pool name 1>
dcache> ls unique <pool name 2>
</pre>

This returns the # of files that are not yet replicated, and thus still unique to pool 1 and pool 2. The pnfsid's of the files that are still unique to the pool(s) are listed in the replicaManager logfile.

Note: "ls unique" looks for files only in pools that are in the 
online state. E.g. other pools in drainoff state are not considered when
checking for uniqueness. You can thus put multiple pools into drainoff state at once without worrying about "ls unique" giving misleading answers.
 
If there are no unique files in the pools then you can take the pool out as follows:

<pre>
dcache> set pool <pool name 1> down
dcache> set pool <pool name 2> down
</pre>

If this doesn't work, complain to support@dcache.org .
Please refer in your email to this twiki page and OSG. 

---++++ What do all of the cells and domains do?
When  trying to debug dcache it is often very confusing because there are so many components. Without knowing what all these components do,
and what hardware resources they stress (cpu, memory, disk, etc.) it is hard to even know which logfile to look at when trying to debug something, and how to improve performance.

Action items:
   * provide a suggested hardware deployment scenario, or two, three.
   * provide a (complete) list of components, and a one paragraph description of what they do, and what hardware resources they stress.
   * provide a (complete) list of how each of us deploys dcache.


---++++ What do I do if I loose a file that still is in pnfs?
You need to rm the file from pnfs as well, and then retransfer the file.

---++++ Is there a way to configure dcap to have a timeout?
Yes there is. It's an option that has to be specified with dcap,
(dccp -o). This could also be hardcoded into libdcap.so and thus fixed
as a site characteristic. Ideally, one would want to have this as a serverside rather than a clientside timeout such that the admins deploying the server control it. As of now, a serverside timeout is not possible. Rob K. finding out more about this from DESY.

---++++ PNFS checking
There are various people who have developed scripts for either searching for lost files, or chksumming all files. There are at least three such systems:

   * http://hepuser.ucsd.edu/twiki/bin/view/Main/PnfsChecker
   * Brian's pnfschecker (fkw will check and link it in here)
   * there are scripts in use at fnal.

---++++ Billing
There is detailed information about "billing" in the dCache book:
http://www.dcache.org/manuals/Book/cb-accounting.shtml

This is implemented as a cell in dCache, and we all have this installed by default. Unfortunately, none of us knows what to do with it.

There is the option to have the billing information stored in a DB, and there is a tool available to display this info from the DB.

The fnal version of that tool is at:
http://cmsdcam.fnal.gov:9090/lps/plots/src/plots2.lzx

The source, and deployment instructions will be available here:
https://plone4.fnal.gov/P0/DCache/dcache/webdcache
(Vladimir will update this page to provide the necessary info!)

---+++ New features coming within the next 6 months

---++++ gPLAZMA
   * [[%ATTACHURL%/gPlazma-Presentation.pdf][Deploying gPlazma dCache cell.]]
---++++ SRM v2.2
   * [[%ATTACHURL%/SRMTalk-DCache-Workshop-June17-2006.pdf][SRMTalk-DCache-Workshop-June17-2006.pdf]]: SRM V2.2 Implementation Status
   * [[%ATTACHURL%/SRMTransferExplained.pdf][Various Types of SRM Transfers from Network Point of View]]

Also, take a look at the new srm monitoring tool:
http://esepc21.fnal.gov:8080/srmwatch/

---++++ other dCache features 

---+++ Operations Support Tools
Discussion of things people need, and what rudiments of these exist today.
Ideally, we'd "self-organize", and commit to contributing some tools,
and have others that we ask for from OSG Extensions.

---++++ OSG Registration
CMS Tier2 sites are currently required to register their storage elements with the GOC.  This allows them to appear in the GridCat catalog.  It would be nice if more OSG storage elements were registered with the GOC.  Registration is as simple as filling out a web form:

http://www.opensciencegrid.org/index.php?option=com_wrapper&Itemid=68&elMenu=Grid%20Support

---++++ Alarms
Nagios setup for dCache monitoring (BNL, Wisconsin).
MonALISA alarms (Nebraska).

---++++ Integrity Checking
Work by Nebraska and UCSD

---++++ Performance Monitoring
Work by FNAL, DESY, Nebraska, others?

-- Main.FkW - 17 Jul 2006
 


   * [[%ATTACHURL%/dcacheIntro-7-17-2006.ppt][dcacheIntro-7-17-2006.ppt]]: Layperson intro to dcache

   * [[%ATTACHURL%/SRMTransferExplained.pdf][SRMTransferExplained.pdf]]: Various Types SRM Transfers from Network Point of View

%META:FILEATTACHMENT{name="gPlazma-Presentation.pdf" attr="" autoattached="1" comment="" date="1153163628" path="gPlazma-Presentation.pdf" size="316082" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SRMTransferExplained.pdf" attr="" autoattached="1" comment="" date="1153154636" path="SRMTransferExplained.pdf" size="88832" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="ASR-dCache-tuning-p1.ppt" attr="" autoattached="1" comment="" date="1153445759" path="ASR-dCache-tuning-p1.ppt" size="67072" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="ASR-dCache-admin-p2.ppt" attr="" autoattached="1" comment="" date="1153445683" path="ASR-dCache-admin-p2.ppt" size="119296" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="SRMTalk-DCache-Workshop-June17-2006.pdf" attr="" autoattached="1" comment="" date="1153152912" path="SRMTalk-DCache-Workshop-June17-2006.pdf" size="76357" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006.ppt" attr="" autoattached="1" comment="" date="1153153475" path="dcacheIntro-7-17-2006.ppt" size="411136" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006" attr="" autoattached="1" comment="" date="1153145214" path="dcacheIntro-7-17-2006" size="407040" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dCache-ROCKS.ppt" attr="" autoattached="1" comment="" date="1153154025" path="dCache-ROCKS.ppt" size="443904" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="UCSD-dCache-topology-diagram.jpg" attr="" autoattached="1" comment="" date="1153445738" path="UCSD-dCache-topology-diagram.jpg" size="66668" user="Main.AbhishekSinghRana" version="2"}%
