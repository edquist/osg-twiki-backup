%META:TOPICINFO{author="AnneHeavey" date="1158084072" format="1.1" version="1.43"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! DcacheWorkshop for storage admins (July 17th 2006, FNAL)

%TOC%

---++ Where?
FCC2, i.e. second floor conference room in Feynman computing center @ FNAL.
Or to join virtually, follow these instructions:

If you will be joining by video, we will be using 
the ESnet ECS Ad-hoc bridge.
The number to dial in to connect by video is
88322243 for 88dcache at 348kps.

If you need to dial in by phone only, 
call 510-883-7860 then enter the Ad-hoc number 
88322243 for 88dcache followed by the # sign.

If you need to join by VRVS see instructions at
http://www-staff.es.net/~mikep/adhoc/vrvsecs.htm

'Storm' VRVS room has been reserved.

There's also a web page at the main OSG events calendar:
(old link: http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=146)
(new link: http:/www.opensciencegrid.org/cms/?pid=1000107)

---++ Agenda

---+++ Intro of Attendees
Please feel free to edit this twiki by adding in your site name, and dCache infrastructure deployed today, and planned for the future, say one year from now.

 <b>Abhishek Singh Rana & Frank Wuerthwein - UCSD </b>
   * <b>[[%ATTACHURL%/UCSD-dCache-topology-diagram.jpg][UCSD dCache Topology Diagram]]</b>
   * USCMS T2, dCache/SRM in production usage since early 2004.
   * 80+ worker nodes, 70+ have dCache pools, 0.2 - 1.7 TB per pool.
   * ~42TB currently, phased growth ongoing. 
   * ~11 dCache infrastructure nodes with various hardware configs.
   * 1 Core + 1 PNFS server & Mgr (DB) + 1 Replica Mgr (DB) + 1 DCap + 1 SRM (DB) + 6 GFTP.
   * <u>Only</u> Nodes on WAN (dual-homed): 1 Core + 1 SRM (DB) + 6 GFTP.
   * PNFS mounted <u>only</u> on infrastructure nodes, not mounted on worker nodes.
   * Multiple mover queues on pools - default LAN (dCap) + WAN (SRM & GFTP).
   * Network: 6 GFTP nodes on 10GbE, 2 of which with 2 GFTP doors each. 
   * Version: dcache-1.6.6-5 with April 2006 jar update from FNAL.
   * <nowiki>PostgreSQL 7.4.6, jdk1.5.0_01</nowiki>
   * Implicit Space reservation enabled (mid 2005). 
   * Replica Mgr in production usage (early 2004) - all pools are resilient. 
   * ext3 as filesystem. 
   * gPlazma non-GUMS RBAC mode in production usage (mid 2005).
   * Download transfer milestone - 13 TB/day FNAL->UCSD, 3rd party SRM, by <nowiki>PhEdEx</nowiki>.

 <b>Suresh Singh, Michael Thomas - Caltech</b>
   * USCMS Tier2 site, dCache used in production
   * ~60 worker nodes, all but a few are dCache pool nodes
   * 34 Tb, growing to ~60 later this year
   * 2 dCache infrastructure nodes
   * all workers on public address space
   * replication on (2 copies?)
   * second, smaller 4-node dCache installation used for development with <nowiki>LambdaStation</nowiki>

 <b>Preston Smith - Purdue</b>
   * USCMS Tier-2, dCache used in production
   * Version: dcache-1.6.6-5 with May10 2006 jar update from FNAL.
   * 28 TB currently
   * Pools live on large blocks of RAID storage (6 pool nodes).
   * 3 GridFTP doors
   * All nodes reside on public network 
   * No replication currently.
   * Upcoming expansion will add pools on worker nodes (with replication), in addition to additional RAID storage. 
   * 2 infrastructure nodes (pnfs on one node, srm/dcap/admin on 2nd)

<b>Dan Schrager - Indiana University</b>
   * Experimental dCache installation at http://bandit.uits.indiana.edu:2288/
   * SC4 (not production yet) dCache installation at http://tier2-d1.uchicago.edu:2288/

<b>Yujun Wu - Fermilab</b>
   * CMS T1 site, dCache/SRM used in production
   * Version: dCache 1.7.0
   *110TB of disk on 23 read/write pool nodes with 2 bonded GE interfaces. Each read/write pool node has multiple mover queues (LAN queue and WAN queueu);
   * 8 nodes with about 9 TB of space are dedicated as stage pool node area for restoring the tape-based data to disk;
   * We also have resilient dCache built with 500 worker nodes with a total space of 55 TB;
   * All nodes reside on public network;
   * 10 dCache infrastructure nodes (Core, dCap, SRM, PNFS, Replica Manager, Informaiton system, and Management);
   * PNFS is mounted on all the worker nodes;

<b>Zhengping(Jane) Liu, Yingzi(Iris) Wu --- Brookhaven National Lab</b>
   * USATLAS Tier1 site, dCache used in production
   * 150TB currently, phased growth ongoing
   * 350+ worker nodes (dCache pools)
   * 1 admin + 1 PNFS server + 4 dCap + 1 SRM + 5 GridFTP
   * HPSS as tape backend.
   * Network: 5 GFTP nodes with 2GFTP doors each on 10GbE, 
   * Version: dcache-1.6.6.5
   * ProsgreSQL 8.1.4, jdk 1.5.0_05
   * file system: ext3 in admin and read pool, xfs in write pool
   * Transfer - 7 TB/day 

<b>Dan Bradley - UWMadisonCMS</b>
   * CMS T2 site, dCache used in production
   * 36TB Xserve RAID + 48TB/2 resilient
   * 6 dCache infrastructure nodes: pnfs, core+httpd+admin, SRM, 3 dcap doors (500 logins each), gridftp, replicaManager
   * 18 2TB Xserve RAID pools
   * 48 1TB resilient pool nodes
   * pnfs is only mounted on infrastructure nodes
   * all nodes on public network
   * dCache 1.6.6-5 with jar update May 10/2006 (Jon's jars)
   * PosgreSQL 7.4.6, jdk 1.5.0_07
   * Multiple mover queues on pools - 1000 default LAN (dCap) + 5 WAN (SRM & GFTP).
   * ext3 filesystem on resilient pools, jfs on Xserves (considering change to xfs)
   * Download transfer milestone - 15 TB/day FNAL->Wisconsin, 3rd party SRM, by <nowiki>PhEdEx</nowiki>.

<b>Anand Padmanabhan - UIowa </b>
   * Experimental dCache Installation
   * 2 dCache Infrastructure nodes ( 1 admin+pnfs+pool + 1 pool+gsiftp-door+srm-door)
   * Currently no replication
   * Version: dcache-1.6.6.5, ProsgreSQL 8.1.4, jdk 1.5.0_07
   * http://rtgrid1.its.uiowa.edu:2288/

<b>Ilya Kravchenko - MIT </b>
   *  USCMS Tier-2 site, dCache is the storage
   *  Version: dcache-1.6.6-5 with May10 2006 jar update from FNAL
   * total storage: ~35 TB
      * 18 worker nodes have 1.7 TB/node in Raid5 (physically 300 GB x8, of which one redudant and one spare)
      * one disk server with 5.4 TB in Raid5 (physically 300 GB x24, minus spares and redundant)
   * the breakdown of services and systems:
      * one Compute Element, runs OSG stuff
      * one Storage Element, runs SRM 
      * one dCache admin node, runs core services
      * one node with 8 GB of memory dedicated to a GridFTP door
      * one node is a web server for Ganglia, etc other monitoring
      * worker nodes (note: Tier2, Tier3 and "nodes of opportunity) all integrated into one pool:
         * 26 dual dual-core opterons
         * 13 dual opterons
         * about 140 assorted Pentiums from 1GHz and up
   * pnfs mounted on all worker nodes
   * all dCache pools are on private network
   * on public network (dual-homed) CE, SE, GridFTP door node, web server
   * no ReplicaManager

 <b> Attendees: Please continue with details .. </b>


---+++ Layperson Intro to dCache
   * [[%ATTACHURL%/dcacheIntro-7-17-2006][dCache Introduction]]
     Goal here is to present a layperson's view of how dCache works.

---+++ Walkthrough of dCache installation (moved to afternoon)
[[http://www.atlasgrid.bnl.gov/workshop_dcache/install_note][Installation Instructions]]

---+++ dCache on Rocks
   * [[%ATTACHURL%/dCache-ROCKS.ppt][dCache on Rocks]]

---+++ dCache performance tuning and basics of admin interface
   * [[%ATTACHURL%/ASR-dCache-tuning-p1.ppt][Basics of Tuning]]
   * [[%ATTACHURL%/ASR-dCache-admin-p2.ppt][Basics of Admin Interface]]

---+++ dCache FAQ and discussion
Here comes your input. Ask whatever questions you want to from off the dCache people.

---++++ How to drain a pool?
You can either use the replicaManager or the CopyManager.
The latter is a relatively new feature.

   * Using the replica manager:
http://cmsdcam2.fnal.gov/dcache/resilient/Resilient_dCache_TroubleShooting.html

   * replica manager documentation at UNL:
http://t2.unl.edu/documentation/dcache_replicas

The replicaManager is repsonsible for managing desired number of replicas. It can be used ot drain a pool as follows:

Use dcache admin interface on the node that has the admin cell.
Then do something like:
<pre>
dcache> set dest replicaManager
dcache> set pool <pool name 1 > drainoff
dcache> set pool <pool name 2 > drainoff
</pre>

You then need to wait, and occasionally check progress:
<pre>
dcache> set dest replicaManager
dcache> ls unique <pool name 1>
dcache> ls unique <pool name 2>
</pre>

This returns the # of files that are not yet replicated, and thus still unique to pool 1 and pool 2. The pnfsid's of the files that are still unique to the pool(s) are listed in the replicaManager logfile.

Note: "ls unique" looks for files only in pools that are in the 
online state. E.g. other pools in drainoff state are not considered when
checking for uniqueness. You can thus put multiple pools into drainoff state at once without worrying about "ls unique" giving misleading answers.
 
If there are no unique files in the pools then you can take the pool out as follows:

<pre>
dcache> set pool <pool name 1> down
dcache> set pool <pool name 2> down
</pre>

If this doesn't work, complain to support@dcache.org .
Please refer in your email to this twiki page and OSG. 

<pre>
Abhishek tried this after the workshop, and found that it didn't quite work as advertized.
Feedback went back to Alex so he can resolve the issues.
Will update this here once resolved.

fkw 8/4/06
</pre>

---++++ What do all of the cells and domains do?
When  trying to debug dcache it is often very confusing because there are so many components. Without knowing what all these components do,
and what hardware resources they stress (cpu, memory, disk, etc.) it is hard to even know which logfile to look at when trying to debug something, and how to improve performance.

Action items:
   * provide a suggested hardware deployment scenario, or two, three.
   * provide a (complete) list of components, and a one paragraph description of what they do, and what hardware resources they stress. This is now done at: https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions
   * provide a (complete) list of how each of us deploys dcache.


---++++ What do I do if I loose a file that still is in pnfs?
You need to rm the file from pnfs as well, and then retransfer the file.

---++++ Is there a way to configure dcap to have a timeout?
Yes there is. It's an option that has to be specified with dcap,
(dccp -o). This could also be hardcoded into libdcap.so and thus fixed
as a site characteristic. Ideally, one would want to have this as a serverside rather than a clientside timeout such that the admins deploying the server control it. As of now, a serverside timeout is not possible. Rob K. finding out more about this from DESY.

---++++ PNFS checking
There are various people who have developed scripts for either searching for lost files, or chksumming all files. There are at least three such systems:

   * http://hepuser.ucsd.edu/twiki/bin/view/Main/PnfsChecker
   * http://t2.unl.edu/cms/storage/test_pfns.py/file_view
   * there are scripts in use at fnal.

---++++ Billing
There is detailed information about "billing" in the dCache book:
http://www.dcache.org/manuals/Book/cb-accounting.shtml

This is implemented as a cell in dCache, and we all have this installed by default. Unfortunately, none of us knows what to do with it.

There is the option to have the billing information stored in a DB, and there is a tool available to display this info from the DB.

The fnal version of that tool is at:
http://cmsdcam.fnal.gov:9090/lps/plots/src/plots2.lzx

The source, and deployment instructions will be available here:
https://plone4.fnal.gov/P0/DCache/dcache/webdcache
(Vladimir will update this page to provide the necessary info!)

---++++ Some info that people sent around after the meeting

Email from Dan Bradley to Ilya Kravchenko explaining the "WAN queue" they implemented
to improve WAN Xfer.

<pre>
What I meant by WAN I/O queues is this:

Each dCache pool has a limit on the number of active movers.  This is  in the pool setup file.  Example:

mover set max active 600

If you just configure it like the above line, than this applies to all  protocols, including dcap and gridftp.  
We are using pull-mode SRM copy  to write files to our pools, which are on the public net, so with max  
active movers of 600, this means we are saying it is ok to have 600  simultaneous gridftp transfers to 
each pool node (and each of those  transfers is using 20 streams!)  Of course, we weren't trying to run  
even 600 transfers all at the same time through phedex, but with even  40 simultaneous transfers going 
on, this still resulted in a single  nodes being overloaded, which resulted in transfer errors, timeouts,  etc.

We therefore needed to reconfigure dCache to limit the maximum number  of transfers to the same node 
to a reasonable level.  We also wanted to  improve the load balancing, because we found that with the 
settings we  were using, transfers were not being scheduled evenly across the  available machines.

To limit the maximum number of gridftp transfers to a node, we could  have simply set the max active 
movers to something lower, like 5.   However, in the past, we found we needed a much larger number 
of dcap  movers in order to support applications such as ORCA.  This is  precisely why the dCache folks 
had added the ability to create multiple  I/O queues, and channel different protocols to different queues.  
The  setup we are using is like this (in very abbreviated form):

pool setup file:
mover set max active 1000 -queue=default
mover set max active 5 -queue=WAN

pool.batch:
-io-queues=default,WAN \

srm-cms-dcache.batch:
-io-queue=WAN \

gridftpdoor-cms-dcache.batch:
-io-queue=WAN \

dcapdoor.batch:
-io-queue=default \


So that says to use a queue named "WAN" for gridftp transfers, and to  use a queue named "default" for dcap transfers.

To improve the load balancing problem, we did two things.  One was  simply to add a lot more nodes to the write pools.  
The other was to  set -spacecostfactor to 0 in PoolManager.conf, because during the load  test, it doesn't make sense to 
try to fill up nodes that are less full,  since the files are continually getting removed, creating a permanent  imbalance in spacecostfactor.

--Dan 
</pre>

---++++ Running with thousands of transfers per DCap door

   At the MIT Tier2 we run the old piece of CMS software called ORCA. A single ORCA executable might open
large number of files that it picks up from dCache here. Sometimes, n*10 files. The access is via the DCap door.
We have noticed that with  large enough number of ORCA jobs running in parallel we sometimes went over the 
max number of movers on a pool or several, which caused everything to get stuck and the jobs had to be killed. 
At that time we had about 20 pools with 100 max movers on each. We increased max movers on each pool to 600,
and still kept only one door. After that we had no problems, and I routinely observed 1800-2000 active transfers,
all going through the DCap door. One should note, however, that in most of these transfers the data flow rate
is rather low. ORCA jobs in almost all files need only a few tens of kB per second.

---+++ New features coming within the next 6 months

---++++ gPLAZMA
   * [[%ATTACHURL%/gPlazma-Presentation.pdf][Deploying gPlazma dCache cell.]]
---++++ SRM v2.2
   * [[%ATTACHURL%/SRMTalk-DCache-Workshop-June17-2006.pdf][SRMTalk-DCache-Workshop-June17-2006.pdf]]: SRM V2.2 Implementation Status
   * [[%ATTACHURL%/SRMTransferExplained.pdf][Various Types of SRM Transfers from Network Point of View]]

Also, take a look at the new srm monitoring tool:
http://esepc21.fnal.gov:8080/srmwatch/

---++++ other dCache features 

---+++ Operations Support Tools
Discussion of things people need, and what rudiments of these exist today.
Ideally, we'd "self-organize", and commit to contributing some tools,
and have others that we ask for from OSG Extensions.

---++++ OSG Registration
CMS Tier2 sites are currently required to register their storage elements with the GOC.  This allows them to appear in the GridCat catalog.  It would be nice if more OSG storage elements were registered with the GOC.  Registration is as simple as filling out a web form:

http://www.opensciencegrid.org/index.php?option=com_wrapper&Itemid=68&elMenu=Grid%20Support (old link)%BR%
%RED%New one not available; see link label "Go register your Storage Element with OSG" under the heading "Publishing your SE" on /Documentation/StorageElementAdmins.  Should it be the ivdgl one: http://osg.ivdgl.org/twiki/bin/view/Operations/RegistrationInstructions?
%ENDCOLOR%

More info Abhishek sent out after the workshop. It's clear that we need to provide step-by-step instructions
in the future. Not sure if they presently exist anywhere. So this is the best fkw knows of right now:
<pre>

[0] Configure SE making sure SRM/GFTP over WAN work. Add storage 
directories (in PNFS) for user 'mis' (Production GridCAT) and user 
'ivdgl' (ITB GridCAT). Find out which DNs/FQANs will be used by 
GridCAT to test transfers, and add them to be authorized r/w on 
corresponding directories. Configure supplementary components*.

[1] Fill in OSG's Resource registration forms.     
(http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=29&elMenu=Grid%20Support) (old)%BR%
(http://osg.ivdgl.org/twiki/bin/view/Operations/RegistrationInstructions)

[2] GridCAT uses LDAP queries (likely with PHP API) to get information 
about SE. For this, we need LDAP server(s) at the site. This is taken 
care of by installing GRIS and BDII (in VDT) and further configuring 
information provider scripts that are run by LDAP servers to pull 
information to be sent to LDAP/SRM/GFTP clients (at GridCAT end). This 
is taken care of by configuring GIP (in VDT). Please note that all 
these will usually reside on host with site's CE.  Configure 
supplementary components*.

[3] Test 'ldapsearch' query to make sure correct LDIF attributes are 
present with correct value for your site's SE.

[3] Some GridCAT tables are static, and thus email may need to be send 
to John Rosheck (jrosheck@indiana.edu) or GOC 
(goc@opensciencegrid.org).

[4] Thereafter, GridCAT will periodically query GRIS/BDII, GIP will be 
executed and will send SE info to GridCAT. GridCAT will then extract 
attributes relevant to forming an 'srmcp' or 'globus-url-copy' query. 
These will be usually host, port, PNFS directories, supported 
protocols. etc.

[5] SE will show up green in GridCAT!

* An SRM/dCache information provider to remotely query PNFS space 
usage (du-like feature) was getting ready, FNAL and UIOWA were working 
on integrating it with GIP. I am not aware of its recent status.

regards,
Abhishek

</pre>

---++++ Alarms
Nagios setup for dCache monitoring (BNL, Wisconsin).
MonALISA alarms (Nebraska).

---++++ Integrity Checking
Work by Nebraska and UCSD already mentioned above under pnfs checking, and here again:
   * http://t2.unl.edu/cms/storage/test_pfns.py/file_view
   * http://hepuser.ucsd.edu/twiki/bin/view/Main/PnfsChecker

---++++ Performance Monitoring
Work by FNAL, DESY, Nebraska, others?

-- Main.FkW - 17 Jul 2006
-- Main.FkW - 04 Aug 2006


   * [[%ATTACHURL%/dcacheIntro-7-17-2006.ppt][dcacheIntro-7-17-2006.ppt]]: Layperson intro to dcache

   * [[%ATTACHURL%/SRMTransferExplained.pdf][SRMTransferExplained.pdf]]: Various Types SRM Transfers from Network Point of View


---++ Action Items

Following are the action items that were compiled from the Workshop.  They are listed in no particular oder, and include who was the
responsible person and what the status of the item is.


| *Action Item* | *Person* | *Status* |
| dCache configuration (hardware, software, etc) on the twiki site | All attendees | In Progress |
| Convert the dCache configuration information into a table | Frank W. & Jorge R. | |
| Add experience using the dcap door with order 1000's of transfers going to 1 door | Ilya | |
| Investigate adding Replica Manager Operational Info to dCache book | Alex K. | In Progress |
| Document deploying dCache on multiple servers | Rob K. & Iris Wu | |
| Document dCache components and info about them | Rob K | Done - https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions |
| Add action items to twiki | Eileen B | In Progress |
| Follow up on implementation of dcap server side timeouts | Rob K | Done - will be in coming dCache release |
| Send list of scripts, tools etc to contribute to Rob K | All attendees | |
| Add info on Brian's pnfs checker on twiki | Frank W | Done - linked in URL to UNL web page in twiki above |
| Add URL of tarball of tools/scripts  | Suresh | |
| Add link to get the dCache Monitoring Package | Vladimir P | Done |
| Send information dCache deployments being monitored by gridcat | Michael T | |

%META:FILEATTACHMENT{name="gPlazma-Presentation.pdf" attr="" autoattached="1" comment="" date="1153163628" path="gPlazma-Presentation.pdf" size="316082" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SRMTransferExplained.pdf" attr="" autoattached="1" comment="" date="1153154636" path="SRMTransferExplained.pdf" size="88832" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="ASR-dCache-tuning-p1.ppt" attr="" autoattached="1" comment="" date="1153445759" path="ASR-dCache-tuning-p1.ppt" size="67072" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="ASR-dCache-admin-p2.ppt" attr="" autoattached="1" comment="" date="1153445683" path="ASR-dCache-admin-p2.ppt" size="119296" user="Main.AbhishekSinghRana" version="5"}%
%META:FILEATTACHMENT{name="SRMTalk-DCache-Workshop-June17-2006.pdf" attr="" autoattached="1" comment="" date="1153152912" path="SRMTalk-DCache-Workshop-June17-2006.pdf" size="76357" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006.ppt" attr="" autoattached="1" comment="" date="1153153475" path="dcacheIntro-7-17-2006.ppt" size="411136" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dcacheIntro-7-17-2006" attr="" autoattached="1" comment="" date="1153145214" path="dcacheIntro-7-17-2006" size="407040" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="dCache-ROCKS.ppt" attr="" autoattached="1" comment="" date="1153154025" path="dCache-ROCKS.ppt" size="443904" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="UCSD-dCache-topology-diagram.jpg" attr="" autoattached="1" comment="" date="1153445738" path="UCSD-dCache-topology-diagram.jpg" size="66668" user="Main.AbhishekSinghRana" version="2"}%
