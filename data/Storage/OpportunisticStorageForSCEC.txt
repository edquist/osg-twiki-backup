%META:TOPICINFO{author="TedHesselroth" date="1256235744" format="1.1" reprev="1.11" version="1.11"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*

---+++++ Purpose

The use of storage in the OSG by the [[http://www.scec.org][Southern California Earthquake Center]] is another step towards fully-automated open storage. Besides actually providing storage and compute resources to SCEC, the goal of this project is to exercise some of the new supporting infrastructure for opportunistic storage, namely, the discovery tools. As such, some managed coordination is needed for the initial production.

---+++++ The Use Case

Southern California Earthquake Center (SCEC) : SCEC’s goal is to understand the physics of the Southern California fault system and develop a model of key aspects of earthquake behavior. One element of SCEC is the CyberSHAke project , which generates probabilistic seismic hazard assessments (PSHA). These are effectively sets of simulated seismograms for the response of a point on the earth to a very large set (~600,000) of potential ruptures (earthquakes). For each seismogram, a peak ground motion can be calculated. This can then be used to gener-ate a probabilistic peak ground motion over the set of potential ruptures.

A reciprocity-based approach is used for this work. This approach has three steps. 
   * 1. Run for-ward wave propagation simulations for a given volume.
   * 2. Extract strain green tensor data for the ruptures that affect a given site.
   * 3. Generate synthetic seismograms for ruptures that affect the given site.

Step 1 is done at USC once. Steps 2 and 3 are done for each site of interest, and are represented as workflows. The workflow for step 2 is 10-15 nodes, with two nodes running 400-cores MPI tasks. The workflow for step 3 is ~800,000 single processors tasks.
Both are currently run on TeraGrid. We will run the first on TeraGrid, and run the second on OSG. Using both CIs should allow more science to be done in the same amount of time with the same amount of human effort. This is the most simple use of both CIs; it does not require new technologies, but does require that CIs actually work together, and this application will be used to ensure that they do work together for an actual user. Specifically, we will port the working code from the SCEC production system to a test system on just the TG, port the step 3 workflow to the OSG, stage the initial large data set to OSG (generated in step 1, does not change from one PHSA curve to another), stage the smaller data that the step 2 workflow produces to OSG (uniquely for each PHSA curve), and determine how to move the overall output from step 3 on OSG into a test database (standing in for the SCEC production database). Other issues that will be solved include authentication, authorization, and accounting.

Maechling, P., Gupta, V., Gupta, N., Field, E. H., Okaya, D., Jordan, T. H., “Grid Computing in the SCEC Com-munity Modeling Environment”, Seismological Research Letters, v. 76, pp. 581-587, 2005. 

---+++++ Summary
The SCEC use case is summarized by

   * 800,000 files (~1 Terabyte of data) from step uploaded to SE once, then read by jobs
   * Smaller number of files from step 2, uploaded to SE, transient
   * 800,000 jobs on CE, equals one run
   * Data is read in job context by posix IO
      * Size per read:
      * Read proximities (i.e., are subsequent reads "near" each other such that caching will be effective):
   * After run, deletion of transient files
   * After run, upload from SE to SCEC test database
   * After project, deletion of 800,000 files from SE
   

The files are in 20 tarred packages. Data placement is done by uploading a tar file to the SE, then unpacking it into the SE by a job running on the computing element. Then consensus from experts is that 700,000 files will not present a problem to Storage Elements, though it is estimated that in a Hadoop installation it will result in an additional memory consumption of 700 MB in the Namenode. We also conclude that dCache would not reliably support posix IO without significant changes to the SCEC codebase; therefore we will not pursue participation by dCache sites for this project.

---+++++ Initial Participating Sites

These sites have agreed to support SCEC

   * Purdue
   * Caltech
   * University of Oklahoma
   * University of Nebraska at Omaha
   * University of Florida (tentative)

The plan is for Purdue to host the proof-of-concept tests and early production, then expansion to production on all the above sites, followed by expansion to other sites in the OSG. The VO to be used is initially the "engage" VO, though at a later point that may be changed to a "scec" VO. The following table shows the status of the project per-site. The table will be maintained by Ted Hesselroth, but site administrators may edit it themselves if they wish.


---++ Key

| *Symbol* | *Meaning* |
| %ICON{led-box-yellow}%  | Partial |
| %ICON{led-box-green}% | Complete |
| %ICON{led-box-red}% | Issue |
| %ICON{led-box-gray}% | Note |

---++ Workflow Checklist

%TABLE{ headerbg="#eeeeee" headercolor="#000000" databg="#ffffff" tableborder="1" columnwidths="120," cellpadding="2" cellspacing="1" dataalign="left" valign="top" sort="off"}%


| *Site*  | *Participation Confirmed* | *SE authz* | *CE authz* | *BDII* | *SRM Copy* | *Space Res* | *First Upload* | *First CE/SE Test* | *Full Upload* | *Full CE/SE Test* | *Site Production*  | *Grid Production* | *VO change* | *Closure* |
| [[http://tiny.cc/5zqNy][Purdue]] | <p>%ICON{led-box-green}% </p> | | | | | | | | | | | | | |
| [[http://tiny.cc/W1A6i][Caltech]] | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | | <p>%ICON{led-box-green}% </p> | | | | | | | | | | |
| [[http://tiny.cc/Yzglz][U of Oklahoma]] | <p>%ICON{led-box-green}% </p> | | | | | | | | | | | | | |
| [[http://tiny.cc/E7s5m][U of Nebraska-Omaha]] | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p>  | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | <p>NA</p> | | | | | | | | |
| [[http://tiny.cc/hgqaq][U of Florida]] | <p>%ICON{led-box-green}% </p> | | | | | | | | | | | | | |

---+++++ Notes on Workflow Checklist Table

   * SE Authz is authorization on the Storage Element for the "engage" VO, by the means provided by the site's particular SRM implementation. SE authorization for space reservation by "engage" VO is encouraged.
   * CE Authz is authorization on the Storage Element for the "engage" VO.
   * Space reservation should be made by "engage" VO unless prohibited by site policy. The cell will be marked "NA" for sites that allow SCEC use without a space reservation.
   * BDII is the value of "BDII Check" of the BDII Checklist table.
   * First Upload and First CE Test refer to the proof-of-concept test using a single SCEC tarfile.
   * Full Upload and Full CE Test refer to realistic jobs using all SCEC tarfiles.
   * Site Production is an optional intermediate step of production by direct submission to the site
   * Grid Production is production by jobs submitted to the OSG Resource Selection Service, to be run on any of the sites which host SCEC data.
   * VO Change is support of a potential "scec" VO, rather than "engage".
   * Closure means that no additional steps are needed by the site. A link to a site's summary or feedback on the project may be placed here.

---++++ BDII Checklist

#BDIIChecklistTable
%TABLE{ headerbg="#eeeeee" headercolor="#000000" databg="#ffffff" tableborder="1" columnwidths="120," cellpadding="2" cellspacing="1" dataalign="left" valign="top" sort="off"}%


| *Site*  | *SE FQAN* | *VO Path* | *SRM Endpoint* | *Available Space* | *SE Mount*  | *Space Token* | *BDII Check* |
| [[http://tiny.cc/5zqNy][Purdue]] |  | | | | | | |
| [[http://tiny.cc/W1A6i][Caltech]] | cit-se2.ultralight.org | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | | | <p>%ICON{led-box-green}% </p> |
| [[http://tiny.cc/Yzglz][U of Oklahoma]] |  | | | | | | |
| [[http://tiny.cc/E7s5m][U of Nebraska-Omaha]] | ff-srm.unl.edu | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | <p>%ICON{led-box-green}% </p> | | |<p>%ICON{led-box-green}% </p> |
| [[http://tiny.cc/hgqaq][U of Florida]] | srmb.ihepa.ufl.edu | | | | | | |

---+++++ Notes on BDII Checklist Table

   * SE FQDN is taken from the service URI on <nop>MyOSG. Click on the site name in the table to view the <nop>MyOSG entry. Needed for checks of BDII info.
   * The columns for VO Path, SRM Endpoint, SE Mount, Available Space and Space Token refer to entries in the OSG BDII.
   * VO Path publishes the rootpath under which the "engage" VO is allowed to read and write.
   * SRM Endpoint is the URI of the storage web service.
   * Available Space indicates that there is enough space for a 1 TB space reservation to be made.
   * SE Mount tells whether the SE is accessible from worker nodes via a mount point. Optional until supported by OSG GIP.
   * Space Token is used by clients to access the reservation. Optional until Space Reservation from Workflow is finished.
   * BDII check will be done by Ted Hesselroth with the OSG Storage Discovery Tool. The tool will then be used in the steps leading up to production. When BDII check is complete, value is copied to BDII column of Workflow Checklist table.

---+++++ BDII Check XPath Expressions

FQDN is defined in the BDII Checklist table.

---++++++ VO Path
<verbatim>
//GlueSE[@GlueSEUniqueID='$FQDN']/GlueSA/GlueVOInfo[@GlueVOInfoAccessControlBaseRule=$vo]/@GlueVOInfoPath
</verbatim>

where $vo is the name of the VO, i.e., 'engage'. The name should be surrounded by single quotes in XPath use.

---++++++ SRM Endpoint
<verbatim>
//GlueSE[@GlueSEUniqueID='$FQDN']/GlueSEControlProtocol[@GlueSEControlProtocolVersion='2.2.0']/@GlueSEControlProtocolEndpoint
</verbatim>

---++++++ SRM Service Endpoint

The SRM service endpoint is defined as the above SRM endpoint with "httpg" replaced by "srm".

---++++++ Available Space
<verbatim>
//GlueSE[@GlueSEUniqueID='$FQDN']/GlueSA/@GlueSAFreeOnlineSize
</verbatim>

---++++++ SE Mount
<verbatim>
//GlueCESEBind[@GlueCESEBindSEUniqueID='$FQDN']/@GlueCESEBindCEAccesspoint
or
//GlueCESEBind[@GlueCESEBindSEUniqueID='$FQDN']/@GlueCESEBindMountInfo
</verbatim>

and in <nop>GlueSE

<verbatim>
//GlueSE[@GlueSEUniqueID='$FQDN']/GlueSEAccessProtocol[@GlueSEAccessProtocolType='file']
</verbatim>

---++++++ Space Token
<verbatim>
//GlueSE[@GlueSEUniqueID='$FQDN']/GlueSA/GlueVOInfo[@GlueVOInfoAccessControlBaseRule=$vo]/@GlueVOInfoTag
</verbatim>



For information on the use of these paths in the discovery tool to be used, see [[https://twiki.grid.iu.edu/bin/view/Storage/OSGStorageDiscoveryTool][OSG Storage Discovery Tool]].

-- Main.TedHesselroth - 08 Jul 2009
