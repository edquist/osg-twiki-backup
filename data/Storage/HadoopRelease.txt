%META:TOPICINFO{author="TedHesselroth" date="1270499009" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Building Hadoop

Note: You DO NOT need to use this page if you are only deploying Hadoop.

We currently use the NMI build and test system to trigger builds for a variety of platforms.

   1. Prereqs: Java 1.6, ant, svn client on the build system
   1. Download and unpack the Hadoop source code.
      * We currently use the 0.19.1 tag:
      <verbatim> svn co http://svn.apache.org/repos/asf/hadoop/core/tags/release-0.19.1/ hadoop </verbatim>
   1. Source an existing VDT install which includes Ant and JDK (do a "ls" on the $VDT_LOCATION and make sure there are "ant" and "jdk1.5" directories).
   1. Set the following variables:
      1. HADOOP_HOME=freshly unpacked source
      1. CLASSPATH variables:
      <verbatim>export CLASSPATH=$HADOOP_HOME/hadoop-0.19.0-core.jar:$HADOOP_HOME/lib/commons-logging-1.0.4.jar:$HADOOP_HOME/lib/commons-logging-api-1.0.4.jar:$HADOOP_HOME/lib/log4j-1.2.15.jar:$CLASSPATH</verbatim>
      1. Library variables:
      <verbatim>export LD_LIBRARY_PATH=$HADOOP_HOME/build/libhdfs:$VDT_LOCATION/jdk1.5/jre/lib/amd64/server:$LD_LIBRARY_PATH</verbatim>
      1. Path variables:
         <verbatim>export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/src/contrib/fuse-dfs/src:$PATH</verbatim>
   1. Patch Hadoop as necessary.  The patches we use are listed below.
   1. (Only on 64-bit nodes).  Edit $HADOOP_HOME/src/c++/libhdfs/Makefile; replace all occurrences of -m32 with -m64.
   1. Export misc. build variables:
      <verbatim>
      export PERMS=1
      export FUSE_HOME=$VDT_LOCATION/fuse
      </verbatim>
      Otherwise, fuse-dfs will not build.
   1. Build Hadoop:
      <verbatim>ant compile-contrib -Dlibhdfs=1 -Dfusedfs=1 jar</verbatim>
      This requires automake >= 1.9.5, which IS NOT AVAILABLE on RHEL4 (for Nebraska builders: this is located in /usr on node001.  Build there.).  I had to download and install it from source, then add /usr/local/bin to the PATH and /usr/local/lib to the LD_LIBRARY_PATH.  You can test your automake version with "automake --version".
   1. Fix link, build/libhdfs/libhdfs.so, to not be absolute.  I 'cd' to the directory $HADOOP_HOME/build/libhdfs, rm the existing libhdfs.so, then perform 'ln -s libhdfs.so.1 libhdfs.so'
   1. cd $HADOOP_HOME/..   Then, make a copy of the entire directory, hadoop-0.x.x/, to hadoop/.  Finally, issue the tar command:
      <verbatim>tar zcf hadoop-0.x.x-RHELy-zzz.tar.gz hadoop/</verbatim>
      Replace x.x with the Hadoop version number; replace y with the RHEL release (4 or 5), and zzz with the platform (i686 or x86_64).
   1. Copy the resulting tarball into t2.unl.edu:/var/www/html/cache.

---+++ Patches we apply to Hadoop

   * HADOOP-4368: https://issues.apache.org/jira/secure/attachment/12399212/hadoop4368.fsstatus.v7.patch (Fixes DF for FUSE)
   * HADOOP-4675: http://osg-test4.unl.edu/store/patches/HADOOP-4675.patch (Fixes Ganglia 3.1)
   * HADOOP-5222: https://issues.apache.org/jira/secure/attachment/12400340/clienttrace-1.patch (Improves client logging)
   * HADOOP-5551: https://issues.apache.org/jira/secure/attachment/12403752/HADOOP-5551-v3.patch (Prevent directory destruction)
   * HADOOP-5579: https://issues.apache.org/jira/secure/attachment/12403721/HADOOP-5579.patch (Propagate the AccessControlException to libhdfs and set errno appropriately)

---++ Building GridFTP-HDFS
Again, we build GridFTP-HDFS using the NMI build and test system to build for all the supported variants.

   1. Pre-requisites:
      1. Valid Hadoop, preferably installed via the UNL pacman cache.
      1. subversion RPM package providing the standard svn client.
   1. Use pacman to pull in the Globus GridFTP SDK:
      <verbatim>
      pacman -get http://vdt.cs.wisc.edu/vdt_1101_cache:Globus-Base-Data-Server
      pacman -get http://vdt.cs.wisc.edu/vdt_1101_cache:Globus-Base-SDK
      </verbatim>
   1. Source the VDT's setup.sh.
      * Make sure $VDT_LOCATION exists in the following steps!
   1. Check out the GridFTP-HDFS sources:
      <verbatim>svn co svn://t2.unl.edu/brian/gridftp_hdfs</verbatim>
   1. Make a backup copy of the makefiles:
      <verbatim>
      cp makefile_header makefile_header.bkp
      cp Makefile Makefile.bkp
      </verbatim>
      This is so the original makefiles can be preserved during the next step.
   1. Replace MAGIC_VDT_LOCATION with the actual contents of $VDT_LOCATION.
      <verbatim>
      sed -i s:MAGIC_VDT_LOCATION:$VDT_LOCATION:g Makefile
      sed -i s:MAGIC_VDT_LOCATION:$VDT_LOCATION:g makefile_header
      </verbatim>
   1. Run make to build the !GridFTP module.
      <verbatim>make</verbatim>
   1. Copy the original makefiles back:
      <verbatim>
      cp Makefile.bkp Makefile
      cp makefile_header.bkp makefile_header
      </verbatim>
   1. Create a tarball, and place it in the pacman cache.


---++ Building !GridFTP-HDFS RPM
   1. Install the VDT rpms
      <verbatim>
      rpm -i http://vdt.cs.wisc.edu/software/gpt//3.2-4.0.8p1/gpt-3.2_4.0.8p1_x86_64_rhap_5-1.x86_64.rpm http://vdt.cs.wisc.edu/software/globus/4.0.8_VDT-1.10.1/RPMS//x86_64/vdt_globus_essentials-VDT1.10.1x86_64_rhap_5-3.x86_64.rpm http://vdt.cs.wisc.edu/software/globus/4.0.8_VDT-1.10.1/RPMS//x86_64/vdt_globus_data_server-VDT1.10.1x86_64_rhap_5-3.x86_64.rpm http://vdt.cs.wisc.edu/software/globus/4.0.8_VDT-1.10.1/RPMS//x86_64/vdt_globus_sdk-VDT1.10.1x86_64_rhap_5-3.x86_64.rpm
      </verbatim>
   1. If you don't already have a build environment, you'll need these RPMs:
      <verbatim>
      yum install gcc libtool automake subversion
      </verbatim>
   1. Install hadoop and the JDK through RPMs.
      <verbatim>
      rpm -i http://newman.ultralight.org/repos/hadoop/4/x86_64/caltech-hadoop-4-1.noarch.rpm
      yum install hadoop-fuse
      </verbatim>
      You're on your own for installing the JDK; download it from http://java.sun.com
   1. Regenerate the globus environment
      <verbatim>
      export GLOBUS_LOCATION=/opt/globus
      export GPT_LOCATION=/opt/gpt
      export PATH=$PATH:$GPT_LOCATION/sbin
      gpt-postinstall
      gpt-build -nosrc gcc64
      </verbatim>
      You may want to replace gcc64 with gcc32.
   1. Checkout the sources
      <verbatim>
      svn co svn://t2.unl.edu/brian/gridftp_hdfs
      </verbatim>
   1. Regenerate Makefile/configure
      <verbatim>
      autoreconf
      automake -a
      ln -s /usr/share/libtool/ltmain.sh
      autoreconf
      </verbatim>
   1. Configure
      <verbatim>
      ./configure --with-java=/usr/java/jdk1.6.0_14/ --with-hadoop=/opt/hadoop
      </verbatim>
   1. Make
      <verbatim>
      make
      </verbatim>
   1. Install
      <verbatim>
      make install
      </verbatim>

---++ !GridFTP RPM TODO
   1. Create RPM from automake-ified version of GridFTP.
   1. Create RPM from binary tarball of PRIMA: http://vdt.cs.wisc.edu/software/prima//0.8.4/
   1. !GridFTP is already in /etc/services on sufficiently recent RHEL5 releases; should we try putting a line in here for some platforms?
   1. Add all this to a yum repository.

-- Main.BrianBockelman - 20 Mar 2009

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = BrianBockelman

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %NO%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = FirstName
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = FirstLast
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %NO%
############################################################################################################
-->
