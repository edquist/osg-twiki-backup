%META:TOPICINFO{author="BrianBockelman" date="1258986406" format="1.1" reprev="1.12" version="1.12"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%



---++ Debugging your Hadoop Instance

This twiki is setup to help diagnose and solve some typical Hadoop issues. 

---++ Running the FUSE mount in Debug mode

It is often useful to run the FUSE mount in debug mode to identify specific errors and problems with the FUSE mount.

<verbatim>
/usr/bin/hdfs -o server=namenode.fqdn,port=9000,rdbuffer=131072,allow_other -d /mnt/hadoop/
</verbatim>
 
Note the use of the -d switch to put the mount in debug mode. CTRL-C to quit the mount after testing. 

---++ Running the Gridftp server in standalone mode

Sometimes diagnosing gridftp server or hadoop errors require you to run the standalone gridftp server which runs in a debug mode. 

*On the Server* 

<verbatim>
gridftp-hdfs-standalone
</verbatim>

This will start a new server in the terminal's foreground; it should also include Hadoop-level errors.

*On the Client, note the use of port 5002*

<verbatim>
source  $VDT_LOCATION/setup.sh
dd if=/dev/zero of=testfile.zero count=10000 bs=1024
globus-url-copy file://localhost/`pwd`/testfile.zero gsiftp://gridftpserver.fqdn:5002/your/path/testfile.zero
</verbatim>

After running the command on the client check the console on the server for any error messages in the transfer that could help diagnose your gridftp server problem. 

---++ Errors and Their Solutions

---+++ Error message: "Superuser privilege is required"

If you encounter this message while administrating your cluster (i.e., executing "hadoop fsck" or "hadoop dfsadmin"), then check the following:
   * You are running the command from the namenode
   * Your current user is the same user that the Hadoop daemons are running as.  If you run the daemons as "root", then you must run administrative commands as "root".  If you run it as "daemon", you must perform these commands as "daemon".

---+++ Incompatible Versions between the Datanode and Namenode

---++++ Problem

Hadoop is very sensitive about the versions and build tags between the different parts of the system. When upgrading hadoop you may get errors of the form.

<verbatim>
2009-03-23 14:06:50,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = cabinet-7-7-28.t2.ucsd.edu/169.228.130.190
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.19.2-dev
STARTUP_MSG:   build =  -r ; compiled by 'mockbuild' on Mon Mar 23 15:50:31 EDT 2009
************************************************************/
2009-03-23 14:06:50,263 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Incompatible build versions: namenode BV = 748415; datanode BV =
2009-03-23 14:06:50,370 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible build versions: namenode BV = 748415; datanode BV=
        at org.apache.hadoop.hdfs.server.datanode.DataNode.handshake(DataNode.java:416)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:265)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:206)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1239)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1194)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1202)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1324)

2009-03-23 14:06:50,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:
</verbatim>

---++++ Solution

Generally you want to upgrade the name node and data nodes at the same time to limit these kinds of version problems. Check the versions of your namenode and datanode to make sure you are running the same build tag of hadoop. 

---+++ FUSE Issues

---++++ Strange filenames with random characters in the FUSE mount

This happens when fs.default.name in your hadoop-site.xml uses the default port (i.e., says hdfs://hostname/ instead of hdfs://hostname:9000/).  Add the port explicitly to the hadoop-site.xml entry and remount FUSE.

---++++ Group permissions issues

Generally, group permission issues fall into one of two categories:
   * Non-existent group name.  If you don't have a group name in /etc/groups or have a system that adds the user to random groups (Condor can do this if you have some rare options turned on, as well as OpenAFS) that aren't mapped to group names.  You can check for this by typing "groups <username>" and verifying it has an exit code 0. Try updating your /etc/groups and/or turn off the software that injects extra group IDs.  It is possible to do this in OpenAFS and Condor and still have a functional install.
   * FUSE/HDFS only looks up a user's group membership the first time you use the file system.  If you're running into group permission issues and you just recently added that user to a group, try remounting the FUSE filesystem

---+++ Gftp Client Errors 

---++++ 500-Failed to open file in HDFS.

<verbatim>
Source URL for copy: file:/data/sam2/.same/SRMv2/testFile.txt
Destination URL:
gsiftp://cithep250.ultralight.org:5000//mnt/hadoop/store/user/test/SAM-cit-itb-se.ultralight.org/lcg-util/testfile-user-20090325-222131.txt
# streams: 1
# set timeout to  0 (seconds)
            0 bytes      0.00 KB/sec avg      0.00 KB/sec
instglobus_ftp_client: the server responded with an error
500 500-Command failed. :
globus_gridftp_server_hdfs.c:globus_l_gfs_hdfs_recv:916:
500-Failed to open file in HDFS.
500 End.
</verbatim>

*Solution:* These errors can be a bit vague, more information may be available however in the gridftp log itself. For example the above may be a permissions problem that can be verified in the Gridftp log. 

If the log is not helpful it might be necessary to run the gridftp server on standalone mode on port 5002. 

---++++ 500-Failed to close file in HDFS.

<verbatim>
      41472 bytes      8.21 KB/sec avg      8.21 KB/sec instglobus_ftp_client: the server responded with an error
500 500-Command failed. : globus_gridftp_server_hdfs.c:globus_l_gfs_hdfs_write_to_storage_cb:926:
500-Failed to close file in HDFS.
500 End.
</verbatim>

*Solution:* Check to make sure the HDFS file system is not full as a full HDFS can cause this issue.

---++++ 530 Login incorrect. : an unknown error occurred

Sometimes a client error is really a problem on the server side.  If globus-url-copy works against another known-working gridftp server, then you should try starting the gridftp server in standalone mode to find the real cause of the problem.

---++++ Solution(s)

Some possibilities that you may find in the output of the standalone gridftp server are:  The gridftp server does not have CLASSPATH set to point to the Hadoop jar files, or the gridftp server is using a jvm version that is incompatible with the Hadoop jars.

---+++ GFTP Server Errors

---++++  500-Allocated all 200 memory buffers; aborting transfer.

This error which may appear in the gftp/srmcp client or the server is caused by clients that use multiple streams consuming all of the 200 available GFTP server buffers. The default is 200 which is a bit low. To help alleviate this problem you can add the following environment variable to the gridftp server. 

In file */etc/gridftp-hdfs/gridftp-hdfs-local.conf*

add the following line

<verbatim>
export VDT_GRIDFTP_BUFFER_COUNT=500
</verbatim>

This increases the available buffers to 500 which should allow more flexibility in how many streams the gftp server will support before it drops the connection to save memory. 

Note: In more recent versions of Hadoop a disk cache is also used to help deal with long transfers

---++++  an end-of-file was reached globus_xio: An end of file occurred (possibly the destination disk is full)

This often shows up in the phedex error logs with the full message as:

<verbatim>
SOURCE error during TRANSFER phase: [GRIDFTP_ERROR] an end-of-file was reached globus_xio: An end of file occurred (possibly the destination disk is full)
</verbatim>

Contrary to the error message, this most often occurs when there is plenty of disk space in Hadoop.  This is likely caused by xinetd refusing incoming gridftp connections due to various throttles that can be set in the xinetd configuration.  Some settings in =/etc/xinetd.conf= and =/etc/xinetd.d/gridftp-hdfs= that you will want to check for are:

<verbatim>
instances = UNLIMITED
per_source = UNLIMITED
</verbatim>

If these two settings are set to small integers (on the order of 20), then it's likely that you're reaching these limits and xinetd is refusing to allow any more incoming connections.  The recommendation is to set these to =UNLIMITED=, or to a larger value that your gridftp server can handle.

---++++ Mkdirs failed to create

<verbatim>
Exception in thread "main" java.io.IOException: Mkdirs failed to create /cms/store/user/tmartin                        
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:358)                                 
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:487)                                                 
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:468)                                                 
Call to org.apache.hadoop.conf.FileSystem::
create((Lorg/apache/hadoop/fs/Path;ZISJ)Lorg/apache/hadoop/fs/FSDataOutputStream;) failed!                 
</verbatim>

Check to make sure the hadoop-site.xml is properly configured, or the CLASSPATH is set correctly.

---++++ Internal error

<verbatim>
[3429] Wed May 27 14
:10:18 2009 :: uaf-4.t2.ucsd.edu:41449: [SERVER]: 500-Command failed. : globus_gridftp_server_hdfs.c:globus_l_gfs_hdfs_recv:1085:
500-System error in Failed to open file /cms/store/user/tmartin/testfile-5035.zero in HDFS for user tmartin due to an internal error in HDFS on server cabinet-7-7-14.t2.ucsd.edu; could be a misconfiguration or bad installation at the site: Unknown error 255
500-A system call failed: Unknown error 255
500 End.
</verbatim>


Check to make sure the hadoop-site.xml is properly configured, or the CLASSPATH is set correctly.


---++ Authors

-- Main.TerrenceMartin - 23 Mar 2009
