%META:TOPICINFO{author="TedHesselroth" date="1274822959" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+*%SPACEOUT{ "%TOPIC%" }%*
%TOC%

The proposal is to allow users to access all opportunistic storage on OSG via a grid centralized storage element. The motivation is to hide from the user the current necessity of making the storage available and to provide the mechanisms of discovery, etc, as a backend service. Technical feasibility of a few software implementations are discussed.

---++IRODS

One of the main drivers of IRODS is the federation of heterogeneous data management technologies. 

It should be noted that federation in IRODS terminology is across Multiple IRODS instances, or zones. Federation allows access to other IRODS instances from an IRODS instance. The IRODS zone hosting the file is the root of the logical path of the file. Therefore federation as defined in IRODS does not in itself make the instances transparent. That may be acceptable, but it seems to defeat the purpose of having logical vs physical names. Furthermore, it would nominally require an IRODS instance at each site, though it is conceivable that a central host could run an IRODS instance for each site.

The logical name does hide the specific storage resource that the file is on. Logical names are mapped to physical names in the iCAT database. An ftp server can be a storage resource, and with SRM support it should be possible to put distributed SRM resources into one zone. So federation of OSG resources would be the creation of an IRODS zone with distributed storage resources. File transfers would be done through the one IRODS server, acting as a cache for files moved to or from the storage resources. It would take several iclient commands to move a file. Alternatively, these might be written in the form of a rule which could be invoked by one client command.  It may be possible to overload some of the existing iclient commands to invoke direct transfer between user and storage resource rather than using the IRODS server as a proxy. At a lower level, the native IRODS transfer mechanism does use direct data channels, but only to and from IRODS servers.
 
That allows flexible file location, though further catalog operations will be necessary to, for example, make sure collections of files are co-located. Support exists in IRODS for the use of custom scripts to execute various file operations, through the Universal Mass Storage System driver. Into these scripts might be placed to perform additional operations as needed.

Another option is to use only the iCAT portion of IRODS, to form a logical namespace and to define collections. Data could then be moved using using the presently-used clients such as globus-url-copy. File transfers would involve additional steps of catalog reading and file registration. Such a use could potentially pose difficulties, since it is not the canonical way of using IRODS. FTS (see below) may be a better fit for this model. 

---+++Directing a collection to a resource

One can use iclient commands to specify the resource to which a file should go. According to the default iCAT schema, a resource may be associated with a file, but not for a collection. This allows a collection to be spread over several resources. One can use rules, such as these from ARGO, to guide an iput to a resource according to the destination collection.

<pre>
1	#ARGO rules
2	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/*/ARGO/*"|msiSetDefaultResc(emii.resource.tpac.org.au,preferred)|nop
3	
4	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/SOOP/*"|msiSetDefaultResc(emii.resource.tpac.org.au,preferred)|nop
5	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/SOTS/*"|msiSetDefaultResc(emii.resource.tpac.org.au,preferred)|nop
6	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/ANFOG/*"|msiSetDefaultResc(arcs-df.ivec.org,preferred)|nop
7	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/AUV/*"|msiSetDefaultResc(arcs-df.vpac.org,preferred)|nop
8	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/ANMN/*"|msiSetDefaultResc(emii.resource.tpac.org.au,preferred)|nop
9	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/ACORN/*"|msiSetDefaultResc(arcs-df.vpac.org,preferred)|nop
10	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/AATAMS/*"|msiSetDefaultResc(arcs-df.vpac.org,preferred)|nop
11	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/FAIMMS/*"|msiSetDefaultResc(arcs-df.vpac.org,preferred)|nop
12	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/eMII/*"|msiSetDefaultResc(emii.resource.tpac.org.au,preferred)|nop
13	acSetRescSchemeForCreate|"$objPath" like "/ARCS/projects/IMOS/staging/SRS/*"|msiSetDefaultResc(arcs-df.ivec.org,preferred)|nop
</pre> 

---+++ A distributed iRODS zone of gridftp storage resources

---++++ Start iRODS.

<pre>
    3  cd /opt/iRODS/
    4  ./irodssetup 
    5  ./irodssetup 
    6  ./irodssetup 
    7  ls
    8  ./irodsctl start
    9  MSS universal driver
</pre>

---++++ Make a resource group with local cache and gridftp server resources.

<pre>
   10  export PATH=$PATH:/opt/iRODS/clients/icommands/bin
   11  iadmin mkresc gwdca03 "MSS universal driver" compound gwdca03.fnal.gov /pnfs/fnal.gov/data
   12  iadmin atrg rg gridworks
   13  iadmin atrg rg data1
   14  iadmin mkresc gridworks "unix file system" cache gw014k1.fnal.gov /data1
   15  iadmin atrg rg gridworks
   16  iadmin atrg rg gwdca03
</pre>

---++++ Add users and user groups

<pre>
   35  iadmin mkuser gwadmin rodsadmin
   36  iadmin mkuser tdh rodsuser
   38  iadmin mkuser testuser rodsuser
   40  iadmin mkgroup gwgroup
   41  iadmin atg gwgroup tdh
   42  iadmin atg gwgroup testuser
</pre>

---++++ Add more resources to the resource group

<pre>
</pre>

---++++ Make a collection

A collection may be thought of as a directory in the logical namespace.

<pre>
</pre>





---++++ 

<pre>
</pre>

---+++ OSG iRODS Model

Discovery, Transfer, Catalog, Management

*Within iRODS framework, or iRODS as a component in another framework?*

Minimize the disparity between this approach and what would be done for direct submission to sites.

---++++Discovery
   
Provide for automatic selection of storage element. Support data-driven computation.

   * On directory creation, assign a storage element to it
      * Schema requires it to be stored in a data object
      * Should include the expected size of the directory
         * May need to move the files to another storage element
      * Space reservation option
      * Storage profile option
   * Use the discovery service to return the TURL or SURL to the user.
   * Associate a compute element with a storage element via data objects
      * Computation profile option
         * Influences where the data is placed
   * Use RSV probe to identify authorized sites
   * Allow for operation with OSGMM

---++++Data Transfer

Desire direct transfer between storage element and user based on non-iRODS storage services. Storage Management policies and Catalog operations should be triggered as in normal iRODS operation.

The fifth option here is recommended, for the specified reasons.

   * Use iRODS standard transfer with gridftp extension microservices
      * Because the gridftp extension is a set of microservices (not a driver) it does not invoke policy rules.
         * Driver is in development.
      * In iRODS a gridftp server is a compound resource; data must hop through the OSG iRODS server
   * Use an SRM transfer with and SRM interface to iRODS
      * Academia Sinica developed SRM interface for SRB.
      * Said by RENCI to be "interested" in SRM interface for iRODS.
      * Presumably would be an adapter for native 
      * Must use gridftp or other non-iRODS storage resources; data must hop through the OSG iRODS server
   * Use microservice-wrapped web service for callbacks
      * Integrate with globus.org-type service
      * Facilitate future integration with Java-based iRODS microservices
      * May do GUMS call
      * Does not invoke policy rules
      * gsoap, complicated, must recompile for each added web service
   * Use iRODS transfer with Universal Mass Storage Driver
      * Storage resources registered as such
      * Uses regular transfer microservices, which will invoke policy rules
      * Must stage through OSG irods server
   * Use iRODS as a shadow storage service
      * Actual transfers in external framework
         * Using same clients as for non-opportunistic storage.        
      * Get TURL or SURL and make the transfer in the next step
         * Override microservice to provide info rather than transfer data
            * Essential that iRODS automatic policy functions are still invoked.
            * Policy functions are invoked before and after invocation of drivers
               * Custom driver using rcExecCmd (same as UMSD)
               * Additional cmd within workflow with msiExecCmd
         * Write UMSD functions to return information rather than do transfers
            * Must avoid false registration on failure
               * Must have a way to block until file completion
            * Currrent nop for file transfer (stage only). Extend?
               * univMSSFileWrite, etc
               * Could write scripts for either gridftp or srm. It may be better in the end to use separate drivers for gridftp and SRM.

---++++Catalog

Database registration should be transparent to the user.

   * Trigger job on completion of pre-defined file set.
   * Add FITS data to metadata (LSST)
      * Database searchability
   * Monitor data movement status
   * Leverage existing iRODS GUIs and web interfaces.
   * Support Storage Management functions, see below

Deploy only iCAT at sites? Or iCAT-enabled iRODS, but only as info server and policy executor?

---++++Management

Should be done through iRODS policy rules.

   * Trigger deletion of expired data collections.
   * Maintain space and availability information.
   * Replication
   * Relocation
   * Aggregation

---+++ OSG iRODS Program of Work.

A plan sketch would be the following. The work starts with the Data Transfer function.

   1. Install iRODS and add several distributed gridftp resources in a resource group.
   1. Run various elementary rule files, with transfers only to local cache resource.
   1. Create and run elementary rules using rulegen.
   1. Run a rule with msiExecCmd invoking example hello script
   1. Write an example script and run a rule with msiExecCmd invoking it.
   1. Check the functionality of the Universal Mass Storage Driver with dummy scripts. 
   1. Copy the Universal Mass Storage Driver as a new driver.
      1. Follow the documentation outlining the creation of a new driver.
   1. Add a storage resource with the type of the new driver.
   1. Before extending, recheck the functionality of the supported command subset.
   1. Write a new univMSSFileWrite function.
      1. Study degree of difficulty before starting program of work.
   1. Check the functionality with a file write and dummy driver script.
      1. Verify that pre and post-processing policy rules are invoked.
      1. *Study issues of why MSS is currently supported only as staging* -asynchronicity?
   1. Design actual univMSSFileWrite function.
      1. Do this design before starting the program of work.
   1. Repeat the previous three steps for all other functions.
   1. Devise and execute thorough transfer tests.
      1. In parallel -test each new function as it is written, continuous integration.
   1. Propose modification of UMSD in iRODS code base to extended version.
   1. Write and test rules using the msiExecCmd microservices for Discovery, Catalog, and Management functions.


---+++ Component Design

How the scripts that invoke specific operations can be written such that iRODS and currently available tools can work together and support the requirements of the model.

The complete list of file operations is: univMSSFileCreate, univMSSFileOpen, univMSSFileRead, univMSSFileWrite,
    univMSSFileClose, univMSSFileUnlink, univMSSFileStat, univMSSFileFstat, univMSSFileLseek,
    univMSSFileFsync, univMSSFileMkdir, univMSSFileChmod, univMSSFileRmdir, univMSSFileOpendir,
    univMSSFileClosedir, univMSSFileReaddir, univMSSFileStage, univMSSFileRename,
    univMSSFileGetFsFreeSpace, univMSSFileTruncate, univMSSStageToCache, univMSSSyncToArch. Some of the operations may not be possible with gridftp or SRM.

---++++univMSSFileRead

   1. preprocessing
   1. fileRead function
      1. Determine if the endpoint is gridftp or SRM.
         1. Look up the database entry for storage resource, or parse the URL.
      1. Obtain TURL or SURL. 
         1. Get server name from iCAT lookup based on the storage resource attributes.
         2. Use OSG Discovery Tool to find the example URL.
         3. Make substitution using logical path to compose URL.
      1. Communicate the URL to the user.
         1.
      1. Block while the user software reads the file.
         1.
      1. Receive a report on the result of the read.
      1. Act based on the success or failure of the user's read.
         1.
         1.
      1. Exit with suitable return code. 
   1. postprocessing
         1. OSG Storage Federation accounting.

---++globus.org

---++FTS

-- Main.TedHesselroth - 19 May 2010
