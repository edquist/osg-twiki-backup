%META:TOPICINFO{author="GarhanAttebury" date="1250700869" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%
---++ Detecting filesystem corruption

Hadoop's fsck utility will show Status: CORRUPT in its output whenever a file is corrupt. The following command will display the status for your entire filesystem namespace.

=# hadoop fsck /=


---++ Causes of corruption

When one or more blocks of a file become corrupt or missing and there are no more 'good' replicas of that block the filesystem is considered CORRUPT. It is often possible to recover from this depending on the cause. Take for example the following scenarios:

* Two or more datanodes crash on your normally 2x replicated filesystem. Files with blocks replicated on those two nodes will become corrupt as there are no replicas available for some of blocks and you cannot retrieve the file in full.

* A datanode crashes, and during replication a hard drive in another datanode starts showing errors and is unable to provide certain blocks. Those blocks now have no 'good' replicas and the filesystem is CORRUPT.

* ... etc. Site admins know there are many other 'unlikely' scenarios that will inevitably occur resulting in blocks being lost.

_The key thing to remember_ is that if you can recover the blocks which make up a file, you can recover the file. Blocks and their associated metadata are simply files on some underlying filesystem (ext3, xfs, reiser, etc...) and you can treat them as such. You can go to great extents to recover the blocks if you need, such as using undelete tools or copying blocks from a bad datanode to another working datanode. So long as you can get the problematic blocks to a working datanode, which in turn can report them to the namenode, you can recover the file.


---++ Locating and repairing corruptions
When you encounter corruptions, use Hadoop's fsck utility to obtain a list of affected files *[should we suggest a parser like my hfscker.py here?]*

Using the =-files=, =-locations=, and =-blocks= parameters you can use fsck to find out which blocks are missing and where they are supposed to be. Sometimes you will find a block simply marked as =MISSING= with no locations given, and other times you will find a single replica with location listed where there should in fact be more.

<verbatim># hadoop fsck /user/gattebury/test.iso -files -locations -blocks
/user/gattebury/test.iso 4613701632 bytes, 35 block(s):  OK
0. blk_-6574099661639162407_21831 len=134217728 repl=3 [172.16.1.115:50010, 172.16.1.128:50010, 129.93.239.178:50010]
1. blk_-8603098634897134795_21831 len=134217728 repl=3 MISSING!
2. ...</verbatim>

In the first case, all possible sources of the second block are gone and the namenode has no knowledge of any host with it. This can happen when nodes are completely off or have no network connection to the namenode. In this case, the easiest solution is to =grep= for the block ID "8603098634897134795" in the namenode logs in hopes of seeing the last place that block lived. Providing you keep namenode logs around and the logging level is set high enough *[what is high enough anyway?]* you will hopefully find a datanode containing the block. If you are able to bring the datanode back up and the blocks are readable from the hard drive(s) the namenode will replicate it back to the appropriate amount and the file corruption will be gone.


<verbatim># hadoop fsck /user/gattebury/test.iso -files -locations -blocks
/user/gattebury/test.iso 4613701632 bytes, 35 block(s):  OK
0. blk_-6574099661639162407_21831 len=134217728 repl=3 [172.16.1.115:50010, 172.16.1.128:50010, 129.93.239.178:50010]
1. blk_-8603098634897134795_21831 len=134217728 repl=3 [172.16.1.83:50010]
2. ...</verbatim>

In the second case you may see a single replica listed for the block where you know there should be more. Under normal operation, this block would eventually (within a few minutes) be replicated back to the replication level of 3. In very rare cases the replication may never happen. This has occurred at Nebraska a few times when a datanode is 'alive' but unable to send the block due to hard drive errors (especially in the cases where the ext3 journal drops and the host enters a rather unpredictable state). This also happens with stuck / under replicated blocks (see Fixing Stuck and Under Replicated Files in HadoopOperations). You should first try to recover the host with the single block listed. If that is possible, the namenode will replicate as needed and the corruption will be gone. If you are unable to recover that host, you can again try greping through the namenode logs (or datanode logs in desparation) to find another replica of that block and recover that host instead. If you find another host with a good copy of the block and bring it up, make sure to disable/decommission the 'stuck' one until you can fix that host as well.

---++ Unrecoverable files
If you are unable to recover at least one datanode containing the missing/corrupt blocks the file is lost as there is no way to retrieve the full file anymore. You can use the =-move= parameter to fsck to move the remaining parts of a file to /lost+found/. *[Not 100% sure, but I believe the missing sections will be zeros leaving the filesize correct]*. 

=# hadoop fsck -move=


_Reminder: having more replicas of 'critical' files such as user data greatly reduces the chance of you losing all replicas of that file's blocks. Nothing prevents you from having 10 replicas on critical files but only one replica of data that is recoverable by another means to save space._

-- Main.GarhanAttebury - 19 Aug 2009
