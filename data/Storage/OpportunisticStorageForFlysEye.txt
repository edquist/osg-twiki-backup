%META:TOPICINFO{author="TanyaLevshina" date="1262727002" format="1.1" version="1.3"}%
---+!! *<noop>Opportunistic Storage For Fly's Eye*
%TOC%

---+++++ Purpose
The Fly's Eye experiment is dealing with search for Highly Energetic Dispersed Radio Transients using the Allen Telescope Array. The collected  data is stored in 36 GB (58 minute) .pcap ethernet capture files on tapes on a remote high performance storage system. Pointers to data files are stored in a !MySQL database.

The Fly's Eye representatives have asked for 20TB of OSG storage for 1 year in order to do event processing. We are trying to come up with "formal" request procedure within OSG to iron out a workable procedure going forward. We have asked Fly's Eye representative the following answers in order to better understand their needs and try to help them with workflow setup for OSG.

---+++++ The Use Case
---++++++ Questions and Answers

<b><i>How much space total do you need for your data?</i></b>
 
20TB of data in total

<b><i>For how long do you want to keep this data in storage?</i></b>

For a year

<b><i>How much data is read by individual analysis job?</i></b>

An individual  analysis job requires about 36GB. 

<b><i>How the input data for an individual job subdivided into individual files?</i></b> 

Our 36 GB files are subdivided into about 250 individual files for analysis,usually processed serially (but these could all be processed in parallel).

<b><i>How many files of what average/ minimum /maximum size make up a 36GB chunk? Is there any required directory hierarchy involved?</i></b> 

 Each file bloats in size by a factor of 4 during subdivision (8 bit  int ->32 bit float).  There isn't any required directory hierarchy, per say.

<b><i>Are the input file(s) read generally sequentially and constantly  throughout the job or is the access more "bursty" or random?</i></b>

Bursty.  Usually we take a 36 GB file, extract ~50 individual files (first 10 minutes of one hour of data), process serially, extract 50 more files (next 10 minutes), process serially, etc...  If we had enough scratch,  it would be more efficient to just extract all 250 files at once.

<b><i>If necessary, could the 20TB be spread across multiple sites (with suitable adaptation to the data marshaling for a job)? Do you have anything  like a data catalog?</i></b>

Yes, we have a data catalog.  The 20 TB could certainly be spread across multiple sites, although it would be most efficient programming-wise if we had a uniform method of access.

<b><i>How long it takes to run each job?</i></b>

The wall time for the jobs is very consistent,  but will of course depend on the compute power of the node.  24 hours on a fast  node, 48 on a slow one.

<b><i>What is the limiting factor here: I/O or CPU capability? Can your jobs be split easily into smaller components or is this already the smallest division possible?</i></b>

It's most convenient for us to split on file boundaries (36 GB files). The jobs themselves are embarrassingly parallel, and could be subdivided ad nauseum.

<b><i>What kind of output data do you produce, and how much per job?</i></b>

Approximately 1GB of event data per job, formatted as mysql insert queries.

<b><i>To where would you ship the output data for interactive access, or would we need to make arrangements?</i></b>

We have facilities (mysql servers w/ sufficient disk space) to ship output data.

<b><i>What is your projected timescale for this operation? Are there  any "drop dead" deadlines?</i></b>

No drop-dead deadlines.

<b><i>Do you require access to the input data on OSG beyond the initial reprocessing exercise?</i></b>

Yes, it would be most useful to us if we had a reasonably static place to keep our data.  Storage of the 20 TB, in an accessible location, is our primary issue.  We understand that disk space isn't yours to promise.

---+++++ Initial Participating Sites

---+++++ Initial Participating Sites

These sites have agreed to support Fly's Eye so far:

   * Omaha, Nebraska - 5TB of storage space
   * Lincoln, Nebraska - 5TB of storage space


-- Main.TanyaLevshina - 30 Dec 2009
