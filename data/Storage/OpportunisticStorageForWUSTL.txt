%META:TOPICINFO{author="TanyaLevshina" date="1274379679" format="1.1" reprev="1.2" version="1.2"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%
---++ Purpose
Human Microbiome Project (HMP) at Washington University in St. Louis (WUSTL)  is dedicated to study of core human 
microbiome. The project analyzes how the changes in the human microbiome can be correlated with changes in human health. 
The new technological and bioinformatic tools  are being developed by HMP team in order to advance this research.

---++ Use Case
Basic Local Alignment Search Tool (blastx) is used to find matches (with a set of special criteria) between DNA sequences (each100-character long strings)  and a
"reference database".  The selection of the datasets depends on the scientific question being asked. The reference database take the form of several
files that, depending on the database, will vary from 3 GB to 10 GB.  The goal is to process tens of billions of DNA sequences. The DNA sequences are split into subsets of "input reads" in order to use them as an input for blastx that runs against a given dataset.
The input to every blastx job is about 67KB files (depends on target runtime/platform) and the reference database (varies: e.g., 3 GB to 10 GB but the same for all jobs in a set).  The job usually runs from 15 minutes to an hour.
The output is a list of matches is in the order of 200 KB per job. The requirements for memory for one job vary: majority of jobs are using less then 2.2 GB of RAM and around 3 GB of virtual memory, with the larger input database blastx could use up to 4 GB of RAM and 7.5 GB of virtual memory. 

---+++ Engage workflow for WUSTL
The first job submitted by Engage on the selected site is staging a database (large text file) and building  references datasets in OSG_DATA shared area. The total size of datasets is 600GB.  Apparently, WUSTL needs just small subset  of these data (10GB) - question to Engage team: Why do all reference datasets need to be staged?

After the reference datasets are successfully built, a pilot job is submitted to a site. Pilot jobs starts user jobs each run from 10 to 1 hours, each runs blastx with input file with subset of DN sequences and subset of the reference datasets. Question to Engage: how do you avoid overloading shared fs when all the jobs start accessing reference data?
 
The requirements:

   1. OSG_DATA allows to store 600GB of data. (question to Engage:  Any other requirement for FS? )
   1. Site should have enough free cpu cycles (question to Engage: what is enough?)
   1. Worker node should allow outbound connection.

---++ Initial Participating Sites
| *Site Name* | *Status *|
|CLEMSON-Palmetto|	functioning|
|FIREFLY|	functioning|
|NEBRASKA_RED|	functioning|
|OSG-RENCI-Engagement|	functioning|
|OSG-UCHC_CBG|	functioning|
|OSG-UFlorida-HPC|	databases will not build, walltime maxes out, other difficulties|
|FNAL_FERMIGRID|	currently debugging|
|FNAL_GPGRID_1|	will attemtp to configure|
|USCMS-FNAL-WC1|            	|
|NYSGRID_CORNELL_NYS1|	could work. Has about 1 TB free|
|OUmissHEP|	not enough CPUs. Has space available, but is misconfigured. OSG_WN_TMP is not writable|
		

-- Main.TanyaLevshina - 20 May 2010
