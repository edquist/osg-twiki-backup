%META:TOPICINFO{author="BrianBockelman" date="1255745294" format="1.1" version="1.9"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ Installation

---++ Quick Start

Quickstart for the impatient.  This assumes you already have [[HadoopInstallation][Hadoop and FUSE]] installed on the Xrootd server.

 <verbatim>
rpm -ivh http://newman.ultralight.org/repos/hadoop/4/x86_64/caltech-hadoop-4-1.noarch.rpm
yum install xrootd
cp /etc/xrootd/xrootd_sample.cfg /etc/xrootd/xrootd.cfg # ...and edit appropriately (see below)
vi /etc/xrootd/Authfile # Edit appropriately (see below)
service xrootd start
</verbatim>

---++ Xrootd/HDFS architecture explanation

Xrootd is a very flexible distributed storage system.  The Xrootd/HDFS integration is designed to allow you to safely export your HDFS system to the wide / metro area network and to allow several sites to share a single, unified namespace.  Currently (and by design), this integration is read-only.  This allows users to collaborate, but provides encouragement to not bypass the "normal" data placement mechanisms.

We currently use two Xrootd daemons: "xrootd" and "cmsd".  The xrootd daemon is the workhorse of the system and provides the data access.  Its job is to read data from disk (with the HDFS module enabled, read data from HDFS) and to export it to the client.  The cmsd is the "Cluster Management System" daemon; these work with the xrootd and each other to determine daemon liveness, file status, and work to create the distributed system.  Each node in Xrootd runs both cmsd and xrootd.

In each distributed Xrootd system, there are at least two "special" roles.  There must be one "manager" cmsd which is in charge of determining overall system status. each manager can manage 64 other nodes; managers can manage other managers to form a tree, or can be peers to form a more resilient system.  If you are familiar with P2P systems, these are similar to "supernodes".  The second role is the "redirector", a special xrootd instance that is on the same host as the manager cmsd.  Clients initiate contact with a redirector instance; this instance does not actually move data, but further instructs the client where to find its desired data.

Here's the approximate way the data location goes:
1) Client contacts the redirector xrootd, asking for file X.
2) Redirector Xrootd asks manager cmsd to find the best copy of file X.
3) Manager cmsd queries all managed cmsd instances to find a copy of X.
4) Each cmsd queries the attached xrootd for file X.
5) If the file is found, this information travels from the datanode xrootd, to the cmsd, then to the manager xrootd
6) The manager determines the best xrootd to serve data (also taking into consideration server load, for example).
7) The redirector xrootd instructs the client where to find file X.
8) The client connects to the optimal data server and starts the session.

The whole process can be done in the order of milliseconds; mostly, it depends on network latencies and the speed of the underlying file systems.

Of course, there are many more details to this; better descriptions of the algorithms for finding data are at http://xrootd.slac.stanford.edu/.

---++ Prerequisites

The Xrootd server has the following prerequisites:

   1 You must also have already [[HadoopInstallation][installed and mounted]] Hadoop using FUSE.

Xrootd is preconfigured to look for the host certificate and key in =/etc/grid-security/xrd/xrd*.pem=.
These files must exist and be readable by the xrootd user.  Using certificates in a different directory or with different names will require modifying =/etc/xrootd/xrootd.cfg=.  Make sure =/etc/grid-security/xrd/xrdcert.pem= exists with mode 644, and =/etc/grid-security/xrd/xrdkey.pem= exists with mode 400 (not 600!)

The installation includes the latest CA Certificates package from the OSG as well as the fetch-crl CRL updater.

It is highly recommended that you make sure your CRLs in =/etc/grid-security/certificates= exist and are up to date by running fetch-crl manually before starting xrootd the first time.  Otherwise xrootd will attempt to download CRLs itself when it starts up.

The rpm/yum installation will create a 'xrootd' system account and group (uid,gid < 500) on the host system for running the xrootd process. If you would like to control the uid/gid that is used, then you should create the 'xrootd'' user and group manually before installing the rpms.

---++ YUM Installation

Remember, in order to use xrootd, you must have the [[HadoopInstallation][Hadoop and the FUSE kernel module]] already installed on your system in order to use the yum installer.

To configure your local installation for the yum repository, [[HadoopInstallation#Yum_install][follow the advice here]] to install the correct =caltech-hadoop= package for your site.

After installing the caltech-hadoop yum configuration package, you can install the xrootd server with:

<verbatim>
yum install xrootd
</verbatim>

Updates can be installed with:

<verbatim>
yum upgrade xrootd
</verbatim>

---+ Configuration (NEEDSWORK)

A sample configuration can be found in

Write the following into xrootd.cfg:

<verbatim>
xrd.port any
xrd.port 1094 if xrootd.unl.edu

xrootd.fslib /usr/lib/libXrdOfs.so
ofs.osslib /usr/lib/libXrdHdfs.so
ofs.authorize 1

oss.localroot /mnt/hadoop
all.export /hello_world forcero
all.export /user
ofs.trace all
xrd.trace all

all.role server
all.role manager if xrootd.unl.edu

all.manager xrootd.unl.edu:1213

cms.allow host *

xrootd.seclib /usr/lib/libXrdSec.so

sec.protocol /usr/lib gsi -certdir:/etc/grid-security/certificates -cert:/etc/grid-security/xrd/xrdcert.pem -key:/etc/grid-security/xrd/xrdkey.pem -crl:3

acc.authdb Authfile
acc.audit deny grant

all.adminpath /var/run/xrootd
</verbatim>


For now, write the following into Authfile:
<verbatim>
u * /store lr
</verbatim>
This allows anyone to read from /store.


---++ Running Xrootd

Start the xrootd and cmsd servers with one command

<verbatim>
service xrootd start
</verbatim>

To start xrootd automatically at boot time:

<verbatim>
chkconfig xrootd on
</verbatim>

---++ Validation and debugging (NEEDSWORK)


Now, test the clients:

<verbatim>
xrdcp root://xrootd.unl.edu//some/path/in/your/HDFS
</verbatim>

Note that you give a path in your HDFS, but you use xrootd.unl.edu as the server.  This is because xrootd will form a global network of servers.

If xrootd won't start, try invoking xrootd manually with the '-d' flag at the end of the command:

<verbatim>
runuser -s /bin/bash - xrootd -c "/usr/bin/xrootd.sh -d"
runuser -s /bin/bash - xrootd -c "cmsd -c /etc/xrootd/xrootd.cfg -d"
</verbatim>

---+ TODO:

   * LFN-to-PFN translation for each site based on the CMS TFC
   * Better authorization.  Currently, your username in the Authfile is HASH.0 where HASH is the output of <verbatim>openssl x509 -noout -hash -in ~/.globus/usercert.pem</verbatim>.  This is really weird - it should integrate with PRIMA.
      * Have the authfile method complemented by native HDFS permissions.

-- Main.BrianBockelman - 14 Oct 2009

%META:FILEATTACHMENT{name="xrootd_make.diff" attachment="xrootd_make.diff" attr="" comment="" date="1255538500" path="xrootd_make.diff" size="801" stream="xrootd_make.diff" tmpFilename="/usr/tmp/CGItemp10439" user="BrianBockelman" version="2"}%
