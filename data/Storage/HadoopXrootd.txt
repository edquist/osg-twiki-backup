%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dWill_20Maier_20286302" date="1296510644" format="1.1" version="1.17"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ Installation

---++ Quick Start

Quickstart for the impatient.  This assumes you already have [[HadoopInstallation][Hadoop and FUSE]] installed on the Xrootd server.

 <verbatim>
rpm -ivh http://newman.ultralight.org/repos/hadoop/4/x86_64/caltech-hadoop-4-1.noarch.rpm
yum install xrootd
cp /etc/xrootd/xrootd_sample.cfg /etc/xrootd/xrootd.cfg # ...and edit appropriately (see below)
vi /etc/xrootd/Authfile # Edit appropriately (see below)
service xrootd start
</verbatim>

---++ Xrootd/HDFS architecture explanation

Xrootd is a very flexible distributed storage system.  The Xrootd/HDFS integration is designed to allow you to safely export your HDFS system to the wide / metro area network and to allow several sites to share a single, unified namespace.  Currently (and by design), this integration is read-only.  This allows users to collaborate, but provides encouragement to not bypass the "normal" data placement mechanisms.

We currently use two Xrootd daemons: "xrootd" and "cmsd".  The xrootd daemon is the workhorse of the system and provides the data access.  Its job is to read data from disk (with the HDFS module enabled, read data from HDFS) and to export it to the client.  The cmsd is the "Cluster Management System" daemon; these work with the xrootd and each other to determine daemon liveness, file status, and work to create the distributed system.  Each node in Xrootd runs both cmsd and xrootd.

In each distributed Xrootd system, there are at least two "special" roles.  There must be one "manager" cmsd which is in charge of determining overall system status. each manager can manage 64 other nodes; managers can manage other managers to form a tree, or can be peers to form a more resilient system.  If you are familiar with P2P systems, these are similar to "supernodes".  The second role is the "redirector", a special xrootd instance that is on the same host as the manager cmsd.  Clients initiate contact with a redirector instance; this instance does not actually move data, but further instructs the client where to find its desired data.

Here's the approximate way the data location goes:
1) Client contacts the redirector xrootd, asking for file X.
2) Redirector Xrootd asks manager cmsd to find the best copy of file X.
3) Manager cmsd queries all managed cmsd instances to find a copy of X.
4) Each cmsd queries the attached xrootd for file X.
5) If the file is found, this information travels from the datanode xrootd, to the cmsd, then to the manager xrootd
6) The manager determines the best xrootd to serve data (also taking into consideration server load, for example).
7) The redirector xrootd instructs the client where to find file X.
8) The client connects to the optimal data server and starts the session.

The whole process can be done in the order of milliseconds; mostly, it depends on network latencies and the speed of the underlying file systems.

Of course, there are many more details to this; better descriptions of the algorithms for finding data are at http://xrootd.slac.stanford.edu/.

---++ Prerequisites

The Xrootd server has the following prerequisites:

   1 You must also have already [[HadoopInstallation][installed and mounted]] Hadoop using FUSE.
   1 Xrootd is preconfigured to look for the host certificate and key in =/etc/grid-security/xrd/xrd*.pem=.  These files must exist and be readable by the xrootd user.  Using certificates in a different directory or with different names will require modifying =/etc/xrootd/xrootd.cfg=.  Make sure =/etc/grid-security/xrd/xrdcert.pem= exists with mode 644, and =/etc/grid-security/xrd/xrdkey.pem= exists with mode 400 (not 600!)
   1 The installation includes the latest CA Certificates package from the OSG as well as the fetch-crl CRL updater.
   1 It is highly recommended that you make sure your CRLs in =/etc/grid-security/certificates= exist and are up to date by running fetch-crl manually before starting xrootd the first time.  Otherwise xrootd will attempt to download CRLs itself when it starts up.
   1 The rpm/yum installation will create a 'xrootd' system account and group (uid,gid < 500) on the host system for running the xrootd process. If you would like to control the uid/gid that is used, then you should create the 'xrootd'' user and group manually before installing the rpms.

---++ YUM Installation

Remember, in order to use xrootd, you must have the [[HadoopInstallation][Hadoop and the FUSE kernel module]] already installed on your system in order to use the yum installer.

Currently, there is a bug which requires FUSE to be mounted on the xrootd node.  This will go away in the future, but FUSE is only used for "stat" calls, not data movement.

To configure your local installation for the yum repository, [[HadoopInstallation#Yum_install][follow the advice here]] to install the correct =caltech-hadoop= package for your site.

After installing the caltech-hadoop yum configuration package, you can install the xrootd server with:

<verbatim>
yum install xrootd
</verbatim>

Updates can be installed with:

<verbatim>
yum upgrade xrootd
</verbatim>

---+ Configuration

---++ File locations

As much as possible, we attempt to use standard binary, library, and file locations in the RPM packaging.

| *Location* | *Needs editing?* | *Description* |
| /etc/xrootd/xrootd.cfg | Yes | The master configuration file for rooted and cmsd |
| /etc/xrootd/Authfile | Yes | The site's authorization file |
| /etc/grid-security/grid-mapfile | Maybe | DN-to-unix username mapping |
| /etc/grid-security/xrd/xrdcert.pem | Yes | Certificate file, PEM formatted |
| /etc/grid-security/xrd/xrdkey.pem | Yes | Certificate keyfile, PEM formatted |
| /var/run/xrootd | No | Inter-process communication sockets and PID files |
| /var/log/xrootd | No | Log files |
| /mnt/hadoop | No | Mount point of HDFS on this node; we assume this location is used in the sample configs. |

---++ Xrootd.cfg

This sample configuration file is shipped with the RPM.  It is configured using the file locations listed above.

This configuration file, if used, will have your server join the global cluster centered at xrootd.unl.edu.  In the future, we will provide sample configs for larger sites that may want to have their own manager as a peer to the UNL one.

<verbatim>
# Port specifications; only the redirector needs to use a well-known port
# "any" will cause rooted to bind to any available port.  Change as needed for firewalls.
xrd.port any
xrd.port 1094 if xrootd.unl.edu

# The roles this server will play.
all.role server
all.role manager if xrootd.unl.edu
# The known managers
all.manager xrootd.unl.edu:1213
#all.manager xrootd.ultralight.org:1213

# Allow any path to be exported; this is further refined in the authfile.
all.export /

# Hosts allowed to use this xrootd cluster
cms.allow host *

# Keep this as a work-around for an xrootd bug
oss.localroot /mnt/hadoop

### Standard directives
# Simple sites probably don't need to touch these.
# Logging verbosity
xrootd.trace emsg login stall redirect
ofs.trace all -debug
xrd.trace conn
cms.trace all

# Turn on authorization
ofs.authorize 1
acc.authdb /etc/xrootd/Authfile
acc.audit deny grant

# Security configuration
sec.protocol /usr/lib gsi -certdir:/etc/grid-security/certificates -cert:/etc/grid-security/xrd/xrdcert.pem -key:/etc/grid-security/xrd/xrdkey.pem -crl:3

# Integrate with CMS TFC, placed in /etc/storage.xml
#oss.namelib /usr/lib/libXrdCmsTfc.so file:/etc/storage.xml?protocol=hadoop

xrootd.seclib /usr/lib/libXrdSec.so
xrootd.fslib /usr/lib/libXrdOfs.so
ofs.osslib /usr/lib/libXrdHdfs.so
all.adminpath /var/run/xrootd
cms.pidpath /var/run/xrootd
</verbatim>

For now, write the following into Authfile:
<verbatim>
u * /store lr
</verbatim>
This allows anyone to read from /store.  You can add specific directives for usernames from the grid-mapfile.  If you would like uscms0313 to have access to /foo, add:
<verbatim>
u uscms0313 /store/foo lr
</verbatim>

When using LCMAPS, note that it is necessary to replace the default value for the *sec.protocol* configuration directive with the value suggested in */etc/xrootd/lcmaps.cfg*, typically:

<verbatim>
sec.protocol /usr/lib64 gsi -certdir:/etc/grid-security/certificates -cert:/etc/grid-security/xrd/xrdcert.pem -key:/etc/grid-security/xrd/xrdkey.pem -crl:3 -
authzfun:libXrdLcmaps.so -authzfunparms:--osg,--lcmapscfg,/etc/xrootd/lcmaps.cfg,--loglevel,0 --gmapopt:2 --gmapto:0
</verbatim>

---++ Running Xrootd

Start the xrootd and cmsd servers with one command

<verbatim>
service xrootd start
</verbatim>

To start xrootd automatically at boot time:

<verbatim>
chkconfig xrootd on
</verbatim>

---++ Validation and Debugging

Now, test the clients:

<verbatim>
xrdcp root://xrootd.unl.edu//some/path/in/your/HDFS /tmp/test
</verbatim>

Note that you give a path in your HDFS, but you use xrootd.unl.edu as the server.  This is because xrootd will form a global network of servers.

If xrootd won't start, try invoking xrootd manually with the '-d' flag at the end of the command:

<verbatim>
runuser -s /bin/bash - xrootd -c "/usr/bin/xrootd.sh -d"
runuser -s /bin/bash - xrootd -c "cmsd -c /etc/xrootd/xrootd.cfg -d"
</verbatim>

---+ TODO:

   * LFN-to-PFN translation for each site based on the CMS TFC.  Some work has been started, but help would be greatly appreciated.
   * Better authorization.  Currently, your username in the Authfile is HASH.0 where HASH is the output of <verbatim>openssl x509 -noout -hash -in ~/.globus/usercert.pem</verbatim>.  This is really weird - it should integrate with PRIMA.
   * Have the authfile method complemented by native HDFS permissions.