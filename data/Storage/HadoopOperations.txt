%META:TOPICINFO{author="BrianBockelman" date="1285856198" format="1.1" version="1.6"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ Daily Operations

All of the admin operations must be done as root on the Hadoop namenode unless
otherwise noted.

---++ Restarting the Namenode
*The namenode is the most critical piece of your infrastructure*.  You _could_ restart it without care, but we recommend you be sufficiently paranoid for production systems.

Prior to restarting the namenode, follow these steps:
   1 Set the namenode into safemode using the following command: =hadoop dfsadmin -safemode enter=.  Wait 1 minute.
   1 Locate the namenode metadata files.  This is usually found in =${hadoop.tmp.dir}/dfs/name/current=, and will be in a different location depending on how you set up your datanode.  You may want to check the last edited timestamp to verify you are looking at the right files.  Copy these to the same directory structure on the secondary namenode.
   1 Start up the name node process manually on the secondary namenode using the command =hadoop-daemon.sh --config /etc/hadoop start namenode=.
   1 Locate the namenode process's log on the secondary namenode (the one you just started manually in the previous step).  It is often in =/var/log/hadoop=, but may differ based on your cluster's configuration.   Wait until the namenode appears to have started normally and fully processed the metadata.  If there are any errors or failures, the manually-started namenode should die with an exception.  In this case, contact the osg-hadoop mailing list _immediately_ - turning off your namenode will definitely damage your site's metadata.
   1 If all goes well, shut off your namenode and continue with your maintenance.

Ninety-nine times out of one hundred, there will be no error, and these extra steps will just cost you an extra 5 minutes of downtime.  This will provide you with the ability to avoid one-in-a-hundred type failures that can cause data loss.

*FAILURE TO FOLLOW THESE STEPS COULD RESULT IN DATA LOSS*.  Regardless of how unlikely such data loss would be, doing the above will eliminate almost all possibility.

---++ Starting and Stopping Hadoop Daemons

---+++ Init Scripts

<verbatim>
/etc/init.d/hadoop_datanode [start|stop]
/etc/init.d/hadoop_namenode [start|stop]
/etc/init.d/hadoop_fuse [start|stop]
</verbatim>

---+++ Manual

We recommend using the init scripts to start and stop daemons.
However, if you must do this manually:

<verbatim>
cd $HADOOP_HOME/bin
source ./hadoop-config.sh
./hadoop-daemon.sh --config $HADOOP_CONF_DIR start datanode
</verbatim>

The valid actions are =start= or =stop=; the valid daemons are
=datanode= or =namenode=.

---+++ Manually Mounting FUSE

To unmount:

<verbatim>
umount /mnt/hadoop
</verbatim>

To mount, after sourcing the Hadoop environment,

<verbatim>
fuse_dfs -oserver=hadoop-name -oport=9000 /mnt/hadoop -oallow_other -ordbufffer=131072
</verbatim>

---++ Hadoop Filesystem

<verbatim>
hadoop fsck / -blocks
</verbatim>

A successful check will end with these words:

<verbatim>
The filesystem under path '/' is HEALTHY
</verbatim>

A unsuccessful check will end with the following:

<verbatim>
The filesystem under path '/' is CORRUPTED
</verbatim>

For general information about HDFS, use =dfsadmin=:

<verbatim>
hadoop dfsadmin -report
</verbatim>

Similar information can be found on the name node's =dfshealth= webpage; for
example: http://dcache-head.unl.edu:8088/dfshealth.jsp.

To get the current safemode status:

<verbatim>
hadoop dfsadmin -safemode get
</verbatim>

To leave or enter safemode:

<verbatim>
hadoop dfsadmin -safemode leave

hadoop dfsadmin -safemode enter
</verbatim>

---++ FUSE

If you see the message "Transport endpoint is not connected" on nodes where
FUSE is mounted, this means that the FUSE mount has died. Connect to the node,
unmount the file system, and remount it:

<verbatim>
umount /mnt/hadoop
fuse_dfs -oserver=hadoop-name -oport=9000 /mnt/hadoop -oallow_other -ordbufffer=131072
ls /mnt/hadoop
</verbatim>

---+ Decommissioning Data Nodes

First, add the node's ip address or fully qualified name to the =hosts_exclude= file. Then, run:

<verbatim>
hadoop dfsadmin -refreshNodes
</verbatim>

The namenode logs will almost immediately log the start of the decommissioning process:

<verbatim>
2009-03-26 10:57:31,794 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Start Decommissioning node 10.3.255.254:50010
</verbatim>

The namenode web interface will also show the node in the state =Decommission In Progress=.
During the decommissioning process you will see lots of messages from the namenode asking to replicate blocks that are located on the decommissioned nodes:

<verbatim>
2009-03-26 11:08:46,814 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* ask 10.3.255.195:50010 to replicate blk_1327555646282055693_11908 to datanode(s) 10.3.255.230:50010
</verbatim>

The decommissioning is complete when you see the following message in the logs:

<verbatim>
Decommission complete for node 172.16.1.55:50010
</verbatim>

Note: The namenode must see the transition from normal host to excluded host in
order for it to realize a node is being decommissioned. If you stop the
namenode, add the file to the hosts_exclude, then start the namenode again, the
namenode will have the following complaints in the log: 

<verbatim>
ProcessReport from unregisterted node: node055:50010
</verbatim>

This is because the namenode thinks it is being contacted by a node
which was never in the system at all, not by a node which should be
decommissioned.

---+ Cleaning Up a CORRUPT Filesystem

When the namenode is in safemode, no edits to the filesystem are
allowed. First, run =fsck= and determine the extent of the
damage.  If it is acceptable to delete or otherwise move aside the
damaged files, turn off safemode, and move the file using the
following command:

<verbatim>
hadoop fsck -move
</verbatim>

This moves any files with problematic blocks into =/lost+found= in
the Hadoop namespace.

---+ Restoring from a checkpoint

First, shut down the namenode. The namenode keeps two checkpoint
images:

   * =dfs/namesecondary/current/=
   * =dfs/namesecondary/previous.checkpoint/=

Copy all the files from one of these directories into
=dfs/name/current/=.

Start the namenode again, and watch the logs for activities.

---+ Fixing Stuck and Under Replicated Files

Sometimes, a small number of blocks may remain under-replicated.
This often corresponds with blocks that were written during or
shortly before or after a namenode crash. Block replications
should occur fairly quickly (no more than 10 minutes); if the block
remains under-replicated longer than that, proceed with the
following instructions.

First, confirm that the file has under-replicated blocks:

<verbatim>
hadoop fsck <file_name>
</verbatim>

Then, set the desired number of replicas to the current
(insufficient) number of observed replicas:

<verbatim>
hadoop fsck -setrep <file_name> <actual_replicas>
</verbatim>

Using =fsck=, verify that the file no longer appears to have
under-replicated blocks. Then, reset the replication policy for the
file to the desired number:

<verbatim>
hadoop fsck -setrep <file_name> <desired_replicas>
</verbatim>

Verify again that Hadoop has begun to replicate the blocks using
=fsck=.

If several files are affected, a variation of the following script
might help:

<verbatim>
hadoop fsck / | awk '{print $1}' | grep user | tr -d ':' | sort | uniq > /tmp/stuck_replicas
cat /tmp/stuck_replicas | xargs -t -i hadoop fs -setrep 2 {}
hadoop fsck / # Make sure everything is happy
cat /tmp/stuck_replicas | xargs -t -i hadoop fs -setrep 3 {}
hadoop fsck / # Watch and see if everything becomes happy
</verbatim>

---+ Port Forwarding for the Hadoop Web Interface

This only needs to be done once:

<verbatim>
/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8088 -i eth0 -j DNAT --to-destination 172.16.100.8:50070
/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8089 -i eth0 -j DNAT --to-destination 172.16.1.3:50030
</verbatim>

---+ Running the Balancer

You may see that your datanode usage may become uneven (this is especially common for heterogeneous sites).  Optimally, all datanodes should have about the same percentage used.

To run the balancer once, execute this from the namenode:
<verbatim>
hadoop-daemon.sh —config /etc/hadoop start balancer -threshold 3
</verbatim>

Several sites have taken to adding this to =/etc/cron.hourly= to make the balancer run at all times.  If the balancer takes more than an hour to run (definitely possible, especially the first time it is run), a second one will refuse to start - so you don't need to worry about the cron job causing a pileup of processes.