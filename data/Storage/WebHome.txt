%META:TOPICINFO{author="TedHesselroth" date="1264459912" format="1.1" reprev="1.50" version="1.50"}%
---+!! *<nop>%SPACEOUT{Data Storage and Management in the OSG}%*
%ATTACHURL%/images.jpg 
%TOC%

StorageSiteMap

---++Introduction

The OSG is both a Data Grid and a Computational Grid. That is to say, management of data (as well as jobs) is a key part of the architecture and services of the common infrastructure.

*Data Driven Science*

Several factors are leading to storage becoming an increasingly important part of grid computation.

   * _Large experiments_: The scale of large experiments has led to unprecedented levels of data collection. The cost of saving data is typically small compared to the overall budget; therefore all potentially useful data is permanently archived. Furthermore, the data may of a generic nature, such that reprocessing in light of new algorithms or associated data may yield scientific results.
   * _Advances in data recording_: As the design template for sensors and scientific instruments becomes increasingly based on the digital computer, devices are now capable of capturing and saving more data than ever before. Because of the economic efficiency of storage, data may be kept for the lifetime of the project. Data reduction as a part of analysis occurs farther along in the methodology and is enacted on larger data sets.
   * _Statistical Paradigms_: New quantitative, data-driven approaches in hitherto "soft" sciences has created entirely new data sources for scientific discovery.

*The Storage Element*

Storage Elements are physical sites where data are stored and accessed, for example, physical file systems, disk caches or hierarchical mass storage systems. Storage Elements manage storage and enforce authorization policies on who is allowed to create, delete and access physical files. They enforce local as well as Virtual Organization policies for the use of storage resources. They guarantee that physical names for data objects are valid and unique on the storage device(s), and they provide data access. 

*Storage Scales*

OSG Stakeholder requirements on data management (movement, storage, access, tracking, control, monitoring) vary widely. One useful measure is the amount of storage involved.

   * _Petabyte scale:_ with a need to store and track up to 10s of petabytes, and stream and store 10s of Terabytes to be accessed locally by or delivered locally from jobs running on OSG sites. 
   * _Terabyte scale:_ with data needing to be moved to moved 'on the fly' to and from jobs scheduled to run on OSG sites. 
   * _Gigabyte scale:_ with typically many ancillary files needed by each job in order to operate on the main data store.

*Storage Environments*

To some degree the means by which storage is operated correlate with the scale.

   * _Owned storage_: This is storage owned by a VO.  Its contents are permanent, and the amount the VO may write to it is limited only by the free space available on the hardware. If the VO is based on an experiment, the owned storage might be data from an experimental apparatus.
   * _Public storage_: This is space on a Storage Element not owned by any VO, but made available by the site.  It has a predetermined size but indeterminate lifetime; a VO which intends to use a large amount of space for a long time might make informal arrangements with the site through the OSG organization. Sites are asked to set aside at least 5 % of their storage for public use. Storage may be designated either by authorization mechanisms or by the use of the space reservation feature of SRM (Storage.OpportunisticStorage).  
   * _Local storage_:  This is a different kind of storage from that of the Storage Element in that it is local to the worker node, and leased for the duration of the batch slot lease only. The working directory of the executable is in this space. Anything left behind after a job vacates a batch slot should be deleted before a new job is given that slot. It’s good practice if a job cleans up before vacating the slot. It’s good practice for a site not to rely on jobs cleaning up after themselves. In OSG we guarantee 10GByte of this space per batch slot in the sense that finding less at runtime is a “ticketable offense”, i.e. the job’s owner may rightfully open a ticket with the GOC for a site that does not provide at least 10GByte per batch slot.

*Storage in the Open Science Grid*

Site numbers. VO numbers. Software roll call. Site/VO/Software triangle. Staff. Responsibilities. Services. See below.
 
---++ Storage for the End User

*Overview*

The end user of storage is primarily concerned with data movement. Other aspects, such as operation of the Storage Element, space management, and policies for retention and access are handled by the Virtual Organization or site. If you are already a member of a Virtual Organization, it may be useful to consult the documentation for your VO regarding data movement, as tools and computing models for your research may be available. 

*Data Movement Patterns*

While one might choose the kind of data movement that best fits the processing, it is desirable that the processing also be designed to facilitate efficient use of storage resources. In addition, the data movement portion of the computational model may depend on other factors, such as the capabilities of co-located Compute Elements and the needs of other projects acting on the same data. There are several patterns of data movement that have evolved and can be found on the Open Science Grid.

   * _Files moved through Automated Data Distribution_: (VDT: RFT, Other: Phedex,FTS) Client don't need to maintain the state. Schedule large numbers. Report status.
   * _Files moved in a staging process_: Has some of the features of Data Distribution, but more closely tied to job submission. DAGMAN+Pegasus, STORK
   * _Files moved during job execution_: Whole files input and output. Results returned to user. Lightweight -wget, squid. Larger, multichannel -see client tools.
   * _Files read during job execution_: Posix. SE mounts on worker nodes. Pre-loaded libraries.

   * ReleaseDocumentation.LocalStorageConfiguration

*Client Tools*

   * gridftp (globus-url-copy, uberftp)
   * SRM clients (Fermliab SRM Clients, LBL SRM Clients (Storage.SRMv2Client), lcg-utils
   * implementation-specific (dccp, xrootd clients)

*Discovery*

   * See secion below on Information Services.

Authorization mechanisms generally rely on the Grid Security Infrastructure; therefore proxies created through VOMS servers are required. 

*Getting Help*



Your Virtual Organization, GOC ticketing system.

For more information, please follow this link to StorageEndUser.


---++Storage for the Application Developer
*Overview*

*Collaborative*
   * Developer Groups
   * VO-developed software
   * Standards

*Software*
   * Client libraries
   * File Transfer Services (VDT: RFT, Other: Phedex,FTS)
   * Catalogs

For more information, please follow this link to StorageApplicationDeveloper.

---++Storage for the Site Administrator
*Overview*

   * Use Patterns
   * Planning and Configuration
   * Installation
   * Validation

For more information, please follow this link to StorageSiteAdministrator (which links to installation docs).


---++ Storage Infrastructure Software
*Overview*

*The Storage Architecture*
   * _Distributed Storage_:
   * _Resource Management_:
   * _Data transfer_:
   * _Replication_:
   * _Archiving_:
   * _Namespace_:

   Information Services

   * _Catalogs_:
   * _Monitoring_:
   * _Discovery_:
   * _Accounting_:

The first four are typically bundled together and make up what is known as a "Storage Element". The latter four are realized as independent components that tie into the job execution stack as well. See "Storage Infrastructure Software", below, for details on specific implementations found in the OSG.


   * *gridftp* (globus, dCache)

   * *xrootd*

   * *Bestman* (filesystem)

   * *Bestman-xrootd* (xrootd)

   * *Hadoop SE* (gateway)

   * *SRM-dCache* tape-backed)

   * *Information Services*
    Catalogs (RLS)
    Accounting (Gratia, dCache Chronicle, Hadoop Chronicle)
    Monitoring (RSV, srmTester)
    Discovery (BDII, Generic Information Provider (GIP), Discovery Tool)

For more information, please follow this link to StorageInfrastructureSoftware.

  
---++Storage Activities in the Open Science Grid


   * _Installation Packages_:
   * _Operations Tools_:
   * _Validation_:
   * _Integration Testing_:
   * _Support_:
   * _Interoperability_:
   * _Policy Coordination_:

The scope of OSG Storage and Data management includes:

   * *Storage Element* implementation, policy, deployment and documentation about storage areas that are accessible to the OSG infrastructure. 
      * The storage resources are of clearly-defined types (tape and disk), and flavor (permanent, persistent, transient, and volatile). 
      * Interfaces for the writing and reading of data by applications to and from storage over the grid. 
      * Policies and agreements for use of Storage Resources on OSG. 
   * *Local Storage* implementation, policy, deployment and documentation of storage accessible to the applications when running on an OSG site. 
      * Definition of the data and storage areas available in the job's execution environment. 
      * Policies and agreements for the use and management of local storage on OSG sites. 
   * *Storage Interface Definitions* with respect to interoperability of storage resources across Grids.

*Weekly Storage Activity Meetings*

Meetings are every Wednesday
   * *Time:* 3:00 pm Central 
   * *Call:* 510-665-5437 
   * *Meeting ID:* 0655 

See also the [[Storage.MeetingMinutes][Minutes from past meetings]]

*OSG Storage Mailing List*
osg-storage@opensciencegrid.org

*OSG Storage Workshops*

   * [[http://indico.fnal.gov/conferenceOtherViews.py?view=standard&confId=2538][2009 OSG Storage Forum]]
   * Storage.HdfsWorkshop

*The OSG Storage Software Roadmap*

The OSG is actively involved in the planning and direction of future storage software. Here is one possible plan for further infrastructure capabilities.

   * Matchmaking.
   * Data Placement.
   * Catalogs.
   * Replication.
   * Grid Buffering.

---++ Further Information

If you still have questions about Storage on the Open Science Grid, more Storage and Data Management documentation on the OSG TWiki may be found in ReleaseDocumentation.

You may also open a GOC ticket, or write directly to the OSG Storage mailing list, see above. Note: you must first be added to the mailing list to post to it.


   * images.jpg: <br />
     <img src="%ATTACHURLPATH%/images.jpg" alt="images.jpg" width='119' height='91' />    

%META:FILEATTACHMENT{name="August_31_2005.txt" attr="h" autoattached="1" comment="Meeting Minutes from 31st August 2005" date="1126120848" path="August_31_2005.txt" size="1968" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="August_17_2005.txt" attr="h" autoattached="1" comment="August 17th 2005 Meeting Minutes" date="1126127229" path="August_17_2005.txt" size="766" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="WhatisaSE.bmp" attr="h" autoattached="1" comment="A SE architecture" date="1126127034" path="WhatisaSE.bmp" size="2045006" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="srm-dcacheactivitystatusplan.rtf" attr="h" autoattached="1" comment="SRM-DCACHE" date="1126128449" path="srm-dcacheactivitystatusplan.rtf" size="4705" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="11thmay2005.txt" attr="h" autoattached="1" comment="11th May 2005 Meeting Minutes" date="1126127198" path="11thmay2005.txt" size="2436" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="SRM1.book.chapter.pdf" attr="h" autoattached="1" comment="DRM-SRM" date="1126128409" path="SRM1.book.chapter.pdf" size="106931" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="September_28_2005.txt" attr="h" autoattached="1" comment="September28th_2005_Updated_Meeting_Minutes" date="1127987797" path="September_28_2005.txt" size="3180" user="SuryaPathak" version="1.2"}%
%META:FILEATTACHMENT{name="SRMsInOSG.pdf" attr="h" autoattached="1" comment="SRM's in OSG" date="1126128425" path="SRMsInOSG.pdf" size="40179" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="August_2005.txt" attr="h" autoattached="1" comment="August 3rd 2005 Meeting Minutes" date="1126127256" path="August_2005.txt" size="1898" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="WhatisLStore.rtf" attr="h" autoattached="1" comment="Overview of LStore project" date="1126128308" path="WhatisLStore.rtf" size="5681" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="October_12_2005.txt" attr="" autoattached="1" comment="October12th Meeting Minutes" date="1129155777" path="October_12_2005.txt" size="2643" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="DRMReadinessplan.txt" attr="h" autoattached="1" comment="DRM Readiness Plan" date="1126128347" path="DRMReadinessplan.txt" size="2363" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="QuickInstallGuide.rtf" attr="h" autoattached="1" comment="DRM Quick Install Guide" date="1126128367" path="QuickInstallGuide.rtf" size="9956" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="SPDInitialDesignDoc.doc" attr="h" autoattached="1" comment="Initial Project plan: SPD (now LStore)" date="1126128530" path="SPDInitialDesignDoc.doc" size="265728" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="Storage-UltraLight.ppt" attr="" autoattached="1" comment="Ultralight Talk at OSG Consortium Meeting" date="1156371070" path="Storage-UltraLight.ppt" size="2144768" user="Main.EileenBerman" version="1"}%
%META:FILEATTACHMENT{name="images.jpg" attachment="images.jpg" attr="" comment="" date="1259104985" path="images.jpg" size="2519" stream="images.jpg" tmpFilename="/usr/tmp/CGItemp7980" user="TanyaLevshina" version="1"}%
