%META:TOPICINFO{author="TedHesselroth" date="1265836777" format="1.1" reprev="1.55" version="1.55"}%
%DOC_STATUS_TABLE%

---+!! *<nop>%SPACEOUT{Data Storage and Management in the OSG}%*
%ATTACHURL%/images.jpg 
%TOC{depth="2"}% 

StorageSiteMap

---++Introduction

The OSG is both a Data Grid and a Computational Grid. That is to say, management of data (as well as jobs) is a key part of the architecture and services of the common infrastructure.

---+++Data driven science

Several factors are leading to storage becoming an increasingly important part of grid computation.

   * _Large experiments_: The scale of large experiments has led to unprecedented levels of data collection. The cost of saving data is typically small compared to the overall budget; therefore all potentially useful data is permanently archived. Furthermore, the data may of a generic nature, such that reprocessing in light of new algorithms or associated data may yield scientific results.
   * _Advances in data recording_: As the design template for sensors and scientific instruments becomes increasingly based on the digital computer, devices are now capable of capturing and saving more data than ever before. Because of the economic efficiency of storage, data may be kept for the lifetime of the project. Data reduction as a part of analysis occurs farther along in the methodology and is enacted on larger data sets.
   * _Statistical Paradigms_: New quantitative, data-driven approaches in hitherto "soft" sciences has created entirely new data sources for scientific discovery.

---+++The Storage Element

Storage Elements are physical sites where data are stored and accessed, for example, physical file systems, disk caches or hierarchical mass storage systems. Storage Elements manage storage and enforce authorization policies on who is allowed to create, delete and access physical files. They enforce local as well as Virtual Organization policies for the use of storage resources. They guarantee that physical names for data objects are valid and unique on the storage device(s), and they provide data access. 

---+++Storage scales

OSG Stakeholder requirements on data management (movement, storage, access, tracking, control, monitoring) vary widely. One useful measure is the amount of storage involved.

   * _Petabyte scale:_ with a need to store and track up to 10s of petabytes, and stream and store 10s of Terabytes to be accessed locally by or delivered locally from jobs running on OSG sites. 
   * _Terabyte scale:_ with data needing to be moved to moved 'on the fly' to and from jobs scheduled to run on OSG sites. 
   * _Gigabyte scale:_ with typically many ancillary files needed by each job in order to operate on the main data store.

---+++Storage environments

To some degree the means by which storage is operated correlate with the scale.

   * _Owned storage_: This is storage owned by a VO.  Its contents are permanent, and the amount the VO may write to it is limited only by the free space available on the hardware. If the VO is based on an experiment, the owned storage might be data from an experimental apparatus.
   * _Public storage_: This is space on a Storage Element not owned by any VO, but made available by the site.  It has a predetermined size but indeterminate lifetime; a VO which intends to use a large amount of space for a long time might make informal arrangements with the site through the OSG organization. Sites are asked to set aside at least 5 % of their storage for public use. Storage may be designated either by authorization mechanisms or by the use of the space reservation feature of SRM (Storage.OpportunisticStorage).  
   * _Local storage_:  This is a different kind of storage from that of the Storage Element in that it is local to the worker node, and leased for the duration of the batch slot lease only. The working directory of the executable is in this space. Anything left behind after a job vacates a batch slot should be deleted before a new job is given that slot. It’s good practice if a job cleans up before vacating the slot. It’s good practice for a site not to rely on jobs cleaning up after themselves. In OSG we guarantee 10GByte of this space per batch slot in the sense that finding less at runtime is a “ticketable offense”, i.e. the job’s owner may rightfully open a ticket with the GOC for a site that does not provide at least 10GByte per batch slot.

---+++Storage in the Open Science Grid

Site numbers. VO numbers. Software roll call. Site/VO/Software triangle. Staff. Responsibilities. Services. See below.

---+++ Comments
| Please fill in the sections for statistical paradigms and Storage in OSG | Main.TedHesselroth | 10 Feb 2010 - 16:41 |
| Local Storage needs link . e.g https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/LocalStorageConfiguration | Main.TanyaLevshina | 10 Feb 2010 - 20:26 |
%COMMENT{type="tableappend"}%

 
---++ Storage for the End User
#IntroStorageEndUser

---+++Overview

The end user of storage is primarily concerned with data movement. Other aspects, such as operation of the Storage Element, space management, and policies for retention and access are handled by the Virtual Organization or site. If you are already a member of a Virtual Organization, it may be useful to consult the documentation for your VO regarding data movement, as tools and computing models for your research may be available. 

---+++Data movement patterns

There are several patterns of data movement that have evolved and can be found in use on the Open Science Grid. In general, these patterns are used in combination, where the access. For details on specific implementations, see the StorageEndUser page. 

   * _Files moved through Automated Data Distribution_: (VDT: RFT, Other: Phedex,FTS) When there are a large number of files to be moved, or data movement is an ongoing process, or a file catalog is employed an extra layer of software to handle data transfers may be used. Such a layer provides features that the simple clients lack. Some features of a data movement services are: taking requests for a set of transfers, handling requests according to file catalog name, subscribing to a published data stream, scheduling and initiating transfers, reporting on the status of transfers, and retrying failed transfers.
   * _Files moved in a staging process_: A staging process may have some of the features of a data distribution service, but is more closely tied to job submission. The data movement request is part of a workflow that may include job execution. As a simple example, the workflow may indicate, "move the data in to the site, if that succeeds, run the analysis on it." File transfers and monitoring are handled by the workflow management system.  DAGMAN+Pegasus, STORK
   * _Files moved during job execution_: Jobs running on worker nodes may also move files within the job context. Clients exist in the software environment for simple data movement and may be invoked as part of a job sequence. This approach has the advantage of simplicity, but may be inefficient when the same file is repeatedly moved by different jobs, or only a fraction of the content of a moved file is actually needed. Whenever data is moved out from a worker node, the network topology needs to be taken into account; when the worker node in on a private network, not all transfer modes will be available. Whole files input and output. Results returned to user. Lightweight -wget, squid. Larger, multichannel -see client tools. firewall
   * _Files read during job execution_: Strictly speaking, this should be referred to as data access rather than data movement.  Rather than moving a whole file, job executables may read read from a file while leaving it in place. Files available from an NFS system as a part of LocalStorageConfiguration may be read through standard language calls. Most Storage Element implementations also support posix IO, though in some cases their use in complicated by the need for preloaded libraries. The Storage Element namespace must also be mounted on the worker node, which is not always the case. Finally, some Storage Elements do not allow appending data to a file, due to difficulties associated with maintaining replicas and tape-backed copies. The use of file access techniques rather than data movement makes for efficient use of bandwidth, and simplifies job management by making data placement a separate process.


   * ReleaseDocumentation.LocalStorageConfiguration

---+++Client tools

Clients for file transfer can be divided into three categories. Documentation for specific implementations may be found on the StorageEndUser page. 

   * Clients for gridftp. Gridftp clients and servers comply with the gsiftp protocol, see the documentation. Included in the OSG software are globus-url-copy and uberftp.
   * Clients for SRM. Storage Resource Manager clients and servers comply with the [[SRM][srm]] specification. Three implementations are included in the OSG software: the Fermliab SRM Clients, the LBL SRM Clients (Storage.SRMv2Client), and glite's lcg-utils.
   * Implementation-specific clients. Storage software implementations often have their own clients which can be used to access the data but do not conform to any external specification. In the OSG software, such clients are dccp for dCache and xrootdutils for xrootd.

SRM and gsiftp rely on the Grid Security Infrastructure for their authorization mechanismsy; therefore proxies created through VOMS servers are required for their clients to operate. 

---+++Discovery

Knowing what sites authorize your VO, what the software environment is, and what the endpoints are, among other information, is a requirement for being able to use storage elements. The OSG runs an information service, the OSG BDII, from which the needed information may be discovered. See the section below on Information Services.


---+++Getting help

Requests for help may be submitted through the [[][GOC ticketing system]]. Tickets are initially assigned to your Virtual Organization, which then may dispatch or escalate the ticket to another organization.

For more information on storage for the end user, please follow this link to StorageEndUser.


---+++ Comments
| Need GOC address. | Main.TedHesselroth | 10 Feb 2010 - 16:49 |
| typo: &#34; be moved to moved&#34;&#60;br /&#62;&#60;br /&#62;GOC address http://www.grid.iu.edu/ | Main.TanyaLevshina | 10 Feb 2010 - 20:20 |
%COMMENT{type="tableappend"}%


---++Storage for the Application Developer

---+++Overview

While one might choose the kind of data movement that best fits the processing, it is also desirable that the processing be designed to facilitate efficient use of storage resources. In addition, the data movement portion of the computational model may depend on other factors, such as the capabilities of co-located Compute Elements and the needs of other projects acting on the same data. 

---+++Collaborative
   * Developer Groups
   * VO-developed software
   * Standards

---+++Software
   * Client libraries
   * File Transfer Services (VDT: RFT, Other: Phedex,FTS)
   * Catalogs

For more information, please follow this link to StorageApplicationDeveloper.


---+++ Comments
%COMMENT{type="tableappend"}%


---++Storage for the Site Administrator

---+++Overview

   * Use Patterns
   * Planning and Configuration
   * Installation
   * Validation

For more information, please follow this link to StorageSiteAdministrator (which links to installation docs).


---+++ Comments
%COMMENT{type="tableappend"}%


---++ Storage Infrastructure Software
*Overview*

*The Storage Architecture*
   * _Distributed Storage_:
   * _Resource Management_:
   * _Data transfer_:
   * _Replication_:
   * _Archiving_:
   * _Namespace_:

   Information Services

   * _Catalogs_:
   * _Monitoring_:
   * _Discovery_:
   * _Accounting_:

The first four are typically bundled together and make up what is known as a "Storage Element". The latter four are realized as independent components that tie into the job execution stack as well. See "Storage Infrastructure Software", below, for details on specific implementations found in the OSG.


   * *gridftp* (globus, dCache)

   * *xrootd*

   * *Bestman* (filesystem)

   * *Bestman-xrootd* (xrootd)

   * *Hadoop SE* (gateway)

   * *SRM-dCache* tape-backed)

   * *Information Services*
    Catalogs (RLS)
    Accounting (Gratia, dCache Chronicle, Hadoop Chronicle)
    Monitoring (RSV, srmTester)
    Discovery (BDII, Generic Information Provider (GIP), Discovery Tool)

For more information, please follow this link to StorageInfrastructureSoftware.


---+++ Comments
%COMMENT{type="tableappend"}%


---++Storage Activities in the Open Science Grid


   * _Installation Packages_:
   * _Operations Tools_:
   * _Validation_:
   * _Integration Testing_:
   * _Support_:
   * _Interoperability_:
   * _Policy Coordination_:

The scope of OSG Storage and Data management includes:

   * *Storage Element* implementation, policy, deployment and documentation about storage areas that are accessible to the OSG infrastructure. 
      * The storage resources are of clearly-defined types (tape and disk), and flavor (permanent, persistent, transient, and volatile). 
      * Interfaces for the writing and reading of data by applications to and from storage over the grid. 
      * Policies and agreements for use of Storage Resources on OSG. 
   * *Local Storage* implementation, policy, deployment and documentation of storage accessible to the applications when running on an OSG site. 
      * Definition of the data and storage areas available in the job's execution environment. 
      * Policies and agreements for the use and management of local storage on OSG sites. 
   * *Storage Interface Definitions* with respect to interoperability of storage resources across Grids.

*Weekly Storage Activity Meetings*

Meetings are every Wednesday
   * *Time:* 3:00 pm Central 
   * *Call:* 510-665-5437 
   * *Meeting ID:* 0655 

See also the [[Storage.MeetingMinutes][Minutes from past meetings]]

*OSG Storage Mailing List*
osg-storage@opensciencegrid.org

*OSG Storage Workshops*

   * [[http://indico.fnal.gov/conferenceOtherViews.py?view=standard&confId=2538][2009 OSG Storage Forum]]
   * Storage.HdfsWorkshop

*The OSG Storage Software Roadmap*

The OSG is actively involved in the planning and direction of future storage software. Here is one possible plan for further infrastructure capabilities.

   * Matchmaking.
   * Data Placement.
   * Catalogs.
   * Replication.
   * Grid Buffering.


---+++ Comments
%COMMENT{type="tableappend"}%


---++ Further Information

If you still have questions about Storage on the Open Science Grid, more Storage and Data Management documentation on the OSG TWiki may be found in ReleaseDocumentation.

You may also open a GOC ticket, or write directly to the OSG Storage mailing list, see above. Note: you must first be added to the mailing list to post to it.


---+++ Comments
%COMMENT{type="tableappend"}%

   * images.jpg: <br />
     <img src="%ATTACHURLPATH%/images.jpg" alt="images.jpg" width='119' height='91' />    


<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = TedHesselroth

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = Storage
   * Local DOC_AREA       = 

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = All

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = Knowledge
   * Local DOC_TYPE       = 
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  TanyaLevshina
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->

%META:FILEATTACHMENT{name="August_31_2005.txt" attr="h" autoattached="1" comment="Meeting Minutes from 31st August 2005" date="1126120848" path="August_31_2005.txt" size="1968" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="August_17_2005.txt" attr="h" autoattached="1" comment="August 17th 2005 Meeting Minutes" date="1126127229" path="August_17_2005.txt" size="766" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="WhatisaSE.bmp" attr="h" autoattached="1" comment="A SE architecture" date="1126127034" path="WhatisaSE.bmp" size="2045006" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="srm-dcacheactivitystatusplan.rtf" attr="h" autoattached="1" comment="SRM-DCACHE" date="1126128449" path="srm-dcacheactivitystatusplan.rtf" size="4705" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="11thmay2005.txt" attr="h" autoattached="1" comment="11th May 2005 Meeting Minutes" date="1126127198" path="11thmay2005.txt" size="2436" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="SRM1.book.chapter.pdf" attr="h" autoattached="1" comment="DRM-SRM" date="1126128409" path="SRM1.book.chapter.pdf" size="106931" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="September_28_2005.txt" attr="h" autoattached="1" comment="September28th_2005_Updated_Meeting_Minutes" date="1127987797" path="September_28_2005.txt" size="3180" user="SuryaPathak" version="1.2"}%
%META:FILEATTACHMENT{name="SRMsInOSG.pdf" attr="h" autoattached="1" comment="SRM's in OSG" date="1126128425" path="SRMsInOSG.pdf" size="40179" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="August_2005.txt" attr="h" autoattached="1" comment="August 3rd 2005 Meeting Minutes" date="1126127256" path="August_2005.txt" size="1898" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="WhatisLStore.rtf" attr="h" autoattached="1" comment="Overview of LStore project" date="1126128308" path="WhatisLStore.rtf" size="5681" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="October_12_2005.txt" attr="" autoattached="1" comment="October12th Meeting Minutes" date="1129155777" path="October_12_2005.txt" size="2643" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="DRMReadinessplan.txt" attr="h" autoattached="1" comment="DRM Readiness Plan" date="1126128347" path="DRMReadinessplan.txt" size="2363" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="QuickInstallGuide.rtf" attr="h" autoattached="1" comment="DRM Quick Install Guide" date="1126128367" path="QuickInstallGuide.rtf" size="9956" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="SPDInitialDesignDoc.doc" attr="h" autoattached="1" comment="Initial Project plan: SPD (now LStore)" date="1126128530" path="SPDInitialDesignDoc.doc" size="265728" user="SuryaPathak" version="1.1"}%
%META:FILEATTACHMENT{name="Storage-UltraLight.ppt" attr="" autoattached="1" comment="Ultralight Talk at OSG Consortium Meeting" date="1156371070" path="Storage-UltraLight.ppt" size="2144768" user="Main.EileenBerman" version="1"}%
%META:FILEATTACHMENT{name="images.jpg" attachment="images.jpg" attr="" comment="" date="1259104985" path="images.jpg" size="2519" stream="images.jpg" tmpFilename="/usr/tmp/CGItemp7980" user="TanyaLevshina" version="1"}%
