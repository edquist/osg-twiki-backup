%META:TOPICINFO{author="BrianBockelman" date="1243451838" format="1.1" version="1.13"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ General Prerequisites

Hadoop will run anywhere that Java is supported. Note that versions
of !OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless
groups on Linux; these groups confuse Hadoop and prevent its
components from starting up successfully. If you plan to install
Hadoop on a Linux !OpenAFS client, make sure you're running at least !OpenAFS 1.4.7.


---+   Pacman Installation

---++    Prerequisites

The installation process is based on [[ReleaseDocumentation.PacmanBestPractices][Pacman]]; 
see the [[ReleaseDocumentation.WebHome][release documentation]] for
help on [[ReleaseDocumentation.PacmanInstall][installing Pacman]] if
you do not have it already.

Next, determine the node which will be your namenode and the list of
the data nodes.  Finally, determine where the Hadoop software will
be installed; we recommend either a pre-existing Pacman install
(such as the OSG) or an NFS mount.

---++    Hadoop

The Hadoop install is relatively automated:

<verbatim>
cd $VDT_LOCATION
echo "http://t2.unl.edu/store/cache" > trusted.caches
echo "http://vdt.cs.wisc.edu/vdt_1101_cache" >> trusted.caches
export VDTSETUP_AGREE_TO_LICENSES=y
pacman -get http://t2.unl.edu/store/cache:Hadoop
pacman -get http://t2.unl.edu/store/cache:Hadoop-Config
</verbatim>

The difference between the Hadoop and Hadoop-Config packages is that
Hadoop-Config includes a set of handy aliases for interacting with
HDFS and a set of init scripts.  You may skip that one if you like.

We recommend also installing FUSE on all servers, and GridFTP-HDFS
on external servers.

---++ Hadoop Configuration

We have included some sane default values for the Hadoop config; you
ought to put any changes in hadoop-site.xml, not hadoop-default.xml.
We recommend the following changes:

   $ =hadoop-site.xml=: site configuration file.
      $ =fs.default.name=: Default filesystem name.  Must be of the form 'hdfs://$HOSTNAME:$PORT'.  The recommended port is 9000.
      $ =hadoop.tmp.dir=: location of the node's disk storage; may be a
         comma-separated list of mount points (I.e., /scratch/hadoop). Note: 
         do not put a space between the commas.
   $ =conf/slaves=: list of datanodes attached to the cluster, one per
      line. Only needed if you plan to use the provided scripts to start and stop the datanodes.
   $ =conf/hadoop-metrics.properties=: Ganglia configuration.
      $ =*.server=: Ganglia location; only needed if you want to use Ganglia to monitor Hadoop.
   $ =conf/hadoop-env.sh=: daemon stdout redirection (default:
      =/var/log=). Note: the log4j logs go to =${hadoop.tmp.dir}/logs=
      and aren't affected by this configuration option.
   $ =${hadoop.tmp.dir}/=: general-purpose directory. Note: if this
      directory does not exist, none of the daemons will start up.
   $ =${hadoop.tmp.dir}/hosts_exclude=: zero-length file. Note: if
      this file does not exist, the namenode will not start up.

The hadoop-site.xml is blank by default. The XML schema used matches that of the hadoop-default.xml.  Here's an example one:

<verbatim>
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://hadoop-name:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/scratch/hadoop</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
</configuration>
</verbatim>

In addition, it may be necessary to increase the open file limit on
all the datanodes to 8192 or more. This configuration varies by
platform.  Usually, the number of open files is proportional to the size of the node; a 4TB node will probably use 10x less file descriptors than a 40TB node.

The namenode must be formatted before it can be used, which can be accomplished by doing the following:
<verbatim>
. $VDT_LOCATION/setup.sh
hadoop namenode -format
</verbatim>

Now, try starting up the namenode and datanode
daemons and reading the logfiles to make sure they can startup
successfully.

<verbatim>
. $VDT_LOCATION/setup.sh
start-dfs.sh
</verbatim>

The logfiles are located in

${hadoop.tmp.dir}/logs

Look for errors at this point.

If you have slaves set up in the slaves file, then the startup
script will attempt to launch daemons through SSH.  If you use this
method, you must have passwordless SSH installed for the user
running Hadoop.  Passwordless SSH is only used for launching the
daemons; it is by no means required to run Hadoop; many sites don't
use this and use the VDT-provided init.d script instead.

---++    FUSE

FUSE provides a POSIX interface to the Hadoop filesystem. Hadoop
itself provides the FUSE-DFS filesystem, but you must install FUSE
separately.


*IMPORTANT*: The version of FUSE (2.7.4) we use is *not* compatible with fuse-2.6.3, which is in the RHEL5 distribution.  Make sure that you don't have any fuse 2.6.3 RPMs (or fuse 2.6.3 kernel modules) installed before proceeding.  If in doubt, the following should not have any RPMs listed except for FUSE 2.7.4:

<verbatim>
rpm -qa | grep fuse
</verbatim>

To install FUSE, you must first install the kernel sources for the
current kernel version running on the nodes where you plan to mount
FUSE. This version must match EXACTLY the version reported by =uname -r=.
If you're using =yum=, you should install either the =kernel-devel=
or =kerenl-smp-devel= RPMS.

If the FUSE installation fails but you are sure that the correct
kernel sources are present, you can override the kernel source check
by setting the following environment variable:

<verbatim>
export VDT_HAS_KERNEL_SOURCES=1
</verbatim>

Then proceed with the installation:

<verbatim>
cd $VDT_LOCATION
pacman -get http://t2.unl.edu/store/cache:FUSE
modprobe fuse
</verbatim>

Before mounting FUSE, make sure that the desired mount point (for
example, =/mnt/hadoop=) exists. Finally, as root:

<verbatim>
source $VDT_LOCATION/setup.sh
export VDT_HDFS_FUSE_MOUNT=/mnt/hadoop
fuse_dfs -oserver=hadoop-name -oport=9000 $VDT_HDFS_FUSE_MOUNT -oallow_other -ordbufffer=65536
</verbatim>

Modify the =VDT_HDFS_FUSE_MOUNT= variable as required for your site.
This needs to be done at boot; sourcing the VDT is a necessary step
as it brings in the correct environment for FUSE.

When you run the mount command, it prints out some scary warnings:

<verbatim>
fuse-dfs didn't recognize /hadoop,-2
fuse-dfs ignoring option allow_other
</verbatim>

Ignore them.

---++    FUSE Configuration

If you installed the Hadoop-Config package above, you can easily
start FUSE on boot. First, tell the VDT what namenode and port you
use.  Add the following to =$VDT_LOCATION/vdt/etc/vdt-local-setup.sh=:

<verbatim>
export VDT_GRIDFTP_HDFS_NAMENODE="hadoop-name"
export VDT_GRIDFTP_HDFS_PORT="9000"
export VDT_HDFS_FUSE_MOUNT="/mnt/hadoop"
</verbatim>

Edit these variables as you see fit.

Enable the VDT packaging of the init script:

<verbatim>
. $VDT_LOCATION/setup.sh
vdt-control --enable hadoop_fuse
vdt-control --on
</verbatim>

---+ RPM installation

Quickstart for the impatient (without fuse):

 <verbatim>
rpm -ivh http://newman.ultralight.org/repos/hadoop/4/i386/caltech-hadoop-4-1.noarch.rpm
yum install hadoop
vi /etc/sysconfig/hadoop
service hadoop-firstboot start
service hadoop start
</verbatim>

---++ Prerequisites and Assumptions

The Hadoop RPMs require Sun java jdk 1.6.0 or later.  You can download this from http://java.sun.com.

The Hadoop init script assumes that you are not running multiple hadoop services (datanode, namenode, secondary namenode) on the same host.

The fuse interface to HDFS requires the fuse kernel module.  The stock RHEL kernels do not include the fuse kernel module.  Atrpms provides kernel modules for the most recent RHEL4 and RHEL5 kernels at www.atrpms.net.  If you are running a custom kernel, then be sure to enable the fuse module with =CONFIG_FUSE_FS=m=.  Building and installing a fuse kernel module for your custom kernel is beyond the scope of this document.

Hadoop and fuse userspace RPMs for RHEL4 and RHEL5 are available from:

<verbatim>
http://newman.ultralight.org/repos/hadoop/4/x86_64
http://newman.ultralight.org/repos/hadoop/4/i386
http://newman.ultralight.org/repos/hadoop/5/x86_64
http://newman.ultralight.org/repos/hadoop/5/i386
</verbatim>

---++ Manual Hadoop RPM install

Download the latest Hadoop RPM from the appropriate directory above.  If you wish to use the fuse interface (recommended), then you should also download the fuse and fuse-libs RPMs.

After downloading the RPMs, install it with:

<verbatim>
rpm -ivh hadoop-0.19.1*.rpm
</verbatim>

Proceed to the Hadoop configuration step below.

---++ Yum install

To further simplify the installation process, a yum repository has been set up.  To configure your local installation for the yum repository, you should install the caltech-hadoop package from:
=http://newman.ultralight.org/repos/hadoop/4/x86_64/caltech-hadoop-4-1.noarch.rpm=

You must have the fuse kernel module already installed on your system in order to use the yum installer.

After installing the caltech-hadoop yum configuration package, you can install hadoop and fuse with:

<verbatim>
yum install hadoop
</verbatim>

Updates can be installed with:

<verbatim>
yum upgrade hadoop
</verbatim>

---++ Manual fuse install

Download the hadoop-fuse, fuse, and fuse-libs rpms from the locations above.  If you downloaded the fuse-kmdl module from Atrpms, then you can install all of the RPMs with a single command:

<verbatim>
rpm -ivh hadoop-fuse*.rpm fuse-*.rpm
</verbatim>

If you obtained your fuse kernel module from elsewhere, or built the fuse module into your custom kernel, then you will need to add =--nodeps= to avoid errors:

<verbatim>
rpm -ivh --nodeps hadoop-fuse*.rpm fuse-*.rpm
</verbatim>

---++ Yum fuse install

The yum installer will attempt to resolve any dependencies during the installation process. 

<verbatim>
yum install hadoop-fuse fuse
</verbatim>

If you installed the fuse kernel module from somewhere other than the Atrpms repository, then you will need to install the fuse rpm manually as described above.  The hadoop-fuse and fuse-libs packages can still be installed with yum:

<verbatim>
rpm -ivh --nodeps fuse*.rpm
yum install hadoop-fuse fuse-libs
</verbatim>

---++ Configuration

The Hadoop RPMs install files into the standard system locations:

| Log files | =/var/log/hadoop/*= |
| PID files | =/var/run/hadoop/*.pid= |
| init scripts | =/etc/init.d/hadoop=, =/etc/init.d/hadoop-firstboot= |
| init script config file | =/etc/sysconfig/hadoop= |
| runtime config files | =/etc/hadoop/*= |
| System binaries | =/usr/bin/hadoop*= |
| JARs | =/usr/share/java/hadoop/*= |

The most common site configuration settings can be changed in =/etc/sysconfig/hadoop=.  In most cases, this file will be identical on the namenode and datanodes.  The configuration settings are documented in the file itself.  After making changes to the file, you must run =service hadoop-firstboot start= to propogate the changes to the hadoop configuration files in =/etc/hadoop=.  =hadoop-firstboot= must be run every time you make changes to /etc/sysconfig/hadoop.

This file will be saved with a =.rpmsave= extension if you ever update your hadoop rpms with rpm or yum.  Make sure to copy your settings from =/etc/sysconfig/hadoop.rpmsave= to =/etc/sysconfig/hadoop= if you ever update your hadoop rpms.  Any manual changes to the hadoop configuration files in =/etc/hadoop/= should be preserved during an upgrade, but may be overwritten when running =hadoop-firstboot=.

---++ Running Hadoop

The Hadoop rpms install a startup script in =/etc/init.d/hadoop=.  The same command is used to start hadoop services on a datanode, namenode, or secondary namenode:

<verbatim>
service hadoop start
</verbatim>

You will also want to configure hadoop to start at boot time with:

<verbatim>
chkconfig hadoop on
</verbatim>

---++ Mounting fuse at boot time

After you have installed the fuse fuse-libs rpms as well as the fuse kernel module, you can mount fuse by adding the following line to =/etc/fstab=:

(Be sure to change the =/mnt/hadoop= mount point and =namenode.host= to match your local configuration.)
<verbatim>
hdfs# /mnt/hadoop fuse server=namenode.host,port=9000,rdbuffer=131072,allow_other 0 0
</verbatim>

...and then run

<verbatim>
mount /mnt/hadoop
</verbatim>

To start the fuse mount in debug mode, you can run the fuse mount command by hand:

<verbatim>
/usr/bin/hdfs  /mnt/hadoop -o rw,server=compute-13-1,port=9000,rdbuffer=131072,allow_other -d
</verbatim>

Debug output will be printed to stderr, which you will probably want to redirect to a file.

-- Main.WillMaier - 19 Mar 2009
-- Main.MichaelThomas - 19 Mar 2009