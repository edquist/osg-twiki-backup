%META:TOPICINFO{author="WillMaier" date="1238591777" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*

---+ HDFS Documentation of the Requirements for a US CMS Tier-2 Facilities Storage Element (SE)

%TOC%

(The original version of this document can be found at: https://twiki.cern.ch/twiki/bin/view/CMS/USCMSTier2SERequirements)

---++ Requirements for a new SE technology

---+++ Management of the SE

   * Submit HDFS to the VDT for inclusion.
   * The adoption of technology into OSG involves a variety of steps.
      * The SE should be tested in the OSG integration testbed (ITB) before being included in the VDT.
      * There should be a thorough review of the critical points of the system by a team of external experts.  This review should study
      the end-to-end functionality of the system with respect to OSG VO applications.
   * In order for the SE to be integrable into existing CMS systems, it must demonstrate the ability to interface with the global data
  transfer system PhEDEx and the transfer technologies of SRM tools and FTS as well as demonstrate the ability to interface to the CMSSW
  application locally through ROOT.
   * There must be sufficient documentation of the SE so that it can be installed and operated by a site with minimal support from the original developers (i.e. nothing more than "best effort").  This documentation should be posted on the OSG Web site, and any specific issues in interfacing the external product to CMS product should be highlighted.
   * For external products, there must be a documented procedure for how problems are reported to the developers of those products, and how these problems are subsequently fixed.
   * Source code required to interface the external product to CMS products must be made available so that site operators can understand what they are operating.  If at all possible, source code for the external product itself should also be available.

---+++ Reliability of the SE

   * The SE must have well-defined and reliable behavior for recovery from the failure of any hardware components.  This behavior should be tested and documented.
   * The SE must have a well-defined and reliable method of replicating files to protect against the loss of any individual hardware system.  Alternately there should be a documented hardware requirement of using only systems that protect against data loss at the hardware level.
   * The SE must have a well-defined and reliable procedure for decommissioning hardware which is being removed from the cluster; this procedure should ensure that no files are lost when the decommissioned hardware is removed.  This procedure should be tested and documented.
   * The SE must have well-defined and reliable procedure for site operators to regularly check the integrity of all files in the SE.  This should include basic file existence tests as well as the comparison against a registered checksum to avoid data corruption.  The impact of this operation (e.g. load on system) should be documented.
   * The SE must have well-defined interfaces to monitoring systems such as Nagios so that site operators can be notified if there are any hardware or software failures.

---+++  Performance of the SE

   * All aspects of performance must be documented.
      * See Brian's white paper to be presented at ISGC; ask for a draft copy.  All of the below performance aspects are documented in this.
   * The SE must be capable of delivering at least 1 MB/s/batch slot for CMS applications such as CMSSW.  If at all possible, this should be tested in a cluster on the scale of a current US CMS Tier-2 system.
   * The SE must be capable of writing files files from the wide area network at a performance of at least 125MB/s while simultaneously   writing data from the local farm at an average rate of 20MB/s.
   * The SE must be capable of serving as an SRM endpoint that can send and receive files across the WAN to/from other CMS sites.  The SRM must meet all WLCG and/or CMS requirements for such endpoints.  File transfer rates within the PhEDEx system should reach at least 125MB/s between the two endpoints for both inbound and outbound transfers.

---++ Roadmap to production deployment at each site

   1. A candidate SE should be subject to all of the regular, low-stress tests that are performed by CMS.  These include appropriate SAM tests, job-robot submissions, and PhEDEx load tests.  The SE should pass these tests 80% of the time over an period of two weeks.  (This is also the level needed to maintain commissioned status.)
      * This has been demonstrated at Nebraska during the first two weeks of March; Nebraska has maintained commissioned status during February and March.
   1. The new storage element should be filled to 90% with CMS data.  These datasets should be chosen such that they are currently "popular" in CMS and will thus attract a significant number of user jobs.  Failures of jobs due to failure to open the file or deliver the data products from the storage systems (as opposed to user error, CE issues, etc.) should be at the level of less than 1 in 10^5 level.
      * This was tested at Nebraska; no failures were reported.  TODO: describe testing process.
   1. In addition, there should be a stress test of the SE using these same files.  Over the course of two weeks, priority should be given to skimming applications that will stress the IO system.
      * Working at Nebraska to demonstrate this with a skimming workflow from Kristian and some Cosmic data.
   1. As part of the stress tests, the site should intentionally cause failures in various parts of the storage system, to demonstrate the recovery mechanisms.
   1. No site will replace an existing SE with a proposed SE without notifying US CMS S&C management.

-- Main.BrianBockelman - 01 Apr 2009