%META:TOPICINFO{author="TanyaLevshina" date="1285967143" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="MeetingMinutes"}%
---+ %WEB% %CALC{"$PROPERSPACE(%TOPIC%)"}%

---++ Attending
<!-- List all of the members of the group who attended. Guests are listed under GUESTS.  -->
   *  Andrew Baranovski 
   *  Brian Bockelman 
   *  Ted Hesselroth 
   *  Abhishek Rana 
   *  Tanya Levshina
   *  Neha Sharma - apologies 

---+++ Guests
   * Suchandra Thapa

---++ Status report
<!-- DESCRIPTION -->

Andrew:
<pre class="screen">
no report
</pre>

Ted:
<pre class="screen">
Worked on coordinating with VDT on pacman installation for discovery
tool. It is the first time that they are using a maven build, so some
explanation was needed. Incorporated latest feedback from first testing
of pacman install by STG. Made svn branch for test cache version of
discovery tool and modified maven build to be applicable to either
branch or trunk. Updated various help messages.

Worked with Yu Jun Wu on BDII publication of non-standard Lustre SE
info. He will also add mount point info to GlueCESEBind. He accidentally
made a non-standard BDII entry that broke the cache loading function of
the discovery tool, so the dicovery tool code was changed to make it
more robust. Have not heard from Ewa on SCEC or from Mats about the
recent request for 20TB of storage. Had asked Mats to get more info from
requester; number of files, number of associated CE jobs. The 20 TB
amount would put that project beyond parameters of purely opportunistic
storage, and require communication with site adimistrators.

Worked with Timur Perelmutov to discover why gPlazma has been found to
be slow under heavy load when using the GUMS callout. The problem was
traced to the callout not being synchronized, so that when the cache
expired several threads could contact GUMS and jointly cause a timeout.
BNL was informed of the fix, and a copy sent to OSG Storage.

Studied tools for representing software architectures, towards the end
of designing the diagnostic tool, and supporting the upcoming reviews of
information service and OSG architectures.

</pre>

Neha:
<pre class="screen">

Working on vdt-dCache release:

Going through the dCache release notes, I think following 6 modifications that need to be done in our package for 1.9.5
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

1) (done) The configuration parameter "PermissionHandlerDataSource" needs to be removed 
because permission handling is now done at PnfsManager level rather than mounted namespace file system

2) (in progress) On dCap and FTP doors only, the file permission checking can now be done by PnfsManager. To enable it, 
following has to be set in the config/dCacheSetup file  permissionPolicyEnforcementPoint=PnfsManager

so, i need to add support for it in our conf file and scripts

3) (done) need to modify pnfs install script to not mount pnfs on ftp doors and node running dir domain

4) (in progress) add support so that dcache can be started as non-root

5) pnfsManager option "-storageinfo-provider" is no longer supported (need to investigate implications of this)

6) In Storage.PoolManager.conf, specifying on-cost value as a number not ending with "%" will result  in odd behavior (need to investigate implications of this)

Ticket support
</pre>

Abhishek:
<pre class="screen">

FUSE/kernel/ROCKS awkward relationship: FUSE can be nontrivial to to setup if a site does not have it as part of stock kernel (I think, RHEL 5.2+). 
In such a situation, an external kernel module needs to be installed which can be loaded at boot time. As you know, loadable kernel modules are tied to 
kernel version very closely (unless a site compiles these itself). Getting module's RPM from a third-party limits the choice of versions. 
Mirrors only host recent few release numbers. Bottomline is, it ends up dictating kernel version itself - such a site has to upgrade kernel itself to match it 
with module. In addition, if original install was ROCKS based, its GRUB loader has to be tuned differently to recognize the new kernel.

YUM based distribution management: I like Caltech's design of YUM repository. Attached is a list of packages in current distribution. 
I am inclined to continue the same model, and see if there is a need to migrate the repository to WISC or FNAL.

Need for VDT's YUM repository for common packages: A current caveat is that some packages have been factored out of regular VDT 
(E.g., Globus subsets, GUMS-client, CA certs) as RPMs. E.g., highlighted pink in attached file. This has simplified the deployment. 
Over long run, if VDT team at WISC has plans to setup a sub-repository, sites can get these packages as official RPMs. 
This can keep things consistent across overall VDT.

Pre-release evaluation and certification: Mike and I had a good discussion yesterday. He identified a need for:

  o Install testing: Possibly test on the full-matrix of flavors/versions/arch's: 32bit/64bit and EL4/EL5. Do we have this facility 
already at WISC and FNAL? E.g., for full matrix, Xen-based virtual machines can be used.

o Runtime testing: Testing on partial matrix can be sufficient.

o Scalability testing: Focus on single or limited platforms will be sufficient. This is within Igor's scope at UCSD.

Documentation: Based on contents of first golden release, current documents will have to be edited and factored into ReleaseDoc. 
I can assist in editing these.

Bestman distribution management: We can fetch the most recent stable version and include it as RPM. However, this will be a factored out version - 
different from VDT. Is this okay?

Long-term packaging changes: If we find Cloudera's distribution of core Hadoop packages appealing, it can replace the current subset of 
RPMs (directly from Apache.org). This is on a 6 month timescale.
</pre>

Alex:
<pre class="screen">
Support
  - support FTS operation from bestman to cache from Florida to UK
  - support RSV probe on bestman
  - support gsiftp server setup - cert/key file issue
  - support on bestman and gums server connection - fqan definition in
gums setup
  - support 3rd party transfer from cache to bestman-gateway - dcau issue
  - support recursive directory copying with srm-copy at NERSC for STAR
</pre>

Tanya
<pre class="screen">
 Meetings:
         2 days  ATLAS T3 meeting at Argonne
         Meeting with Ruth to discuss current status 
         Monthly meeting with xrootd developers  
         Storage Documentation meeting 
         
  Email discussions:
       local mount point 
       xrootd release and bug submission
       storage rsv probes
       storage scalability testing
      
   
  Technical work:
        Investigated problems reported by Rob Snihur on  our Xrootd test stand: 
           * cnsd is failing after ~24 of PhEDEx tests, send all necessary information to xrootd developers
           * copy is failing with obscure message if the space on server node is less then allowed limit
        Installed discovery tools from VDT - reported several  problems
        Installed gridftp - xrootd on ATLAS T3 cluster, have written "step-by-step" guide - needs more work
   
</pre>
---++ First agenda item
<!-- DESCRIPTION -->
RFC for config.ini changes (S. Thapa)
---+++ Discussion
<!-- describe the discussion that took place -->

Discussed [[https://imapserver2.fnal.gov/attach/se_changes.pdf][Suchandra's presentation] .

Summary of discussion:
   * Looks like very useful idea will simplify configuration for OSG site admins
   *  Xrootd: 
      * xrdr-storage-cache     is a mandatory  field that is missing
   * BeStMan
      * gums-host, gums-port are needed and gums_dn is not
      * globus-tcp-port-range  should be present in case of firewall
      * If this bestman installation is ONLY for xrootd that's all
      * If you plan to use it for BeStMan-gateway installation you will need to add the following fields:
         * with-allowed-paths
         * with-blocked-paths
   * BeStMan-xrootd installation should trigger xrootdfs installation
   * Gridftp installation may trigger gratia transfer probe installation
   * Gridftp
      * redirector - should be host and port (or default will be used?)
      * redirector_storage_path  is the same that redirector_storage_path in Xrootd
         * Should we worry about Prima (to connect to GUMS) here and tcp port range in case of firewall?
---++ Second Agenda Item
<!-- DESCRIPTION -->

Storage probe re-design
---+++ Discussion
<!-- describe the discussion that took place -->

We will need to re-design dcache storage probe to:
   * report information StorageElement (partially static information) and !StorageElementRecord (dynamic measurements related to link group, pools, etc)
   * get data from Information Provider instead of srm database and dcache admin API

Brian is developing similar probes for hadoop. Andrew will start working on probes for dcache. We will need to figure out what to do with xrootd.

---++ Third agenda item
<!-- DESCRIPTION -->
Tickets
---+++ Discussion
<!-- describe the discussion that took place -->

++++++++++++++++++++++++
New tickets
++++++++++++++++++++++++

Closed:

7698 - OSG-Storage: dCache: PNFS Manager not functioning properly after restart

7700 - OSG-Storage: PostgreSQL: Performance Tuning Tips

Follow up in progress:

7697 - OSG-Storage: srm-copy: "mkdir" argument failure when using -f and file:// as target url

7699 - OSG-Storage: Bestman->dCache: "failed to get initial request status" error

++++++++++++++++++++++++
Old tickets
++++++++++++++++++++++++

Closed:

7615 - Hadoop- EOF errors while doing third party transfer using srmcp/srm-copy   (Is it really closed?)

7659 - dCache - Conflicting values w.r.t space reservation

Follow up in progress:

7505/7574 - dCache - Site losing data due to files being marked as non-sticky 

6967/7010 - dCache - File replicas not being removed

6908 - dCache - log4j errors when restarting dcache 1.9.2-5

6971 - dCache - Problem with pnfs register command

No update:

7448 - dCache - Poor PNFS/Chimera performance on upgrade from 1.9.2-5 to 1.9.4.-x

7326 - dCache - misleading error message when using dccp to transfer  data

---++ Created by:
<!-- Paste your signature (below) here -->
-- Main.TanyaLevshina - 04 Nov 2009