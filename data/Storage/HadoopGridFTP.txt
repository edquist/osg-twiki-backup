%META:TOPICINFO{author="MichaelThomas" date="1280879460" format="1.1" reprev="1.12" version="1.12"}%
%META:TOPICPARENT{name="Hadoop"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---+    Installation

The installation of the Globus-based Hadoop !GridFTP server is based on the RPM format.  There used to be a [[HadoopGridFTPPacman][Pacman install]], but that has been deprecated.

---++   Quick Start

Quickstart for the impatient.  This assumes you already have the *jdk* 1.6.0 RPM installed on all relevant nodes.

 <verbatim>
rpm -ivh http://vdt.cs.wisc.edu/hadoop/osg-hadoop-1-2.el5.noarch.rpm
yum install gridftp-hdfs
vi /etc/sysconfig/hadoop # Edit appropriately.
service hadoop-firstboot start
vi /etc/grid-security/prima-authz.conf # Add gums hostname
service xinetd restart
</verbatim>

---++ Prerequisites and Assumptions

The following are the prerequisites for following this guide

   1. This page assumes you have read the [[HadoopUnderstanding][HDFS planning guide]]
   1. This page assumes that your gridftp node already has the hadoop RPM installed, /etc/sysconfig/hadoop edited, and hadoop-firstboot run.  If you have not done this yet, [[HadoopInstallation][follow the directions here]].
   1. *You do not need hadoop-fuse, FUSE kernel module, or FUSE libraries installed*

To verify that HDFS core is installed and configured correctly on your system, the following command should return results and have exit code 0:

<verbatim>
hadoop fs -ls /
</verbatim>

*IF* the results of that command are the same files as those shown in =ls -l /=, then your site has not been configured; you probably forgot to run =hadoop-firstboot=.

We make the following assumption:
   1. GUMS must be used for grid user mappings.  While a grid-mapfile may be used, the configuration of its use is not currently tested or supported.

The Gridftp server for hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration.  You should plan accordingly to provision enough gridftp servers to handle the bandwidth that your site can support.

The installation includes the latest CA Certificates package from the OSG as well as the fetch-crl CRL updater.

*Note:* You do not need FUSE mounted on !GridFTP nodes,

---++ Yum-based Installation Method
To configure your local installation for the yum repository, [[HadoopInstallation#Yum_install][follow the advice here]] to install the =osg-hadoop= package at your site.

After installing the osg-hadoop yum configuration package, you can install the gridftp-hdfs server with:

<verbatim>
yum install gridftp-hdfs
</verbatim>

Updates can be installed with:

<verbatim>
yum upgrade gridftp-hdfs
</verbatim>

Proceed to the Configuration section.

---++ RPM-based Install method

This is not the recommended method due to the inter-rpm dependencies for gridftp-hdfs.  Hadoop and gridftp-hdfs RPMs for RHEL4 and RHEL5 are available from:

<verbatim>
http://vdt.cs.wisc.edu/hadoop/stable/1.0/rhel4/i386/
http://vdt.cs.wisc.edu/hadoop/stable/1.0/rhel4/x86_64/
http://vdt.cs.wisc.edu/hadoop/stable/1.0/rhel5/i386/
http://vdt.cs.wisc.edu/hadoop/stable/1.0/rhel5/x86_64/
</verbatim>

Download the =gridftp-hdfs= package from the appropriate directory above.  Install using the following command:

<verbatim>
rpm -ivh gridftp-hdfs*.rpm
</verbatim>

---+ Configuration

The installation of gridftp-hdfs and its dependencies creates several directories.  In addition to the Hadoop installation files, you will also find:

| Log files | =/var/log/gridftp-auth.log=, =/var/log/gridftp.log= |
| xinetd files | =/etc/xinetd.d/gridftp-hdfs= |
| runtime config files | =/etc/gridftp-hdfs/*= |
| System binaries | =/usr/bin/gridftp-hdfs*= |
| System libraries | =/usr/lib64/libglobus_gridftp_server_hdfs_gcc64dbg.so*= |
| prima auth configuration | =/etc/grid-security/prima-authz.conf= |
| CA certificates | =/etc/grid-security/certificates/*= |

gridftp-hdfs reads the hadoop configuration file to learn how to talk to Hadoop.  As per the prerequisites section, you should have already edited =/etc/sysconfig/hadoop= and run =service hadoop-firstboot start=.  If you did not follow the directions, please do that now.

It is *not* necessary to start any hadoop services with =service hadoop start= if you are running a dedicated gridftp server (that is, no datanode or namenode services will be run on the host).

In =prima-authz.conf= you will need to enter the url for your gums server, as well as the path to your host certificate and key:

<verbatim>
imsContact https://your.gums.host:8443/gums/services/GUMSAuthorizationServicePort
...
serviceCert /etc/grid-security/hostcert.pem
serviceKey  /etc/grid-security/hostkey.pem
</verbatim>

The default settings in =/etc/gridftp-hdfs/*.conf= should be ok for most installations.  The file =gridftp-inetd.conf= is used by the xinetd service for starting up the gridftp server.  The file =gridftp.conf= is used by =/usr/bin/gridftp-hdfs-standalone= for starting up the gridftp server in a testing mode.  =gridftp-hdfs-local.conf= contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the xinetd and standalone gridftp server.  Some of the environment variables that can be used in =gridftp-hdfs-local.conf= include:

| Option Name | Needs Editing? | Suggested value|
| GRIDFTP_HDFS_REPLICA_MAP | No | File containing a list of paths and replica values for setting the default # of replicas for specific file paths |
| GRIDFTP_BUFFER_COUNT | No | The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop |
| GRIDFTP_FILE_BUFFER_COUNT | No | The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop |
| GRIDFTP_SYSLOG | No | Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application) |
| GRIDFTP_HDFS_MOUNT_POINT | Maybe | The location of the FUSE mount point used during the Hadoop installation.  Defaults to /mnt/hadoop.  This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native hadoop paths. *Note:* this does not imply you need FUSE mounted on !GridFTP nodes! |
| GRIDFTP_LOAD_LIMIT | No | GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20. |
| TMPDIR | Maybe | The temp directory where the file-based buffers are stored.  Defaults to /tmp. |

---++ Running gridftp-hdfs

If you were not already running the xinetd service (by default it is not installed on RHEL5), then you will need to start it with the command =service xinetd restart=.  Otherwise, the gridftp-hdfs service should be configured to run automatically as soon as the installation is finished.  If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:

<verbatim>
/usr/bin/gridftp-hdfs-standalone
</verbatim>

The standalone server runs on port 5002, handles a single gridftp request, and will log output to stdout/stderr.

---+ Next Steps
Congratulations! At this point, you should have a working Hadoop installation and !GridFTP server. Please proceed to the validation steps or the next guide, [[HadoopSRM][Hadoop SRM install]].

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = BrianBockelman

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %NO%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = FirstName
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = FirstLast
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %NO%
############################################################################################################
-->
