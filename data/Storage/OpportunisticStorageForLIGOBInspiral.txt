%META:TOPICINFO{author="TanyaLevshina" date="1282772139" format="1.1" version="1.10"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+!! *<noop>Opportunistic Storage for LIGO Binary Inspiral*
%TOC%
Requirements:
   * periodically pre-stage 1 week of data (2 weeks minimal per site)
   * 1 week of data appears to be around O(100GB).
   * a job running on the worker node needs to access to the whole weekly dataset 
   * *Optional*: data should be deleted after processing of the weekly dataset is done.  During testing, we frequently reuse data.
   * RLS catalog needs to be populated after successful transfer.
   * Data should be easily replicated to multiple sites.
   * 100K jobs at 5-15 minutes per job.
   * 25K hours per workflow.

Current Restrictions:
   * Have to run on site with posix access to the files from the worker nodes, so only sites with hdfs, lustre or panasas may be considered 
   * Currently using the -rfc compliant VOMS proxy.  This is not accepted by current !BeStMan-gateway, but accepted by Globus Gridftp servers.

Immediate Goals:
   * Document the procedure for kicking off transfers (Britta)
   * Investigate the current transfer procedure, see if it could be optimized (Brian & Derek)
   * See if the -rfc requirements can be dropped or use two proxy (change proxy location env.variable to specify in command line). The load balancing provided by SRM could be use for gridftp server selection. (Brian & Derek).
   *  Measure the transfer of weekly dataset from LIGO to Nebraska. The goal is ~ 8 hours. (Britta, Brian, & Derek)
   *  Write replication script that allowed transfer of data between sites. Provide measurements. (Derek? Brian? Tanya?)
   *  Identify the sites that LIGO could run and verify that they are ready for hosting LIGO data  (Tanya). 19 SEs claim to support LIGO. Only 5 of them have storage area mounted on worker nodes according to BDII:
| *Site Name*| *SE_ID*| *Storage Type* | *SURL* | *Test Status* | *Local !Mount* | *SA free space (8/23/10)* |Ticket|
|UCSDT2|bsrm-1.t2.ucsd.edu|!BeStMan/HDFS|srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/hadoop/ligo|%GREEN%Success%ENDCOLOR%|/hadoop/ligo|~136TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9125][9125]]|
|CIT_CMS_T2|cit-se.ultralight.org|!BeStMan/HDFS|srm://cit-se.ultralight.org:8443/srm/v2/server?SFN=/mnt/hadoop/osg/LIGO|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/osg/LIGO| ~90TB||
|Nebraska|red-srm1.unl.edu|!BeStMan/HDFS|srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/user/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/user/ligo|~156TB||
|Firefly|ff-se.unl.edu|!BeStMan/panasas|srm://ff-se.unl.edu:8443/srm/v2/server?SFN=/panfs/panasas/CMS/data/ligo|%GREEN%Success%ENDCOLOR%|/panfs/panasas/CMS/data/ligo|~40TB||
|TTU-ANTAEUS|sigmorgh.hpcc.ttu.edu|!BeStMan/Lustre|srm://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server?SFN=/lustre/hep/osg/ligo|%GREEN%Success%ENDCOLOR%| /lustre/hep/osg/ligo|~239TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9126][9126]]|
|OUHEP_OSG|ouhep2.nhn.ou.edu|!BeStMan/Lustre|srm://ouhep2.nhn.ou.edu:8443/srm/v2/server?SFN=/raid1/data/griddata|%GREEN%Success%ENDCOLOR%|raid1/data/griddata| ~1.5TB||
|UFlorida-PG|srmb.ihepa.ufl.edu|!BeStMan/Lustre|srm://srmb.ihepa.ufl.edu:8443/srm/v2/server?SFN=/lustre/raidl/user/ligo/|%RED%Error%ENDCOLOR%|!MountInfo set to none|~1TB||
|!GridUNESP_CENTRAL|se.grid.unesp.br|!BeStMan/?|srm://se.grid.unesp.br:8443/srm/v2/server?SFN=/store/ligo|%GREEN%Success%ENDCOLOR%|/store/ligo|N/A||
|!UMissHEP|umiss005.hep.olemiss.edu|!BeStMan/?|srm://umiss005.hep.olemiss.edu:8443/srm/v2/server?SFN=/osgremote/osg_data/ligo|%GREEN%Success%ENDCOLOR%|/storage005| ~7TB||
|NWICG_NDCMS|nwicg.crc.nd.edu|!BeStMan/?|srm://nwicg.crc.nd.edu:49084/srm/v2/server?SFN=/dscratch/osg/bestman|%RED%Error%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|NWICG_NotreDame|osg.crc.nd.edu|!BeStMan/?|srm://osg.crc.nd.edu:8443/srm/v2/server?SFN=/dscratch/osg/bestman|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|SBGrid-Harvard-East|osg-east.hms.harvard.edu|!BeStMan/?|srm://osg-east.hms.harvard.edu:10443/srm/v2/server?SFN=/osg/storage/data/ligo|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~3TB||
|NERSC-PDSF|pdsfsrm.nersc.gov|!BeStMan/?|srm://pdsfsrm.nersc.gov:62443/srm/v2/server?SFN=/srmcache/ligo|%RED%Error%ENDCOLOR%|!MountInfo is not defined|N/A||
|FNAL_FERMIGRID|fndca1.fnal.gov|dCache|srm://fndca1.fnal.gov:8443/srm/managerv2?SFN=/pnfs/fnal.gov/usr/fermigrid/volatile/|%GREEN%Success%ENDCOLOR%||N/A||
|GLOW|cmssrm.hep.wisc.edu|dCache|srm://cmsdcache03.hep.wisc.edu:8443/srm/managerv2?SFN=/pnfs/hep.wisc.edu/data5/LIGO|%RED%Error%ENDCOLOR%||N/A||
|BNL-ATLAS|dcsrm.usatlas.bnl.gov|dCache|srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/osg/|%GREEN%Success%ENDCOLOR%||N/A||
|Purdue-RCAC|srm-dcache.rcac.purdue.edu|dCache|srm://srm-dcache.rcac.purdue.edu:8443/srm/managerv2?SFN=/VO/ligo|%RED%Error%ENDCOLOR%||~1TB||
|SPRACE|osg-se.sprace.org.br|dCache|srm://osg-se.sprace.org.br:8443/srm/managerv2?SFN=/pnfs/sprace.org.br/data|%RED%Error%ENDCOLOR%||~30TB||
                                                                                                                                           
   * Create a !GlideInWMS workflow to run in 1-3 days at any particular site that can provide 10K-20K opportunistics hours per day to LIGO (Mats)
   * Create a standard "transfer test" which we can do between sites to show we understand how to manage transfers.
   * Discuss second level staging with Pegasus team.