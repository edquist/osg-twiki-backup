%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dBritta_20Daudert_20869038" date="1291142668" format="1.1" version="1.25"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+!! *<noop>Opportunistic Storage for LIGO Binary Inspiral*
%TOC%
Requirements:
   * periodically pre-stage 1 week of data (2 weeks minimal per site)
   * 1 week of data appears to be around O(100GB).
   * a job running on the worker node needs to access to the whole weekly dataset 
   * *Optional*: data should be deleted after processing of the weekly dataset is done.  During testing, we frequently reuse data.
   * RLS catalog needs to be populated after successful transfer.
   * Data should be easily replicated to multiple sites.
   * 100K jobs at 5-15 minutes per job.
   * 25K hours per workflow.

Current Restrictions:
   * Have to run on site with posix access to the files from the worker nodes, so only sites with hdfs, lustre or panasas may be considered 
   * Currently using the -rfc compliant VOMS proxy.  This is not accepted by current !BeStMan-gateway, but accepted by Globus Gridftp servers.

Immediate Goals:
   * Document the procedure for kicking off transfers (Britta) %GREEN%DONE%ENDCOLOR%  [[http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/FILE-TRANSFERS/][File transfer instructions]]
   * Investigate the current transfer procedure, see if it could be optimized (Brian & Derek)
   * See if the -rfc requirements can be dropped or use two proxy (change proxy location env.variable to specify in command line). The load balancing provided by SRM could be use for gridftp server selection. (Brian & Derek).
   *  Measure the transfer of weekly dataset from LIGO to Nebraska. The goal is ~ 8 hours. (Britta, Brian, & Derek) 
   *  Write replication script that allowed transfer of data between sites. Provide measurements. (Derek? Brian? Tanya?)
   *  Identify the sites that LIGO could run and verify that they are ready for hosting LIGO data  (Tanya). 19 SEs claim to support LIGO. Only 5 of them have storage area mounted on worker nodes according to BDII. %GREEN%DONE%ENDCOLOR%
   * Create a !GlideInWMS workflow to run in 1-3 days at any particular site that can provide 10K-20K opportunistics hours per day to LIGO (Mats) %GREEN%DONE%ENDCOLOR%
   * Create a standard "transfer test" which we can do between sites to show we understand how to manage transfers. 
   * Discuss second level staging with Pegasus team. %BLUE%In progress%ENDCOLOR%

Britta's wish list

   * srm-copy should work with rfc complient proxy
   * srm-copy -mkdir should create directories recursively
   * throttling at BestMan gateways
   * srm-copy error handling should be improved, e.g.
      * Pad passphrase --> one of the gridftp servers not operational
      * EOF error -->  incorrect mount point or port
   * turn  OUHEP_OSG into 64 bit cluster

|*Testing Status*| *Site Name*| *SE_ID*| *Storage Type* | *SURL* | *Test Status* | *Local !Mount* | *SA free space (8/23/10)* |Ticket|
|%RED%Fail%ENDCOLOR%|UCSDT2|bsrm-1.t2.ucsd.edu|!BeStMan/HDFS|srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/hadoop/ligo|%GREEN%Success%ENDCOLOR%|/hadoop/ligo|~136TB||
|%GREEN%Success%ENDCOLOR%|CIT_CMS_T2|cit-se.ultralight.org|!BeStMan/HDFS|srm://cit-se.ultralight.org:8443/srm/v2/server?SFN=/mnt/hadoop/osg/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/osg/ligo| ~90TB||
|%GREEN%Success%ENDCOLOR%|Nebraska|red-srm1.unl.edu|!BeStMan/HDFS|srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/user/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/user/ligo|~156TB||
|%GREEN%Success%ENDCOLOR%|Firefly|ff-se.unl.edu|!BeStMan/panasas|srm://ff-se.unl.edu:8443/srm/v2/server?SFN=/panfs/panasas/CMS/data/ligo|%GREEN%Success%ENDCOLOR%|/panfs/panasas/CMS/data/ligo|~40TB||
|%RED%Fail%ENDCOLOR%|TTU-ANTAEUS|sigmorgh.hpcc.ttu.edu|!BeStMan/Lustre|srm://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server?SFN=/lustre/hep/osg/ligo|%GREEN%Success%ENDCOLOR%| /lustre/hep/osg/ligo|~239TB||
|%GREEN%Success%ENDCOLOR%|OUHEP_OSG|ouhep2.nhn.ou.edu|!BeStMan/Lustre|srm://ouhep2.nhn.ou.edu:8443/srm/v2/server?SFN=/raid1/data/griddata|%GREEN%Success%ENDCOLOR%|/raid1/data/griddata| ~1.5TB||
|%RED%Omitted%ENDCOLOR%|UFlorida-PG|srmb.ihepa.ufl.edu|!BeStMan/Lustre|srm://srmb.ihepa.ufl.edu:8443/srm/v2/server?SFN=/lustre/raidl/user/ligo/|%RED%Error%ENDCOLOR%|!MountInfo set to none|~1TB||
|%GREEN%Success%ENDCOLOR%|!GridUNESP_CENTRAL|se.grid.unesp.br|!BeStMan/?|srm://se.grid.unesp.br:8443/srm/v2/server?SFN=/store/ligo|%GREEN%Success%ENDCOLOR%|/store/ligo|N/A||
|%RED%Fail%ENDCOLOR%|!UMissHEP|umiss005.hep.olemiss.edu|!BeStMan/?|srm://umiss005.hep.olemiss.edu:8443/srm/v2/server?SFN=/osgremote/osg_data/ligo|%GREEN%Success%ENDCOLOR%|/osgremote/osg_data/ligo| ~7TB||
|%RED%Omitted%ENDCOLOR%|NWICG_NDCMS|nwicg.crc.nd.edu|!BeStMan/?|srm://nwicg.crc.nd.edu:49084/srm/v2/server?SFN=/dscratch/osg/bestman|%RED%Error%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|%GREEN%Success%ENDCOLOR%|NWICG_NotreDame|osg.crc.nd.edu|!BeStMan/?|srm://osg.crc.nd.edu:8443/srm/v2/server?SFN=/dscratch/osg/bestman|%GREEN%Success%ENDCOLOR%||~0.2TB||
|%GREEN%Success%ENDCOLOR%|SBGrid-Harvard-East|osg-east.hms.harvard.edu|!BeStMan/?|srm://osg-east.hms.harvard.edu:10443/srm/v2/server?SFN=/osg/storage/data/ligo|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~3TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9571][9571]]|
|%RED%Omitted%ENDCOLOR%|NERSC-PDSF|pdsfsrm.nersc.gov|!BeStMan/?|srm://pdsfsrm.nersc.gov:62443/srm/v2/server?SFN=/srmcache/ligo|%RED%Error%ENDCOLOR%|!MountInfo is not defined|N/A||
|%RED%Omitted%ENDCOLOR%|FNAL_FERMIGRID|fndca1.fnal.gov|dCache|srm://fndca1.fnal.gov:8443/srm/managerv2?SFN=/pnfs/fnal.gov/usr/fermigrid/volatile/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|GLOW|cmssrm.hep.wisc.edu|dCache|srm://cmsdcache03.hep.wisc.edu:8443/srm/managerv2?SFN=/pnfs/hep.wisc.edu/data5/LIGO|%RED%Error%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|BNL-ATLAS|dcsrm.usatlas.bnl.gov|dCache|srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/osg/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|Purdue-RCAC|srm-dcache.rcac.purdue.edu|dCache|srm://srm-dcache.rcac.purdue.edu:8443/srm/managerv2?SFN=/VO/ligo|%RED%Error%ENDCOLOR%||~1TB||
|%RED%Omitted%ENDCOLOR%|SPRACE|osg-se.sprace.org.br|dCache|srm://osg-se.sprace.org.br:8443/srm/managerv2?SFN=/pnfs/sprace.org.br/data|%RED%Error%ENDCOLOR%||~30TB||
                                                                                                                                           
 Details of file transfers:

   * 11/05/10 SUBMIT HOST CRASH
      * 4 work-flows submitted at the time, each transferring 5000 files (100 per dag job)
         * Grid_UNESP_CENTRAL, OUHEP_OSG, NWICG_NotreDame, SBGrid-Harvard-East
      * Robert: due to each lbnl srm client using 200MB in RAM for each transfer process
      * Alex Sim: you need to increase the MX value in the srm-copy script for java vm size

   * SBGrid-Harvard-East
      * Resubmitted dag after submit host crash
      * Transfers fails, permission issue
      * /osg/storage/data/ligo/frames belongs to root, should be ligo
      * Opened GOC ticket 11/12
      * Disk and hardware failures at site issues at site, fixed 11/23
      * Data transfer successful

   * NWICG_NotreDame
      * bad pass phrase error cause by source red-gridftp1.unl.edu being non-operational
      * re-run work-flow, 5000 files transferred successfully, closed GOC ticket


   * UCSDT2
      * test work-flow transferring 20 files
      * can't turn off remote_initialdir with current version of Pegasus
      * remote_initialdir = /hadoop/ligo: md5sum checks fail: no execute permission of pegasus kickstart 
      *  manually removing remote_initialdir from md5sum job submit file: stage_out of data fails, pegasus:transfer sets source directory of output files to $DATA
      * Additionally: $DATA not visible from WNs --> can't set remote_initialdir to $DATA --> ihope work-flow will fail

   * TTU_ANTAEUS
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again

   * OUHEP_OSG
      * test work-flow transferring 20 files succeeds
      * test work-flow transferring 5000 files fails
         * repeated srm-copy fail
         * SRM-CLIENT: Thu Oct 28 15:39:23 PDT 2010 Server did not respond and client is exiting. SrmPrepareToPut
      * Alex Sim says:
<pre class="screen">
if the machine is too busy, the connection
will likely take a long time to be established (in part it's GSI and
another busy network), and if it's too long, srm-copy might get
timed-out. You can increase the timeout value in that case. It doesn't
seem that the SRM server is too busy or crashed in this particular case.
</pre>

      * Resubmitted work-flow with time out 3600s (default is 1800)
      * Work-flow completes without fails

   * GridUNESP_CENTRAL
      * test work-flow transferring 20 files succeeds
      * 10/29 - present: site down, MYOSG: CE service is in UNKNOWN status
      * 11/11 submitted rescue dag which succeeds without further fails 
      * Observed repeated time outs when transferring 3 day data set
     
   * UMiss_HEP     
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again

 Running IHOPE with SRM setup on a 3 day data set:

   * work-flows ran successfully at 2 OSG sites and the local ITB cluster (LIGO_CIT):
      * Nebraska: 11 days
      * FF: 10 days 18 hours
      * LIGO_CIT: 14 hours 20 minutes
   * work-flows are running at 2 OSG sites:
      * CIT_CMS_T2: submitted  11/19 12:39:56
      * NWICG_Notredame: submitted 11/24 13:19:19
   * work-flow run unsuccessful at OUHEP_OSG
      * 32 bit cluster
   * To be submitted: Harvard-East