%META:TOPICINFO{author="BrittaDaudert" date="1288642955" format="1.1" version="1.17"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+!! *<noop>Opportunistic Storage for LIGO Binary Inspiral*
%TOC%
Requirements:
   * periodically pre-stage 1 week of data (2 weeks minimal per site)
   * 1 week of data appears to be around O(100GB).
   * a job running on the worker node needs to access to the whole weekly dataset 
   * *Optional*: data should be deleted after processing of the weekly dataset is done.  During testing, we frequently reuse data.
   * RLS catalog needs to be populated after successful transfer.
   * Data should be easily replicated to multiple sites.
   * 100K jobs at 5-15 minutes per job.
   * 25K hours per workflow.

Current Restrictions:
   * Have to run on site with posix access to the files from the worker nodes, so only sites with hdfs, lustre or panasas may be considered 
   * Currently using the -rfc compliant VOMS proxy.  This is not accepted by current !BeStMan-gateway, but accepted by Globus Gridftp servers.

Immediate Goals:
   * Document the procedure for kicking off transfers (Britta) %GREEN%DONE%ENDCOLOR%  [[http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/FILE-TRANSFERS/][File transfer instructions]]
   * Investigate the current transfer procedure, see if it could be optimized (Brian & Derek)
   * See if the -rfc requirements can be dropped or use two proxy (change proxy location env.variable to specify in command line). The load balancing provided by SRM could be use for gridftp server selection. (Brian & Derek).
   *  Measure the transfer of weekly dataset from LIGO to Nebraska. The goal is ~ 8 hours. (Britta, Brian, & Derek) 
   *  Write replication script that allowed transfer of data between sites. Provide measurements. (Derek? Brian? Tanya?)
   *  Identify the sites that LIGO could run and verify that they are ready for hosting LIGO data  (Tanya). 19 SEs claim to support LIGO. Only 5 of them have storage area mounted on worker nodes according to BDII. %GREEN%DONE%ENDCOLOR%
   * Create a !GlideInWMS workflow to run in 1-3 days at any particular site that can provide 10K-20K opportunistics hours per day to LIGO (Mats) %GREEN%DONE%ENDCOLOR%
   * Create a standard "transfer test" which we can do between sites to show we understand how to manage transfers. 
   * Discuss second level staging with Pegasus team. %BLUE%In progress%ENDCOLOR%

|*Testing Status*| *Site Name*| *SE_ID*| *Storage Type* | *SURL* | *Test Status* | *Local !Mount* | *SA free space (8/23/10)* |Ticket|
|%RED%Fail%ENDCOLOR%|UCSDT2|bsrm-1.t2.ucsd.edu|!BeStMan/HDFS|srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/hadoop/ligo|%GREEN%Success%ENDCOLOR%|/hadoop/ligo|~136TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9125][9125]]|
|%GREEN%Success%ENDCOLOR%|CIT_CMS_T2|cit-se.ultralight.org|!BeStMan/HDFS|srm://cit-se.ultralight.org:8443/srm/v2/server?SFN=/mnt/hadoop/osg/LIGO|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/osg/LIGO| ~90TB||
|%GREEN%Success%ENDCOLOR%|Nebraska|red-srm1.unl.edu|!BeStMan/HDFS|srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/user/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/user/ligo|~156TB||
|%GREEN%Success%ENDCOLOR%|Firefly|ff-se.unl.edu|!BeStMan/panasas|srm://ff-se.unl.edu:8443/srm/v2/server?SFN=/panfs/panasas/CMS/data/ligo|%GREEN%Success%ENDCOLOR%|/panfs/panasas/CMS/data/ligo|~40TB||
|%RED%Fail%ENDCOLOR%|TTU-ANTAEUS|sigmorgh.hpcc.ttu.edu|!BeStMan/Lustre|srm://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server?SFN=/lustre/hep/osg/ligo|%GREEN%Success%ENDCOLOR%| /lustre/hep/osg/ligo|~239TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9126][9126]]|
|%GREEN%Success%ENDCOLOR%/%RED%Fail%ENDCOLOR%|OUHEP_OSG|ouhep2.nhn.ou.edu|!BeStMan/Lustre|srm://ouhep2.nhn.ou.edu:8443/srm/v2/server?SFN=/raid1/data/griddata|%GREEN%Success%ENDCOLOR%|/raid1/data/griddata| ~1.5TB||
|%RED%Omitted%ENDCOLOR%|UFlorida-PG|srmb.ihepa.ufl.edu|!BeStMan/Lustre|srm://srmb.ihepa.ufl.edu:8443/srm/v2/server?SFN=/lustre/raidl/user/ligo/|%RED%Error%ENDCOLOR%|!MountInfo set to none|~1TB||
|%GREEN%Success%ENDCOLOR%/%RED%Fail%ENDCOLOR%|!GridUNESP_CENTRAL|se.grid.unesp.br|!BeStMan/?|srm://se.grid.unesp.br:8443/srm/v2/server?SFN=/store/ligo|%GREEN%Success%ENDCOLOR%|/store/ligo|N/A||
|%RED%Fail%ENDCOLOR%|!UMissHEP|umiss005.hep.olemiss.edu|!BeStMan/?|srm://umiss005.hep.olemiss.edu:8443/srm/v2/server?SFN=/osgremote/osg_data/ligo|%GREEN%Success%ENDCOLOR%|/osgremote/osg_data/ligo| ~7TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9147][9147]]|
|%RED%Omitted%ENDCOLOR%|NWICG_NDCMS|nwicg.crc.nd.edu|!BeStMan/?|srm://nwicg.crc.nd.edu:49084/srm/v2/server?SFN=/dscratch/osg/bestman|%RED%Error%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|%BLUE%In progress%ENDCOLOR%|NWICG_NotreDame|osg.crc.nd.edu|!BeStMan/?|srm://osg.crc.nd.edu:8443/srm/v2/server?SFN=/dscratch/osg/bestman|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|%BLUE%In progress%ENDCOLOR%|SBGrid-Harvard-East|osg-east.hms.harvard.edu|!BeStMan/?|srm://osg-east.hms.harvard.edu:10443/srm/v2/server?SFN=/osg/storage/data/ligo|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~3TB||
|%RED%Omitted%ENDCOLOR%|NERSC-PDSF|pdsfsrm.nersc.gov|!BeStMan/?|srm://pdsfsrm.nersc.gov:62443/srm/v2/server?SFN=/srmcache/ligo|%RED%Error%ENDCOLOR%|!MountInfo is not defined|N/A||
|%RED%Omitted%ENDCOLOR%|FNAL_FERMIGRID|fndca1.fnal.gov|dCache|srm://fndca1.fnal.gov:8443/srm/managerv2?SFN=/pnfs/fnal.gov/usr/fermigrid/volatile/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|GLOW|cmssrm.hep.wisc.edu|dCache|srm://cmsdcache03.hep.wisc.edu:8443/srm/managerv2?SFN=/pnfs/hep.wisc.edu/data5/LIGO|%RED%Error%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|BNL-ATLAS|dcsrm.usatlas.bnl.gov|dCache|srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/osg/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|Purdue-RCAC|srm-dcache.rcac.purdue.edu|dCache|srm://srm-dcache.rcac.purdue.edu:8443/srm/managerv2?SFN=/VO/ligo|%RED%Error%ENDCOLOR%||~1TB||
|%RED%Omitted%ENDCOLOR%|SPRACE|osg-se.sprace.org.br|dCache|srm://osg-se.sprace.org.br:8443/srm/managerv2?SFN=/pnfs/sprace.org.br/data|%RED%Error%ENDCOLOR%||~30TB||
                                                                                                                                           
 Details:

   * UCSDT2
      * test work-flow transferring 20 files
      * can't turn off remote_initialdir with current version of Pegasus
      * remote_initialdir = /hadoop/ligo: md5sum checks fail: no execute permission of pegasus kickstart 
      *  manually removing remote_initialdir from md5sum job submit file: stage_out of data fails, pegasus:transfer sets source directory of output files to $DATA
      * Additionally: $DATA not visible from WNs --> can't set remote_initialdir to $DATA --> ihope work-flow will fail

   * TTU_ANTAEUS
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again

   * OUHEP_OSG
      * test work-flow transferring 20 files succeeds
      * test work-flow transferring 5000 files fails
         * repeated srm-copy fail
         * SRM-CLIENT: Thu Oct 28 15:39:23 PDT 2010 Server did not respond and client is exiting. SrmPrepareToPut

   * GridUNESP_CENTRAL
      * test work-flow transferring 20 files succeeds
      * 10/29 - present: site down, MYOSG: CE service is in UNKNOWN status
     
   * UMiss_HEP     
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again