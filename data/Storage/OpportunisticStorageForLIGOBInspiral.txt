%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dBritta_20Daudert_20869038" date="1292360604" format="1.1" reprev="1.29" version="1.29"}%
%META:TOPICPARENT{name="OpportunisticStorage"}%
---+!! *<noop>Opportunistic Storage for LIGO Binary Inspiral*
%TOC%
%BLUE%Requirements:%ENDCOLOR%
   * periodically pre-stage 1 week of data (2 weeks minimal per site)
   * 1 week of data appears to be around O(100GB).
   * a job running on the worker node needs to access to the whole weekly dataset 
   * *Optional*: data should be deleted after processing of the weekly dataset is done.  During testing, we frequently reuse data.
   * RLS catalog needs to be populated after successful transfer.
   * Data should be easily replicated to multiple sites.
   * 100K jobs at 5-15 minutes per job.
   * 25K hours per workflow.

%BLUE%Current Restrictions:%ENDCOLOR%
   * Have to run on site with posix access to the files from the worker nodes, so only sites with hdfs, lustre or panasas may be considered 
   * Currently using the -rfc compliant VOMS proxy.  This is not accepted by current !BeStMan-gateway, but accepted by Globus Gridftp servers.

%BLUE%Immediate Goals:%ENDCOLOR%
   * Document the procedure for kicking off transfers (Britta) %GREEN%DONE%ENDCOLOR%  [[http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/FILE-TRANSFERS/][File transfer instructions]]
   * Investigate the current transfer procedure, see if it could be optimized (Brian & Derek)
   * See if the -rfc requirements can be dropped or use two proxy (change proxy location env.variable to specify in command line). The load balancing provided by SRM could be use for gridftp server selection. (Brian & Derek).
   *  Measure the transfer of weekly dataset from LIGO to Nebraska. The goal is ~ 8 hours. (Britta, Brian, & Derek) 
   *  Write replication script that allowed transfer of data between sites. Provide measurements. (Derek? Brian? Tanya?)
   *  Identify the sites that LIGO could run and verify that they are ready for hosting LIGO data  (Tanya). 19 SEs claim to support LIGO. Only 5 of them have storage area mounted on worker nodes according to BDII. %GREEN%DONE%ENDCOLOR%
   * Create a !GlideInWMS workflow to run in 1-3 days at any particular site that can provide 10K-20K opportunistics hours per day to LIGO (Mats) %GREEN%DONE%ENDCOLOR%
   * Create a standard "transfer test" which we can do between sites to show we understand how to manage transfers. 
   * Discuss second level staging with Pegasus team. %BLUE%In progress%ENDCOLOR%

%BLUE%Britta's wish list%ENDCOLOR%

   * srm-copy should work with rfc complient proxy
   * srm-copy -mkdir should create directories recursively
   * throttling at BestMan gateways
   * srm-copy error handling should be improved, e.g.
      * Pad passphrase --> one of the gridftp servers not operational
      * EOF error -->  incorrect mount point or port
   * turn  OUHEP_OSG into 64 bit cluster

|*Testing Status*| *Site Name*| *SE_ID*| *Storage Type* | *SURL* | *Test Status* | *Local !Mount* | *SA free space (8/23/10)* |*Ticket*|
|%RED%Fail%ENDCOLOR%|UCSDT2|bsrm-1.t2.ucsd.edu|!BeStMan/HDFS|srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN=/hadoop/ligo|%GREEN%Success%ENDCOLOR%|/hadoop/ligo|~136TB||
|%GREEN%Success%ENDCOLOR%|CIT_CMS_T2|cit-se.ultralight.org|!BeStMan/HDFS|srm://cit-se.ultralight.org:8443/srm/v2/server?SFN=/mnt/hadoop/osg/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/osg/ligo| ~90TB||
|%GREEN%Success%ENDCOLOR%|Nebraska|red-srm1.unl.edu|!BeStMan/HDFS|srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/user/ligo|%GREEN%Success%ENDCOLOR%|/mnt/hadoop/user/ligo|~156TB||
|%GREEN%Success%ENDCOLOR%|Firefly|ff-se.unl.edu|!BeStMan/panasas|srm://ff-se.unl.edu:8443/srm/v2/server?SFN=/panfs/panasas/CMS/data/ligo|%GREEN%Success%ENDCOLOR%|/panfs/panasas/CMS/data/ligo|~40TB||
|%RED%Fail%ENDCOLOR%|TTU-ANTAEUS|sigmorgh.hpcc.ttu.edu|!BeStMan/Lustre|srm://sigmorgh.hpcc.ttu.edu:49443/srm/v2/server?SFN=/lustre/hep/osg/ligo|%GREEN%Success%ENDCOLOR%| /lustre/hep/osg/ligo|~239TB||
|%RED%Fail%ENDCOLOR%|OUHEP_OSG|ouhep2.nhn.ou.edu|!BeStMan/Lustre|srm://ouhep2.nhn.ou.edu:8443/srm/v2/server?SFN=/raid1/data/griddata|%GREEN%Success%ENDCOLOR%|/raid1/data/griddata| ~1.5TB/actual 400GB||
|%RED%Omitted%ENDCOLOR%|UFlorida-PG|srmb.ihepa.ufl.edu|!BeStMan/Lustre|srm://srmb.ihepa.ufl.edu:8443/srm/v2/server?SFN=/lustre/raidl/user/ligo/|%RED%Error%ENDCOLOR%|!MountInfo set to none|~1TB||
|%RED%Fail%ENDCOLOR%|!GridUNESP_CENTRAL|se.grid.unesp.br|!BeStMan/?|srm://se.grid.unesp.br:8443/srm/v2/server?SFN=/store/ligo|%GREEN%Success%ENDCOLOR%|/store/ligo|N/A||
|%RED%Fail%ENDCOLOR%|!UMissHEP|umiss005.hep.olemiss.edu|!BeStMan/?|srm://umiss005.hep.olemiss.edu:8443/srm/v2/server?SFN=/osgremote/osg_data/ligo|%GREEN%Success%ENDCOLOR%|/osgremote/osg_data/ligo| ~7TB||
|%RED%Omitted%ENDCOLOR%|NWICG_NDCMS|nwicg.crc.nd.edu|!BeStMan/?|srm://nwicg.crc.nd.edu:49084/srm/v2/server?SFN=/dscratch/osg/bestman|%RED%Error%ENDCOLOR%|!MountInfo is not defined|~0.2TB||
|%GREEN%Success%ENDCOLOR%|NWICG_NotreDame|osg.crc.nd.edu|!BeStMan/?|srm://osg.crc.nd.edu:8443/srm/v2/server?SFN=/dscratch/osg/bestman|%GREEN%Success%ENDCOLOR%||~0.2TB||
|%GREEN%Success%ENDCOLOR%|SBGrid-Harvard-East|osg-east.hms.harvard.edu|!BeStMan/?|srm://osg-east.hms.harvard.edu:10443/srm/v2/server?SFN=/osg/storage/data/ligo|%GREEN%Success%ENDCOLOR%|!MountInfo is not defined|~3TB|[[https://ticket.grid.iu.edu/goc/viewer?id=9571][9571]]|
|%RED%Omitted%ENDCOLOR%|NERSC-PDSF|pdsfsrm.nersc.gov|!BeStMan/?|srm://pdsfsrm.nersc.gov:62443/srm/v2/server?SFN=/srmcache/ligo|%RED%Error%ENDCOLOR%|!MountInfo is not defined|N/A||
|%RED%Omitted%ENDCOLOR%|FNAL_FERMIGRID|fndca1.fnal.gov|dCache|srm://fndca1.fnal.gov:8443/srm/managerv2?SFN=/pnfs/fnal.gov/usr/fermigrid/volatile/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|GLOW|cmssrm.hep.wisc.edu|dCache|srm://cmsdcache03.hep.wisc.edu:8443/srm/managerv2?SFN=/pnfs/hep.wisc.edu/data5/LIGO|%RED%Error%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|BNL-ATLAS|dcsrm.usatlas.bnl.gov|dCache|srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/osg/|%GREEN%Success%ENDCOLOR%||N/A||
|%RED%Omitted%ENDCOLOR%|Purdue-RCAC|srm-dcache.rcac.purdue.edu|dCache|srm://srm-dcache.rcac.purdue.edu:8443/srm/managerv2?SFN=/VO/ligo|%RED%Error%ENDCOLOR%||~1TB||
|%RED%Omitted%ENDCOLOR%|SPRACE|osg-se.sprace.org.br|dCache|srm://osg-se.sprace.org.br:8443/srm/managerv2?SFN=/pnfs/sprace.org.br/data|%RED%Error%ENDCOLOR%||~30TB||
                                                                                                                                           

%BLUE%Throttling/concurrency testing %ENDCOLOR%

   * Implemented condor throttle 4:  max transfer jobs running simultaneously  
   * Throttling the number of simultaneous srm-copy calls is necessary to avoid excessive memory usage causing submit host crash
   * Increase timeout limit from 1800 secs to 3600 sec to avoid timeout errors
   * Testing different concurrency settings to optimizes file transfers 

|%BLUE%Total run time of transfers%ENDCOLOR%| *LDG-->OUHEP*| *LDG-->OUHEP*| *LDG-->Nebraska* | *LDG-->Nebraska* | 
|*Concurrency 1*|8 hrs 56 mins|10 hrs 50 mins|9 hrs 31 mins|9 hrs 18 mins|
|*Concurrency 3*|3 hrs 33 mins|3 hrs 31 mins|3 hrs 15 mins|3 hrs 23 mins|
|*Concurrency 4*|2 hrs 51 mins|4 hrs|2 hrs 39 mins|2 hrs 31 mins|
|*Concurrency 5*|2 hrs 15 mins|2 hrs 33 mins|2 hrs 7 mins|2 hrs 6 mins|
|%BLUE%Runtime range in secs per transfer dag job (100 files)%ENDCOLOR%| *LDG-->OUHEP*| *LDG-->OUHEP*| *LDG-->Nebraska* | *LDG-->Nebraska* |
|*Concurrency 1*|2621 - 2705|2625 - 4077|2510 - 2761|2509 - 2693|
|*Concurrency 3*|910 - 1045|898 - 1151|859 - 886|861 - 977| 
|*Concurrency 4*|675 - 914|672 - 837|632 - 728|635 - 679|
|*Concurrency 5*|575 - 912|578 - 895|508 - 599|507 - 610|

%BLUE%Details of file transfers:%ENDCOLOR%

   * 11/05/10 SUBMIT HOST CRASH
      * 4 work-flows submitted at the time, each transferring 5000 files (100 per dag job)
         * Grid_UNESP_CENTRAL, OUHEP_OSG, NWICG_NotreDame, SBGrid-Harvard-East
      * Robert: due to each lbnl srm client using 200MB in RAM for each transfer process
      * Alex Sim: you need to increase the MX value in the srm-copy script for java vm size

   * SBGrid-Harvard-East
      * transferred 3 day data set successfully.

   * NWICG_NotreDame
      * bad pass phrase error cause by source red-gridftp1.unl.edu being non-operational
      * re-run work-flow, 5000 files transferred successfully, closed GOC ticket


   * UCSDT2
      * test work-flow transferring 20 files
      * can't turn off remote_initialdir with current version of Pegasus
      * remote_initialdir = /hadoop/ligo: md5sum checks fail: no execute permission of pegasus kickstart 
      *  manually removing remote_initialdir from md5sum job submit file: stage_out of data fails, pegasus:transfer sets source directory of output files to $DATA
      * Additionally: $DATA not visible from WNs --> can't set remote_initialdir to $DATA --> ihope work-flow will fail

   * TTU_ANTAEUS
      * test work-flow transferring 20 files
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again

   * OUHEP_OSG
      * test work-flow transferring 20 files succeeds
      * test work-flow transferring 5000 files fails
         * repeated srm-copy fail
         * SRM-CLIENT: Thu Oct 28 15:39:23 PDT 2010 Server did not respond and client is exiting. SrmPrepareToPut
      * Alex Sim says:
<pre class="screen">
if the machine is too busy, the connection
will likely take a long time to be established (in part it's GSI and
another busy network), and if it's too long, srm-copy might get
timed-out. You can increase the timeout value in that case. It doesn't
seem that the SRM server is too busy or crashed in this particular case.
</pre>

      * Resubmitted work-flow with time out 3600s (default is 1800), completes
      * Transferring 3 day data set for ihope testing, I observe repeated time-out/handshake  errors and am unable to transfer the complete data set
      * NOTE: OUHEP only has 400 GB storage space in $DATA which is listed above as the storage location 

   * GridUNESP_CENTRAL
      * test work-flow transferring 20 files succeeds
      * 10/29 - present: site down, MYOSG: CE service is in UNKNOWN status
      * 11/11 submitted rescue dag which succeeds without further fails 
      * Observed repeated time out/handshake errors when transferring 3 day data set, unable to transfer the complete data set
     
   * UMiss_HEP     
      * test work-flow transferring 20 files;
      * data transferred successfully
      * md5sum jobs sit in queue for days without executing
      * dagman log entries switch between:
         * The job's remote status is unknown
      * and
         * The job's remote status is known again

 %BLUE%Running IHOPE with SRM setup on a 3 day data set:%ENDCOLOR%

   * work-flows ran successfully at 2 OSG sites and the local ITB cluster (LIGO_CIT):
      * Nebraska: 11 days
      * FF: 10 days 18 hours
      * LIGO_CIT: 14 hours 20 minutes
   * work-flows are running at 2 OSG sites:
      * CIT_CMS_T2: submitted  11/19 12:39:56
      * NWICG_Notredame: submitted 11/24 13:19:19
      * SBGrid-Harvard-East: submitted: 11/30 13:19:40
   * work-flow run unsuccessful at OUHEP_OSG
      * 32 bit cluster
 

%BLUE%Running IHOPE with SRM setup on a 3 day data set GLIDEINS:%ENDCOLOR%

   * Ten work-flows submitted to FF simultaneously:
      * Runtimes vary from 17 hours to 106 hours
      * Latest run of ten shows runtimes of 69 - 82 hours for 8 of the ten runs 
      * More stats here: 

http://www.ligo.caltech.edu/~bdaudert/INSPIRAL/GLIDEINS/STATS/SCALE-TEST/
