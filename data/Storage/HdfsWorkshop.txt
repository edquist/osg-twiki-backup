%META:TOPICINFO{author="MichaelThomas" date="1236657768" format="1.1" version="1.11"}%
%META:TOPICPARENT{name="StorageForum"}%
---+ HdfsWorkshop, March 11-13, UCSD


---++ Logistics

Information concerning travel and accommodations can be found at the webpage for the recent
[[http://hepuser.ucsd.edu/twiki2/bin/view/UCSDTier2/SandCweekApril2009][Software and Computing]] meeting at UCSD.
   * UCSD will host a hands-on workshop focused on integration of hdfs (the Hadoop file system) at the USCMS Tier2s from March 11 to March 13, 2009.
   * Most attendees will be arriving around noon on Wednesday.  Most will be at the Sheraton La Jolla.

---++ Agenda

   * Meet Wednesday noon at Mayer Hall 5517 (Terrence's office). 
      * start with lunch, in case either Will or Brian are delayed with their flights.
      * Followed by the TODO as it is below.

---++ TODO

   * Introductory remarks from Ken Bloom (30 min)
   * Reality check: Time to ask upfront questions and let sites define any goals they have for the meeting. (30 min)
      * Small roundtable of "what we want to do with Hadoop (and why)" from each site.
   * Detailed outline of Hadoop components (logfiles, what's installed where, etc). (1 hr)
      * Demonstration and outline of the Nebraska system
   * Install-fest: (most of morning)
      * Help folks install small evaluation testbeds.
      * Get all the important components going (HDFS, FUSE, GridFTP, SRM)
      * Demonstrate file transfers and CMSSW usage
   * Feedback / update documentation and packaging based on the install-fest (1 hr)
   * Outline how the transition process at Nebraska proceeded.  (1 hr)
      * Discuss what tests your SE must pass and how to set them up with Hadoop.
   * Discuss monitoring of Hadoop through: (1 hr)
      * JMX.  Make sure everyone can run JMX console.
      * Ganglia.  Make sure everyone's HDFS cluster gets integrated into Ganglia
      * Nagios.  Share Nebraska probes and offer advice from Nerbaska.
      * Monalisa. Share custom monalisa+ganglia module from Caltech
   * Demonstrate reliability of HDFS in various circumstances: (30 min)
      * restart of namenode
      * restoration from checkpoint/backup
      * client behavior during namenode restart
      * log atomicity during namenode failure (=kill -9=)
   * Discuss what goes on during a HDFS upgrade. (30 min)
   * Discuss Rocks integration (1 hr)
      * RPMs of hadoop, fuse packages
      * Rocks kickstart xml fragments
      * Hands-on help integrating Hadoop with existing Rocks clusters
      * Todo: build hadoop rpm from source
      * Todo: integration bestman/gridftp (RPMS + kickstart xml)
   * Test Scalability / Inter-site PhEDEx transfers?  (1 hr)
   * Plan a "storage challenge" to be held before data taking?

---++ Coordination

   * Jabber/XMPP: uscms-t2@conference.fnal.gov (multi-user chat)

-- Main.WillMaier - 06 Mar 2009