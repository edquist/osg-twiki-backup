%META:TOPICINFO{author="DougOlson" date="1239304560" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="SecurityTeamWorkingArea"}%
%RED% *OSG Restricted Information*

Information on this web page is restricted to authorized individuals only.  It should not be copied or distributed without
the express consent of the OSG Security Team and only to individuals, and while retaining this information protection notice.
%ENDCOLOR%

<!--
   * Set TABLEATTRIBUTES = sort="off"
-->

---++!! Contents

%TOC%

---+ Contingency Planning for the dependencies of OSG on the DOEGrids CA service.
The Computer Security Risk Assessment of OSG ([[https://osg-docdb.opensciencegrid.org:440/cgi-bin/ShowDocument?docid=488][doc488]]) identified the
DOEGrids CA and RA services as one of the few elements capable of causing significant disruption to overall OSG operations.
That assessment considered only human sources of threats for the core OSG assets because  physical damage caused by fire or other
natural disasters is dealt with in the security plans of the organizations hosting the services that OSG relies on.
For this contingency planning we also consider the effects of any type of disaster or disruption to the CA service operated by ESnet
on OSG operations.
To evaluate this risk more clearly and to mitigate it we want to understand the status of the ESnet PKI contingency plan and disaster recovery plan
as well as develop the OSG contingency plan in case of disruption of the CA and certificate services.

This contingency planning is modeled on the [[http://csrc.nist.gov/publications/nistpubs/800-34/sp800-34.pdf][NIST publication SP800-34]]
"Contingency Planning Guide for Information Technology Systems".
This defines a seven step process consisting of:
   1. Develop the contingency planning policy statement.
   2. Conduct the business impact analysis (BIA).
   3. Identify preventive controls.
   4. Develop recovery strategies.
   5. Develop an IT contingency plan.
   6. Plan testing, training, and exercises.
   7. Plan maintenance.
The recommended three phases of actions to take following a system disruption are:
   * The *Notification/Activation* Phase describes the process of notifying recovery personnel and performing a damage assessment. 
   * The *Recovery* Phase discusses a suggested course of action for recovery teams and personnel to restore IT operations at an alternate site or using contingency capabilities. 
   * The final phase, *Reconstitution*, outlines actions that can be taken to return the system to normal operating conditions.

The purpose of a contingency plan is illustrated nicely in this figure.

<img src="%ATTACHURLPATH%/sp800-34-fig2.1.gif" alt="sp800-34-fig2.1.gif" width='612' height='219' />

The contigency plan should "serve as a user’s manual for executing the strategy in the event of a disruption".

It is useful to read Section 2 "Background" of [[http://csrc.nist.gov/publications/nistpubs/800-34/sp800-34.pdf][NIST SP800-34]]
for a description of how a contingency plan fits into the broader picture of a Business Continuity Plan.

---++ Status

|  *Step*  |  *Status*  | *Comments*   |
|  1  |  In Progress  | Needs development & review by security team and ET |
|  2  |  In Progress  | Needs review of risk analysis by security team, completion of resource characterization, then impacts and recovery priorities |
|  3  |  Not Started  | But maybe nothing needs to be done. |
|  4  |  Not Started  |  |
|  5  |  Not Started  |  |
|  6  |  Not Started  |  |
|  7  |  Not Started  |  |

---++ Steps in Contingency Planning Process
The section below hold the materials and descriptions of activities that go into developing the contingency plan but are not
the contingency plan itself.

---+++ Develop the contingency planning policy statement.
We should figure out what to write here.
In principle, it should relate to the guidelines used in performing the risk assessment in defining quantitatively what is
acceptable risk so the conditions that can cause a contingency plan to be put into action can be identified.
From the Risk Assessment document (doc488) we have the following definitions of impact.

<BLOCKQUOTE>
A security event has LOW impact if it occurs less than 10 times per year and does not disrupt the perception of the OSG as a computational facility that can be relied on AND no single occurrence of the event disables the substantially all OSG’s operational Compute Element service for more than two days.

A security event has MODERATE impact if it occurs less than 20 times/ year disables the compute element service for up to a week.

A security event has SEVERE impact if it occurs 20 or more times/year or disables the compute element service for more than a week.
</BLOCKQUOTE>


---+++ Conduct the business impact analysis (BIA).
_The BIA enables the Contingency Planning Coordinator to fully characterize the system requirements, processes, and interdependencies and use this information to determine contingency requirements and priorities. The BIA purpose is to correlate specific system components with the critical services that they provide, and based on that information, to characterize the consequences of a disruption to the system components._

According to the NIST SP800-34 we should identify the critical business processes and use that to identify the critical IT resources.
We will invert that process a bit and consider what the effects are of various IT resources being disrupted.
To perform the analysis we will use the following template to characterize each resource.
---++++ Resource characterization template
   * Brief description of the resource, i.e., name, purpose
   * Points of contact for the resource
   * People directly affected by the resource being disrupted
   * Other resources being directly affected by the resource being disrupted
   * List of specific functions of the resource including TTL, periodicity, and other time-related characteristics. 

---++++ Risk Analysis
Excerpts from OSG Doc488, Table 4, Risk Management.  Grades D and E are considered acceptable risk. Grade C requires monitoring. Grades A and B require mitigation.

| *No* | *Description of Risk*  | *Frequency* | *Severity* | *Grade* |
| 14 | OSG staff publicizing false information about User Process  | L | L | E |
| 15 | Careless 3rd party mistakenly granting agent privilege to incorrect person, possible impact on OSG since all agents of DOE Grids have privilege for all certificates  | L | L | E |
| 16 | Malicious authorization of agent privilege for illegitimate person, by impersonating a valid RA, malicious agent can revoke all certificates causing DNS  | L | H | C |
| 17 | Hack into CA website and perform malicious actions, install fake agents, revoke certificates, cause DNS  | L | H | C |
| 18 | Careless or incomplete training for agents, agents do not follow correct procedures, potential exploit for malicious user if combined with exploit of VOMS server | L | M | D |
| 19 | Malicious mis-information provided for agent training, like 10 but implies intent to exploit  | L | M | D |
| 20 | Careless or incomplete user training for PKI, user may not handle private key correctly  | M | L | D |
| 21 | Accidental corruption of RA process audit log, leads to incomplete of lack of audit ability | L | L | E |
| 22 | Intentional corruption of RA process audit log to disguise audit trail, potential privacy compromise  | L | L | E |
| 23 | Malicious disruption of PMA process, potential damage to credibility of PKI, potential delays in handling authentication and revocation requests | L | L | E |
| 24 | Careless or incomplete identity vetting in certificate process, potential exploit for malicious user can permit abuse of resources if identity theft occurs  | L | M | D |
| 25 | Malicious abuse of identity vetting, insider participation in identity theft by agents | L | M | D |
| 26 | Careless management of CA services, potential DNS caused by CRL lack of availability | L | H | C |
| 27 | Loss of CA integrity due to physics access to CA, potential compromise of CA key and need for re-issuing thousands of certificates | L | H | C |
| 38 | Third party careless agent authorization (CA managers mistake)Risk of mistaken agent authorization, potential damage to trust of grid PKI if it happens too often  | L | L | E |
| 39 | Third party agent authorization (impersonate OSG RA to install malicious agent)Risk that malicious agent is authorized via impersonation of legit RA, immediate impact is low, potential threat to PKI trust, potential DNS by revocation of certificates  | L | H | C |
| 40 | OSG RA careless authZ of agent (Accidental authorization of agent that does not want agent access, low impact unless agent also exploits VOMS)  | L | L | E |
| 41 | Software exploit agent authZ (CA managers issue)  | L | H | C |
| 42 | Agent authZ physical access (CA managers issue) Malicious physical access to CA machines  | L | H | C |
| 43 | OSG staff alarmist – removal of agent authZ (Agent authorization is removed due to incorrect OSG staff complaints , may delay some certificate requests)  | L | L | E |
| 44 | 3rd party careless agent training (Agents in VOs give incorrect info to other agents, requests handled incorrectly)  | L | L | E |
| 45 | 3rd party malicious agent training (Intent to give agents incorrect instructions, low impact unless malicious person also has access to VOMS)  | L | L | E |
| 46 | OSG staff careless agent training (OSG RA provides incorrect training info to agents, agents do not follow correct procedures)  | L | L | E |
| 47 | Malicious remote access agent training (change web instructions)Hacker modifies agent instructions and post bad info  | L | L | E |
| 48 | Physical access & agent training (OSG staff trashing training materials)  | L | L | E |
| 49 | Careless 3rd party user PKI training (Users don’t get proper instructions on how to handle private keys)  | M | L | D |
| 50 | 3rd party careless user support for PKI (Users get mad, try to circumvent controls that are not working)  | L | L | E |
| 62 | ID vetting 3rd party carelessness(Accidental incomplete ID verification, or accidental duplicate DN issuance, impact on PKI trust if happens too often)  | L | L | E |
| 63 | ID vetting 3rd party vandal(Malicious sponsor or agent violates authentication process, issues incorrect DN, low impact unless malicious party has access to VOMS admin)  | L | M | D |
| 64 | ID vetting malicious remote access (Hacker access to Registration Manager server, DNS potential from installing vandal agent doing revocation or certificates)  | L | H | C |
 | 65 | ID vetting physical access  | L | H | C |
| 66 | ID vetting OSG staff alarmist (OSG staff making incorrect statements about quality of ID vetting, potential damage to trust in PKI)  | L | L | E |
| 67 | CA integrity 3rd party carelessness (Potential disruption of services, such as CRL’s, and delays in request processing)  | L | M | D |
| 68 | CA integrity 3rd party vandal (Potential disruption of services, like CRL distribution, or violation of CA key)  | L | H | C |
| 69 | CA integrity malicious remote access(Potential disruption of services like CRL distribution, potential DNS from certificate revocation)  | L | H | C |
| 70 | CA integrity physical access(Potential loss of PKI) | L | H | C |
| 71 | CA integrity OSG staff alarmist(Mis-statements about PKI may damage trust in PKI)  | L | L | E |
| | | | | |
||  *Additional sources of risk (not from OSG doc488) include:*  |||
| A1 | fire (suppression systems already in place) | L | L | E |
| A2 | earthquake (Hayward fault is close by)  | L | H | C |
| A3 | flood (LBL sits on hillside with no water above)  | L | L | E |
| A4 | airplanes falling from sky (LBL is near flight path of east-bound flights from SFO)  | L | ? | ? |
| A5 | power failure (UPS of limited duration in place)   | M | L | D |

From the risk analysis it is apparent that there are many scenarios where something may go wrong somehow 
without stopping the service and the primary question is "how can we tell it went wrong?".
These issues are not part of the Contingency Plan.  In the Contingency Plan we want to consider only those risks which can  cause a service outage for
an extended period of time.  Based on the impact definitions from OSG Doc488 we should consider time periods of two days, one week, one month, and greater than one month.  Note that one month is appropriate to consider since there are CA processes on that time scale (CRL lifetime, cert expiration notices, etc.).

---++++ Resources
---+++++ DOEGrids CA (pki1.doegrids.org)
   * DOEGrids CA - issues X509 certificates for people and services of OSG
   * Mike Helm for CA, Doug Olson for RA
   * Affected people are CA operators, Agents, Gridadmin, Subscribers, relying parties
   * Other resources affected include all hosts & services trusting the DOEGrids CA and using DOEGrids CRLs, i.e., all of OSG.
   * Functions
      * CA signing function - internal with no direct external exposure, occurs on demand from Agents, Gridadmins and subscribers' replacement interfaces; averages about 30 time/day
      * Agent interface - 
      * Gridadmin interface -
      * Public submission interface -
      * Email notifications -
      * CRL generation -
      * Publish to LDAP -

---+++++ CRL Publishing
   * CRL published by CA at crl.doegrids.org, accessed about 70K/day from 13K unique hosts
   * Mike Helm, Dhiva
   * Affected people - like CA
   * Affected services - like CA
   * Functions
      * CRL  published at http://crl.doegrids.org/1c3f2ca8/1c3f2ca8.r0  (DOEGrids CA 1)
      * CRL published at http://es.net/CA/d1b603c3/CRL/d1b603c3.r0 (ESnet Root CA)

---+++++ Public ldap certificate repository
---+++++ Email notifications

---++++ Impacts of outages
This section quantifies the impact of service outages for the relevant time periods.

---++++ Recovery Priorities
This section will show within what time period services must be restored or work arounds adopted following a service disruption.

---+++ Identify preventive controls.
This should already exist in the ESnet PKI CSPP so we probably don't need to add anything here.

---+++ Develop recovery strategies.
This should provide the meat (or TVP) of the contingency plan, and include describing what actions OSG should take until the service si restored.

---+++ Develop an IT contingency plan.
This is "write the document".

---+++ Plan testing, training, and exercises.
This means the contingency plan developed should be included in the OSG security ST&E process.

---+++ Plan maintenance.
DItto.

---++ Acknowledgements

Help and assistance provided by
   * Dan Peterson - ESnet Security Officer
   * Mike Helm - ESnet ATF Lead
   * Dhiva Muruganantham - DOEGrids CA operations

Also [[http://csrc.nist.gov/][NIST Computer Security Resource Center]].

-- Main.DougOlson - 07 Apr 2009


   * Set ALLOWTOPICVIEW = Main.SecurityTeamGroup, Main.MichaelHelm
   * Set ALLOWTOPICCHANGE = Main.SecurityTeamGroup    
   * Set DENYTOPICVIEW = Main.TwikiGuest

%META:FILEATTACHMENT{name="sp800-34-fig2.1.gif" attachment="sp800-34-fig2.1.gif" attr="" comment="Contingency Plan illustration" date="1239145106" path="sp800-34-fig2.1.gif" size="18194" stream="sp800-34-fig2.1.gif" tmpFilename="/usr/tmp/CGItemp4912" user="DougOlson" version="1"}%
