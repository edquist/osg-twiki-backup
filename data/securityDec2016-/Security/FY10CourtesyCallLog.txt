%META:TOPICINFO{author="JimBasney" date="1251467873" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="FY10SecuritySupport"}%
<!--
   * Set ALLOWTOPICVIEW = Main.SecurityTeamGroup
   * Set ALLOWTOPICCHANGE = Main.SecurityTeamGroup
-->
%TOC%
---+ Courtesy Call Log

This is a log of courtesy calls for the FY10SecuritySupport program.

---++ Jeff !DeReus (GROW VO)

Email	jeffrey-dereus@uiowa.edu

Phone	1-319-335-6079

Call scheduled 2pm Mon Aug 17 2009.

http://www.its.uiowa.edu/research/grow

https://grow-voms.its.uiowa.edu:8443/voms/GROW

GROW applications are coming from several disciplines including Atmospheric Science, Bioinformatics, Computational Science, Environmental Science, Geographic Information Science, High Energy and Particle Physics, Management Science, Medical Imaging, and Statistical Computing. GROW will provide computing resources to OSG as well as be interested in using both computing and data resources on OSG.

Community:	Grid computing/application communities within the state of Iowa and their affiliated stakeholders

Jeff attended OSG AHM earlier this year. Didn't come to site admins workshop due to limited funding.
Using same name as old GROW VO (based on OSG recommendation) but new install.
User community not appearing. Discussing with Abhishek.
Bringing up a new cluster.

Jeff is responsible for OSG system admin and other systems.
Bringing the VO up was easy.
Some OSG 1.2 upgrade documentation troubles, though. 
Got certificates OK. Became RA agent.

VO membership will be based on Iowa faculty sponsorship.
Will run VO jobs on campus first.
New cluster will mainly serve GROW VO members. Backfill with campus Condor. Willing to serve other OSG VOs while waiting for GROW VO membership to appear.

No security concerns.

---++ Scott Beardsley (UCD_T3: UC Davis T3 CMS SE and CE nodes)

Email	sbeardsley@ucdavis.edu

Phone	+1 530 752 0547

Call scheduled 1:30pm Mon Aug 17 2009.

Bill Broadley <bill@ucdavis.edu> will also join.

Supported VOs	cms mis ops osg

cms.tier3.ucdavis.edu:2119

Scott and Bill are the cluster team at UC-Davis. They're officially in the Geology department and manage 10-15 clusters across campus departments supporting computational science and engineering. Brought CMS T3 cluster up in collaboration with the Physics department. They just recently got the cluster hardware. Other clusters in other departments are not in OSG and have separate accounting.

Scott was at the recent site admins workshop in Indy. His first OSG meeting. He joins the T3 calls. Bill is less focused on grid aspects but is knowledgeable about cluster security.

Scott asked about FNAL KCA using MD5. I told him about new FNAL KCA coming online with IGTF approval using SHA-1.

Concerns:
   * CA bundle signing. move to https.
   * signing of VDT/OSG software. They use signed RPM/Debian packages on their systems.
   * Brought the CMS cluster up using SL5. ISO on HTTP site. No checksum. Can we raise this as a SL security concern? Could OSG Security Team provide checksums for SL distributions? Could put MD5SUM or SHA1SUMS of ISO directory on https server. Provide https page for verifying packages / CAs.
   * They're using DLV: publish dnssec keys with ISC. Could OSG use it?
   * Where to find a summary of different services and ports required for OSG? Squid needs a monitoring port open. Can we provide firewall rules?
   * They are using GUMS. Do they still need a grid-mapfile? CEMon requires it?

---++ Patrick !McGuigan (DOSAR VO and UTA_DPCC/UTA_SWT2/SWT2_CPB)

mcguigan@uta.edu (UTexas Arlington)

Phone	+1-817-272-1051

secondary contact: Mark Sosebee <sosebee@uta.edu> (not in OIM?)

Distributed Organization for Scientific and Academic Research

A community of physics researchers interested in furthering the development of grid based analyses.

Call 2:00pm Tue Aug 25 2009.

Need to update OIM info. He's receiving reminder emails.

We met at OSG AHM in livingston.
Patrick involved since before it was OSG.
Started with Grid2003. Computing Site.
Focus on ATLAS and HEP.
2 clusters part of ATLAS T2. UTA_WT2. UTA_DPCC.
First site started in 2003.
Mark taking more active role with clusters.
Patrick administers DOSAR VOMS server.
Talk to Horst (OU) about DOSAR.
Preparing to move VOMS server to Lousiana Tech.

Other DOSAR contacts are
Dick Greenwood at LTU,
Jayu at UTA,
Joel Snow at Langston.

Running an older version of VOMS.

DOSAR membership is based on recommendation from someone in the group.
Email to Patrick or Horst. People in DOSAR know each other.

UTA security officer: Shaun Lam.

CA/CRL updates. Want one site installation. Push that out to machines.
RSV complains about CA/CRLs not up-to-date.
Is this fixed in OSG 1.2? Should be...

At OSG 1.0 now. Moving to 1.2 within a month.
Using GUMS 1.2.16. Last upgrade changed configuration syntax.
Will update GUMS within a month.

Use PBS/Torque.

Storage Element: Bestman/Gateway because using xrootd.
ATLAS supports: dcache, xrootd, lustre/POSIX

DOSAR used Condor-G in the past. Ask Horst for details.

Feedback to security team:
   * Concern about revoked certificates. Making sure we're handing VOMS updates. Making sure GUMS is polling VOMS. Maybe more monitoring/verification?
   * Security notifications are helpful.

---++ David Lesny (!IllinoisHEP Tier3gs)

Email:	ddl@illinois.edu

Phone:	1-217-333-4972

In person meeting 2:00pm Mon Aug 24 2009 (461 Loomis on UIUC campus).

Mark Neubauer is the PI.

T3gs = T3 + grid services (which makes them an "unofficial" Tier2)

Grid services include CE, SE, Local File Catalog, DQ2. In other words, Panda jobs run there.
They allow ATLAS production jobs from Panda production queue and other jobs from the Panda analysis queue.
They may disable the Panda analysis queue in the future and just support ATLAS production jobs and local jobs.

Dave is the primary sysadmin for the systems and handles security issues.
He's assisted by Larry Nelson.
Dave attended an OSG storage workshop at Fermi and the admin workshop in Indy this month.
They're running the latest GUMS (v1.3.1.6).
He updated his contact info in OIM today. Registered their GUMS server.

For security issues, Dave would report them to the Physics department IT administrator, who would escalate to campus IT security.

He has IPtables configured to allow SSH only locally.
Was interested in GUMS banning. I pointed him to TWiki page about it.
He finds VDT pacman installs work well. Can easily manage multiple installed versions. VDT dCache install worked well for him.

He can always use better monitoring tools.
He watches RSV, Panda, and ATLAS web dashboards.
Larry watches messages at a central syslog host.
They're interested in syslog-ng.

---++ Tufts (Atlas T3)

<verbatim>
OSG security call with Tufts.
Aug 27 2009 2pm CT

Attending:
  Jim Basney (OSG Security)
  Peter Doherty (SBGrid site admin)
  John Hover (BNL - ATLAS)
  Torre Wenaus (BNL - Panda)
  Durwood Marshall (Tufts)
  Lionel Zupan (Tufts)
  Vik Solem (Tufts)
  Paul Nash (Tufts)


Summary:

Tufts is a long-time ATLAS participant but is new to OSG.  They are
joining OSG as a Tier3, in collaboration with the Boston University
and Harvard Tier2 sites. There was a misconception that Tufts needed
to grant worldwide access to their systems to participate, but we
discussed that they can configure their systems to serve local Tufts
researchers only without accepting outside jobs or unrestricted
external network connections.

We discussed the old OpenSSL version in the Globus Toolkit. OSG sites
should install Globus from VDT (http://vdt.cs.wisc.edu/), which links
Globus against the system's OpenSSL libraries. This is functionality
provided in the latest Globus releases. The OSG security and VDT teams
actively participate in the Globus Security Committee.

We also discussed VO software administrators and VO-specific software
areas. In ATLAS, Xin maintains the ATLAS analysis software in the
ATLAS software area on ATLAS sites. Tufts can control this access. In
any case, no outside OSG or VO personnel can modify a site's system
software or VDT installation. VO software administrators only have
access to their specific VO software directory. OSG strongly supports
the principal of local site control.

We discussed the best avenues for Tufts to receive support. OSG
mailing lists provide a forum for site administrators to share their
expertise. Rik Yoshida is the ATLAS T3 coordinator.


Jim promised to pass along the following information:

Compute Element Firewalls configuration:
https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ComputeElementFirewalls

OSG mailing lists:
http://www.opensciencegrid.org/Consortium_Mailing_Lists

OSG security:
https://twiki.grid.iu.edu/twiki/bin/view/Security/WebHome

Condor vulnerability assessment:
http://pages.cs.wisc.edu/~kupsch/vuln_assessment/


Minutes:
Peter has talked with Lionel and Durwood in the past.
SBGrid is doing outreach work in Boston area. Got in touch with Tufts.

Tufts ATLAS Tier3 for long time. Joining OSG now.
Boston University and Harvard act as Tier2.
Working to reproduce BU/Harvard setup at Tufts.

John: Typically OSG gatekeepers allow access from all IP addresses.

Lionel - Associate Dir of Research Technology (Tufts)

Tufts received a recommendation to allow access from all Class B IP
addresses. 524,000 IP addresses
We (Jim/John/Torre/Peter) think this is not a requirement.

Tufts is installing OSG/ATLAS software stack on shared systems.
Valuable equipment inside Tufts border.
Cluster. Storage system.
Center for scientific visualization.
Central/Shared resources.

Concern: An OSG/ATLAS security issue could take down the shared Tufts
resources.

As a T3, Tufts will support local Tufts usage.
Could consider opening up in the future.
Small group at Tufts can make submissions to gatekeeper.
Group was expanded to include Saul from BU.
BU and Harvard say restricting at IP level is not possible?
If user is at a conference, their IP would be blocked?
We think IP restricts are possible.
Perhaps Tufts researchers can use a VPN when off campus.

Jim: Follow-up with install documentation.

Connections open:
Job submissions. Using Panda?
Data from BU: remotely mount via GPFS or upload/download.
10 Gbps connection to BU.

Panda can allow a restricted number of sites.
Or you can run your own submitter with pilot jobs.

Any restrictions on outbound access? No.

Where can our system administrators find support?
OSG mailing lists. Email security team.
Rik Yoshida is ATLAS T3 coordinator.

Security reviews of software?
Formal review of Panda.
ATLAS analysis software stack? Has EGEE done a review? Not that we know.
Paul noticed issues of old SSL libraries.
Globus had old OpenSSL version.
VDT install uses system OpenSSL.
We work with Globus Security Committee.

Email address for VDT updates?

Opening up ports? Should just open up port range.

Local Panda pilot submissions will not require inbound access.

Interested in opening collaboration, at Bioinformatics level.
Talking with Ian Stokes-Rees.

Can an outside OSG administrator upgrade or change system software?
That's not the case.
VO software administrator can have rights to install VO software.
ATLAS software can be installed this way.
Are the Pilots handing the ATLAS installation?
Xin does the install using Pilots.
Tufts could do this through a local submission.
Tufts would be in charge of software upgrades.
VOs can have VO-specific software area.
Panda is outside VO software area.
Tufts would upgrade their own scheduler.
Will talk with Rik Yoshida.
</verbatim>
