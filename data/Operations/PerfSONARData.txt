%META:TOPICINFO{author="RobQ" date="1495119490" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="OpsRootCauseAnalysis"}%
---+ Root Cause Analysis of !PerfSONAR Data Store Upgrade

---++ Introduction
This document is to describe the data corruption and recovery incident on !PerfSONAR data stores. All parties have been notified to properly and in a timely manner resolve the issue to the extent possible.

The Service Level Agreement for the service was inoperative at the time of the events described here.

---++++ Definitions:
   * The effected machines are psds0, psds1 and psds2, read as "PerfSonar !DataStore 0|1|2"
   * The original configuration of these machines had 4x4 TB drives.
      * These were configured as RAID10 giving 8 TB useful space per instance
      * The available 8 TB space was partitioned as 4 TB for system use, 4 TB for data storage 
   * The new configuration was planned to have 16 TB per instance, 4 TB for system use, 12 TB for storage. (Tripling available storage space)

---++ Timeline
   * Early 2017: It was determined that psds0 would run out of disk space in approximately 14 months
      * 12x8 TB disks were obtained.
   * 28/Feb: Scott suggested the SLA for the !PerfSONAR service be deprioritized due to poor availability statistics.
   * 2/Mar: SLA suspension was presented to the ET and no objections were expressed.
      * Operations treats all components as having development status.
      * Service administrators have access to all machines without constraints of change management.
   * 21/Apr: (Friday) 1st set of disk replacements occurs.
   * 26/Apr: (Wednesday) 3rd set of replacements occurs.
   * 28/Apr: First notice of any sort of problem, [[https://ticket.grid.iu.edu/33631][33631]]. Files system errors on /var and /usr/local
      * fsck was used and the immediate problem was solved, access to the machine was possible.
   * 30/Apr: psds components go to read only state, service is unusable [[https://ticket.grid.iu.edu/33641][33641]]
      * This is taken to be the start time of the problem.
      * Efforts to recover are initiated by Shawn !McKee. Documents regarding the efforts are linked below.
   * 8/May: Shawn contacts operations to inquire as to the existence of backups for the data.
      * Shawn is informed no backups exist.
   * Ongoing: 
      * Documents regarding recovery efforts/progress to date [[http://steige.grid.iu.edu/steige/OSG-postgresql-corruption.rtf][1]] and [[http://steige.grid.iu.edu/steige/Recover-corrupt-DB.rtf][2]] were generated.
      * External expertise has been sought. The above documents are in the possession of the IU database administration group and are being evaluated

---++ Analysis

The initial design of the service determined that sufficient redundancy existed that operation without backup was acceptable. Additionally, data archiving was was to be used.
If a data archiving plan had existed, the action of replacing the drives for the service would have been avoided entirely. Additionally, if archives had existed recovery would have been greatly facilitated.

---++ Follow Up Actions

   * Any recovery activities recommended by available experts should be pursued. 
   * Data archiving must be implemented at highest possible priority.

---++ Reference

   * Executive team, Shawn !McKee, Rob Quick
      * [[http://steige.grid.iu.edu/steige/OSG-postgresql-corruption.rtf][OSG-postgresql-corruption.rtf]], Shawn !McKee
      * [[http://steige.grid.iu.edu/steige/Recover-corrupt-DB.rtf][Recover-corrupt-DB.rtf]] Shawn !McKee.