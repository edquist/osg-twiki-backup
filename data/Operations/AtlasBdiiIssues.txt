%META:TOPICINFO{author="JohnHover" date="1248901443" format="1.1" version="1.4"}%
This page is intended to serve as a record of some information publishing issues that ATLAS/BNL has encountered. 
<font color=blue>Burt's comments in blue .. </font>
<font color=red>Brian's comments in red ... </font>
<font color=green>John's follow-up comments in green ... </font>

---+++ Background

ATLAS has several OSG resources at BNL: BNL_ATLAS_1 and BNL_ATLAS_2 (gatekeepers), BNL_ATLAS_SE (dCache/SRM), and BNL-LCG2(a gatekeeper used to publish dCache, FTS, LFC, and MyProxy information for forwarding to the WLCG BDII). Initially each of these has been in their own resource groups in OIM, as has traditionally been the case. In an effort to consolidate the information publishing and accounting, we made a plan, with Arvind's help, to merge them. 

   * Place BNL_ATLAS_1 and BNL_ATLAS_SE under the BNL-LCG2 resource group in OIM. 
   * Moving the specialized providers for LFC, MyProxy, and FTS to the BNL_ATLAS_1 CEMon instance. 
   * Remove the BNL-LCG2 gatekeeper (as it was only created as a place-holder).

This migration effort caused problems for the FTS service, particularly the central FTS at CERN.  My understanding is that on each FTS server there is an automatic periodic update where the system queries BDII for the storage endpoints for all sites that participate in FTS channels configured in the server. In this scenario, the channels are staticly defined, and refer only to sitenames. It is the storage endpoints (e.g. SRM urls) that are re-configured dynamically (once per day at most sites, every hour for central CERN). I'm waiting confirmation, but we think that the 'glite-sd-query' tool is the one used to look up this info.  <font color=blue>The FTS servers run glite-sd-query as a cron job (part of glite-sd2cache) and cache the results in /opt/glite/etc/services.xml.</font>

We thought that the FTS configuration update did its site lookup based on the mds-vo-name=XXX in the base DN for each GlueSchema object in BDII. It appears that instead the tool examines the value of an attribute within the objects: SiteUniqueID. <font color=red> In general, mds-vo-name=XXXX is ignored; its existence is side-effect from representing the GLUE in LDAP, and has no intrinsic meaning.</font> In our case, even though records published from BNL_ATLAS_1 had their mds-vo-name properly converted from "local" to "BNL-LCG2", the value within SiteUniqueID was still "BNL_ATLAS_1". We recognized this only in retrospect. At the time we just switched everything back to the previous configuration in order to restore function.  <font color=blue>Confirmed: glite-sd-query populates the site field with the result from <nop>GlueForeignKey: <nop>GlueSiteUniqueID from the <nop>GlueServicesUniqueID stanza.</font>

While investigating and getting help from OSG folk, we learned that we should have adjusted the config.ini file and switched the "site_name" variable in  /monitoring/config.ini from BNL_ATLAS_1 to BNL-LCG2.

---+++ Remaining Issue(s) & Concerns

   * If we begin using OSG resource group names as site_names for individual resources, what are the consequences of no longer having distinct resource names?
      * <font color=blue>The resource names are still unique -- <nop>GlueCEUniqueID corresponds to the gatekeeper hostname, for instance. 
      * One issue is that multiple CEs can overwrite shared entries, such as <nop>GlueSite, some of the SE entries if you've got both CEs configured to publish the SE, etc.  This should be ok but might be confusing when troubleshooting.</font>  <font color=red>This should primarily affect matchmaking based on the BDII; i.e., gLite WMS, some CMS tools.  I'm not aware of an ATLAS one which uses the OSG BDII.</font>
   * If that is a problem, would it make sense to consider dynamically replacing resource name strings with resource group strings in BDII contents before it is forwarded to the WLCG BDII?
   * On a related subject, when multiple resources of a given type (e.g. a CE) are combined in a resource group, in some cases the service availability should be calculated as OR, i.e. if *any* CE is up the site availability is OK (100%). While for other types of resources (maybe storage), availability calculation should be AND, e.g., if half the storage is offline, then perhaps the the site should be considered 50% available.  <font color=red>This might be a valid point, but we go by the algorithm as defined by WLCG... this various tweak has been discussed before, but I've never seen any serious action on it.</font><font color=green>So, is the current algorithm AND or OR? This will affect what resources we choose to include in the resource group.</font>
   * I spoke with the Gratia team, and they say that Gratia probes are nearly entirely independent of the Resource name. The exception is that the probe is associated with <font color=red>site name in the ProbeConfig (which is populated from the config.ini)</font> the *first time* the probe connects. Henceforth the collector uses that resource name. The resource name can be changed by the Collector administrator at any time--nothing needs to be done at the site. 

---+++ Plans

BNL plans to try the resource/resource group consolidation again during the next major scheduled downtime. This time we are also going to plan to shift everything under a new resource group name "BNL_ATLAS", with ATLAS FTS re-defining all channels at all sites to refer to that sitename (rather than BNL-LCG2). These recent complications have had the positive effect of having the FTS team put together a clear recipe for changing site names globally. <font color=red>I believe this will work, assuming that all the other T1s work in a coordinated fashion during the downtime.</font>


-- Main.JohnHover - 29 Jul 2009
