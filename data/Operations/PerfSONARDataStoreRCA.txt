%META:TOPICINFO{author="TomLee" date="1473872860" format="1.1" reprev="1.12" version="1.12"}%
%META:TOPICPARENT{name="WebHome"}%
---+ Root Cause Analysis of Reported Sno+ Issue

---++ Introduction
After the 23-Aug-2016 Maintenance Window the !PerfSONAR datastore at psds failed to restart collecting and sending network metrics.  

---++ Timeline
   * 4 Jul Marian B discovered a bug that potentially sends duplicated summaries to CERN AMQ and creates a [[https://github.com/opensciencegrid/rsv-perfsonar/pull/1][PR]]
   * 6 Jul Edgar tested Marian's PR on dev node fermicloud164.fnal.gov with no duplicate summaries created. 
   * 7 Jul Edgar tags rsv-perfsonar 1.1.3 with the fix
   * 4 Aug [[https://jira.opensciencegrid.org/browse/SOFTWARE-2385][SOFTWARE-2385]] is closed and released on OSG August's release.
   * 16 Aug ITB master datastore (psds-itb1) updated and frozen; appears to be functioning properly
   * 23 Aug Normal Maintenance window update on master datastore (psds0)
      * yum update esmond
      * yum update rsv-perfsonar
      * Edgar reports problems with RSV posting
      * ITB master datastore continues to operate without problems
      * Shawn flying to Shanghai and out of contact.
   * 24 Aug Operations restarts Postgres based on Edgar's request
      * Edgar reports there is still a problem
   * 26 Aug Ticket Closed prematurely
      * Ticket reopened same day when Shawn reports there are still problems
   * 31 Aug Operations posts some log contents to ticket
      * GOC checked esmond and pgsql versions, psql versions are different 9.4.6 on prod, 9.4.8 on itb
   * 1 Sep ESNet added to ticket for guidence
      * Brian adds this is a problem for a student using the data from UNL
   * 3 Sep Shawn returns from China trip (where he had limited access to email and remote systems)
   * 6 Sep Troubleshooting call determines Apache was not restarted after upgrade attributing to at least some of the issue
      * esmond had new configuration /etc/httpd/conf.d/apache-esmond.conf
      * new files were moved to /usr/lib/esmond/esmond
      * Service restarted and normal operations for ~4 hours
      * After 4 hours memory swapping begins and service stops publishing
      * Shawn begins the process of discussing data recovery from the outage
   * 7 Sep Full reboot performed and memory available to the VM updated from 12G to 16G
   * 8 Sep 17:56 CEST After Edgar suggests checking RPM versions of condor, condor-cron, and rsv packages, Tom notes, "The version of condor is the same, but production lags behind ITB on condor-cron and rsv."
   * 8 Sep 18:16 CEST Shawn reports additional RPMS that were older on Production vs ITB
   * 8 Sep 19:05 CEST By Edgar's request Tom looks in /var/log/rsv/consumers/html-consumer.err and discovers "insecure string pickle" error
   * 8 Sep 21:10 CEST Edgar notices that several RPMs besides the ones updated are out of date compared to ITB
   * 8 Sep 21:15 CEST On advice from both Shawn and Edgar, Tom restarts simplevisor on psds0, which appears to reset the memory usage of stompclt for now, but doesn't restore data flow to CERN
   * 8 Sep 21:36 CEST By request Tom updates all the RPMs on production that Shawn noted were lagging and reboots psds0 (this seemed to be an important part of getting back to a working state)
   * 8 Sep 22:17 CEST Tom deletes /usr/share/rsv/www/state.pickle, stops simplevisor and stompclt, and runs stompclt from the command line as a test
   * 8 Sep 22:26 CEST Shawn posts graph showing that data is being published to CERN (Hendrik Borras confirms this)
   * 8 Sep 22:33 CEST On advice from Marian, Tom interrupts command-line stompclt to obtain its stats (and starts simplevisor again), producing the following:
      * =# 2016/09/08-20:31:48 stompclt[47740]: main lingering=
      * =processed 31731 messages in 1636.728 seconds (0.019 k msg/sec)=
      * =throughput is around 0.140 MB/second=
      * =# 2016/09/08-20:31:48 stompclt[47740]: main stopping=
      * =stompclt: unexpected ERROR frame received: User CN=OSG Operations Center, OU=People, O=Open Science Grid, DC=DigiCert-Grid, DC=com is not authorized to write to: topic://perfsonar.summary.packet-loss-rate-bidir=
   * 8 Sep 23:45 CEST CERN stops receiving data again, according to both Hendrik Borras and Shawn
   * 9 Sep 18:22 CEST Tom deletes /usr/share/rsv/www/state.pickle and restarts simplevisor; Shawn reports that this starts data being published to CERN again (but it doesn't last)
   * 10 Sep Service restarted and memory usage shows signs of returning to normal. 
      * Memory usage visualization in attachments.
      * Memory usage continues to rise and is associated with the "stompclt" process that is supposed to send data to the ActiveMQ at CERN
   * 12 Sep 10:00 CEST Marian reports that the CERN broker was re-configured to accept topic "packet-loss-bidir" following Tom's earlier report on stompclt run from command line. In July the CERN broker was re-configured "to introduce new topics and have per-topic authorisation"; this predates the appearance of the issue but cannot have caused it, as it was working until late August.
   * 12 Sep 20:56 CEST Scott reboots psds0; stompclt starts publishing data to CERN and doesn't stop this time
   * 12 Sep 21:31 CEST After that reboot, Tom reports the following stompclt errors in syslog (times are UTC; CEST = UTC + 2):
      * =Sep 12 18:38:21 psds0 stompclt[2911029]: [ERROR] failed to flush outgoing buffer (13469824136 bytes)!=
      * =Sep 12 19:02:08 psds0 stompclt[2589]: [WARNING] removing too old volatile file: /usr/local/rsv-perfsonar//57d6f9c8/57d6f9ef92b9eb.tmp=
   * 13 Sep 10:00 CEST Lionel points out that outgoing buffer in stompclt should be normally empty as it's only used to buffer data before sending them to a socket. Buffer this big (13469824136 bytes) suggests communication with socket is not working (possible cause might be a network problem on psds0). Lionel suggests to re-configure stompclt to limit the maximum size of this buffer (by adding window=100 in its configuration) and also suggests "reliable = true" setting
   * 13 Sep 20:30 CEST Tom adds "window = 100" and "reliable = true" to stompclt configuration in /etc/rsv/stompclt/default.conf and restarts simplevisor/stompclt

---++ Analysis 
      The analysis is ongoing.

   * Hypothesis: stompclt was not authorized to write to the CERN broker, so it cached its messages in memory, causing greater and greater memory usage until the VM went deep into swap and the kernel's oom-killer killed it. On 12 Sep, a configuration change was made to the CERN broker that fixed the issue; however, stompclt on psds0 had already been killed by oom-killer. When psds0 was rebooted, however, data began to flow normally and the problem did not recur. (As part of this hypothesis, state.pickle and the out-of-date RPMs were actually unrelated.)
      * Advantages of this hypothesis: It explains why restarting stompclt/simplevisor restored service, but each time it only lasted for an hour or somewhat more before it stopped again. It explains the memory usage issue as well. Also, it explains why the reboot of 12 Sep restored service and why the memory issues did not then rematerialize, despite the fact that psds0 hadn't been changed.
      * Problems with this hypothesis: It does not explain how the problem originally began. There was supposedly a change to the configuration on the CERN broker in July, but it doesn't make sense that this would have no effect on the production datastore master until its reboot of 23 Aug. Also, shouldn't simplevisor automatically restart stompclt if oom-killer stops it? The hypothesis needs revision or replacement.

---++ Follow Up Actions

One issue noted was that the ITB instance of the OSG network service was not having problems. When we compared the software package versions on the ITB vs Production we noticed that a number of them were (much) older on the Production systems. Another issue was that Production systems were not restarted after updates were applied. Because of the very old version of one of the RPMS a fix (from February) was missing that would have restarted a reconfigured service when the new RPM was installed. Since that fix was missing and the services/system were not restarted, the Production instances were running with a mix of old and new services.  

   * We should have a policy of updating RPMs to match ITB versions on Production as we move updates in place (to make this possible, we must keep careful track of the RPMs updated on ITB so these changes can be replicated on production)
   * Production updates should be applied before the system is restarted (stated another way, we should ensure that the system is rebooted after the production updates, whether or not it was rebooted prior to them)

---++ Reference

[[https://ticket.grid.iu.edu/30823][Primary Ticket]]

-- Main.RobQ - 13 Sep 2016

%META:FILEATTACHMENT{name="Screen_Shot_2016-09-14_at_8.59.55_AM.png" attachment="Screen_Shot_2016-09-14_at_8.59.55_AM.png" attr="" comment="" date="1473858134" path="Screen Shot 2016-09-14 at 8.59.55 AM.png" size="181212" stream="Screen Shot 2016-09-14 at 8.59.55 AM.png" tmpFilename="/usr/tmp/CGItemp38434" user="RobQ" version="1"}%
