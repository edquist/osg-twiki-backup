%META:TOPICINFO{author="ShawnMcKee" date="1473860087" format="1.1" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+ Root Cause Analysis of Reported Sno+ Issue

---++ Introduction
After the 23-Aug-2016 Maintenance Window the !PerfSONAR datastore at psds failed to restart collecting and sending network metrics.  

---++ Timeline

   * 23 Aug Normal Maintenance window update on master datastore (psds0)
      * yum update esmond
      * yum update rsv-perfsonar
      * Edgar reports problems with RSV posting
      * Shawn flying to Shanghai and out of contact.
   * 24 Aug Operations restarts Postgres based on Edgars request
      * Edgar reports there is still a problem
   * 26 Aug Ticket Closed prematurely
      * Ticket reopened same day when Shawn reports there are still problems
   * 31 Aug Operations posts some log contents to ticket
   * 1 Sep ESNet added to ticket for guidence
      * Brian adds this is a problem for a student using the data from UNL
   * 3 Sep Shawn returns from China trip (where he had limited access to email and remote systems)
   * 6 Sep Troubleshooting call determines Apache was not restarted after upgrade attributing to at least some of the issue
      * Service restarted and normal operations for ~4 hours
      * After 4 hours memory swapping began and service stopped publishing
      * Shawn began the process of discussing data recovery from the outage
   * 7 Sep Full reboot performed and memory available to the VM updated from 12G to 16G
<del>   * 8 Sep 22:33 CEST Tom reported that running stompclt by hand produced the following:
</del><ins>   * 8 Sep 22:33 CEST Thom reported that running stompclt by hand produced the following:
</ins>      * Thom noted "The version of condor is the same, but production lags behind ITB on condor-cron and rsv."
      * # 2016/09/08-20:31:48 stompclt[47740]: main lingering
      * processed 31731 messages in 1636.728 seconds (0.019 k msg/sec)
      * throughput is around 0.140 MB/second
      * Shawn reported additional RPMS that were older on Production vs ITB
      * Thom updates all the RPMS on Production that Shawn noted were lagging (this seemed to be an important part of getting back to a working state)
      * stompclt: unexpected ERROR frame received: User CN=OSG Operations Center, OU=People, O=Open Science Grid, DC=DigiCert-Grid, DC=com is not authorized to write to: topic://perfsonar.summary.packet-loss-rate-bidir
   * 10 Sep Service restarted and memory usage shows signs of returning to normal. 
      * Memory usage visualization in attachments.
      * Memory usage continues to rise and is associated with the "stompclt" process that is supposed to send data to the ActiveMQ at CERN
   * 12 Sep 10:00 CEST CERN broker was re-configured to accept topic topic://perfsonar.summary.packet-loss-rate-bidir
<del>   * 12 Sep 21:31 CEST Tom reported the following stompclt errors were found in syslog:
</del><ins>   * 12 Sep 21:31 CEST Thom reported the following stompclt errors were found in syslog:
</ins>      * Sep 12 18:38:21 psds0 stompclt[2911029]: [ERROR] failed to flush outgoing buffer (13469824136 bytes)!
      * Sep 12 19:02:08 psds0 stompclt[2589]: [WARNING] removing too old volatile file: /usr/local/rsv-perfsonar//57d6f9c8/57d6f9ef92b9eb.tmp

---++ Analysis 
      The analysis is ongoing.   
---++ Follow Up Actions

<del>---++ Reference
</del><ins>One issue noted was that the ITB instance of the OSG network service was not having problems.   When we compared the software package versions on the ITB vs Production we noticed that a number of them were (much) older on the Production systems.  Another issue was that Production systems were not restarted after updates were applied.  Because of the very old version of one of the RPMS a fix (from February) was missing that would have restarted a reconfigured service when the new RPM was installed.  Since that fix was missing and the services/system were not restarted, the Production instances were running with a mix of old and new services.  
</ins>
<del>   * Per email from Marian Babik, 12 September:
</del><ins>---++ Follow Up Actions
</ins>
<del>      * <verbatim>we had a look at the issue with Lionel this morning and we have the following comments/questions:
</del><ins>   * We should have a policy of updating RPMS to match ITB versions on Production as we move updates in place
</ins><del>- production broker at CERN was re-configured to introduce new topics and have per-topic authorisation. In the test that Tom did in earlier in this thread, stompclt complains on failure to send message with topic packet-loss-bidir, which wasn’t configured as I didn’t expect this even type to be sent to the message bus. We have now re-configured the broker to accept packet-loss-bidir topics as well, however it’s important that you double-check syslog (/var/log/messages) to see if there aren't any other issues reported by stompclt</verbatim>
</del><ins>   * Production updates should be applied before the system is restarted
</ins>---++ Reference

[[https://ticket.grid.iu.edu/30823][Primary Ticket]]

-- Main.RobQ - 13 Sep 2016

%META:FILEATTACHMENT{name="Screen_Shot_2016-09-14_at_8.59.55_AM.png" attachment="Screen_Shot_2016-09-14_at_8.59.55_AM.png" attr="" comment="" date="1473858134" path="Screen Shot 2016-09-14 at 8.59.55 AM.png" size="181212" stream="Screen Shot 2016-09-14 at 8.59.55 AM.png" tmpFilename="/usr/tmp/CGItemp38434" user="RobQ" version="1"}%
