%META:TOPICINFO{author="RobQ" date="1258139652" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="WebHome"}%
---+ What is the GOC?

The Grid Operation Center (GOC) based at Indiana University is tasked with supporting the operational needs of the users, resource providers, and collaborators of the [[http://www.opensciencegrid.org][Open Science Grid]] (OSG). The GOC consists of human and compute services provided to maximize operational functionality of the OSG. The OSG is the only current grid infrastructure supported by the GOC, but the GOC remains open to hosting grid related services for other national and regional research grids. 

---++ Introduction

This document is being written to describe in depth the scope and responsibilities of the Grid Operations Center. It will also cover some of the experiences and challenges of hosting operational components of a national grid infrastructure such as the OSG. This document will provide facts about services supplied by the GOC but also opinions based on experience of the individuals who have helped develop and provide these services. 

---+++ Purpose of this Document

The explicit purpose of this document is to explain and clarify for the customers, collaborators, and contributing members of the GOC what the GOC does. The scope is not only for those unfamiliar with the GOC and it's services, but a chance for those people providing the services to clarify their responsibility to the GOC's customer base. 

---+++ Definitions

   * Global Research Network Operations Center (GRNOC) - Is a premier provider of highly responsive network coordination, engineering, and installation services that support the advancement of R&E networking.
   * Grid Computing - The combination of computer resources from multiple administrative domains applied to a common task, usually to a scientific, technical or business problem that requires a great number of computer processing cycles or the need to process large amounts of data. (Wikipedia)
   * Open Science Grid (OSG) - A national, distributed computing grid for data intensive research. OSG is based in the US and services the research and eduction community. 
   * Virtual Organization (VO) - refers primarily to a collection of people and also includes the group's computing/storage resources and services.

---+++ Relationships

To service a distributed computing infrastructure like the OSG the GOC has formed many relationship, both internal and external to the OSG. These include relationships with individual institutions, peering grids, OSG collaborators and other OSG activities. This portion of the document will attempt to cover the most important of these relationship. 

---++++ Open Science Grid

The OSG collaborators provide the customer base of the GOC. These customers require several services to maintain operations for their VO and are the prime motivators in the direction the GOC evolved and continues to move. The GOC provides well defined services and service levels to OSG based on [[ServiceLevelAgreements][SLAs]] agreed to by both OSG Operations and the OSG Stakeholders. OSG funds approximately two-thirds of the GOC staff.

---++++ Indiana University - Research Technologies

Indiana University hosts the GOC and leverages local services to provide high levels of service to OSG. The [[http://pti.iu.edu/rt][Research Technologies]] group provides the remainder of the funding for GOC personnel along with most equipment and hosting costs. 

---++++ Indiana University - Global Research Network Operations Center

The [[http://globalnoc.iu.edu/][GRNOC]] provides two critical operational services for the GOC, the trouble ticketing system and 24x7x365 phone and email support. Due to the existing Network Operations Structure at Indiana University these services are utilized at minimal cost to the GOC. 

---++++ Worldwide LHC Computing Grid (WLCG) 

The [[http://lcg.web.cern.ch/LCG/][WLCG]] is an umbrella collaboration existing of the VOs participating in the Large Hadron Collider experiment. 

---++++ European Grid for E-SciencE (EGEE)

[[http://www.eu-egee.org/][EGEE]] is a peering grid with similar goals and structure to OSG and has resources in many countries worldwide. The GOC interacts via the WLCG with EGEE operations, some services span the boundaries of OSG and EGEE. 

---++++ OSG Operational Partners

The GOC interacts with several OSG participants. Fermilab, University of Wisconsin, and University of Nebraska host GOC operationally dependent services. Other OSG activities that are co-dependent on GOC services include Software, Security, Integration, VO Group, Eduction, Engagement, Metrics, and Documentation. 

---+++ Historical Notes

The GOC began before the existence of OSG and serviced previous projects such as Gird2003, Grid3, Worldgrid, and the Trillium projects (Particle Physics Data Grid, Grid Physics Network, and International Virtual Data Grid Labortory). 

---++ Structure

The GOC is split into two main areas Support Services and Infrastructure Services, the decision to split into these areas was primarily for personnel focus. The growth in the GOC over the past several years allowed for this split, during earlier days when there was less staff everyone helped out on everything and handled problems and requests as they came. This new model allows some of the group to be solely responsible in customer support and the other part of the group to concentrate on compute services. This also gives members of the GOC clear guidelines of their responsibilities, while overlap does occur when needed, it has clarified GOC roles. Two individuals have been given the roles of Support Lead and Infrastructure Lead to clarify reporting lines to the GOC Manager and OSG Operation Coordinator. 

---+++ Support Services

Support services provided at the GOC include several types of human interaction needed between the GOC and its customers. These include front line support, ticket creation and tracking, and communicating the state of operations to the community serviced. 

---++++ Front Line Support

The GOC provides front line support to its customers via phone, email and webform entry points. The OSG, in general, favors email based communication though the other forms have proven useful. The GOC encourages use of the webform as it allows us to gather meta-data about the submitter as well as that of the resource provider. The partnership with the GRNOC allows us to offer 24x7x365 phone and email ticket creation and emergency response for problems deemed critical. Operators are on the lookout during this time for emergency notifications to the email box or taken by phone. The webform allows submitters to specify if the problem they are submitting is critical, and is routed directly to a GOC staff members phone. 

The following problems are deemed critical in nature:
   * An security incident report that causing an immediate risk to one or more OSG resources or users.
   * An outage of the [[TheGOC#Information_Services][BDII Information Service]].
   * An outage of the [[TheGOC#Health_Monitoring_Services][MyOSG XML Data Source]].

---++++ Trouble Ticketing

One of the core functionalities of the GOC is to track to resolution Operational issues that affect the OSG. To do this tickets are tracked leveraging the existing GRNOC trouble ticket system. The GOC uses the guidelines set forth in TicketExpectations document. This document sets forth not only GOC expectations but customer expectations. The GOC staff expends a significant portion of it's effort in communicating with the OSG community through these tickets. A public interface for the tickets, developed by the GOC is located at the [[https://ticket.grid.iu.edu][Ticket Interface]]. Details of this interface will be included later in this document at [[https://twiki.grid.iu.edu/bin/view/Operations/TheGOC#Communication_Services][Communications Services]].

Metric reports for tickets can be found at http://www.grid.iu.edu/reports/ticketreports.html.

---+++++ Trouble Ticket Exchange

Trouble ticket exchange has been set up with WLCG, Fermilab (CMS), BNL, VDT, and others to provide direct communication between ticket systems local to these collaborators. This allows collaborators to operate in their local environment when dealing with the GOC and issues reported to OSG. Ticket exchange has been historically difficult for the GOC to maintain high services levels, this is due to the ticket systems on both ends are usually not under the administrative domain of the participants of the exchange. Example: GRNOC administers the GOC ticket system and the Fermilab Helpdesk administers the Fermigrid ticket system. 

---++++ Communication

Several other avenues of communication also occur between the GOC and other OSG entities. These 

---+++++ Notification

It is important to keep the community notified of events affecting OSG Operations. To do this the GOC has set up a blog with RSS feed at http://osggoc.blogspot.com/. This can be subscribed to by anyone in the OSG community. All notifications also go out to corresponding contacts and mailing lists. These announcements include scheduled and unscheduled outages, software releases, peering collaborator events and other events that affect the Operations of the OSG. 

---+++++ Meetings

Various meetings include weekly calls hosted by OSG Operations and attendance in WLCG Operations meetings. Other participation includes Production Meetings, Software Tools Group Meetings, VO Group Meetings, Accounting Meetings, Integration Meetings, and other ad-hoc and management level meetings in which Operations initiates or participates. OSG Operations and the GOC are also represented on the OSG Executive Board and Council. 

The GOC also participates at many events in the OSG and Grid Computing physically. 

---+++++ Collaborative Documentation

The GOC also hosts the [[http://twiki.grid.iu.edu][OSG TWiki]] which is used to collect any documentation pertinent to the OSG. This includes technical documents, policy and procedure documents, and notes and minutes various OSG meetings. It also includes details middleware release documentation and installation instructions. 

---+++ Infrastructure Services

The GOC Infrastructure group is responsible for the compute services needed by the OSG to conduct daily operations. These include several forms of information, health, administrative, and communication tools. While some tools are shared open source tools readily available, several have been customized or build from scratch to handle the specific environment of the OSG. 

---++++ Physical Infrastructure

The GOC houses 18 physical servers in two racks. 16 of these are on the Bloomington campus of IU in the [[http://it.iu.edu/datacenter/index.php][Data Center]] and 2 on the Indianapolis campus of IU in the ITCT complex. The separate physical environments allow survivability mechanisms that prevent a failure on either campus to knock out service to the OSG. Details of these mechanisms will be described below in the individual service section. 

---+++++ Network

I am really too ill informed to write this section. Tom or Arvind can you fill in some details about the IU network, also the use of DNS RR should go here as it is a network tool, not a GOC tool. 

---+++++ Power and Cooling

Once again, I'm pretty ill informed and will rely on Tom here. 

---+++++ Disaster Recovery

The GOC uses three basic mechanisms for disaster recovery. 
   * Institutional Backups - TSM is used to backup the internal backups. 
   * Internal Backups - Internal backup details here. 
   * Virtual Machines - For quick restoration of services that are often updated and restoration of data is unnecessary. ie BDII. 

---+++++ Computer Operations at IU

IU provides 24x7x365 coverage of the facilities at both campuses. This allows the GOC technical staff to have the ability to perform hands-on activities in case of an emergency without being personally present at the GOC hardware. 

---++++ Compute Infrastructure

OSG Operations compute based services are hosted, maintained, and sometime developed by the GOC staff. This set of services composed the OSG Operations infrastructure. While each service will be described in detail below, the GOCs overall philosophy is to host services which are important to the OSG community as a whole, we try not to host VO or resource specific services. The entire GOC takes pride in our ability to provide these services with minimal downtime and keep the community through visible and reliable change management. Services developed by the GOC are released to the community through a testing and integration process described in this [[https://twiki.grid.iu.edu/bin/view/Operations/ReleaseSchedule][document]]. 

---+++++ Hardware

Details of the model numbers of the servers at the GOC go here, along with rack peripherals. 

---+++++ Operating System and Virtualization

Details of both base OS and VMs go here. 

---+++++ Information Services

Details of BDII and !ReSS go here.

---+++++ Health Monitoring Services

Details of RSV go here. Also brief description of evolution of !GridCat to VORS to RSV.

---+++++ Administrative Services

Details of OIM and the Software Cache go here. 

---+++++ Communication Services

Details of !MyOSG, Footprints, TWiki, Ticket, RSS Feed, TTX, JIRA go here.

---+++++ Metric and Reporting Services

Details of Gratia and RSV/SAM reports go here. 

---++++ Service Level Agreements for Compute Services

Each GOC service will have an SLA. The current SLA page is located [[https://twiki.grid.iu.edu/bin/view/Operations/ServiceLevelAgreements][here]]. Writing, reviewing and getting approval for these SLAs has proven to be a long and tedious process, which is still in progress. 

---++++ Development of Compute Services

While the GOC is not a software development group, some services are unique to the OSG environment and do require development. In these cases the GOC strives to work with the OSG community to meet the needs of the users. 

---++ Discussion

-- Main.RobQ - 04 Nov 2009
