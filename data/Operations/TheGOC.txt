%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dElizabeth_20Chism_20100912" date="1295049177" format="1.1" reprev="1.14" version="1.14"}%
%META:TOPICPARENT{name="WebHome"}%
%DOC_STATUS_TABLE%

%TOC%

---+ The OSG GOC
The Grid Operation Center (GOC) based at Indiana University is tasked with supporting the operational needs of the users, resource providers, and collaborators of the Open Science Grid (OSG). The GOC consists of human and compute services provided to maximize operational functionality of the OSG. The OSG is the only current grid infrastructure supported by the GOC, but the GOC remains open to hosting grid related services for other national and regional research grids. 

---+ What is the GOC? 
The Grid Operation Center (GOC) based at Indiana University is tasked with supporting the operational needs of the users, resource providers, and collaborators of the Open Science Grid (OSG). The GOC consists of human and compute services provided to maximize operational functionality of the OSG. The OSG is the only current grid infrastructure supported by the GOC, but the GOC remains open to hosting grid related services for other national and regional research grids. The GOC is first point of contact for members of the OSG needing assistance and tracks those problems through their resolution. The GOC also provides numerous services to the OSG community that support usage of the Open Science Grid.

---++ Introduction 
This document is being written to describe in depth the scope and responsibilities of the Grid Operations Center. It will also cover some of the experiences and challenges of hosting operational components of a national grid infrastructure such as the OSG. This document will provide facts about services supplied by the GOC but also opinions based on experience of the individuals who have helped develop and provide these services. 
---++ Purpose of this Document 
The explicit purpose of this document is to explain and clarify for the customers, collaborators, and contributing members of the GOC what the GOC does. The scope is not only for those unfamiliar with the GOC and it's services, but a chance for those people providing the services to clarify their responsibility to the GOC's customer base. 
---++ Definitions 
•	Global Research Network Operations Center (GRNOC) – Is Indiana University's premier provider of highly responsive network coordination, engineering, and installation services that support the advancement of R&E networking. 
•	Grid Computing - The combination of computer resources from multiple administrative domains applied to a common task, usually to a scientific, technical or business problem that requires a great number of computer processing cycles or the need to process large amounts of data. (Wikipedia) 
•	Open Science Grid (OSG) - A national, distributed computing grid for data intensive research. OSG is based in the US and services the research and education community. NB: OSG itself does not own the compute and storage resources that form the OSG. These owned by the stakeholders.
•	Stakeholder – Any group that has an interest in the OSG such as a virtual organization or resource provider.
•	Virtual Organization (VO) - refers primarily to a collection of people and also includes the group's computing/storage resources and services. 
---++ Relationships 
To service the worldwide distributed computing infrastructure, the GOC has formed many relationships both internal and external to the OSG. These include relationships with individual institutions, peering grids, OSG collaborators and other OSG activities. This portion of the document will attempt to cover the most important of these relationships. 
---+++ Open Science Grid 
The OSG collaborators are the customer base of the GOC. These customers require services to maintain operations for their VO and are the primary stakeholders in the GOC and guide the direction of the GOC’s work. The GOC provides well defined services and service levels to OSG based on SLAs agreed to by both OSG Operations and the OSG VOs. OSG funds approximately two-thirds of the GOC staff. 
---+++ Indiana University - Research Technologies 
Indiana University hosts the GOC and leverages local services to provide a high level of resources to OSG. The Research Technologies group provides the remainder of the funding for GOC personnel along with most equipment and hosting costs. 
---+++ Indiana University - Global Research Network Operations Center 
The GRNOC provides two critical operational services for the GOC, the trouble ticketing system and 24x7x365 phone and email support. Due to the existing network operations infrastructure at Indiana University these services are utilized at minimal cost to the GOC. 
---+++ Worldwide LHC Computing Grid (WLCG) 
The WLCG is an umbrella collaboration existing of the VOs participating in the Large Hadron Collider experiment. WLCG resources are located in Canada, Europe, South America, and Asia.
---+++ European Grid for E-SciencE (EGEE) 
EGEE is a peering grid with similar goals and structure to OSG and has resources in many countries worldwide. The GOC interacts with the EGEE via the WLCG and some services span the boundaries of OSG and EGEE. The resources currently coordinated by EGEE will be managed through the European Grid Initiative (EGI) as of 2010.
---+++ OSG Operational Partners 
The GOC interacts with several OSG participants. Fermilab, University of Wisconsin, and University of Nebraska host services that the GOC is operationally dependent on. Other OSG activities that are co-dependent on GOC services include Software, Security, Integration, VO Group, Education, Engagement, Metrics, and Documentation. 
---+++ Previous Partners
The GOC began before the existence of OSG and serviced previous projects such as Gird2003, Grid3, WorldGrid, and the Trillium projects (Particle Physics Data Grid, Grid Physics Network, and International Virtual Data Grid Laboratory). 
---++ Structure 
The GOC is split into two main areas: Support Services and Infrastructure Services. The decision to split into these areas was primarily to allow GOC personnel to focus on specific areas of expertise. The growth in the size of the GOC staff over the past several years allowed this split. Previously when the staff was smaller, the entire staff worked on the handling of problems and requests as they came in. The current model allows one part of the staff to focus solely on customer support and the other part of the staff to concentrate on developing and operating services and providing second level support. This also gives members of the GOC clear guidelines of their responsibilities and while overlap does occur occasionally, it has clarified roles held by the GOC team members. Two individuals have been given the roles of Support Lead and Infrastructure Lead to clarify reporting lines to the GOC Manager and OSG Operation Coordinator. 
---+++ Support Services 
Support services provided at the GOC include several types of human interaction needed between the GOC and its customers. These include front line support, ticket creation and tracking, and communicating the state of operations to the community serviced. 
---++++ Front Line Support 
The GOC provides front line support to its customers via phone and email. Customers can also open issues with GOC using a webform but subsequent interaction with the customer will be via telephone and/or email. The OSG, in general, favors email based communication though the other communication methods have proven useful. The GOC encourages use of the webform as it allows us to gather meta-data about the submitter as well as that of the resource provider. The partnership with the GRNOC allows us to offer 24x7x365 phone and email ticket creation and emergency response for problems deemed critical. During off-hours, the GRNOC operators are available to receive emergency notifications via email or telephone. The webform allows submitters to specify if the problem they are submitting is critical, and the GRNOC routes critical issues immediately to a GOC staff members phone. 
The following issue classes are deemed critical in nature: 
•	A security incident report that is deemed to be causing an immediate risk to one or more OSG resources or users. 
•	An outage of the BDII Information Service. 
•	An outage of the MyOSG XML Data Source. 
---++++ Trouble Ticketing 
One of the core GOC responsibilities is to track to their final resolution operational issues that affect the OSG. To do this tickets are tracked leveraging the existing GRNOC trouble ticket system. The GOC uses the guidelines set forth in TicketExpectations document. This document sets forth not only GOC expectations but also customer expectations. The GOC staff expends a significant effort in communicating with the OSG community through these tickets. A public interface for viewing the tickets, developed by the GOC can be fount at Ticket Interface. Details of this interface can be found below in Communications Services section.
Numerous metrics about the trouble ticketing are available. The metrics include items such as numbers of tickets, how quickly tickets are resolved, etc. The ticket metric reports can be found at http://www.grid.iu.edu/reports/ticketreports.html. 
---+++++ Trouble Ticket Exchange 
Trouble ticket exchange has been set up with WLCG, Fermilab (CMS), BNL (ATLAS), VDT, and others to provide direct communication between ticket systems local to these collaborators. This allows collaborators to operate in their local environment when dealing with the GOC and issues reported to OSG. Ticket exchange has historically been difficult and it has been difficult for the required GOC to maintain high service levels. This difficulty is caused by the independence (from the GOC) of the ticketing systems that create and/or receive the tickets being exchanged. It is constant work to make sure that the ticket exchange continue to function properly as the various ticketing systems are maintained and updated. Additionally, many different ticketing systems are in use: Remedy at FNAL, RT at BNL, FootPrints at the GOC etc.
---++++ Communication 
Several other avenues of communication also occur between the GOC and other OSG entities. These 
---+++++ Notification 
The GOC keeps the OSG community notified of events affecting OSG Operations. The GOC provides a blog with RSS feed at http://osggoc.blogspot.com/ that OSG community members can subscribed to. All notifications sent to various mailing lists that include contact personnel for each OSG resource. These announcements include scheduled and unscheduled outages, software releases, peering collaborator events and other events that affect the Operations of the OSG. 
---+++++ Meetings 
The GOC staff attends various meetings and host a weekly OSG Operations phone conference. Among the meeting that the GOC attend is the weekly WLCG Operations meeting. Other meetings that GOC staff attend include Production Meetings, Software Tools Group Meetings, VO Group Meetings, Accounting Meetings, Integration Meetings, and other ad-hoc and management level meetings in which Operations initiates or participates. OSG Operations and the GOC are also represented on the OSG Executive Board and Council.
The GOC also participates at many events in the OSG and Grid Computing physically. 
---+++++ Collaborative Documentation 
The GOC also hosts the OSG TWiki which is used to collect any documentation pertinent to the OSG. This includes technical documents, policy and procedure documents, and notes and minutes various OSG meetings. It also includes details middleware release documentation and installation instructions. 
---+++ Infrastructure Services 
The GOC Infrastructure group is responsible for the compute services needed by the OSG to conduct daily operations. These include information, health, administrative, and communication tools. While some tools are shared open source tools readily available, several have been customized or build from scratch to handle the specific environment of the OSG. 
---++++ Physical Infrastructure 
The GOC houses 18 physical servers in two racks. 16 of these are on the Bloomington campus of IU in the Data Center and 2 on the Indianapolis campus of IU in the ICTC complex. The separate physical environments allow survivability mechanisms that prevent a failure on either campus to knock out service to the OSG. Details of these mechanisms will be described below in the individual service section. 
---+++++ Network 
The Bloomington GOC servers are on an exclusively allocated VLAN; the 
Indianapolis servers are likewise on a similar but separate VLAN. The 
Bloomington servers also have a backend private VLAN for inter-server communication; the Indianapolis servers link into this via a VPN, using one of the Bloomington servers as a gateway. All GOC servers are behind a university-wide Juniper Netscreen firewall system for security.

GOC services for which this is possible are duplicated at both the Bloomington and Indianapolis locations. A DNS round-robin approach is used, dividing traffic between the two instances of the same service and permitting some degree of continued access immediately if one of the two go down, while GOC staff work to bring access back online fully.
---+++++ Power and Cooling 
The Bloomington servers are housed in the IU Data Center, a facility that went online in 2009 and provides infrastructure for a wide variety of applications, from Indiana University departmental work to international projects. The power system is backed up with a UPS system consisting of two flywheel-based energy storage devices and a battery-based device, and with two generators. In the case of a failure of utility power, the flywheel devices can provide power for 20 seconds at full load, the battery system can then provide power for 8 minutes, and then the generators will take over if the power has not returned by that time. Each server rack contains two redundant network-monitored PDUs, and each GOC server has two power supplies, one connected to each PDU, allowing all servers to continue operation if one PDU were to go down. The Data Center currently has 1100 tons of cooling with 2 cooling towers designed to withstand 140-mph winds.

The Indianapolis servers are in the Informatics and Communications Technology 
Complex, with similar power and cooling conditions as the IU Data Center described above. 
---++++ Disaster Recovery 
The GOC uses three basic mechanisms for disaster recovery. 
•	Institutional Backups - TSM is used to backup the internal backups. 
•	Internal Backups - Internal backup details here. 
•	Virtual Machines - For quick restoration of services that are often updated and restoration of data is unnecessary. ie BDII. 
---+++++ Computer Operations at IU 
IU provides 24x7x365 coverage of the facilities at both campuses. This allows the GOC technical staff to have the ability to perform hands-on activities in case of an emergency without being personally present at the GOC hardware. 
---++++ Compute Infrastructure 
OSG Operations compute based services are hosted, maintained, and sometime developed by the GOC staff. This set of services composes the OSG Operations infrastructure. The GOC’s overall philosophy is to host services which are important to the OSG community as a whole and we try not to host VO or resource specific services. The entire GOC takes pride in our ability to provide these services with minimal downtime and keep the community through visible and reliable change management. Services developed by the GOC are released to the community through a testing and integration process described in this document. Updates are done on a regular schedule (currently every second Tuesday) so that the OSG community knows when to expect changes. Changes are announced with sufficient warning for the stakeholders to validate that the changes don’t cause problems.
---+++++ Hardware 
Details of the model numbers of the servers at the GOC go here, along with rack peripherals. 
---+++++ Operating System and Virtualization 
Details of both base OS and VMs go here.  
GOC Service Architecture Diagram 
  

---++++ Information Services 
---+++++ BDII
The GOC BDII service consumes raw GLUE information from CEMon client located at the OSG Resource level and serves it to OSG users in ldap friendly format. The BDII service consists of two independent BDII software structures available using DNS round-robining techniques.
The Service Level Agreement for the OSG BDII can be found at this link. The BDII server is located at this link. 
---+++++ ReSS
Need to get this from FNAL. 
---++++ Health Monitoring Services 
---+++++ RSV Collector
The GOC Resource and Service Validation (RSV) Collector receives and distributes RSV health monitoring records from local OSG Resources to several reporting services including but not limited to MyOSG and WLCG SAM. The Service Level Agreement for the OSG RSV Collector can be found at this link.
---++++ Administrative Services 
---+++++ OIM 
The OSG Information Management (OIM) service is a database and web API which contains administrative data collected from OSG collaborators describing resource, organization, and personal entities. Registration in OIM is required for technical resources to be part of OSG and strongly suggested for human resources connected to the OSG. 
The OIM entry page is available at this link. 
An SLA for OIM does not yet exist, but will be added upon its completion.
---+++++ Software Cache 
The GOC Production Software cache hold files necessary to update the OSG Certification Authority distribution and current OSG Production and ITB Middleware stacks.
The SLA for the Software Cache can be found at this link. Access to download from this cache is available at this link.
---++++ Communication Services 
---+++++ MyOSG
The GOC Production MyOSG service presents information from several OSG information sources and presents the information in various forms including web page, Universal Widget API, and XML. The MyOSG service consists of a web server and software consolidators to translate incoming data from several sources to be displayed in MyOSG. 

The MyOSG presentation website can be found at this link. The SLA for MyOSG can be found at this link.

---+++++ Footprints Ticketing 
The GOC ticketing system is based on software provided commercially to the GRNOC by Numara Software. This service is used to provide consistency to the operators that open tickets 24x7. Standard IT ticketing functionality is provided by this software. Footprints is protected by GRNOC and IU authentication mechanisms.
The GOC has developed a public interface see “Ticket” section below. We have also developed several email based exchange services with OSG collaborators, this is described in the “TTX” section below.  
---+++++ Ticket
The GOC ticket service provides public viewing and interaction, including ticket submission, with the Footprints ticketing system described above. It also provides several tools used by the GOC Support Staff and OSG Security Team to send notification and standard email forms to the OSG community. This interface is available at this link.
---+++++ TTX
---+++++ JIRA
---+++++ TWiki 
---+++++ Blogspot RSS Feed . 
---++++ Metric and Reporting Services 
---+++++Details of Gratia and RSV/SAM reports go here. 
---++++ Service Level Agreements for Compute Services 
Each GOC service will have an SLA. The current SLA page is located here. Writing, reviewing and getting approval for these SLAs has proven to be a long and tedious process, which is still in progress. 
---+++ Development of Compute Services 
While the GOC is not a software development group, some services are unique to the OSG environment and do require development. In these cases the GOC strives to work with the OSG community to meet the needs of the users. 
---++++ Upcoming Development Projects



<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = RobQ

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|General|Integration|Monitoring|Operations|Security|Storage|Tier3|User|VO)
   * Local DOC_AREA       =  Operations

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager)
   * Local DOC_ROLE       =  SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting)
   * Local DOC_TYPE       = Knowledge

   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = ElizabethChism

   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = ElizabethChism

   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->

---+++ Comments
|  | Main.SoichiHayashi | 14 Jan 2011 - 20:37 |
%COMMENT{type="tableappend"}%

%META:FILEATTACHMENT{name="TheGOC.doc" attachment="TheGOC.doc" attr="" comment="" date="1295037899" path="What is the GOC.doc" size="140800" stream="What is the GOC.doc" tmpFilename="/usr/tmp/CGItemp11049" user="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dSoichi_20Hayashi_20461343" version="3"}%
