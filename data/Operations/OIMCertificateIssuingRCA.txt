%META:TOPICINFO{author="RobQ" date="1493222712" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="OpsRootCauseAnalysis"}%
---+ Root Cause Analysis of OIM Cert Issues 25-26 April 2017

---++ Introduction

---++ Timeline

---++ Follow Up Actions

---++ Reference

Rob, Susan, Von,

May I request an informal post-incident analysis for last night's CILogon OSG CA outage (https://ticket.opensciencegrid.org/33592)? I think the outage lasted <24 hours, so our monthly uptime is still above the 95% monthly uptime target under our SLA (https://twiki.grid.iu.edu/bin/view/Operations/CILogonServiceLevelAgreement). For this reason, I think a formal root cause analysis is not mandated per the SLA, but still I think there are some lessons learned that we can take away from this outage to improve the service we deliver to OSG users.

Here's my view of the outage:

Tue 11am OIM software upgrade begins. CILogon OSG CA stops working.
Tue  5pm GOC ticket opened. CILogon support requested.
Wed 10am Tomcat restarted on OIM to resolve the problem.

Some observations:
It appears the cause was an OIM software upgrade without a needed Tomcat restart.
It appears the problem was not detected during post upgrade testing in the GOC business-hours service update window.
Individual CILogon staff were contacted directly for off-hours assistance without going through the XSEDE helpdesk (i.e., SLA procedures were not followed).
CILogon staff were asked to diagnose the problem before OIM staff, even though an OIM change occurred immediately preceding the outage, and no CILogon service change was scheduled for that day.
A similar problem has occurred before (https://ticket.opensciencegrid.org/31112).

I welcome any comments/corrections on the above and your additional thoughts on this outage and how we might have avoided it and/or handled it more quickly/efficiently.

Thanks,
Jim


-- Main.RobQ - 26 Apr 2017
