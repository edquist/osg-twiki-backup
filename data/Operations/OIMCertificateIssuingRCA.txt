%META:TOPICINFO{author="RobQ" date="1494078381" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="OpsRootCauseAnalysis"}%
---+ Root Cause Analysis of OIM Cert Issues 25-26 April 2017

---++ Introduction
This document is to describe the issues raised from the user and host certificate issuance through OIM (OSG information management) portal. All parties involved in the certificate issuance have been notified to properly and timely resolve the issue.

---++ Timeline

<h3 style="margin-top: 0; padding-top: 0;">4/25/2017 </h3>


<ul> <li> 9:02 AM EST OIM update branch 3.69. Field rename to accommodate gratiaweb retirement. Tested to verify the changes took into an effect. All the changes have been deployed in to an appropriate webapp location
<li>12:40 PM EST OS updates are run.
<li> 3:47 PM EST A certificate request from David Sanders was received.  It generated the error (Failed to sign certificate -- CertificateProviderException :: From CILogon: Failed to make cilogon/rest request: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure)
<li> 3:50 PM EST Internal investigation starts. Marina Krenz started to troubleshoot the issue.
<li> 4:00 PM EST Production meeting. It was decided to include CILogon on the issue, while the internal investigation was on the way.
<li> 5:05 PM EST Kyle Gross opens the ticket to track/resolve the issue https://ticket.grid.iu.edu/33592
      * While trying to diagnose at GOC I open a ticket and also contact Jim and Terry to let them know that we are seeing issues and asked if there was a known ongoing issue. I did not think there was a problem with CILogon, but I just wanted a response to completely rule it out.- Kyle
</ul>
<h3 style="margin-top: 0; padding-top: 0;">4/26/2017 </h3>
<ul>
<li> 8:00 AM EST Internal investigation continues(investigation of the current certificates, redeployment of the changes)
<li> 9:45 AM EST Sarah Schmiechen suggests to restart tomcat, as in the past it needed to be restarted more than once.
<li> 10:00 AM EST Tomcat restart resolved the issue.
<li> 10:00 AM EST Certficate was issued by Sarah Schmiechen to confirm the proper functionality of OIM
<li> 10:09 AM EST Kyle Gross confirmed the functionality as well.
<li> 10:24 AM EST Kyle Gross announces all clear via email.
</ul>
---++ Follow Up Actions

   * Certificate flow should be check end-to-end with each update. 
   * Upon discussion within internal staff, we will be adding a policy to extend the restart of Tomcat not only after OIM software update but after OS updates as well. 
   * For tickets opened against the CILogin service we should simply use the sychronize with XSEDE ticketing option, not add individual names to the ticket. This will be reiterated to staff at the next standup meeting. 

---++ Reference

Rob, Susan, Von,

May I request an informal post-incident analysis for last night's CILogon OSG CA outage (https://ticket.opensciencegrid.org/33592)? I think the outage lasted <24 hours, so our monthly uptime is still above the 95% monthly uptime target under our SLA (https://twiki.grid.iu.edu/bin/view/Operations/CILogonServiceLevelAgreement). For this reason, I think a formal root cause analysis is not mandated per the SLA, but still I think there are some lessons learned that we can take away from this outage to improve the service we deliver to OSG users.

Here's my view of the outage:

Tue 11am OIM software upgrade begins. CILogon OSG CA stops working.
Tue  5pm GOC ticket opened. CILogon support requested.
Wed 10am Tomcat restarted on OIM to resolve the problem.

Some observations:
It appears the cause was an OIM software upgrade without a needed Tomcat restart.
It appears the problem was not detected during post upgrade testing in the GOC business-hours service update window.
Individual CILogon staff were contacted directly for off-hours assistance without going through the XSEDE helpdesk (i.e., SLA procedures were not followed).
CILogon staff were asked to diagnose the problem before OIM staff, even though an OIM change occurred immediately preceding the outage, and no CILogon service change was scheduled for that day.
A similar problem has occurred before (https://ticket.opensciencegrid.org/31112).

I welcome any comments/corrections on the above and your additional thoughts on this outage and how we might have avoided it and/or handled it more quickly/efficiently.

Thanks,
Jim


-- Main.RobQ - 26 Apr 2017
