%META:TOPICINFO{author="MichaelWilde" date="1151104196" format="1.0" version="1.5"}%
%META:TOPICPARENT{name="SummerGridSyllabus2006"}%
---+Exercises for Lecture 1: Introduction to the Grid

This exercise explores some simple Grid operations to start developing a few very basic techniques for distributed computing.

We assume you're using the default “bash” shell set up on your logins on the laptops and lab servers. We recommend you don't change your shell.

%TOC{depth="3"}%

---++Getting to know your Mac Laptop

You will be working for the week in teams of two, which have been pre-assigned and are listed on a handout titled Team List.  Please find your partner, and locate (from the handout) the laptop that you have been assigned to.

You and your partner will share the same userid on the laptop, and use the same userid to log into the Linux lab systems that we will be using.  These IDs are of the form “trainingN” – listed on the Class Team Roster. You’ll need to work on the laptop with the same number – that’s where your login has been established. These numbers are also marked on stickers on the bottom of the laptop, and on each desk in the lab.

You will be using the following tools on your laptop:

-			“Terminal” tool to connect to the Linux lab systems.

-			“FireFox” browser.  Since some things require FireFox and don’t work well with Safari or Internet Explorer, its best to stay in FireFox and keep all your bookmarks in this one browser.

-			Occasionally, a simple text editor.

Please take a moment and learn how to start these tools from the MacOS “Dock” – the tool selector at the bottom of the laptop. To create a new Terminal window from within Terminal, use File -> New Shell.

A handy tip: you can use the Apple-Key (has an “Apple” and “Propeller” symbol – just to the left of the space bar) for fast cut and paste. Apple-C = “copy the text selected by the cursor”, Apple-V = “paste text selected by the cursor”, Apple-X = “delete text selected by the cursor”.

Set a bookmark in your FireFox browser to the Workshop Syllabus page:

http://osg.ivdgl.org/twiki/bin/view/SummerGridWorkshop/SummerGridSyllabus2006

 

General Information about the workshop will be posted at:

http://osg.ivdgl.org/twiki/bin/view/SummerGridWorkshop/WebHome

 

Now, start a “Terminal” window from the desktop Dock at the bottom of your screen (it may have been moved to the sides or even the top, though…). The icon for Terminal is a black screen with the characters “>_” on it.

 

Note the conventions for command-line dialogs that will be used throughout these exercises:

What you type is in bold

The system’s responses are in regular weight text

Our comments to you (which you should NOT enter) are:  #  italics after a "#"

 
<pre>
training42$ pwd			  # You type "pwd" but NOT this comment!

/home/training42			 # the system's response

training42$					# the system's command prompt
</pre>
 

 

Practice cutting text from the browser to run a command or set of commands: cut the “pwd” command from the box above and paste it into your terminal window to execute it.

This is a good way to avoid making typos while entering commands from the examples (also a good way to replicate our typos precisely – please let us know when you find errors in the exercise handouts! :) 

 
---++Getting set up on the Lab Linux System
You will be doing almost all of lab exercises this week from a Linux computer (“host”) named “gridlab1” (its “fully qualified host name” is “gridlab1.phys.utb.edu”).  From this machine, we will run Grid jobs, transfer files, and explore Grid sites.

To access this machine, from your laptop Terminal window, we will use the “secure shell” utility, ssh, to do a “remote login” from your laptop to the gk1 Linux server:

<pre>
CCT-Training14s-Computer:~ training14$ ssh gridlab1.phys.utb.edu

The authenticity of host 'gridlab1.phys.utb.edu (206.76.233.104)' can't be established.

RSA key fingerprint is 36:74:78:a8:ed:6b:38:96:63:20:01:df:46:9b:59:3b.

Are you sure you want to continue connecting (yes/no)? yes

Warning: Permanently added 'gk1.phys.utb.edu,206.76.233.104' (RSA) to the list of known hosts.

training14@gk1.phys.utb.edu's password: training14 # not echoed !!!

[training14@gk1 training14]$	  # Now you’re on the gk1 server!
</pre>

After the first time you do this, you won’t get the “Are you sure…” prompt. Some of you will never see this, as your computers were used for testing this material, and the “yes” reply was already supplied by a tester.  So it will look like:

<pre>
CCT-Training14s-Computer:~ training14$ ssh gridlab1.phys.utb.edu

Password: training14 <enter trainingN here, same as your laptop name>

[training14@gk1 training14]$
</pre>

For the rest of this exercise, we will use simpler prompts: $ for your laptop and gridlab$ for the gridlab1 lab server.

Check that various important variables are set correctly in your environment:

 
<pre>
gridlab$ echo $GLOBUS_LOCATION
/opt/globus

gridlab$ echo $CONDOR_LOCATION
/opt/condor

gridlab$
</pre>
 

Next, create a Grid security proxy for this tutorial.	(A proxy is like a temporary ticket to use the Grid – in this case, for the next 12 hours.  This will be explained in Lecture 2.)

You will need to obtain your passphrase from the Team List handout. Use the passphrase from the Team List for the team member listed in the “Your Identity” prompt, as illustrated below.

 
<pre>
gridlab$ grid-proxy-init

Your identity: /C=US/O=Globus/O=University of Wisconsin/OU=Computer Sciences Department/CN=Alan De Smet
Enter GRID pass phrase for this identity: YourPassPhrase
Creating proxy ........................................... Done
Your proxy is valid until Thu Jul 10 16:06:13 2003

gridlab$ 
</pre>
 
---++Submitting jobs with Globus Commands

Now, if everything is set correctly, you should be able to run “Grid jobs” (here, mostly “commands”) on the lab Grid. First, check that the gatekeeper “knows” you – has your Grid identity authorized to run jobs:

<pre>
gridlab$ globusrun –a –r gk2/jobmanager-fork

GRAM Authentication test successful

gridlab$
</pre>

Look for the “test successful” message. This means you are “authorized” to use the gk2 “resource”. If this still fails, there is a problem with your Grid certificate, so contact an instructor.

Now, run your first very simple Grid job with the command “globus-job-run”:

 
<pre>
gridlab$ globus-job-run gridlab2 /bin/hostname

gridlab2.phys.utk.edu

gk1$
</pre>

You’ve just submitted a “job” (the command “hostname”) to the GRAM gatekeeper on gridlab2, from the “submit host” gridlab1! Trivial, perhaps, but a building block to more powerful capabilities.

What else can you learn with this? E.g, how can you find what user ID your job ran under?	Also try /usr/bin/env (real important!), uptime, pwd, ls? (Note: you must use full paths!)

Try running a shell command:

<pre>
/bin/sh –c 'cmd here'				 (use this, e.g. to find your $PATH with echo!)
</pre>

try this on gk2 as well

 
---++ Explore the Lab Grid

Here are the sites that you can access at this point:

<pre>

Site Name				  Gatekeeper					 Sched	  CPUS  GridFTP
LAB1						 gridlab1.phys.utb.edu	  condor	 1	  gridlab1.phys.utb.edu
LAB2						 gridlab1.phys.utb.edu	  condor	 1	  gridlab2.phys.utb.edu
LAB3						 gridlab1.phys.utb.edu	  condor	 1	  gridlab3.phys.utb.edu
LAB4						 gridlab1.phys.utb.edu	  condor	 1	  gridlab4.phys.utb.edu
ISI						  skynet-login.isi.edu		pbs		  90	skynet-login.isi.edu
IUPUI-ITB				  feynman.uits.iupui.edu	 condor	  ?	 feynman.uits.iupui.edu
CIT_ITB_1				  citgrid3.cacr.caltech.edu condor	  ?	 citgrid3.cacr.caltech.edu
UFlorida-EO				ufgrid05.phys.ufl.edu	  condor	  20	ufgrid05.phys.ufl.edu
FNAL_FERMIGRID_TEST	 fgtest1.fnal.gov			 condor	  10	fgtest1.fnal.gov
FIUPG						fiupg.ampath.net			 condor	  76	fiupg.ampath.net
ANL-UC					  tg-grid.uc.teragird.org	 pbs		 5
NCSA						 tg-login.ncsa.teragrid.org pbs		 5
SDSC						 tg-login.sdsc.teragrid.org pbs		 5	  

</pre>

gridlab$ gsites -g
Site					 Gatekeeper						  Job-Managers		  
---------------------------------------------------------------------
dartmouth-physics	phys-01.grid.dartmouth.edu	 fork,condor			
dartmouth-pbs		 pbs-01.grid.dartmouth.edu	  fork,sge				
uchicago-compsci	 evitable.uchicago.edu			fork,condor			
utb-summergrid		gk2.phys.utb.edu				  fork,condor			
dartmouth-math		math-01.grid.dartmouth.edu	 fork,condor			
uchicago-chemistry  terminable.uchicago.edu		 fork,condor			
uchicago-sociology  ept.uchicago.edu				  fork,condor			
uchicago-education  chalant.uchicago.edu			 fork,condor			
uchicago-astronomy  gainly.uchicago.edu			  fork,condor			
uchicago-business	sheveled.uchicago.edu			fork,condor			

gridlab$
</pre>

To dig deeper, try the various other gsites options to learn more about these sites.

Now, try running some simple jobs on a remote machine:

<pre>
gridlab$ grun –s uchicago-compsci –c /bin/hostname
</pre>

See what happens if the command name to run (-c option) is not fully qualified.  (i.e., just “-c hostname”)

To make things easier, set up aliases. You’ll need to make a “.gstar” directory in your home directory. You can paste the commands from “cat” through the line containing just “END”:

<pre>
gridlab$ cd
gridlab$ mkdir .gstar # Note the "."!
gridlab$ cat > $HOME/.gstar/aliases <<END
dmphys dartmouth-physics
dmpbs dartmouth-pbs	
uccs uchicago-compsci
lab utb-summergrid
dmmath dartmouth-math
ucchem uchicago-chemistry
ucsoc uchicago-sociology
ucedu uchicago-education
ucastro uchicago-astronomy
ucbus uchicago-business
END

gridlab$
</pre>

Now try using the aliases:

<pre>
gridlab$ grun -s lab -c /bin/hostname
 
# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/hostname
gk2

gridlab$
</pre>

(Note that if you give grun a bad alias, it fails silently, since no hosts matched. It’s a prototype…)

Here’s the same command run under a shell:

<pre>
gridlab$ grun -s lab -c '/bin/sh -c hostname'		  
# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c hostname
gk2
gridlab$
</pre>

Note that here we did not need to specify a full path for hostname – so running /bin/sh on the remote site gives you a PATH.  Can you find out what it is? (Hint: run echo $PATH…)

If you’re careful with single and double quotes, you can also pass a list of commands:

<pre>
gk1$ grun -s lab -c '/bin/sh -c "ls /etc | grep cron"'

# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c "ls /etc | grep cron"
anacrontab
cron.d
cron.daily
cron.hourly
cron.monthly
cron.weekly
crontab

gridlab$
</pre>

Try using the grun –f option to put your scripts in a file.

Now, you can explore the remote execution environment.  Learn what you can about remote environment  variable settings (env command), local directory (pwd), and what files are out there (ls).  Can you create and remove a file in the remote directory?  Can you cd to /tmp and do the same?

To run a command on all Grid sites, try grun with no “-s” option.  The sites in a Grid are seldom all “up” at any given time, and out lab Grid is no exception!	Try to find out which sites are responding correctly.  What different GRAM errors do you get?

Then, make an include (or an exclude) list to run a command only on the “good” sites. An include-list is a simple file of site names, placed in a file ending in “.i”.  An exclude list is similar but ends in “.x”. (NOTE: you need to use full site names in these lists – not aliases).  For example:

<pre>
gridlab$ cat >sites.i <<END
sitename1 # Put the real, *full* site names here (not aliases)
sitename2
END
</pre>
 

Try running the simple “numerical application” bc to see how fast the different processors are out in the Grid:

<pre>
echo 1234^50000 | time bc >/dev/null
</pre>
 

Try this on several of the “working” sites.

<pre>
gridlab$ grun -S sites.i -c '/bin/sh -c "echo 1234^50000 | time bc >/dev/null"'
# processing uchicago-compsci, 50 CPUs
uchicago-compsci: globus-job-run evitable.uchicago.edu/jobmanager-fork /bin/sh -c "echo 1234^50000 | time bc >/dev/null"
4.00user 0.00system 0:04.15elapsed 96%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (152major+306minor)pagefaults 0swaps

# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c "echo 1234^50000 | time bc >/dev/null"
4.15user 0.02system 0:04.58elapsed 90%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (160major+307minor)pagefaults 0swaps

gridlab$
</pre>

---++Digging Deeper

 

Look into “RSL” –  the Globus GT2 GRAM “Resource Specification Language” – Google for more info. Try a simple globus-job-run with the option “–dumprsl”, then try globusrun with your own RSL. 

o		  globusrun – r gk1 –o '&(executable=/bin/pwd)'
o		  Write a small shell script to dump more about your environment in one command. Try it locally.
§			#!/bin/bash
§			date
§			hostname -f
§			uptime
§			env
§			cat /etc/issue

Then: 
<pre>
globus-job-run gk1 –s my-scriptname
</pre>

Extra extra credit: Try “staging” a binary – copying it from your job submission host to the execution host (such as the local /bin/date)

·			What RSL does globus-job-run create?

·			Submit a simple job such as /bin/hostname to the Condor jobmanager using globus-job-run and globusrun.	The “contact string” is gk2/jobmanager-condor rather than just “gk1” (which is a default name for gk2/jobmanager-fork)

Create a new program in your favorite language. C, Bourne shell, Perl, and Python are reasonable choices. This program should take one argument, and integer, and return the square of that argument. Run this program on our local grid for value. You'll need to stage the executable--see the -s command for globus-job-run. (What RSL do you get now?) Run this program for n in [10, 30], and collect the results in distinct files. Better yet, make a script to do all of this for you.

