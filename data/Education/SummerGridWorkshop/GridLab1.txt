%META:TOPICINFO{author="MichaelWilde" date="1151270592" format="1.0" version="1.9"}%
%META:TOPICPARENT{name="SummerGridSyllabus2006"}%
---+Exercises for Lecture 1: Introduction to the Grid

This exercise explores some simple Grid operations to start developing a few very basic techniques for distributed computing.

We assume you're using the default "bash" shell set up on your logins on the laptops and lab servers. We recommend you don't change your shell.

%TOC{depth="3"}%

---++Getting to know your Mac Laptop

You will be working for the week in teams of two, which have been pre-assigned and are listed on a handout titled Team List.  Please find your partner, and locate (from the handout) the laptop that you have been assigned to.

You and your partner will share the same userid on the laptop, and use the same userid to log into the Linux lab systems that we will be using.  These IDs are of the form "trainingN" - listed on the Class Team Roster. You'll need to work on the laptop with the same number - that's where your login has been established. These numbers are also marked on stickers on the bottom of the laptop, and on each desk in the lab.

You will be using the following tools on your laptop:

	* "Terminal" tool to connect to the Linux lab systems.
	* "FireFox" browser.  Since some things require FireFox and don't work well with Safari or Internet Explorer, its best to stay in FireFox and keep all your bookmarks in this one browser.
	* Occasionally, a simple text editor.

Please take a moment and learn how to start these tools from the MacOS "Dock" - the tool selector at the bottom of the laptop. To create a new Terminal window from within Terminal, use File -> New Shell.

A handy tip: you can use the Apple-Key (has an "Apple" and "Propeller" symbol - just to the left of the space bar) for fast cut and paste. Apple-C = "copy the text selected by the cursor", Apple-V = "paste text selected by the cursor", Apple-X = "delete text selected by the cursor".

Set a bookmark in your FireFox browser to the Workshop Syllabus page: http://osg.ivdgl.org/twiki/bin/view/SummerGridWorkshop/SummerGridSyllabus2006

General Information about the workshop will be posted at: http://osg.ivdgl.org/twiki/bin/view/SummerGridWorkshop/WebHome

Now, start a "Terminal" window from the desktop Dock at the bottom of your screen (it may have been moved to the sides or even the top, though.). The icon for Terminal is a black screen with the characters ">_" on it.

Note the conventions for command-line dialogs that will be used throughout these exercises:

What you type is in *bold*. The system's responses are in regular weight text.

Our comments to you (which you should *not* enter) look like:  <pre>#  some comment here</pre>

<pre>
laptop$ <b>pwd</b>	 # You type "pwd" but NOT this comment!
/home/ccttraining42	# the system's response
laptop$					# the system's command prompt
</pre>

In these exercises, we'll use the prompt "laptop$ " for responses from the shell on the lab laptops, and "gridlab$ " or "gridlab1$ ", etc., for responses from remote shells. The actual shell prompts you'll see may be different from those we show here.

To start, practice cutting text from your this page in your web browser to run a command or set of commands: cut the "pwd" command from the box above and paste it into your terminal window to execute it. This is a good way to avoid making typos while entering commands from the examples.
 
---++Getting set up on the Lab Linux System
You will be doing almost all the lab exercises this week on a Linux computer ("host") named "gridlab1" (its "fully qualified host name" is "gridlab1.phys.utb.edu").  From this machine, we will run Grid jobs and explore various Grid sites.

To access this machine, from your laptop Terminal window, we will use the "secure shell" utility, ssh, to do a "remote login" from your laptop to the gridlab1 Linux server:

<!-- ping gridlab1?  Do we need fqdn's? -->

<pre>
laptop$ ssh train42@gridlab1.phys.utb.edu
The authenticity of host 'gridlab1.phys.utb.edu (206.76.233.104)' can't be established.
RSA key fingerprint is 36:74:78:a8:ed:6b:38:96:63:20:01:df:46:9b:59:3b.
Are you sure you want to continue connecting (yes/no)? <b>yes</b>
Warning: Permanently added 'gridblab1.phys.utb.edu,206.76.233.104' (RSA) to the list of known hosts.
train42@grodlab1.phys.utb.edu's password: <b>summergrid</b> # not echoed !!!
gridlab1$	  # Now you're talking to a shell on the gridlab1 server
</pre>

After the first time you do this, you won't get the "Are you sure." prompt. Some of you will never see this, as your computers were used for testing this material, and the "yes" reply was already supplied by a tester.  So it will look like:

<pre>
laptop$ <b>ssh gridlab1.phys.utb.edu</b>
Password: <b>summergrid</b> # enter trainingN or NN here, same as your laptop name
gridlab1$
</pre>

For the rest of this exercise, we will use simpler prompts: $ for your laptop and gridlab1$ for the gridlab1 lab server.

Now, on the gridlab1 host, check that various important variables are set correctly in your environment:

<!-- do we need these?  Will we have a condor_location? --> 
<pre>
gridlab$ <b>echo $GLOBUS_LOCATION</b>
/opt/globus

gridlab$ <b>echo $CONDOR_LOCATION</b>
/opt/condor

gridlab1$
</pre>

Next, create a "Grid security proxy" for this tutorial.  A proxy is
like a temporary ticket to use the Grid - in this case, for the next
12 hours.  (Grid proxies will be explained in Lecture 2).

The Grid pass phrase for our workshop is <b>summergrid</b>

<!-- fixme show the proxy they will really get here -->
<pre>
gridlab1$ <b>grid-proxy-init</b>

Your identity: /C=US/O=SDSC/OU=SDSC/CN=Account Train35/UID=train35
Enter GRID pass phrase for this identity:<b>summergrid</b>
Creating proxy ............................................. Done
Your proxy is valid until: Mon Jun 26 04:24:20 2006
gridlab1$

</pre>
 
---++Submitting jobs with Globus Commands

Now, if everything is set correctly, you should be able to run "Grid
jobs" on the hosts in the lab Grid. First, check that the "gatekeeper"
- the Grid component that accepts and executes remote jobs - "knows"
you - that your Grid identity is authorized to run jobs:

<pre>
gridlab1$ <b>globusrun -a -r gridlab2/jobmanager-fork</b>

GRAM Authentication test successful

gridlab1$
</pre>

<!-- fixme does -a test authorization (ie am in gridmap) or just authentication (ie it accepts my cert) ? -->

The "test successful" message means you are "authorized" to use the
gridlab2 "resource" or "job execution service". If this simple test
fails, there is a problem with your Grid certificate, so contact an
instructor for help.

Now, run your first very simple Grid job with the command
"globus-job-run". This command can run other commands on remote sites,
but it expects them to be "fully qualified" path names (i.e., they
must start with a "/"). Lets say we want to run the Linux command
"hostname" on the remote site to verify that we're talking to the
resource we think we are.

First, run it locally to make sure you are invoking it correctly.
Then use the command "which" to find out what fully qualified path
name your shell located this command in (i.e., which system-supplied
directory of executable tools was it found in)?

<pre>
gridlab1$ <b>hostname</b>
gridlab1.phys.utb.edu
gridlab1$ <b>which hostname</b>
/bin/hostname
gridlab1$
</pre>

Common Linux system commands are typically, but not always, found in
the same directory on all Linux systems, and this week we'll be using
Grid systems with exclusively Linux-based hosts. In real work,
organizations establish conventions for such things.

<pre>
gridlab1$ globus-job-run gridlab2 /bin/hostname

gridlab2.phys.utb.edu

gridlab1$
</pre>

<!-- fixme For example, try another gatekeeper: Fermi? -->

You've just submitted a "job" (the Linux command "hostname") to the
GRAM gatekeeper on gridlab2, from the "submit host" gridlab1! Trivial,
perhaps, but a building block to more powerful capabilities.

Now, see what else can you learn with this.  Find what user ID your job
ran under ("id" command).  Find out what environment variables are set ("env" command),
what the load on the remote Grid server is ("uptime"), and what the default working dirsctory your remote job will run in ("pwd" command). Do an "ls" of this working directory.  Use "df" to find out how much space is there, and how much space exists in the remote "/tmp" directory.

<!-- fixme make a script, set of remote commands, set of local commands -->

<!-- exercise sequence:
  try several commands
  make a script to run them
  run the script remotely
  try fork and scheduler JM
  compare times of different sites
  compare pwd of different sites
  what ID on different sites
  run from that ID to another one

  stage a data file
  wc -l the data file
 
  is the 
  Multihop?

$ cat countem.c
main()
{
  int i, sum=0, rc;
  while ( (rc=scanf("%d\n", &i)) != -1 ) {
	 sum += i;
  }
  printf("%d\n",sum);
}

awk '{ sum += $1 } END { print sum }' <data

awk 'BEGIN ~{ print int(rand()*100) }'

seq 500 600

/bin/time --verbose ./countem 300 <data

genr:

$ cat genr
NLINES=$1
NDIGITS=$2
awk "BEGIN {
  for (i = 0 ; i < $NLINES; i++ ) {
	 print int(rand()*10**$NDIGITS)
  }
}"

genr usage: genr nlines ndigits

$ ./genr 5 8
23778751
29106573
84581385
15220829
58553734

countem usage: countem num-compute-cycles <data

./genr 300 4 | /bin/time --verbose ./countem 300

-->

<!-- how to dertermine the default jobmanager? -->

(Hint: you must use full pathnames for all commands you run with globus-job-run - its default environment has no useful paths )

<!-- fixme look at the def env PATH and describe it more clearly than "no useful paths" ! -->

The reason we have had to use only fully qualified

Try running a shell command:

<pre>
/bin/sh -c 'cmd here'				 (use this, e.g. to find your $PATH with echo!)
</pre>

try this on gridlab2 as well

<!-- fixme: how many virtual CPUs on gridlabs 1-4? -->
 
---++ Explore the Lab Grid

Here are the sites that you can access at this point:

<pre>

Site Name				  Gatekeeper					 Sched	  CPUS GridFTP

LAB1						 gridlab1.phys.utb.edu	  condor	 1	 gridlab1.phys.utb.edu
LAB2						 gridlab1.phys.utb.edu	  condor	 1	 gridlab2.phys.utb.edu
LAB3						 gridlab1.phys.utb.edu	  condor	 1	 gridlab3.phys.utb.edu
LAB4						 gridlab1.phys.utb.edu	  condor	 1	 gridlab4.phys.utb.edu
ISI						  skynet-login.isi.edu		pbs		 90	skynet-login.isi.edu
IUPUI-ITB				  feynman.uits.iupui.edu	 condor	 1	 feynman.uits.iupui.edu
CIT_ITB_1				  citgrid3.cacr.caltech.edu condor	 1	 citgrid3.cacr.caltech.edu
UFlorida-EO				ufgrid05.phys.ufl.edu	  condor	 20	ufgrid05.phys.ufl.edu
FNAL_FERMIGRID_TEST	 fgtest1.fnal.gov			 condor	 10	fgtest1.fnal.gov
FIUPG						fiupg.ampath.net			 condor	 76	fiupg.ampath.net
ANL-UC					  tg-grid.uc.teragird.org	 pbs		5	 tg-grid.uc.teragird.org	 
NCSA						 tg-login.ncsa.teragrid.org pbs		5	 tg-login.ncsa.teragrid.org
SDSC						 tg-login.sdsc.teragrid.org pbs		5	 tg-login.sdsc.teragrid.org

</pre>

The official list of Grid sites for the workshop will be located here:  
[[%ATTACHURL%/sites][Workshop Grid site list]]

<!-- fixme copy this to a well-known place like /train/sites on each machine -->

Now, try running some simple jobs on a remote machine:

<pre>
gridlab$ grun -s uchicago-compsci -c /bin/hostname
</pre>

See what happens if the command name to run (-c option) is not fully qualified.  (i.e., just "-c hostname")

To make things easier, set up aliases. You'll need to make a ".gstar" directory in your home directory. You can paste the commands from "cat" through the line containing just "END":

<pre>
gridlab$ cd
gridlab$ mkdir .gstar # Note the "."!
gridlab$ cat > $HOME/.gstar/aliases <<END
dmphys dartmouth-physics
dmpbs dartmouth-pbs	
uccs uchicago-compsci
lab utb-summergrid
dmmath dartmouth-math
ucchem uchicago-chemistry
ucsoc uchicago-sociology
ucedu uchicago-education
ucastro uchicago-astronomy
ucbus uchicago-business
END

gridlab$
</pre>

Now try using the aliases:

<pre>
gridlab$ grun -s lab -c /bin/hostname
 
# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/hostname
gk2

gridlab$
</pre>

(Note that if you give grun a bad alias, it fails silently, since no hosts matched. It's a prototype.)

Here's the same command run under a shell:

<pre>
gridlab$ grun -s lab -c '/bin/sh -c hostname'		  
# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c hostname
gk2
gridlab$
</pre>

Note that here we did not need to specify a full path for hostname - so running /bin/sh on the remote site gives you a PATH.  Can you find out what it is? (Hint: run echo $PATH.)

If you're careful with single and double quotes, you can also pass a list of commands:

<pre>
gk1$ grun -s lab -c '/bin/sh -c "ls /etc | grep cron"'

# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c "ls /etc | grep cron"
anacrontab
cron.d
cron.daily
cron.hourly
cron.monthly
cron.weekly
crontab

gridlab$
</pre>

Try using the grun -f option to put your scripts in a file.

Now, you can explore the remote execution environment.  Learn what you can about remote environment  variable settings (env command), local directory (pwd), and what files are out there (ls).  Can you create and remove a file in the remote directory?  Can you cd to /tmp and do the same?

To run a command on all Grid sites, try grun with no "-s" option.  The sites in a Grid are seldom all "up" at any given time, and out lab Grid is no exception!	Try to find out which sites are responding correctly.  What different GRAM errors do you get?

Then, make an include (or an exclude) list to run a command only on the "good" sites. An include-list is a simple file of site names, placed in a file ending in ".i".  An exclude list is similar but ends in ".x". (NOTE: you need to use full site names in these lists - not aliases).  For example:

<pre>
gridlab$ cat >sites.i <<END
sitename1 # Put the real, *full* site names here (not aliases)
sitename2
END
</pre>
 

Try running the simple "numerical application" bc to see how fast the different processors are out in the Grid:

<pre>
echo 1234^50000 | time bc >/dev/null
</pre>
 

Try this on several of the "working" sites.

<pre>
gridlab$ grun -S sites.i -c '/bin/sh -c "echo 1234^50000 | time bc >/dev/null"'
# processing uchicago-compsci, 50 CPUs
uchicago-compsci: globus-job-run evitable.uchicago.edu/jobmanager-fork /bin/sh -c "echo 1234^50000 | time bc >/dev/null"
4.00user 0.00system 0:04.15elapsed 96%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (152major+306minor)pagefaults 0swaps

# processing utb-summergrid, 101 CPUs
utb-summergrid: globus-job-run gk2.phys.utb.edu/jobmanager-fork /bin/sh -c "echo 1234^50000 | time bc >/dev/null"
4.15user 0.02system 0:04.58elapsed 90%CPU (0avgtext+0avgdata 0maxresident)k
0inputs+0outputs (160major+307minor)pagefaults 0swaps

gridlab$
</pre>


---++ Checking out the Grid

- verify that you can log into each gridlab host (1-4)

- try to ssh from one to the other

- try scp to copy files

- try keygen


---++Digging Deeper

-- run jobs through sched and non sched
-- compare times
-- look at qstat and condor_q
-- look at env vars, UID, pwd, etc

---++ Exploring Internet Tools

Ping (via DNS and not)
ssh
traceroute

Look into "RSL" -  the Globus GT2 GRAM "Resource Specification Language" - Google for more info. Try a simple globus-job-run with the option "-dumprsl", then try globusrun with your own RSL. 

o		  globusrun - r gk1 -o '&(executable=/bin/pwd)'
o		  Write a small shell script to dump more about your environment in one command. Try it locally.
			#!/bin/bash
			date
			hostname -f
			uptime
			env
			cat /etc/issue

Then: 
<pre>
globus-job-run gk1 -s my-scriptname
</pre>

Extra extra credit: Try "staging" a binary - copying it from your job submission host to the execution host (such as the local /bin/date)

Å∑			What RSL does globus-job-run create?

Å∑			Submit a simple job such as /bin/hostname to the Condor jobmanager using globus-job-run and globusrun.	The "contact string" is gk2/jobmanager-condor rather than just "gk1" (which is a default name for gk2/jobmanager-fork)

Create a new program in your favorite language. C, Bourne shell, Perl, and Python are reasonable choices. This program should take one argument, and integer, and return the square of that argument. Run this program on our local grid for value. You'll need to stage the executable--see the -s command for globus-job-run. (What RSL do you get now?) Run this program for n in [10, 30], and collect the results in distinct files. Better yet, make a script to do all of this for you.



%META:FILEATTACHMENT{name="sites" attr="h" comment="" date="1151179338" path="sites" size="1141" user="MichaelWilde" version="1.1"}%
