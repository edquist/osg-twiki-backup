%META:TOPICINFO{author="MichaelWilde" date="1150241440" format="1.0" version="1.1"}%
%META:TOPICPARENT{name="ToDo"}%

<html>

<head>
<title>Student notes for VDS tutorial</title>
<style>
.transcript
{
	 background-color: #FFFFBA;
	 border: thin solid #02007F;
	 padding: 3pt;
	 width: auto;
}
  .in {color:#0000FF; font-weight: bold;}
  .out {color:#006600;}
</style>

</head>

<body>

<h1>Student notes for VDS tutorial</h1>

<h2>Introduction</h2>

<p>These are the student notes for the VDS tutorial at TeraGrid '06. They
are designed to be used in conjunction with instructor presentation and
support.</p>

<p>The tutorial is structured as four chapters of 45 minutes each. Each
chapter will consist of a presentation by the instructors, and then 
time to work on the exercises in this file.</p>

<p>You will see two styles of machine text here:</p>
<pre class="in transcript">Text like this is input that you should type.</pre>
<pre class="out transcript">Text like this is the output you should get.</pre>

<p>For example:</p>

<p class="transcript">
<code><span class="out">$</span> <span class="in">date</span><br />
<span class="out">Wed May 31 11:54:58 BST 2006</span></code>
</p>

<h2>Chapter 1: Basic usage</h2>

<p>The exercises in this chapter will familiarize you with basic use
of the VDS and with the physics example code that we will be using in
later exercises.</p>

<p>You will need to log into the tutorial machine, using an ssh client and 
the login name and password supplied separately.</p>

<p>On Linux or Mac OS X, open a terminal window and type:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">ssh vdsuser-9999@skynet-login.isi.edu</span>
[welcome message]
<span class="out">vdsuser-3@skynet-login:~$</span>
</pre>

<p>On Windows, 
	<a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/">PuTTY</a> 
	is recommended as an ssh client.</p>


<p>Files for each exercise are stored in subdirectories:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">ls</span>
<span class="out">
BIGGERVDL	 PROBINTRO	 SIMPLEVDL	 VDLINTRO
COMPOUNDVDL  GRIDINTRO	 QuarkCode	 </span>
</pre>

<p>You may also see some other files here.</p>

<h3>Exercise 1.1: VDLINTRO</h3>

<p>In this exercise, you will use the VDL tools to generate a very simple
'Hello World' message.</p>

<p>There is only one file for this exercise, located in the 
	<code>VDLINTRO/</code> subdirectory.
</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cd VDLINTRO/</span>

<span class="out">$</span> <span class="in">ls</span>
<span class="out">helloworld.vdl</span>
</pre>

<p><code>helloworld.vdl</code> is a VDL (Virtual Data Language file) which
defines one <em>transformation</em> (like a function definition), and one
<em>derivation</em> (like a function call). <br />

The transformation defined here takes a message and outputs it to a file.<br />
The derivation defined here calls this transformation, with a specific
message (hello world!) and a specific output file.</p>

<p>The first step in running helloworld is to compile the VDL file into an
<em>abstract DAG</em> (or <em>DAX</em>). Use the vdlc command for this:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">vdlc helloworld.vdl </span>

<span class="out">20060531T145705.890: [app] using label "helloworld"
20060531T145705.891: [app] using output file "helloworld.dax"</span>
</pre>

<p>An abstract DAG has been generated and output in XML format into 
<code>helloworld.dax</code>. Open <code>helloworld.dax</code> in a file
viewer:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cat helloworld.dax</span>

</pre>

<p>Inside the DAX, you should see three sections.
<ol>
<li>list of all referenced files - this shows the input and output files 
used by this workflow. In this case, there is only one file, the output 
file <code>greeting.txt</code>.
</li>

<li>definition of all jobs - each job in the workflow. In this exercise,
there is only one. The derivation specified in the VDL file has been converted
into a single &lt;job&gt; element in the DAX.</li>

<li>list of control-flow dependencies - this section specifies a partial
order in which jobs are to executed. As there is only one job in this 
DAX, there is nothing here. In later exercises, you will see more 
information here.</li>

</ol>
</p>

<p>Now that you have your abstract DAG, you can run it through the 
<em>planner</em> to generate a plan for the workflow, using the
<code>vds-plan</code> command.
</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">vds-plan --pegasus -g QuarkCode --option pools=local --option output=local --option force helloworld.dax --base ./dags</span>

[...many lines of output...]
<span class="out">
2006.06.10 20:14:57.108 PDT: [INFO] Generating submit files for the concrete workflow (completed)
2006.06.10 20:14:57.108 PDT: [INFO] Generating submit files for the cleanup workflow
2006.06.10 20:14:57.114 PDT: [INFO] Generating submit files for the cleanup workflow (completed)
2006.06.10 20:14:57.115 PDT: [INFO] Time taken to execute is 1.05 seconds

I have concretized your abstract workflow. The workflow has been entered
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is

vds-run  /nfs/home/vdsuser-2/VDLINTRO/dags/QuarkCode/helloworld/run0007
</span>
</pre>

<p>The workflow has been planned. You can start it running by typing the
command shown in the above output (note that the numbers will probably be
different for you).</p>

<p>You can start it running using the <code>vds-run</code> 
command:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">vds-run  /nfs/home/vdsuser-2/VDLINTRO/dags/QuarkCode/helloworld/run0007</span>

<span class="out">
Checking all your submit files for log file names.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor			  : helloworld-0.dag.condor.sub
Log of DAGMan debugging messages					  : helloworld-0.dag.dagman.out
Log of Condor library debug messages				 : helloworld-0.dag.lib.out
Log of the life of condor_dagman itself			 : helloworld-0.dag.dagman.log

Condor Log file for all jobs of this DAG			: /tmp/helloworld-037504.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 212.
-----------------------------------------------------------------------
</span>
</pre>

<p>You can monitor which jobs are running using the <code>condor_q</code> 
command and by watching the jobstate log.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">tail -f jobstate.log</span>
[...log information appears over time...]

<span class="out">1150052758 INTERNAL *** DAGMAN_FINISHED ***
1150052762 INTERNAL *** TAILSTATD_FINISHED 0 ***</span>
</pre>

<p>You will know when your job is completed when the 
<code>TAILSTATD_FINISHED</code> message appears. If something has gone 
wrong, you might see an error message instead.</p>

<p>When you job has completed, you can examine the results as follows.
You will need to substitute your user number in twice, and the run number
that you used above. Replace 2 below with your user number and 0005 with the
run number.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cat /nfs/scratch01/train</span>02<span class="in">/QuarkCode/vdsuser-</span>2<span class="in">/helloworld/run</span>0005<span class="in">/greeting.txt</span>

<span class="out">Hello World</span>
</pre>

<p>and now you can see the greeting.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cat greeting.txt</span>
<span class="out">Hello World</span>
</pre>

<p>Congratulations! You've run your first VDL workflow.</p>



<h3>Exercise 1.2: PROBINTRO</h3>

<p>This exercise introduces the physics example code, taken from 
QuarkNet. You will run some of the code by hand to analyse data
and plot it on a graph. In later exercises, you will run the 
same code but in other ways, taking advantage of VDS.</p>

<p>The QuarkNet code is written in PERL and kept in the <code>QuarkCode/</code>
subdirectory.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">ls QuarkCode/</span>

<span class="out">Combine.pl		  Flux.pl			  Plot.pl			  Split.pl 
CommonSubs.pl	  Frequency.pl		RawAnalyze.pl	  ThresholdTimes.pl 
EventChoice.pl	 Getopt				ShowerArgs.pl	  WireDelay.pl 
EventSearch.pl	 Lifetime.pl		 SingleChannel.pl  gps_data.pl 
ExtraFunctions.pl Math				  Sort.pl</span></pre>

<p>The data files which will be analysed have been collected from cosmic
ray detectors at various sites around the US. Some of that is available in
the <code>Data/</code> subdirectory.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">ls Data/</span>
<span class="out">180
180.2004.0819.0.raw
180.2004.0819.0.raw.meta
180.2004.0819.1.raw
180.2004.0819.1.raw.meta</span>
[...]

</pre>

<p>The commands to run are in the <code>PROBINTRO/</code> subdirectory, 
in a shell script called <code>manual.sh</code>.  You can run the whole
analysis by executing that shell script.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cd PROBINTRO</span>
<span class="out">$</span> <span class="in">ls</span>

<span class="out">manual.sh
$</span> <span class="in">./manual.sh</span>
<span class="out">min max step num bin 0.01050 9.53400 0.95235 |10|10|
numbins: 10 #bins: 10||
DEBUG: 0.01050 9.53400 0.01050 9.53400
$</span> <span class="in">ls</span>
<span class="out">gnuplotFitParam943  int-lifetimes  int-thresh-3  int-wire-2  manual.sh
int-combined		  int-params	  int-thresh-4  int-wire-3  plot.png
int-extra			  int-sorted	  int-thresh-5  int-wire-4
int-extra-raw		 int-thresh-1	int-thresh-6  int-wire-5
int-freqs			  int-thresh-2	int-wire-1	 int-wire-6</span>
</pre>

<p>The output graph is in plot.png. In addition, several intermediate files
have been created - each step in the analysis created intermediate files
that were then used by later steps.
</p>

<p>To view the plot, you can copy <code>plot.png</code> to your
skynet webspace, and view it in your web browser:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cp plot.png ~/public_html/probintro.png</span>
<span class="out">$</span> 
</span>
</pre>
<p>Point your web browser to:
http://skynet-login.isi.edu/~vdsuser-9999/probintro.png</p>

<h2>Chapter 2: More advanced VDL</h2>
<p>In this chapter you will be introduced to more advanced VDL usage.</p.

><h3>Exercise 2.1: SIMPLEVDL</h3>

<p>In this exercise, you will write your own VDL to run one step in the
QuarkNet problem code, which takes the processed data and generates a graph.
</p>

<p>You are provided with
 <ul> <li>the input data files for this step, <code>simplevdl-extra</code> and
			 <code>simplevdl-freqs</code> (these are the same as 
			 some of the intermediate files that were generated in your
			 <code>PROBINTRO/</code> 
			 directory). These have already been registered with the replica
			 location system (which you will see in greater depth in a later
			 exercise)</li>

		<li>a transformation definition that defines how to use Plot.pl from 
			 the <code>QuarkCode/</code> directory to convert input data 
			 files into a graph, in <code>transform.vdl</code>.</li>
 </ul>
</p>

<p>Armed with these tools, you will need to write a derivation which 
	uses the supplied transformation on the supplied data files to produce
	the output graph.</p>
			
<p>Instructions:

<ul>
  <li>move into the <code>SIMPLEVDL/</code> directory, and create a file
		called <code>simple.vdl</code> using an editor of your choice.<br />

<pre class="transcript">
<span class="out">$</span> <span class="in">cd SIMPLEVDL/</span>
<span class="out">$</span> <span class="in">vi simple.vdl</span>

</pre>
</li>
<li>Add a DV line to the VDL file. You can use 
	 <code>VDLINTRO/helloworld.vdl</code> as a syntax example, and if 
	 you get stuck the correct DV can be found in 
	 <code>SIMPLEVDL/cheat/simple.vdl</code>.  You will need to define the
	 derivation to use these parameters, with your user number 
	 substituted in for USER:
	 <pre>
		  plot_caption="plot from bigger.vdl",
		  plot_extraFunctions=@{input:"simplevdl-extra"},
		  plot_highX="",
		  plot_highY="",
		  plot_highZ="",
		  plot_infile=@{input:"simplevdl-freq"},
		  plot_lowX="",
		  plot_lowY="",
		  plot_lowZ="",
		  plot_outfile_param=@{output:"simplevdl-USER-param"},
		  plot_outfile_image=@{output:"simpleVDL-USER.png"},
		  plot_plot_type="3",
		  plot_title="bigger VDL plot",
		  plot_xlabel="xlabel",
		  plot_ylabel="ylabel",
		  plot_zlabel="" 
	 </pre>


</li>

<li>Now try compiling, planning and running your workflow:

<pre class="transcript">
<span class="out">$</span> <span class="in">vdlc simple.vdl transform.vdl</span>
<span class="out">20060609T152414.181: [app] using label "simple"
20060609T152414.182: [app] using output file "simple.dax"
$</span> <span class="in">vds-plan --pegasus -g QuarkCode --option pools=local --option output=local --option force simple.dax --base ./dags</span>
[...]
<span class="out">
2006.06.09 19:18:14.059 PDT: [INFO] Time taken to execute is 1.029 seconds

I have concretized your abstract workflow. The workflow has been entered
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is

vds-run  /nfs/home/vdsuser-2/SIMPLEVDL/dags/QuarkCode/simple/run0017

<span class="out">$</span> <span class="in">vds-run /nfs/home/vdsuser-2/SIMPLEVDL/dags/QuarkCode/simple/run0017</span>
[...]

<span class="out">
I have started your workflow, committed it to DAGMan, and updated its
state in the work database. A separate daemon was started to collect
information about the progress of the workflow. The job state will soon
be visible. Your workflow runs in base directory

cd /nfs/home/vdsuser-2/SIMPLEVDL/dags/QuarkCode/simple/run0017
</span>
<span class="out">$</span> <span class="in">cd /nfs/home/vdsuser-2/SIMPLEVDL/dags/QuarkCode/simple/run0017</span>
<span class="out">$</span> <span class="in">tail -f tailstatd.log</span>
[... output over time ...]
<span class="out">20060611T122510.413 [172]: DAGMan finished with exit code 0
20060611T122510.413 [173]: processed chunk of 1094 byte
# updated state in workman
20060611T122510.416 [173]: skipping plots
20060611T122510.416 [173]: finishing, exit with 0
20060611T122510.418 [173]: copied common log to /nfs/home/vdsuser-2/SIMPLEVDL/dags/QuarkCode/simple/run0017
</span>
<span class="out">$</span> cd /nfs/scratch01/train02/QuarkCode/vdsuser-2/simple/run0017/</span>

<span class="out">$</span> <span class="in">cp simplevdl-myuser9999.png ~/public_html/</span>
</pre>
<p>Now go to http://skynet-login.isi.edu/~vdsuser-99999/simplevdl-myuser9999.png
</li>

</ul>
</p>

<h3>Exercise 2.2: BIGGERVDL</h3>

<p>In this exercise, you will expand the VDL used in the SIMPLEVDL exercise
to include everything necessary to run the whole problem code (as seen in 
the PROBINTRO exercise).</p>

<p>We introduce several more transformations, and make derivations to 
process the raw input data into a graph.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cd ~/BIGGERVDL</span>
<span class="out">$</span> <span class="in">ls</span>
<span class="out">derivations.vdl	  transformations.vdl</span>
<span class="out">$</span> <span class="in">less transformations.vdl</span>

[...observe the new transformations...]
<span class="out">$</span> <span class="in">vdlc derivations.vdl transformations.vdl</span>
<span class="out">20060610T111452.577: [app] using label "derivations"
20060610T111452.578: [app] using output file "derivations.dax"</span>
<span class="out">$</span> <span class="in">vds-plan --pegasus -g QuarkCode --option pools=local --option output=local --option force derivations.dax --base ./dags</span>
[...]
<span class="out">I have concretized your abstract workflow. The workflow has been entered
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is

vds-run  /nfs/home/vdsuser-2/BIGGERVDL/dags/QuarkCode/derivations/run0004
</span>

<span class="out">$</span> <span class="in">vds-run  /nfs/home/vdsuser-2/BIGGERVDL/dags/QuarkCode/derivations/run0004</span>

<span class="out">
I have started your workflow, committed it to DAGMan, and updated its
state in the work database. A separate daemon was started to collect
information about the progress of the workflow. The job state will soon
be visible. Your workflow runs in base directory

cd /nfs/home/vdsuser-2/BIGGERVDL/dags/QuarkCode/derivations/run0004
</span>
<span class="out">$</span> <span class="in">cd /nfs/home/vdsuser-2/BIGGERVDL/dags/QuarkCode/derivations/run0004</span>
<span class="out">$</span> <span class="in">tail -f tailstatd.log</span>
[...wait...]
<span class="out">20060611T200900.583 [500]: DAGMan finished with exit code 0
20060611T200900.583 [501]: processed chunk of 1095 byte
# updated state in workman
20060611T200900.585 [501]: skipping plots
20060611T200900.585 [501]: finishing, exit with 0
20060611T200900.588 [501]: copied common log to /nfs/home/vdsuser-2/BIGGERVDL/dags/QuarkCode/derivations/run0004

$</span> <span class="in">cd /nfs/scratch01/train02/QuarkCode/vdsuser-2/derivations/run0004</span>

<span class="out">$</span> <span class="in">cp bigger.png ~/public_html/</span>
</pre>


<h3>Exercise 2.3: COMPOUNDVDL</h3>  

<p>In this exercise, you will combine the transforms of the previous
BIGGERVDL exercise into a single compound transform. When you have 
done this, you will be able to invoke the entire analysis process using 
a single derivation.
</p>

<p>The files for this exercise are in the <code>COMPOUNDVDL/</code>
directory.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cd COMPOUNDVDL/</span>
<span class="out">$</span> <span class="in">ls</span>
<span class="out">compound.vdl  transformations.vdl</span>
</pre>

<p>The most interesting file here is <code>compound.vdl</code>. This 
takes the various different transformations that we did separately in 
the previous exercise, and makes a single transform.</p>

<p>Then instead of specifying many derivations, we can specify just a 
single derivation to run the whole analysis.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">vdlc compound.vdl transformations.vdl</span>
<span class="out">20060531T121629.999: [app] using label "compound"
20060531T121630.011: [app] using output file "compound.dax"</span>
</pre>

<p>At this point, a file called <code>compound.dax</code> should have 
been generated. Look inside and see that it is very similar to

<code>biggervdl.dax</code>, the DAX file from the BIGGERVDL exercise.
</p>

<p>Now plan and run the workflow:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">vds-plan --pegasus -g QuarkCode --option pools=local --option output=local --option force compound.dax --base ./dags
</span>
[...] <span class="out">
2006.06.11 17:22:28.471 PDT: [INFO] Time taken to execute is 1.091 seconds

I have concretized your abstract workflow. The workflow has been entered
into the workflow database with a state of "planned". The next step is 
to start or execute your workflow. The invocation required is

vds-run  /nfs/home/vdsuser-2/COMPOUNDVDL/dags/QuarkCode/compound/run0002</span>
</pre>

<pre class="transcript">
<span class="out">$</span> <span class="in">vds-run /nfs/home/vdsuser-2/COMPOUNDVDL/dags/QuarkCode/compound/run0002</span>
<span class="out">I have started your workflow, committed it to DAGMan, and updated its
state in the work database. A separate daemon was started to collect
information about the progress of the workflow. The job state will soon
be visible. Your workflow runs in base directory

cd /nfs/home/vdsuser-2/COMPOUNDVDL/dags/QuarkCode/compound/run0002</span>

<span class="out">$</span> <span class="in">cd /nfs/home/vdsuser-2/COMPOUNDVDL/dags/QuarkCode/compound/run0002</span>
<span class="out">$</span> <span class="in">$ tail -f tailstatd.log </span>

[...wait...]
<span class="out">20060611T172658.603 [327]: DAGMan finished with exit code 0
20060611T172658.603 [328]: processed chunk of 1095 byte
# updated state in workman
20060611T172658.606 [328]: skipping plots
20060611T172658.606 [328]: finishing, exit with 0
20060611T172658.609 [328]: copied common log to /nfs/home/vdsuser-2/COMPOUNDVDL/dags/QuarkCode/compound/run0002</span>

<span class="out">$</span> <span class="in">cd /nfs/scratch01/train02/QuarkCode/vdsuser-2/compound/run0002/</span>
<span class="out">$</span> <span class="in">cp vdsuser-2-plot.png ~/public_html/</span>
</pre>

<p>Now you can observe your plot in a web browser, at 
http://skynet-login.isi.edu/vdsuser-9999/vdsuser-9999-plot.png</p>

<h2>Chapter 3: Running on the GRID using Pegasus</h2>

<p>In this chapter you will be introduced to running the abstract
workflow generated through Pegasus on the Teragrid. You will take a
variant of the compound dax generated in the previous exercises and
run it on the GRID.</p>

<p> All the exercises in this Chapter will be run from the $HOME/PEGASUS/
directory. All the files that are required reside in this sub
directory </p>  

<pre class="transcript">
<span class="out">$</span> <span class="in">cd $HOME/PEGASUS</span>
<span class="out">$</span> 

</span>
</pre>

<h3>Exercise 3.1 SETTING UP THE REPLICA CATALOG (RLS)</h3>
<p>In this exercise you will insert entries into the Replica Catalog.
The replica catalog that we use is GLOBUS RLS. </p>

<p> A Replica Catalog maintains the lfn to pfn mapping for the input
files of your workflow. Pegasus queries it to determine the locations
of the raw input data files required by the workflow. Additionally,
all the materialized data is registered into RLS for data reuse later
on.  </p>

<p>You can use the <code>rc-client</code> command to insert , query
and delete from the replica catalog.</p>

<p>The input data to be used for your workflow resides in the
$HOME/Data directory. We are going to insert entries into the replica
catalog that point to the files in this directory.</p>


<p>The instructors have provided:
 <ul> <li> A file replicas.in, the input data file
 for the rc-client that  contains the mappings that need to be
 populated in the RLS.</li> 
 </ul>
</p>
	  

<p>You will need to write some things yourself, by following the instructions
below:
<ul>
<!--
<li>Copy the template file to a file (replicas.in) in the PEGASUS subdirectory.</li>
<li>Do @USER@ substitution on the copied file. Replace @USER@ with your
username of the form vdsuser-X.</li>
-->
<li>Insert the entries in the RLS and verify if they are correctly inserted</li>

</ul>
</p>

<p>Instructions:
<ul>
<!--
  <li>copy the template file. Run the perl command ( <b>perl -pi -e
  's/\@USER\@/XXX/g' replicas.in </b>) substituting XXX  with your
  username.<br /> 

<pre class="transcript">
<span class="out">$</span> <span class="in">cp replicas.in.template replicas.in</span>
<span class="out">$</span> <span class="in">perl -pi -e 's/\@USER\@/vdsuser-4/g' replicas.in</span>
</pre>

  </li>
-->
  <li>Let us see what the file looks like.

<pre class="transcript">
<span class="out">$</span> <span class="in">cat replicas.in</span>
<span class="out">vdsuser-4_180.2004.1014.0.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2004.1014.0.raw pool="isi_skynet"</span> 
<span class="out">vdsuser-4_180.2004.1014.2.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2004.1014.2.raw pool="isi_skynet"</span> 

<span class="out">vdsuser-4_180.2004.1017.1.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2004.1017.1.raw pool="isi_skynet"</span> 
<span class="out">vdsuser-4_180.2005.0126.0.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2005.0126.0.raw pool="isi_skynet"</span> 
<span class="out">vdsuser-4_180.2005.0926.0.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2005.0926.0.raw pool="isi_skynet"</span> 
<span class="out">vdsuser-4_180.2005.0126.1.raw gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180.2005.0126.1.raw pool="isi_skynet"</span> 
<span class="out">vdsuser-4_180.geo gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180/180.geo pool="isi_skynet"</span> 
</pre>

</li>

<li> Now we are ready to run <code>rc-client</code> and populate the
data. Since each of you have uniques lfn's that are being registered,
all the 7 entries should be successfully registered.

<pre class="transcript">
<span class="out">$</span> <span class="in">rc-client --insert replicas.in</span>
<span class="out"></span>
<span class="out">#Successfully worked on	 : 7 lines</span>
<span class="out">#Worked on total number of : 7 lines.</span> 
</pre>
</li>

<li> Now the entries have been successfully inserted into the LRC. We
should confirm whether the mappings have propogated to the RLI or
not. Let us wait about 30 seconds and then query the replica catalog
for a particular lfn. <br/>
Run <br/>

<b>rc-client lookup @USER@_180.geo</b> where @USER@ is replaced by your username e.g vdsuser-4
</li>

<pre class="transcript">
<span class="out">$</span> <span class="in">rc-client lookup vdsuser-4_180.geo</span>
<span class="out">vdsuser-4_180.geo gsiftp://skynet-login.isi.edu/nfs/home/vdsuser-4/Data/180/vdsuser-4_180.geo pool="isi_skynet"</span>
</pre>
</li>
</ul>

<p> Congratulations!! You have the replica catalog setup correctly for
use. This is the catalog which you will tinker with most, while
running Pegasus.</p>


<h3>Exercise 3.2 SETTING UP THE SITE CATALOG AND THE TRANSFORMATION</h3>
<p>In this exercise you will setup your Site Catalog and the
Transformation Catalog.</p>

<p>The transformation catalog maintains information about where the
application code resides on the grid. In our case, it contains the
locations where the QuarkCode is installed on the various Teragrid
sites.</p>

<p> The site catalog contains information about the layout of your
grid where you want to run your workflows. In our case it contains
site information about the various Teragrid sites. For each site
information like workdirectories, jobmanagers to use, gridftp servers
to use and other site wide information like environment variables to
be set is maintained.
</p>

<p>
<ul>
<li>You can use the <code>vds-get-sites</code> command to generate a
site catalog and a transformation catalog to be used.

<pre class="transcript">
<span class="out">$</span> <span class="in">vds-get-sites -t $HOME/my.tc.data -s $HOME/my.sites.xml --default-rls rlsn://smarty.isi.edu</span>
<span class="out"># using default transformation mappings.</span>
<span class="out"># assembling information for grid "tg".</span>
<span class="out"># using URI dbi:SQLite2:dbname=/nfs/software/vds/default/contrib/OurGrids/tg.db</span>
<span class="out"># processing isi-skynet, 93 CPUs</span>
<span class="out"># processing tg_ncsa, 10 CPUs</span>
<span class="out"># processing tg_uc, 10 CPUs</span>

<span class="out"># processing tg_sdsc, 10 CPUs</span>
<span class="out"># adding myself as local site</span>
<span class="out"># dumping catalogs...</span>
<span class="out"># dumping SC into my.sites.xml...</span>
<span class="out"># backup /nfs/home/vdsuser-4/tc.data</span>
<span class="out"># -> /nfs/home/vdsuser-4/tc.data.0</span>
<span class="out"># dumping TC into /nfs/home/vdsuser-4/tc.data..</span>
</pre>
</li>
</ul>

</p>

<p>The instructors have provided:
 <ul> <li> A ready site catalog(sites.xml) in the
 $HOME directory.</li> 
 <li> A ready transformation catalog (tc.data) in the
 $HOME directory.
 </ul>
</p>
	  
<p>You can look at them to have an idea as to what they look like. But
for now we will move ahead and plan your workflow through Pegasus. We
need to get running on the GRID fast :). Time is short!!

</p>


<h3>Exercise 3.3 Running <code>vds-plan</code> to generate concrete workflow (condor
submit files) and <code>vds-run</code> to submit the workflow to a grid resource</h3> 

<p>In this exercise we are going to run <code>vds-plan</code>to generate a
concrete workflow from the abstract workflow (compound.dax). The
concrete workflow generated, are condor submit files that are
submitted to remote grid resources using Condor DAGMan and
CondorG. Then we will submit the workflow to the grid using
<code>vds-run</code> 
</p>

<p>Firstly, we are slightly going to change the compound.dax that was
generated in earlier steps. The changes which we will be doing are to
ensure that each of yours DAX refers to unique lfns. This way you
cannot clobber your fellow students runs.</p>

<p>The instructors have provided:
 <ul> <li> A dax (compound.dax) in the $HOME/PEGASUS
 directory. This dax has filenames with starting with your userid.
	  </li> 
 </ul>
</p>

	  
<p>You will need to write some things yourself, by following the instructions
below:
<ul>
<!--
<li>Copy the template file (compound.dax.template) to another file
(compound.dax) in the PEGASUS subdirectory.</li> 
<li>Do @USER@ substitution on the copied file. Replace @USER@ with your
username of the form vdsuser-X for e.g vdsuser-4</li>
-->
<li>Run vds-plan to generate the condor submit files out of the dax.</li>
</ul>
</p>

<p>Instructions:
<ul>
<!--
  <li>copy the template file. Run the perl command ( <b>perl -pi -e
  's/\@USER\@/XXX/g' compound.dax </b>) substituting XXX  with your
  username.<br /> 

<pre class="transcript">
<span class="out">$</span> <span class="in">cp compound.dax.template compound.dax</span>
<span class="out">$</span> <span class="in">perl -pi -e 's/\@USER\@/vdsuser-4/g' compound.dax</span>
</pre>

  </li>

-->
  <li>Let us see what the dax looks like.


<pre class="transcript">
<span class="out">$</span> <span class="in">more compound.dax</span>

</pre>

</li>

  <li> Let us run vds-plan on the compound.dax. Now with slightly more
  options than before. You need to replace @GRIDSITE@ with any one of the
  following <br/>
				isi_skynet<br/>
		 sdsc_tg<br/>
		 uc_tg<br/>

  <pre class="transcript">
  <span class="out">$</span>vds-plan --pegasus  --base ./dags --option pools=@GRIDSITE@ --option output=local --option force compound.dax </span>
  </pre>

  <p>
  The above command says that we need to plan the compound.dax using
  pegasus on the grid site @GRIDSITE@. The output data needs to be
  transferred back to the local submit host (i.e
  skynet-login.isi.edu). In addition we want the condor submit files
  to be generated in a directory structure whose base is ./dags/grid.
  </p>

 
  Here is the output of vds-plan running on site isi_skynet.
  <pre class="transcript">

  <span class="out">$</span>vds-plan --pegasus  --base ./dags --option pools=isi_skynet --option output=local --option force  compound.dax </span>
  <span class="out"># Warning: relative base directory ./dags</span>
  <span class="out">2006.06.09 20:42:50.707 PDT: [INFO] Parsing of the DAX</span>
  <span class="out">2006.06.09 20:42:51.217 PDT: [INFO] Parsing of the DAX (completed)</span>
  <span class="out">2006.06.09 20:42:51.238 PDT: [INFO] Parsing the site catalog</span>
  <span class="out">2006.06.09 20:42:51.316 PDT: [INFO] Parsing the site catalog (completed)</span>

  <span class="out">2006.06.09 20:42:51.687 PDT: [INFO] Querying Replica Catalog</span>
  <span class="out">2006.06.09 20:42:51.697 PDT: [INFO] Querying Replica Catalog (completed)</span>
  <span class="out">2006.06.09 20:42:51.697 PDT: [INFO] Doing site  selection</span>
  <span class="out">[..]</span>
  <span class="out">2006.06.09 20:42:51.7I have concretized your  abstract workflow. The workflow has been entered</span>
  <span class="out">into the workflow database with a state of
  "planned". The next step is to start or execute your workflow. The
  invocation required is </span>

  <span class="out">vds-run  /nfs/home/vdsuser-4/tutorial/master/PEGASUS/dags/ivdgl1/QuarkCode/run0001</span>
  <span class="out">
  </pre>

  </li>

  <li> Now run vds-run as mentioned in the output of vds-plan on the compound.dax.

  <pre class="transcript">
  <span class="out">$</span>vds-run /nfs/home/@USER@/tutorial/master/PEGASUS/dags/ivdgl1/QuarkCode/runXXX</span>

  <span class="out">[..]</span>
  <span class="out"># parsing properties in /nfs/home/vdsuser-4/.wfrc...</span>
  <span class="out"># slurped /nfs/home/vdsuser-4/tutorial/master/PEGASUS/dags/grid/ivdgl1/QuarkCode/run0001/braindump.txt</span>
  <span class="out"></span>
  <span class="out">I have started your workflow, committed it to DAGMan, and updated its</span>
  <span class="out">state in the work database. A separate daemon was started to collect</span>

  <span class="out">information about the progress of the workflow. The job state will soon</span>
  <span class="out">be visible. Your workflow runs in base directory</span>
  <span class="out"></span>
  <span class="out">cd /nfs/home/vdsuser-4/tutorial/master/PEGASUS/dags/ivdgl1/QuarkCode/run0001</span>
  </pre>

  The above command submits the workflow to Condor
  DAGMAN/CondorG. After submittting it starts a monitoring daemon
  tailstatd that parses the condor log files to update the status of
  the jobs and push it in a work database.
  </li>

</ul>


 <h3>Exercise 3.4 Tracking the progress of the workflow and debugging
 the workflows.</h3>

<p>In this exercise we are going to list ways to track your workflow,
and give some debugging hints when something goes wrong.
</p>

<p>We will change into the directory, that was mentioned by the
vds-run command.</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cd /nfs/home/@USER@/tutorial/master/PEGASUS/dags/ivdgl1/QuarkCode/run000X</span>

</pre>

<p>In this directory you will see a whole lot of files. That should
not scare you. Unless things go wrong, you need to look at just a very
few number of files to track the progress of the workflow</p>

<p>At the first go you should be concerned with only one file</p>
<ul>
 <li>
 jobstate.log
 </p> This is the output file of the monitoring daemon that is parsing
 all the condor log files to determine the status of the
 jobs. It logs the events seen by Condor into a more readable form for
 us.
 </p>

 <pre class="transcript">

 <span class="out">$</span><span class="in">more jobstate.log</span>
 <span class="out">1149911774 INTERNAL *** TAILSTATD_STARTED ***</span>
 <span class="out">1149911773 INTERNAL *** DAGMAN_STARTED ***</span>
 <span class="out">1149911773 QuarkCode_0_pegasus_concat UN_READY - - -</span>
 <span class="out">1149911773 Plot_ID000008 UN_READY - - -</span>
 <span class="out">1149911773 Lifetime_ID000005 UN_READY - - -</span>

 <span class="out">1149911773 WireDelay_ID000002 UN_READY - - -</span>
 <span class="out">1149911773 rc_tx_WireDelay_ID000002_0 UN_READY - - -</span>
 <span class="out">[..]</span>
 </pre>

 <p>
 In the starting of the jobstate.log, when the workflow has just
 started running you will see a lot of entries with status
 UN_READY. That designates that DAGMan has just parsed in the .dag
 file and has not started working on any job as yet. Initially all the
 jobs in the workflow are listed as UN_READY
 </p>


 <p> After sometime you will see entries in jobstate.log, that shows a
 job is being executed etc </p>

 <pre class="transcript">
 <span class="out">1149911774 QuarkCode_0_isi_skynet_cdir SUBMIT 264.0 isi_skynet -</span>
 <span class="out">1149911789 QuarkCode_0_isi_skynet_cdir EXECUTE 264.0 isi_skynet -</span>
 <span class="out">1149911789 QuarkCode_0_isi_skynet_cdir GLOBUS_SUBMIT 264.0 isi_skynet -</span>

 <span class="out">1149911789 QuarkCode_0_isi_skynet_cdir GRID_SUBMIT 264.0 isi_skynet -</span>
 <span class="out">1149911794 QuarkCode_0_isi_skynet_cdir JOB_TERMINATED 264.0 isi_skynet -</span>
 <span class="out">1149911794 QuarkCode_0_isi_skynet_cdir POST_SCRIPT_STARTED - isi_skynet -</span>
 <span class="out">1149911799 QuarkCode_0_isi_skynet_cdir POST_SCRIPT_TERMINATED 264.0 isi_skynet -</span>
 <span class="out">1149911799 QuarkCode_0_isi_skynet_cdir POST_SCRIPT_SUCCESS - isi_skynet -</span>
 </pre>

 <p>
 The above shows the create dir job being submitted and then executed
 on the grid. In addition it lists that job is being run on the grid
 site <code>isi_skynet</code> The various states of the job while it
 goes through submission to execution to postprocessing are in UPPERCASE.
 <p>

 </li>
</ul>

<br/>

<p> To see which job of yours is being executed, you can also use
condor_q. By default condor_q list all the jobs on the submit
host. However, we are just interested in our own respective jobs. So
we will use some classad magic to narrow the results.</p>

<p> Run command <b> condor_q -const '(Owner == "@USUER@")'</b> with
@USER@ replaced by the username.

 <pre class="transcript">
 <span class="out">$</span> condor_q -const '(Owner == "vdsuser-4")'</span>
 <span class="out">252.0	vdsuser-4		 6/9  17:04	0+00:11:21 R  0	9.8  condor_dagman -f -</span>
 <span class="out">260.0	vdsuser-4		 6/9  17:15	0+00:00:00 I  0	9.8  kickstart -n Quark</span>

 </pre>

 <p> The above indicates that currently we have two jobs running</p>
 <br/>

 <p>
 Keep a lookout on the condor_q to track whether a workflow is running
 or not. If you do not see any of your job in the condor_q for
 sometime (say 30 seconds), we know the workflow has finished. We need
 to wait, as there might be delay in CondorDAGMAN releasing the next
 job into the queue after a job has finished successfully.</p>


 <p>If condor_q is empty, then either your workflow has </br>
	  - successfully completed</br>
	  - stopped midway due to non recoverable error</br>
 </p>


 <ul>
 <li> Successfully Completed
 <p> Let us again look at the jobstate.log. This time we need to look
 at the last few lines of jobstate.log</p>

 <pre class="transcript">
 <span class="out">$</span> tail jobstate.log</span>
 <span class="out">1149894263 new_rc_tx_Plot_ID000008_0 POST_SCRIPT_TERMINATED 248.0 local -</span>
 <span class="out">1149894263 new_rc_tx_Plot_ID000008_0 POST_SCRIPT_SUCCESS - local -</span>
 <span class="out">1149894268 new_rc_register_Plot_ID000008 SUBMIT 249.0 local -</span>

 <span class="out">1149894273 new_rc_register_Plot_ID000008 EXECUTE 249.0 local -</span>
 <span class="out">1149894273 new_rc_register_Plot_ID000008 JOB_TERMINATED 249.0 local -</span>
 <span class="out">1149894273 INTERNAL *** DAGMAN_FINISHED ***</span>
 <span class="out">1149894274 INTERNAL *** TAILSTATD_FINISHED 0 ***</span>
 </pre>

 <p>

 Looking at the last two lines we see that DAGMAN finshed, and
 tailstatd finished successfully with a status 0. This means workflow
 ran successfully. Congratulations you ran your workflow on the grid successfully.
 </p>

 <p>
 The output plot generated by the workflow is <code>.png file</code> that resides
 in the directory <b>/nfs/storage01/trainXX/vdsuser-Y_plot.png</b> where
 XX is the 2 digit version of Y.  for e.g if user is vdsuser-4 then
 the path will be /nfs/storage01/train04/vdsuser-4_plot.png
 </p>
 <p>To view the plot, you can copy <code>vdsuser-Y_plot.png</code> to your
skynet webspace, and view it in your web browser:</p>

<pre class="transcript">
<span class="out">$</span> <span class="in">cp /nfs/storage01/train04/vdsuser-4_plot.png ~/public_html/newplot.png</span>
<span class="out">$</span> 
</span>
</pre>
<p>Point your web browser to:
http://skynet-login.isi.edu/~@USER@/newplot.png where @USER@ is your
userid</p> 
</li>

<li> Unsuccessfully Completed (Workflow execution stopped midway)
 <p> Let us again look at the jobstate.log. Again we need to look
 at the last few lines of jobstate.log</p>

 <pre class="transcript">
 <span class="out">$</span> tail jobstate.log</span>
 <span class="out">1149912756 Frequency_ID000006 SUBMIT 277.0 isi_skynet -</span>
 <span class="out">1149912766 Frequency_ID000006 GLOBUS_SUBMIT 277.0 isi_skynet -</span>
 <span class="out">1149912766 Frequency_ID000006 GRID_SUBMIT 277.0 isi_skynet -</span>

 <span class="out">1149912861 Frequency_ID000006 EXECUTE 277.0 isi_skynet -</span>
 <span class="out">1149912901 Frequency_ID000006 JOB_TERMINATED 277.0 isi_skynet -</span>
 <span class="out">1149912901 Frequency_ID000006 POST_SCRIPT_STARTED - isi_skynet -</span>
 <span class="out">1149912906 Frequency_ID000006 POST_SCRIPT_TERMINATED 277.0 isi_skynet -</span>
 <span class="out">1149912906 Frequency_ID000006 POST_SCRIPT_FAILURE 1 isi_skynet -</span>
 <span class="out">1149912906 INTERNAL *** DAGMAN_FINISHED ***</span>

 <span class="out">1149912911 INTERNAL *** TAILSTATD_FINISHED 1 ***</span>
 </pre>

  <p>
 Looking at the last two lines we see that DAGMAN finshed, and
 tailstatd finished unsuccessfully with a status 1.
 </p>

 <p>
 We can easily determine which job failed. It is Frequency_ID000006 in
 this case.<br/>

 To determine the reason for failure we need to look at
 it's kickstart output file which is $JOBNAME.out i.e
 <a href="http://skynet-login.isi.edu/~vdsuser-4/Frequency_ID000006.out">Frequency_ID000006.out</a>
 <br/>
 Looking at it's output we see it failed because of a perl library not
 installed on isi_skynet!.
 </p>

 <p>
  We correct that and submit the workflow again.
  </p>

 </li>

 </ul>

<pre>
The End
</pre>
</body>
</html>
-- Main.MichaelWilde - 13 Jun 2006

