%META:TOPICINFO{author="BalamuruganDesinghu" date="1437689089" format="1.1" reprev="1.3" version="1.3"}%
%META:TOPICPARENT{name="UserSchool15Materials"}%
<style type="text/css">
pre em { font-style: normal; background-color: yellow; }
pre strong { font-style: normal; font-weight: bold; color: #008; }
</style>


---+ Thursday  Exercise 2: Spliting  database 
In this example, we touch the basics of handling large input data using blast sequence similarity search. Blast search against a large sequence data base is a challenging task. Because analysis of large database may require large memory. So we split the reference database into chunks and perform the blast search. The present idea of splitting the database for blast analysis may be applied to several data analytics problems. 

---++ Data chunks and workflow
The database is a large file,  ~20 GB in size. It is split into 18 chunks. You could check the files
<pre class="screen">
%UCL_PROMPT_SHORT% <strong>ls /stash/public/blast/database/nt.5-30-2014/*.gz   </strong>
</pre> 
<pre class="file">
nt.00.tar.gz, nt.01.tar.gz, nt.02.tar.gz ... # the split data chunks in compressed format
</pre>

Each chunk is less than 1 GB in is size. We want to perform analysis on these smaller chunks.  Our task is to create 18 independent jobs.  Tell condor to transfer the data chunk from the public directory to worker machine.  In each job wrapper script, uncompress the data chunk before the job execution.  For convenience, we utilize bash script to generate job files. First we work out  an example and then you hack this workflow and run a query on your own. 

---++ Get and explore example files
Create a BlastSplitSearch directory on the OSG submit node, and download the required files.
<pre class="screen">
%UCL_PROMPT_SHORT% <strong>wget http://stash.osgconnect.net/+dbala/Storage/MultiInputsMultiJobs-blastn/BlastSplitCT.tar.gz  </strong>
%UCL_PROMPT_SHORT% <strong>tar -xzf  BlastSplitCT.tar.gz  </strong>
</pre>

<pre class="screen">
%UCL_PROMPT_SHORT% <strong>cd  BlastSplitCT  </strong>
%UCL_PROMPT_SHORT% <strong> ls  </strong>
</pre>

<pre class="file">
generate_job_files.bash   # Helper script  to set up jobs
StartUpFiles/  # Contains input files and scripts for the job
</pre>

Let us see what is inside the directory ==StartUpFiles==
<pre class="screen">
%UCL_PROMPT_SHORT% <strong>ls StartUpFiles/  </strong>
</pre>

<pre class="file">
HLA.txt    # query input file
blast_exe.bash    # Job wrapper script
blast_gen.bash    #  Generates job submit file for a list of data chunks using the file "blast.submit.template" as a template
blast.submit.template    # The template file used to create job submission files
</pre>

---++ Generate job files and submit

We utilize  the helper script  ==generate_job_files.bash== to set up the job. 
<pre class="screen">
%UCL_PROMPT_SHORT% <strong>  chmod +x generate_job_files.bash  </strong>
%UCL_PROMPT_SHORT% <strong> ./generate_job_files.bash  </strong>
</pre>
would produce the following output on the screen
<pre>
..........
To submit all condor jobs 
cd JobDirectory 
./jobsubmit.bash 
..........
</pre>
and creates a directory ==JobDirectory== for job submissions.  Inside ==JobDirectory==, we see the following list of files
<pre class="file">
blast_00.submit, blast_01.submit, ...   # 18 job script files for 18 data chunks
HLA.txt    # query input file
blast_exe.bash    # Job wrapper script
jobsubmit.bash    # script to submit all jobs
Log/    # directory to store the standard output, log and error files
</pre>

To submit the jobs 
<pre class="screen">
%UCL_PROMPT_SHORT% <strong> ./jobsubmit.bash  </strong>
</pre>
OR
<pre class="screen">
%UCL_PROMPT_SHORT% <strong> condor_submit blast*.submit  </strong>
</pre>








-- Main.BalamuruganDesinghu - 23 Jul 2015
