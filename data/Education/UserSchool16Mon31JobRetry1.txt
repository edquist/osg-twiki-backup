%META:TOPICINFO{author="GregThain" date="1469116432" format="1.1" version="1.2"}%
<style type="text/css">
pre em { font-style: normal; background-color: yellow; }
pre strong { font-style: normal; font-weight: bold; color: #008; }
</style>

---+ Monday Exercise 3.1: A job that needs retries

The goal of this exercise is to demonstrate running a job that intermittently fails, and could benefit from having HTCondor automatically retry it.

This first part of the exercise should take only a few minutes, and is designed to setup the next exercises.


---++ An ill-behaved job

Let's assume that a colleague has shared with you this simple python program, which occasionally fails.  In the real world, we'd probably just fix
the program, but this example simulates problems that may happen unexpectedly, because of incorrectly configured machines, or other surprises.

Make a new directory for this example, cd into it, then copy and paste this example into a file named "murphy.py".  In python, indentation matters, so make sure to preserve the indentation.

<pre class="file">
#!/usr/bin/env python

# murphy.py simulate a real program with real problems
import random
import time

# Create a random number seeded by system entropy
r = random.SystemRandom()

# One time in three, simulate a runtime error
if (r.randint(0,2) == 0):
	# intentionally print no output
	exit(15)
else:
	time.sleep(3)
	print "All work done correctly"

# By convention, zero exit code means success
exit(0)

</pre>

Even if you aren't a Python expert, you should be able to guess what this program does.

In the same directory as "murphy.py", create an HTCondor submit file which submits a cluster
of 20 instances of this job.  The condor submit file below does this.

<pre class="file">
universe = vanilla
executable = murphy.py
arguments = $(PROCESS)


should_transfer_files = yes
when_to_transfer_output = on_exit

output = out.$(CLUSTER).$(PROCESS)
#error  = err.$(CLUSTER).$(PROCESS)
log    = log

request_memory = 128M
request_disk   = 128M
request_cpus   = 1
queue 20

</pre>

Use this submit file to submit 20 instances of this job.  Then, wait for all the jobs in the cluster to finish. What output do you expect?  What output did you get?  If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute.  The following command will show the ExitCode for a given cluster of jobs:

<pre class="screen">

%UCL_PROMPT_SHORT% <strong>condor_history</strong> clusterID -af ClusterId ExitCode
</pre>

That is, for cluster 84, the command would be
<pre class="screen">
%UCL_PROMPT_SHORT% <strong>condor_history</strong> 84 -af ClusterId ExitCode
</pre>

How many of these jobs succeeded?  How many failed?

---++ Retrying the ill-behaved job

Now let's see if we can solve this problem.  Re-running any failed instance will have a good shot at a correct run, so that's what we'll try.  By only changing the submit file, try to tell HTCondor to re-run any job whose ExitCode attribute is non-zero.  After making this change to the submit file, run condor_submit again, and wait for all the jobs to exit.  Did your change work?  

After the jobs have all exited, one way to see what happened is to look at the log file, "log" in this case.  Did any jobs need to be restarted?  Another way to see how many restarts there were is to look at the NumJobStarts attribute in a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier.  Does the number of retries seem correct?  For those jobs which did need to be retried, what is their ExitCode -- and what about the ExitCode from earlier execution attempts?

---++ A (too) Long Running job

Sometime, an ill-behaved job will get stuck in a loop, and run indefinitely, instead of exiting with a non-zero exit code.  We can modify our python program to do this with the following file.  Cut and paste this to a new file, called "murphy2.py".  Again, you should be able to figure out what this new program does.

<pre class="file">
#!/usr/bin/env python

# murphy.py simulate a real program with real problems
import random
import time

# Create a random number seeded by system entropy
r = random.SystemRandom()

# One time in three, simulate an infinite loop
if (r.randint(0,2) == 0):
        # intentionally print no output
        time.sleep(3600)
        exit(15)
else:
        time.sleep(3)
        print "All work done correctly"

# By convention, zero exit code means success
exit(0)
</pre>

Now edit your submit file to change the executable command from murphy.py to murphy2.py.  You don't need to submit these new jobs just yet, but if you do, you may want to condor_rm the whole cluster, rather than waiting up to an hour for some of these jobs to finish.

For this example, let's instruct HTCondor to automatically remove any jobs that run for more than one minute.  This should take just one line change to your existing condor_submit file.  Instead of editing your existing (correct!) file in place, try making a copy, and working on the copy, so that you have the original saved.

condor_submit your corrected submit file.  Do the long running jobs get removed?  What does condor_history show for the cluster after all jobs are done?

---++ Bonus exercise

If you have time, edit your submit file so that instead of removing long running jobs, have HTCondor automatically put the long-running job on hold, and then automatically release it.


-- Main.GregThain - 19 Jul 2016
