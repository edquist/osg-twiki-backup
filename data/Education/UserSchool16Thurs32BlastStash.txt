%META:TOPICINFO{author="LaurenMichael" date="1469397943" format="1.1" reprev="1.7" version="1.7"}%
%META:TOPICPARENT{name="UserSchool16Materials"}%
---+ Using !StashCache for Large Shared Data

This exercise will use a [[http://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastHome][BLAST]] workflow to demonstrate the functionality of !StashCache for transferring input files to jobs on OSG.

Because our individual blast jobs from [[UserSchool16Thu31BlastProxy][Exercise 3.1]] would take a bit longer with a larger database (too long for an workable exercise!), we'll imagine for this exercise that our =pdbaa_files.tar.gz= file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from [[UserSchool15Thu32BlastProxy][fExercise 3.2]], but instead of using the web proxy for the =pdbaa= database, we will place it in !StashCache via the OSG Connect server, and demonstrate that you can then submit !StashCache-dependent jobs from any OSG submit point, like =osg-learn.chtc.wisc.edu=.

!StashCache is a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed.

%ATTACHURL%/Screen_Shot_2016-07-06_at_4.15.32_PM.png

There are two methods of pulling data from !StashCache, within jobs.  We are in the middle of a transition from one to another.  They are:

   1. *stashcp* (old): A command line program to copy files from !StashCache with ==cp== like syntax.
   2. *StashCache-over-CVMFS* (new): A much more intuitive and fault tolerant method for accessing !StashCache files.  But only available on a few resources, for now.

In the below tutorials, we will use *stashcp*, but you can also try out *StashCache-over-CVMFS* in the Bonus exercise.

---++ Setup

   1. Make sure you're logged in to =osg-learn.chtc.wisc.edu=
   1. Transfer the following files from [[UserSchool16Thu31BlastProxy][Exercise 3.1]] to a new directory called =thur-data-3.2=: =blast_wrapper.sh=, =mouse_rna.fa.1=, =mouse_rna.fa.2=, =mouse_rna.fa.3=, and the most recent submit file.

---++ Place the Database in !StashCache

---+++ Copy to your =public= space on OSG Connect
!StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your blast database into this public directory. If you remember the "public" directory in your home directory on the OSG Connect server =login.osgconnect.net=, it's this location where you will place files that need to end up in the !StashCache data origin. So, from =osg-learn.chtc.wisc.edu= you can place the pdbaa_files.tar.gz from the previous exercise into !StashCache using a command like the following, which refers to the current location of the =pdbaa_file.tar.gz= file on =osg-learn.chtc.wisc.edu=, and to your =username= and =public= location on =login.osgconnect.net=:

<pre class="screen">
%UCL_PROMPT_SHORT% <strong>scp ../thur-data-blast/pdbaa_files.tar.gz %RED%username%ENDCOLOR%@login.osgconnect.net:~/public/</strong>
</pre>

As the =public= directory name indicates *your files placed in the =public= directory will be accessible to anyone's jobs if they know how to use =stashcp=*, though no one else will be able to edit the files, since only you can _place_ or _change_ files in your =public= space. For your own work in the future, make sure that you never put any sensitive data in such locations. 

---+++ Check the file on OSG Connect

Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node:

<pre class="screen">
%UCL_PROMPT_SHORT% <strong>ssh %RED%username%ENDCOLOR%@login.osgconnect.net</strong>
%UCL_PROMPT_SHORT% <strong>ls public</strong>
</pre>

Now, load the =stashcp= module, which will allow you to test a copy of the file from !StashCache into your home directory on =login.osgconnect.net=:

<pre class="screen">
%UCL_PROMPT_SHORT% <strong>module load stashcp</strong>
%UCL_PROMPT_SHORT% <strong>stashcp /user/%RED%username%ENDCOLOR%/public/pdbaa_files.tar.gz ./</strong>
</pre>

You should now see the =pdbaa_files.tar.gz= file in your current directory. Notice that we had to include the ==/user== and ==username== in the file path for =stashcp=, which make sure you're copying from *your* =public= space.

---++ Modify the submit file and wrapper script

Return to your =thur-data-stash= directory on =osg-learn.chtc.wisc.edu= where you will modify the files as described below:

1. At the top of the wrapper script, add the above two lines to load the =stashcp= module and to copy the =pdbaa_files.tar.gz= file into the current directory of the job.

2. Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing =rm= command, if you're confident) to make sure the =pdbaa_files.tar.gz= file is also deleted and not copied back as perceived output.

<pre class="file">
rm pdba_files.tar.gz
</pre>

3. Delete the =http= address from =transfer_input_files= in the submit file.

4. Add the following line to the submit file, somewhere before the word =queue= to make sure your job lands on a server with StashCache.

<pre class="file">
+WantsStashCache = true
</pre>

5. Last, create a new =list.txt= file, to list only the =mouse_rna.fa.*= files that you copied into the =thur-data-stash= directory.

---++ Submit the Job

In the previous exercise, you put each file in the submit file and used the HTTP plugin built into HTCondor in order to transfer files to remote worker nodes.  In this exercise, we will use the wrapper script that you wrote above to download each of the files.




<!-- Saving for now, to be used in the Bonus exercise, later

---++ Using !StashCache-over-CVMFS

Next, we will show the use of !StashCache-over-CVMFS.  This is an abbreviated tutorial.

   1. First log into the OSG Connect submit host (login.osgconnect.net), download the tutorial files using the *tutorial* command, and cd into the newly created directory:
   <pre class="screen">
$ tutorial stashcache-blast
$ cd tutorial-stashcache-blast
</pre>
   1. The tutorial-stashcache-blast directory contains a number of files, described below:
      * HTCondor submit script: *blast.submit*
      * Job wrapper script: *blast_wrapper.sh*
      * Query files: *query_0.fa  query_1.fa*
   2. In addition to these files, the following input files are needed for the jobs:
      * database file: *nt.fa*
      * database index files: *nt.fa.nhr  nt.fa.nin  nt.fa.nsq*

   These files are currently being stored in ==/cvmfs/stash.osgstorage.org/user/eharstad/public/blast_database/==.

---++ The CVMFS Submit File

First, let's take a look at the HTCondor job submission script:

<pre class="file">
universe = vanilla

executable = blast_wrapper.sh
arguments  = blastn -db /cvmfs/stash.osgstorage.org/user/eharstad/public/blast_database/nt.fa -query $(queryfile)
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = $(queryfile)

+WantsCvmfsStash = true
requirements = (GLIDEIN_ResourceName == "MWT2" || GLIDEIN_ResourceName == "Nebraska" || GLIDEIN_ResourceName ==  "Sandhills")
	
output = job.out.$(Cluster).$(Process)
error = job.err.$(Cluster).$(Process)
log = job.log.$(Cluster).$(Process)

# For each file matching query*.fa, submit a job
queue queryfile matching query*.fa
</pre>

The executable for this job is a wrapper script, `blast_wrapper.sh`, that takes as arguments the blast command that we want to run on the compute host.  We specify which query file we want transferred (using HTCondor) to each job site with the *transfer_input_files* command.

Note the one additional line that is required in the submit script of any job that uses !StashCache:

<pre class="file">
+WantsCvmfsStash = true
</pre>

Finally, since there are multiple query files, we submit them with the command `queue queryfile matching query*.fa` command.  Because we have used the $(queryfile) macro in the name of the query input files, only one query file will be transferred to each job.

---++ The Wrapper Script

Now, let's take a look at the job wrapper script which is the job's executable:

<pre class="file">
#!/bin/bash
# Load the blast module
module load blast

"$@"
</pre>

The wrapper script loads the blast modules so that it can access the Blast software on the compute host.

You are now ready to submit the jobs:

<pre class="screen">
$ condor_submit blast.submit
</pre>

 Each job should run for approximately 3-5 minutes.  You can monitor the jobs with the condor_q command:

<pre class="screen">
$ condor_q <userid>
</pre>

-->

%META:FILEATTACHMENT{name="Screen_Shot_2016-07-06_at_4.15.32_PM.png" attachment="Screen_Shot_2016-07-06_at_4.15.32_PM.png" attr="" comment="" date="1467839812" path="Screen Shot 2016-07-06 at 4.15.32 PM.png" size="375951" stream="Screen Shot 2016-07-06 at 4.15.32 PM.png" tmpFilename="/usr/tmp/CGItemp55860" user="DerekWeitzel" version="1"}%
%META:TOPICMOVED{by="LaurenMichael" date="1468861232" from="Education.UserSchool16Thurs22BlastStash" to="Education.UserSchool16Thurs32BlastStash"}%
