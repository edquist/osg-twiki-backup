%META:TOPICINFO{author="LaurenMichael" date="1438055422" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="UserSchool15Materials"}%
<style type="text/css">
pre em { font-style: normal; background-color: yellow; }
pre strong { font-style: normal; font-weight: bold; color: #008; }
</style>

---+ Monday Exercise 1.2: Plan Joe's Workflow

Your goal is to plan out Joe’s workflow based upon small-scale test jobs (this first, 30-minute session) and, later, to write a full-scale DAG to actually run his workflow in production (the second, 90-minute session after the next lecture). Please make sure to first read Exercise 1.1, which has important background information about Joe's intended work and how he has submitted jobs so far.

---+++Your goal, by the end of exercises 1.2 and 1.3, is to put Joe's work for all three traits into a single DAG by doing the following:

* A) Optimize the submit files for the _permutation_ jobs of each trait,* such that each trait takes advantage of high-throughput parallelization with some number of job _processes_ that each calculate a portion of a total of 100,000 permutations (versus the 10,000 permutations per trait that Joe has been working with previously).

* B) Optimize submit file values for "request_memory" and "request_cpus"* for each _permutation_ and _QTL mapping_ step, for each trait.

* C) Create a single DAG file, with _permutation_ and _QTL mapping_ jobs for each of the three traits,* including PRE and/or POST scripts for the =tar= scripts that need to be run.

*NOTE: You will not need to modify any of Joe’s input files or programs, and will only need to modify the submit files as directly instructed. It is advisable that you split up the work within your pair or group in order to be time-efficient.*


---++ Steps to Take:

1) Based upon what you [[https://www.opensciencegrid.org/bin/view/Education/UserSchool15Fri11LearnJoe'sWork][learned from Joe]], draw the _general workflow_ that you would make for Joe (on paper), keeping in mind that there are 3 traits for which the _permutation_ and _QTL mapping_ steps need to be completed. The tar steps will need to be PRE or POST scripts (you decide which is best). Think about what Joe's intended workflow means for the shape of the DAG, PARENT-CHILD dependencies for JOBs in the DAG, and the fact that the _permutation_ step could be broken up into multiple processes of fewer total permutations, each.

2) Test the HTC optimization of the _permutation_ step:

- You will need to determine the number of permutations that could be batched in a single job process, if each process needs to run in ~30 minutes for good HTC scaling. To do this, first modify one of the permutation submit files to “queue” 10 processes (so that you can average time between the 10 test processes), add "request_cpus = 1" according to Joe's indication, and add reasonable first guesses for "request_memory" and "request_disk" (say, 1 GB?). Then, copy this submit file so that you can change the last argument (the number of permutations) from "10000" to "10", "100", or "1000". For time's sake, you should test all three of these variations at the same time! 

- After each set of _permutation_ tests finishes, you’ll need to use =tarit.sh= (with the correct argument!) before running the test jobs for the QTL step.

3) Test each of the _QTL mapping_ jobs by submitting the three submit files after simply adding lines for "request_memory", "request_disk", and "request_cpus". The resource needs (RAM and disk space) and execution time of each _QTL mapping_ job will likely increase with the total number of permutations from the previous _permutation_ step. You'll test optimized _permutation_ and _QTL mapping_ steps in the next exercise.

4) Optimizing the _permutation_ step: 
Calculate the number of permutations that should be run _per job process_, such that the runtimes per process will be about 30 minutes (not exact, but closer to 30 than to 5 or 60). You can then calculate the number of processes that should be queued such that 100,000 permutations are calculated for each trait, so you want _job processes_ X _permutations_ to equal 100,000 total permutations for each of the three phenotype traits.
*Hint: You can use the "condor_history" feature (similar to condor_q, but for completed jobs) to easily view the "RUNTIME" for each job in a "cluster", by typing "=condor_history _cluster_=".

5) Optimizing memory and disk:
Make sure to examine the log files of your _permutation_ and _QTL_ test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.


-- Main.LaurenMichael - 26 Jul 2015

%META:TOPICMOVED{by="LaurenMichael" date="1438007785" from="Education.UserSchool15Fri11PlanFlow" to="Education.UserSchool15Fri12PlanFlow"}%
