%META:TOPICINFO{author="ForrestChristian" date="1174856774" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="MidwestSyllabus"}%
%LINKCSS%

---+!! Tutorial: Data Mining
%TOC%
%STARTINCLUDE%


<a name="Getting_set_up"></a>
---++ Getting set up

Check that you have a valid proxy.

<pre class="screen">
%LOGINHOST%$ <userinput>grid-proxy-init</userinput>
Your identity: %CERTSUBJECT%
Enter GRID pass phrase for this identity:
Creating proxy ...................................... Done
Your proxy is valid until: Fri Mar 23 00:20:40 2007
</pre>

Now, make a working directory for this exercise. For the rest of this exercise, all your work should be done in there.

<pre class="screen">
%LOGINHOST%$ <userinput>mkdir dmex</userinput>
%LOGINHOST%$ <userinput>cd dmex</userinput>
</pre>



<a name="Patterns 1 and 2"></a> 
---++ The Basic Pattern
Recall the two basic patterns from the lecture that we will deal with in this tutorial:

   * _Pattern 1_ divides the data into several different segments, runs the same analysis on each to produce a report or model, and gathers the results.  
   * _Pattern 2_ divides the parameter space into several different parameter ranges, runs the same analysis on each to produce a report or model, and gathers the results.  

(Of course, the two Patterns can be combined.)

To support these patterns, make the following directories on _each_ of the nodes in the cluster you will be using.  These exercises assume that they exist. 

   * data
   * analysis 
   * models
   * reports 
   * collect


Here is an example of how to create the directory data on workshop4. 

<pre class="screen">
%LOGINHOST%$ <userinput>globus-job-run workshop4.lac.uic.edu "/bin/mkdir"  &#92;
  "/home/%LOGINNAME%/data" </userinput>
Submitting job...Done.
Job ID: uuid:63bf25c6-d895-11db-a43b-00e081749872
Termination time: 03/23/2007 16:50 GMT
Current job state: Active
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
</pre>

Create the other directories in a similar fashion.


<a name="Using R to Compute k-Means Clusters"></a>
---++ Using R to Compute k-means Clusters

Before using the grid for you computation, make sure you can do the computation first on a single machine that you can log in to, using R, but not Globus. 

<pre class="screen">
%LOGINHOST%$ <userinput>R</userinput>
> <userinput>library(pmml)</userinput>
Loading required package: XML
> <userinput>data(iris)</userinput>
> <userinput>a.data &lt;- iris </userinput>
> <userinput>summary(a.data)</userinput>
 Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50  
> <userinput> a.data.1 &lt;- a.data[, c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")]</userinput>
> <userinput> m1 &lt;- kmeans(a.data.1, centers=3) </userinput>
> <userinput> m1$centers </userinput>
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     5.901613    2.748387     4.393548    1.433871
2     5.006000    3.428000     1.462000    0.246000
3     6.850000    3.073684     5.742105    2.071053
> <userinput> p1 &lt;- pmml(m1) </userinput>
> <userinput> saveXML(p1, file="iris-clusters.pmml") </userinput>
> <userinput> quit() </userinput>

</pre>

In this example, we use the Iris data that is part of the R distributionand available through the data() function.  We compute three k-means clusters for the four features: 
   * Sepal.Length
   * Sepal.Width
   * Petal.Length
   * Petal.Width 

Save the resulting model as a PMML file. To make sure that this works, you should examine the PMML file and it should look like the following: 

<pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;PMML version="3.1" xmlns="http://www.dmg.org/PMML-3_1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;
 &lt;DataDictionary numderOfFields="4"&gt;
  &lt;DataField name="Sepal.Length" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Sepal.Width" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Petal.Length" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Petal.Width" optype="continuous" dataType="double"/&gt;
 &lt;/DataDictionary&gt;
 &lt;ClusteringModel modelName="KMeans_Model" functionName="clustering" algorithmName="KMeans" mo
delClass="centerBased" numberOfClusters="3"&gt;
  &lt;MiningSchema&gt;
   &lt;MiningField name="Sepal.Length" usageType="active"/&gt;
   &lt;MiningField name="Sepal.Width" usageType="active"/&gt;
   &lt;MiningField name="Petal.Length" usageType="active"/&gt;
   &lt;MiningField name="Petal.Width" usageType="active"/&gt;
  &lt;/MiningSchema&gt;
  &lt;ComparisonMeasure kind="distance"/&gt;
  &lt;Cluster name="1" size="62"&gt;
   &lt;Array n="4"&gt;5.90161290322581 2.74838709677419 4.39354838709678 1.43387096774194&lt;/Array&gt;
  &lt;/Cluster&gt;
  &lt;Cluster name="2" size="50"&gt;
   &lt;Array n="4"&gt;5.006 3.428 1.462 0.246&lt;/Array&gt;
  &lt;/Cluster&gt;
  &lt;Cluster name="3" size="38"&gt;
   &lt;Array n="4"&gt;6.85 3.07368421052632 5.74210526315789 2.07105263157895&lt;/Array&gt;
  &lt;/Cluster&gt;
 &lt;/ClusteringModel&gt;
&lt;/PMML&gt;
</pre>

<a name="Using R and Globus to Compute k-Means Clusters"></a>
---++ Using R and Globus to Compute k-Means Clusters

Once you can use R to compute clusters locally on a node that you can login to and invoke R directly, it is relatively easy to invoke the script on a remote node.  Here is an example.  

First, we invoke the script directly on a machine that we can login to: 

<pre class="screen">
$ <userinput>R &lt; angle-analysis.r --no-save </userinput>

R version 2.4.0 Patched (2006-11-25 r39997)
Copyright (C) 2006 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.
 
> <userinput>a.data.file &lt;- "data/angle-data.csv"</userinput>

<em>[etc]</em>
</pre>

Next, we perform the following steps. We assume that we have a coordinating node and one or more compute nodes.

<ol>
<li> Divide the data into segments.
<li> Copy a segment of the data to a compute node.
<li> Copy the corresponding analysis script.
<li> Invoke R, with the input coming from the analysis script.
<li> Copy the resulting PMML model back to the coordinating node.
</ol>


To obtain the angle data files, execute following commands on the controlling node:


<pre class="programlisting">
$ <userinput>wget http://angle.ncdm.uic.edu/globus/angle-data-%LOGINHOST%.csv</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/angle-data-workshop2.csv</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/angle-data-workshop3.csv</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/angle-data-workshop4.csv</userinput>
</pre>

Here is a Python script which will do this. The assumption is that the data is divided into several parts, labeled =angle-data-1.csv=, =angle-data-2.csv=, etc.  The same R analysis will be run on each segment to produce a separate PMML file, which will be collected and combined into a single model. In the example below, we look at the analysis for =angle-data-4.csv=, which we assume is one of the segments. 

---+++ ==run-job.py==

<pre class="programlisting">
#! /usr/bin/env python                                                       
                                                                     
from os import system                             
 
# copy local data file to remote machine 
cmds = "globus-url-copy file:///home/%LOGINNAME%/data/angle-data-workshop4.csv " 
cmds = cmds + "gsiftp://workshop4.lac.uic.edu/home/%LOGINNAME%/data/angle-data.csv" 
print "*** running: " + cmds + "\n"
system(cmds) 
 
# copy r script to compute clusters to remote machine 
cmds = "globus-url-copy file:///home/%LOGINNAME%/angle-analysis.r " 
cmds = cmds + "gsiftp://workshop4.lac.uic.edu/home/%LOGINNAME%/angle-analysis.r" 
print "*** running: " + cmds + "\n"
system(cmds) 
 
# invoke R script on remote machine 
cmds = "globus-job-run workshop4.lac.uic.edu " 
cmds = cmds + " /bin/sh -c '/usr/bin/R &lt /home/%LOGINNAME%/angle-analysis.r --no-save' " 
print "*** running: " + cmds + "\n"
system(cmds) 
 
# collect PMML file 
cmds = "globus-url-copy gsiftp://workshop4.lac.uic.edu/home/%LOGINNAME%/models/angle-model.pmml " 
cmds = cmds + "file:///home/%LOGINNAME%/collect/angle-model-4.pmml" 
print "*** running: " + cmds + "\n"
system(cmds) 
</pre>

Here is the R script that is being run on the node. 

---+++ ==angle-analysis.r==

<pre class="programlisting">

library(pmml) 
 
# data file 
a.data.file &lt;- "data/angle-data.csv" 
 
# output file 
a.model.file &lt;- "models/angle-model.pmml" 
 
a.col &lt;- list(ts=0,  
  src="",  
  sprt=0,  
  sCOUNTRY="",  
  dst="",  
  dprt=0,  
  dCOUNTRY="",  
  ip.4min=0,  
  ip.2min=0,  
  ip.1min=0,  
  ipprt.4min=0,  
  ipprt.2min=0,  
  ipprt.1min=0,  
  clusters="" 
) 
 
a.raw &lt;- scan(file=a.data.file, what=a.col, skip=1, sep=',') 
a.data &lt;- as.data.frame(a.raw) 
 
a.data.1 &lt;- a.data[,c('ip.4min', 'ip.2min', 'ip.1min',  
  'ip.4min', 'ip.2min', 'ip.1min')] 
 
 
m1 &lt;- kmeans(a.data.1, centers=10) 
 
p1 &lt;- pmml(m1) 
saveXML(p1, file=a.model.file) 
</pre>


<a name="Generalizing to a List of Nodes"></a>
---++ Generalizing to a List of Nodes

It is now easy to modify the Python code so that it will work with any list of available nodes, say a list provide by the Globus MDS service. Here is a slightly modified Python script that does this: 

---+++ ==run-multiple.py==

<pre class="programlisting">
#! /usr/bin/env python

from os import system

nodes = ["%LOGINHOST%", "workshop2", "workshop3", "workshop4"]

# local files are of the form filebase/filename
filebase = "file:///home/%LOGINNAME%"

# remote files are of the form: gsiftp:///filename
nodebase = ".lac.uic.edu/home/%LOGINNAME%"


# copy local data file to remote machine
for node in nodes:
  cmds = "globus-url-copy " + filebase + "/data/angle-data-"
  cmds = cmds + node + ".csv "
  cmds = cmds + "gsiftp://"
  cmds = cmds + node
  cmds = cmds + nodebase + "/data/angle-data.csv"
  print "*** running: " + cmds + "\n"
  system(cmds)

# copy r script to compute clusters to remote machine
for node in nodes:
  cmds = "globus-url-copy " + filebase + "/analysis/angle-analysis.r "
  cmds = cmds + "gsiftp://"
  cmds = cmds + node
  cmds = cmds + nodebase + "/angle-analysis.r"
  print "*** running: " + cmds + "\n"
  system(cmds)

# invoke R script on remote machine
for node in nodes:
  cmds = "globus-job-run "
  cmds = cmds + node
  cmds = cmds + ".lac.uic.edu "
  cmds = cmds + " /bin/sh -c '/usr/bin/R &lt /home/%LOGINNAME%/angle-analysis.r --no-save' "
  print "*** running: " + cmds + "\n"
  system(cmds)

# collect PMML file
for node in nodes:
  cmds = "globus-url-copy gsiftp://"
  cmds = cmds + node
  cmds = cmds + nodebase + "/models/angle-model.pmml "
  cmds = cmds + filebase + "/collect/angle-model-"
  cmds = cmds + node
  cmds = cmds + ".pmml"
  print "*** running: " + cmds + "\n"
  system(cmds)
</pre>


<a name="Assignment 1"></a>
---++ Assignment 1

This assignment has two parts: 

<ol>
  <li>Take several Angle data files, and use R and Globus to compute k-means clusters, with k=5, where different processors compute the k-means clusters for different data files. Collect all the resulting PMML files and analyze how different the clusters are. </li>
  <li>Repeat the task above, but this time use the same data file, but use different processors to compute clusters for different values of k. </li>
</ol>


<a name="Using R to Compute CART Trees"></a>
---++ Using R to Compute CART Trees

---+++ Example - IRIS Data 

To warm up, we begin by reviewing how to compute a classification and regression (CART) tree using R.  First, we load the Iris data, which, as we have already seen, can be accessed with the data() command.  Next, we compute the tree using the rpart library.  Finally, we save the tree as a PMML file.  

<pre class="screen">
> <userinput>data(iris)</userinput>
> <userinput>a.data &lt;- iris</userinput>
> <userinput>summary(a.data)</userinput>
  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500                  

> <userinput>library(rpart)</userinput>
> <userinput>m1 &lt;- rpart(Species ~ ., data=a.data)</userinput>
> <userinput>print(m1)</userinput>
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length&lt; 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width&lt; 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *

> <userinput>library(pmml)</userinput>
Loading required package: XML
> <userinput>p1 &lt;- pmml(m1)</userinput>
> <userinput>saveXML(p1, "iris.pmml")</userinput>
</pre>


Here is a slightly more general version of the command used to compute a classification tree using R: 

<pre class="programlisting">
m2 &lt;- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
  data=a.data, method = "class", control=rpart.control(cp=0, maxdepth=5))
</pre>

Here is the PMML file produced: 

<pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;PMML version="3.1" xmlns="http://www.dmg.org/PMML-3_1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;
 &lt;DataDictionary numderOfFields="5"&gt;
  &lt;DataField name="Species" optype="categorical" dataType="string"&gt;
   &lt;Value value="setosa"/&gt;
   &lt;Value value="versicolor"/&gt;
   &lt;Value value="virginica"/&gt;
  &lt;/DataField&gt;
  &lt;DataField name="Sepal.Length" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Sepal.Width" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Petal.Length" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="Petal.Width" optype="continuous" dataType="double"/&gt;
 &lt;/DataDictionary&gt;
 &lt;TreeModel modelName="RPart_Model" functionName="classification" algorithmName="rpart" splitCharacteristic="binarySplit"&gt;
  &lt;MiningSchema&gt;
   &lt;MiningField name="Species" usageType="predicted"/&gt;
   &lt;MiningField name="Sepal.Length" usageType="active"/&gt;
   &lt;MiningField name="Sepal.Width" usageType="active"/&gt;
   &lt;MiningField name="Petal.Length" usageType="active"/&gt;
   &lt;MiningField name="Petal.Width" usageType="active"/&gt;
  &lt;/MiningSchema&gt;
  &lt;Node score="setosa" recordCount="150"&gt;
   &lt;True/&gt;
   &lt;Node score="setosa" recordCount="50"&gt;
    &lt;SimplePredicate field="Petal.Length" operator="lessThan" value="2.45"/&gt;
   &lt;/Node&gt;
   &lt;Node score="versicolor" recordCount="100"&gt;
    &lt;SimplePredicate field="Petal.Length" operator="greaterOrEqual" value="2.45"/&gt;
    &lt;Node score="versicolor" recordCount="54"&gt;
     &lt;SimplePredicate field="Petal.Width" operator="lessThan" value="1.75"/&gt;
    &lt;/Node&gt;
    &lt;Node score="virginica" recordCount="46"&gt;
     &lt;SimplePredicate field="Petal.Width" operator="greaterOrEqual" value="1.75"/&gt;
    &lt;/Node&gt;
   &lt;/Node&gt;
  &lt;/Node&gt;
 &lt;/TreeModel&gt;
&lt;/PMML&gt;
</pre>


---+++ Overview of Building a Model

It is helpful to structure the process of building a data mining or statistical model into several discrete steps.  Here is a standard way to this.  For more complicated problems, there would be more steps and in turn these steps would be broken up further. 

<ol>
   <li> _Reading the data._  In the examples below, we will use a script called =read_data.r= to do this. </li>
   <li> _Computing derived attributes._  For simplicity, in the examples below, we omit this step, but it is usually one of the most important.   </li>
   <li> _Building the model._  In the examples below, we will use a script called =build_model.r= to do this. </li>
  <li> Validate the model.  In the examples below, we will use a script called =validate_model.r= to do this. </li>
</ol>


---+++ Example -  Forest Cover Data

We next look a slighly more complicated example - computing a classification tree on file with about 580,000 data records and 54 attributes that can be used to classify 7 types of forest cover.

This data is from a study of Jock A. Blackard from Colorado State University and is available from the UCI Data Archive. 


To obtain the data files, execute following commands on the controlling node:


<pre class="programlisting">
$ <userinput>cd ~/</userinput>
$ <userinput>mkdir data</userinput>
$ <userinput>cd data</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover-full.txt</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover.txt</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover-%LOGINHOST%.txt</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover-workshop2.txt</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover-workshop3.txt</userinput>
$ <userinput>wget http://angle.ncdm.uic.edu/globus/forest-cover-workshop4.txt</userinput>
$ <userinput>cd ~/</userinput>
</pre>


Here is the R program to read the data: 

---+++ ==~/analysis/read_data.r==

<pre class="programlisting">
a.data.file &lt;- "data/forest-cover.txt"

a.col &lt;- list(elevation=0,
  aspect=0,
  slope=0,
  h.hydrology=0,
  v.hydrology=0,
  h.roadways=0,
  shade.0900=0,
  shade.1200=0,
  shade.1500=0,
  h.fire=0,
  wilderness.1=0,
  wilderness.2=0,
  wilderness.3=0,
  wilderness.4=0,
  soil.01=0,
  soil.02=0,
  soil.03=0,
  soil.04=0,
  soil.05=0,
  soil.06=0,
  soil.07=0,
  soil.08=0,
  soil.09=0,
  soil.10=0,
  soil.11=0,
  soil.12=0,
  soil.13=0,
  soil.14=0,
  soil.15=0,
  soil.16=0,
  soil.17=0,
  soil.18=0,
  soil.19=0,
  soil.20=0,
  soil.21=0,
  soil.22=0,
  soil.23=0,
  soil.24=0,
  soil.25=0,
  soil.26=0,
  soil.27=0,
  soil.28=0,
  soil.29=0,
  soil.30=0,
  soil.31=0,
  soil.32=0,
  soil.33=0,
  soil.34=0,
  soil.35=0,
  soil.36=0,
  soil.37=0,
  soil.38=0,
  soil.39=0,
  soil.40=0,
  cover=0)

a.raw &lt;- scan(file=a.data.file, what=a.col, skip=1, sep=',')
a.data &lt;- as.data.frame(a.raw)
</pre>


Here is how to start R and to invoke this R script.  Note that we have divided the data into four segments. This reads only the first segment of data.


<pre class="screen">
%LOGINNAME%@%LOGINHOST%:~$ <userinput>R</userinput>

R version 2.4.1 (2006-12-18)

<em>[etc.]</em>

%LOGINNAME%@%LOGINHOST%:~$ <userinput>source('~/analysis/read_data.r')</userinput>
Read 145252 records

> <userinput>summary(a.data)</userinput>
   elevation        aspect          slope        h.hydrology    
 Min.   :1863   Min.   :  0.0   Min.   : 0.00   Min.   :   0.0  
 1st Qu.:2747   1st Qu.: 54.0   1st Qu.: 7.00   1st Qu.:  95.0  
 Median :2909   Median :108.0   Median :11.00   Median : 212.0  
 Mean   :2874   Mean   :141.1   Mean   :11.93   Mean   : 251.7  
 3rd Qu.:3004   3rd Qu.:216.0   3rd Qu.:15.00   3rd Qu.: 362.0  
 Max.   :3849   Max.   :360.0   Max.   :61.00   Max.   :1343.0  
  v.hydrology        h.roadways     shade.0900      shade.1200   
 Min.   :-146.00   Min.   :   0   Min.   :  0.0   Min.   : 99.0  
 1st Qu.:   7.00   1st Qu.:1842   1st Qu.:207.0   1st Qu.:216.0  
 Median :  23.00   Median :3416   Median :222.0   Median :226.0  
 Mean   :  34.54   Mean   :3311   Mean   :217.4   Mean   :224.9  
 3rd Qu.:  51.00   3rd Qu.:4671   3rd Qu.:232.0   3rd Qu.:236.0  
 Max.   : 554.00   Max.   :7117   Max.   :254.0   Max.   :254.0  

[etc.] 

</pre>


Here is a simple R script called =~/analysis/build_model.r= to build a classification tree using some of the variables to get started. 

<pre class="programlisting">
a.formula &lt;- y.cover ~ elevation + aspect + slope + 
  h.hydrology + v.hydrology + h.roadways + 
  shade.0900 + shade.1200 + shade.1500 + h.fire +
  wilderness.1 +  wilderness.2 + wilderness.3 +  wilderness.4

a.tree.method &lt;- 'class'
a.tree.depth &lt;- 5

m1 &lt;- rpart(formula = a.formula, data = a.data.tr, 
  method=a.tree.method, control=rpart.control(cp=0, maxdepth=a.tree.depth))

# summary(m1)
# plot(m1)
# text(m1, use.n=TRUE)

p1 &lt;- pmml(m1)
saveXML(p1, file="models/forest-cover.pmml")
</pre>


Next, we run the following R script called =~/analysis/go.r= to split the data into training and validation sets and to invoke the =build_model.r= function. 

<pre class="programlisting">
library(rpart)
library(pmml)

a.data.file &lt;- "data/forest-cover.txt"
source('analysis/read_data.r')

a.data[, 'y.cover'] &lt;- factor(a.data[, 'cover'])

# determine number of records in training and valiation data sets
control.tr &lt;- 100000
control.vr &lt;- 40000

# Set training and validation sets
a.data.tr &lt;- a.data[1:control.tr,]
vs &lt;- control.tr+1
ve &lt;- control.tr + control.vr
a.data.validate &lt;- a.data[vs:ve,]

source('analysis/build_model.r')
</pre>

Here is the result of running the R script =go.r=.  

<pre class="screen">
$ <userinput>source('~/analysis/go.r')</userinput>

Loading required package: XML
Call:
rpart(formula = a.formula, data = a.data.tr, method = a.tree.method, 
    control = rpart.control(cp = 0, maxdepth = a.tree.depth))
  n= 100000 

             CP nsplit rel error    xerror        xstd
1  0.1938103402      0 1.0000000 1.0000000 0.004480635
2  0.0430088123      1 0.8061897 0.8065506 0.004213390
3  0.0423170622      2 0.7631808 0.7520828 0.004118668
4  0.0346476586      3 0.7208638 0.7247135 0.004067487
5  0.0343468977      4 0.6862161 0.7038407 0.004026768
6  0.0270083311      5 0.6518692 0.6527715 0.003920677
7  0.0121507414      6 0.6248609 0.6249812 0.003858883
8  0.0096243496      9 0.5839875 0.5975819 0.003794982
9  0.0081806972     10 0.5743631 0.5794159 0.003750912
10 0.0080002406     11 0.5661824 0.5761376 0.003742809
11 0.0065565882     12 0.5581822 0.5675058 0.003721254
12 0.0056543054     13 0.5516256 0.5603778 0.003703207
13 0.0047520226     15 0.5403170 0.5488887 0.003673641
14 0.0016241090     16 0.5355650 0.5436254 0.003659896
15 0.0011729676     17 0.5339409 0.5421817 0.003656104
16 0.0003759512     18 0.5327679 0.5421216 0.003655946
17 0.0003609131     20 0.5320160 0.5419712 0.003655550
18 0.0002706848     21 0.5316551 0.5419110 0.003655392
19 0.0000000000     23 0.5311137 0.5417607 0.003654996

Node number 1: 100000 observations,    complexity param=0.1938103
  predicted class=2  expected loss=0.33249
    class counts: 22027 66751  2160  2160  2582  2160  2160
   probabilities: 0.220 0.668 0.022 0.022 0.026 0.022 0.022 
  left son=2 (86939 obs) right son=3 (13061 obs)
  Primary splits:
      elevation    &lt; 3077.5 to the left,  improve=7006.051, (0 missing)
      wilderness.1 &lt; 0.5    to the right, improve=5918.671, (0 missing)
      wilderness.4 &lt; 0.5    to the left,  improve=3943.152, (0 missing)
      wilderness.3 &lt; 0.5    to the left,  improve=2545.846, (0 missing)
      h.fire       &lt; 1557.5 to the right, improve=2157.961, (0 missing)
  Surrogate splits:
      wilderness.2 &lt; 0.5    to the left,  agree=0.874, adj=0.036, (0 split)
      h.roadways   &lt; 6811.5 to the left,  agree=0.871, adj=0.012, (0 split)
      h.hydrology  &lt; 965    to the left,  agree=0.870, adj=0.008, (0 split)
      v.hydrology  &lt; 259.5  to the left,  agree=0.870, adj=0.006, (0 split)

<em>[etc.]</em>
</pre>


This produces the following PMML model: 


<pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;PMML version="3.1" xmlns="http://www.dmg.org/PMML-3_1" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt;
 &lt;DataDictionary numderOfFields="15"&gt;
  &lt;DataField name="y.cover" optype="categorical" dataType="string"&gt;
   &lt;Value value="1"/&gt;
   &lt;Value value="2"/&gt;
   &lt;Value value="3"/&gt;
   &lt;Value value="4"/&gt;
   &lt;Value value="5"/&gt;
   &lt;Value value="6"/&gt;
   &lt;Value value="7"/&gt;
  &lt;/DataField&gt;
  &lt;DataField name="elevation" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="aspect" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="slope" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="h.hydrology" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="v.hydrology" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="h.roadways" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="shade.0900" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="shade.1200" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="shade.1500" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="h.fire" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="wilderness.1" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="wilderness.2" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="wilderness.3" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="wilderness.4" optype="continuous" dataType="double"/&gt;
 &lt;/DataDictionary&gt;
 &lt;TreeModel modelName="RPart_Model" functionName="classification"
  algorithmName="rpart" splitCharacteristic="binarySplit"&gt;
  &lt;MiningSchema&gt;
   &lt;MiningField name="y.cover" usageType="predicted"/&gt;
   &lt;MiningField name="elevation" usageType="active"/&gt;
   &lt;MiningField name="aspect" usageType="active"/&gt;
   &lt;MiningField name="slope" usageType="active"/&gt;
   &lt;MiningField name="h.hydrology" usageType="active"/&gt;
   &lt;MiningField name="v.hydrology" usageType="active"/&gt;
   &lt;MiningField name="h.roadways" usageType="active"/&gt;
   &lt;MiningField name="shade.0900" usageType="active"/&gt;
   &lt;MiningField name="shade.1200" usageType="active"/&gt;
   &lt;MiningField name="shade.1500" usageType="active"/&gt;
   &lt;MiningField name="h.fire" usageType="active"/&gt;
   &lt;MiningField name="wilderness.1" usageType="active"/&gt;
   &lt;MiningField name="wilderness.2" usageType="active"/&gt;
   &lt;MiningField name="wilderness.3" usageType="active"/&gt;
   &lt;MiningField name="wilderness.4" usageType="active"/&gt;
  &lt;/MiningSchema&gt;
  &lt;Node score="2" recordCount="50000"&gt;
   &lt;True/&gt;
   &lt;Node score="2" recordCount="29928"&gt;
    &lt;SimplePredicate field="h.fire" operator="greaterOrEqual" value="2862"/&gt;
    &lt;Node score="2" recordCount="28917"&gt;
     &lt;SimplePredicate field="elevation" operator="lessThan" value="3094"/&gt;
     &lt;Node score="2" recordCount="5520"&gt;
      &lt;SimplePredicate field="v.hydrology" operator="greaterOrEqual" value="62.5"/&gt;
      &lt;Node score="2" recordCount="2660"&gt;
       &lt;SimplePredicate field="h.roadways" operator="lessThan" value="5188"/&gt;
       &lt;Node score="1" recordCount="1598"&gt;
        &lt;SimplePredicate field="shade.0900" operator="lessThan" value="224.5"/&gt;
       &lt;/Node&gt;
       &lt;Node score="2" recordCount="1062"&gt;
        &lt;SimplePredicate field="shade.0900" operator="greaterOrEqual" value="224.5"/&gt;
       &lt;/Node&gt;
      &lt;/Node&gt;
      &lt;Node score="2" recordCount="2860"&gt;
       &lt;SimplePredicate field="h.roadways" operator="greaterOrEqual" value="5188"/&gt;
       &lt;Node score="1" recordCount="34"&gt;
        &lt;SimplePredicate field="h.fire" operator="lessThan" value="3030"/&gt;
       &lt;/Node&gt;
       &lt;Node score="2" recordCount="2826"&gt;
        &lt;SimplePredicate field="h.fire" operator="greaterOrEqual" value="3030"/&gt;
       &lt;/Node&gt;
      &lt;/Node&gt;
     &lt;/Node&gt;
     &lt;Node score="2" recordCount="23397"&gt;

    [etc.]

      &lt;Node score="6" recordCount="2867"&gt;
       &lt;SimplePredicate field="elevation" operator="greaterOrEqual" value="2370"/&gt;
       &lt;Node score="2" recordCount="220"&gt;
        &lt;SimplePredicate field="wilderness.1" operator="greaterOrEqual" value="0.5"/&gt;
       &lt;/Node&gt;
       &lt;Node score="6" recordCount="2647"&gt;
        &lt;SimplePredicate field="wilderness.1" operator="lessThan" value="0.5"/&gt;
       &lt;/Node&gt;
      &lt;/Node&gt;
     &lt;/Node&gt;
    &lt;/Node&gt;
   &lt;/Node&gt;
  &lt;/Node&gt;
 &lt;/TreeModel&gt;
&lt;/PMML&gt;
</pre>


Here is a R program called =~/analysis/validate_model.r= to validate models, which can be run directly from the command line.  This script could be executed by including the line =source('analysis/validate_model.r')= in the =go.r= script above.  

<pre class="programlisting">
a.report &lt;- "reports/report-validate.txt"
a.factors &lt;- c(1, 2, 3, 4, 5, 6, 7)
a.factors.n &lt;- length(a.factors)

a.data.validate[, 'actual.cover'] &lt;-  factor(a.data.validate[, 'cover'])
y &lt;- predict(m1, newdata=a.data.validate, type='class')
a.data.validate[, 'predicted.cover'] &lt;- y

b.count &lt;- matrix(0, ncol = a.factors.n, nrow = a.factors.n)
b.ratio &lt;- matrix(0, ncol = a.factors.n, nrow = a.factors.n)

for (jj in 1:a.factors.n) {
 jj.f &lt;- a.factors[jj]
 actual.f &lt;- a.data.validate[ a.data.validate$actual.cover == jj.f,
'predicted.cover']
for (ii in 1:a.factors.n) {
  p &lt;- (jj-1)*a.factors.n + ii
  b.count[p] &lt;- length(actual.f[actual.f == a.factors[ii]])
  b.ratio[p] &lt;- (b.count[p] / length(actual.f)) * 100
  }
}
 
cat('\nConfusion Matrix (Counts)\n\n', file = a.report, append=T)
for (jj in 1:a.factors.n) {
 m.start &lt;- (jj-1)*a.factors.n + 1
 m.end &lt;- (jj-1)*a.factors.n + a.factors.n
 rline &lt;- sprintf("%i\t", b.count[m.start:m.end])
 cat(rline, file = a.report, append=T)
 cat('\n', file = a.report, append=T)
}
 
cat('\nConfusion Matrix (Percent)\n\n', file = a.report, append=T)
for (jj in 1:a.factors.n) {
 m.start &lt;- (jj-1)*a.factors.n + 1
 m.end &lt;- (jj-1)*a.factors.n + a.factors.n
 rline &lt;- sprintf("%.1f\t", b.ratio[m.start:m.end])
 cat(rline, file = a.report, append=T)
 cat('\n', file = a.report, append=T)
}
</pre>


Running the =validate_model.r= R script (after go.r) produces the following confusion matrix (stored in =reports/report-validate.txt=), which clearly shows that this is not a very good model. 

<pre class="screen">
$ <userinput>source('~/analysis/validate_model.r')</userinput>

Confusion Matrix (Counts)

11239	 10989	 0	 0	 0	 0	 443	
18280	 58387	 0	 0	 9	 0	 0	
0	 0	 0	 0	 0	 0	 0	
0	 0	 0	 0	 0	 0	 0	
123	 530	 0	 0	 0	 0	 0	
0	 0	 0	 0	 0	 0	 0	
0	 0	 0	 0	 0	 0	 0	

Confusion Matrix (Percent)

49.6	 48.5	 0.0	 0.0	 0.0	 0.0	 2.0	
23.8	 76.1	 0.0	 0.0	 0.0	 0.0	 0.0	
NaN	 NaN	 NaN	 NaN	 NaN	 NaN	 NaN	
NaN	 NaN	 NaN	 NaN	 NaN	 NaN	 NaN	
18.8	 81.2	 0.0	 0.0	 0.0	 0.0	 0.0	
NaN	 NaN	 NaN	 NaN	 NaN	 NaN	 NaN	
NaN	 NaN	 NaN	 NaN	 NaN	 NaN	 NaN	
</pre>

We can easily see what is going on by looking at the  distribution of the factors in the training and validation data: 


<pre class="screen">
$ <userinput>summary(a.data.tr$y.cover)</userinput>
    1     2     3     4     5     6     7 
10151 28794  2160  2160  2415  2160  2160 
$ <userinput>summary(a.data.validate$y.cover)</userinput>
    1     2     3     4     5     6     7 
22671 76676     0     0   653     0     0 
</pre>

The validation set doesn't have instances of data from all the classes that are present. 

To improve the model, we need to define better features, more carefully tune the parameters in the tree, and to make sure that each training and validation set reflects the overall class distribution.  In addition, we need to define a hold out validation set, that also reflects the class distribution of the entire data set.  This hold out validation will be used to validate the ensemble model we will be building. 


<a name="Using R and Globus to Compute CART Trees"></a>
---++ Using R and Globus to Compute CART Trees 

The pattern here is the same as we used for computing k-means clusters.  We need to do the following steps: 

   1. <em>Set up. </em> 
      a. Create the requried directories on each node: data, analysis, models, and reports.
      a. Use Grid-FTP to move the required R scripts to each processor.  We will be using four R scripts: =go.r=, =read_data.r=, =build_model.r=, and =validate_model.r=.
   1. _Partition the data._ Partition the data into several different data segments.
   1. _Scatter the data._ Use Grid-FTP to move each data segment to the appropriate processor. 
   1. _Build the models._  Use GRAM to invoke R using input from the  =go.r= script on each processor. 
   1. _Gather the models._  Use Grid-FTP to gather each of the resulting PMML files.
   1. _Gather and analyze the reports._ Use Grid-FTP to gather each of the resulting report files.
   1. _Build and ensemble model._ Build an ensemble model by assembling each of the PMML files.
   1. _Validate the ensemble model._ Evaluate the ensemble model on a hold out set.

Here is an R script that can be used to compute CART trees using R and and Globus for a single compute node.  In the Assignment 2 below, you will modify this so that it can work using multiple compute nodes.  

Before running the script, prepare a project directory:

<pre class="programlisting">
$ <userinput>cd ~/</userinput>
$ <userinput>mkdir forest-cover/</userinput>
$ <userinput>tar cf - data analysis | tar xvf - -C ~/forest-cover/</userinput>
$ <userinput>mv forest-cover/data forest-cover/control_data</userinput>
$ <userinput>mv forest-cover/analysis forest-cover/control_analysis</userinput>
</pre>

---+++ ==run-cart-globus.py==

<pre class="programlisting">
#!/usr/bin/env python
from os import system

nodes = ['workshop4.lac.uic.edu']
home_dir = '/home/%LOGINNAME%/'
project = 'forest-cover'
rfiles = ["go.r", "read_data.r", "build_model.r", "validate_model.r"]

node = nodes[0]
node_name  = node.split('.')[0]

# prepare remote directories
cmds = "globus-job-run " + node + " /bin/mkdir -p " + home_dir + "/data";
print "*** running: " + cmds + "\n"
system(cmds)

cmds = "globus-job-run " + node + " /bin/mkdir -p " + home_dir + "/analysis";
print "*** running: " + cmds + "\n"
system(cmds)

cmds = "globus-job-run " + node + " /bin/mkdir -p " + home_dir + "/models";
print "*** running: " + cmds + "\n"
system(cmds)

cmds = "globus-job-run " + node + " /bin/mkdir -p " + home_dir + "/collect";
print "*** running: " + cmds + "\n"
system(cmds)

# copy local data file to remote machine
cmds = "globus-url-copy file://" + home_dir + project + "/" + "control_data/"
cmds = cmds + project + "-" + node_name + ".txt "
cmds = cmds + "gsiftp://" + node + home_dir + "data/" + project + ".txt"
print "*** running: " + cmds + "\n"
system(cmds)

# copy r script to compute clusters to remote machine
for rfile in rfiles:
  cmds = "globus-url-copy file://" + home_dir + project + "/"
  cmds = cmds + "control_analysis/" +  rfile + " "
  cmds = cmds + "gsiftp://" + node + home_dir + "analysis/" + rfile
  print "*** running: " + cmds + "\n"
  system(cmds)

# invoke R script on remote machine
cmds = "globus-job-run " + node + " "
cmds = cmds + " /bin/sh -c '/usr/bin/R &lt " + home_dir + "/analysis/go.r --no-save' "
print "running: " + cmds + "\n"
system(cmds)

# collect PMML file
cmds = "globus-url-copy gsiftp://" + node + home_dir + "models/" + project + ".pmml "
cmds = cmds + "file://" + home_dir + "collect/" + project + "-" + node_name + ".pmml"
print "running: " + cmds + "\n"
system(cmds)
</pre>

The script works by taking the data in =forest-cover/control_data= and the scripts in =forest-cover/control_analysis= and copying them, using GridFTP, to the appropriate compute node.  

%NOTE% This script processes data sequentially for demonstrations purposes. Globus jobs can be submitted in parallel and their status can be monitored until results are ready for retrieval.

<pre class="screen">
<userinput>%LOGINNAME%@%LOGINHOST%:~$ ls -R forest-cover/ <userinput>

forest-cover/:
control_analysis  control_data

forest-cover/control_analysis:
build_model.r  go.r  read_data.r  validate_model.r

forest-cover/control_data:
forest-cover.txt            forest-cover-workshop2.txt  forest-cover-workshop4.txt
forest-cover-%LOGINHOST%.txt  forest-cover-workshop3.txt

<em>[etc.]</em>
</pre>

The Python script then uses globus-job-run to invoke R on the compute node using the =go.r= R script.  The PMML model is then collected using Grid-FTP to the collect directory where it can be assembled into an ensemble model. 


<pre class="screen">
%LOGINNAME%@%LOGINHOST%:~$ <userinput>ls collect</userinput>
forest-cover-%LOGINHOST%.pmml
<em>[etc.]</em>
</pre>



<a name="Assignment 2"></a>
---++ Assignment 2

This assignment has three parts: 

   1. Generalize the Python code above that builds CART trees using Globus to work with several compute nodes.
   1. Expand the Python code above that builds CART trees using Globus to also include the =validate_model.r= scripts.  Write the confusion matrix and related information to the =reports= directory and then gather them them into the =collect= directory.
   1. Write a function that given several PMML functions, produces an ensemble model.


<a name="Hints"></a>
---++ Hints

Here are some hints to keep in mind. 

   * Make sure that your analysis program (for example, your R script) works on your login machine first.
   * Make sure Globus script works on your login machine first, before trying it on another machine.
   * Make sure you have created all the proper directories required.
   * Make sure all your files have the proper permissions.


%STOPINCLUDE%

<!--                                                                            
      * Set LOGINHOST = workshop1.lac.uic.edu
      * Set LOGINIP = 131.193.181.56
      * Set GRIDHOST = tg-login.sdsc.teragrid.org
      * Set OTHERHOST = workshop2.lac.uic.edu
      * Set CERTSUBJECT = /O=Grid/OU=OSG/CN=Training User 99
      * Set LOGINNAME = train99
      * Set HOMEDIR = /home/%LOGINNAME%


Main.MichaelWilde - 22 Mar 2007
Main.ForrestChristian - 24 Mar 2007 - Added VARIABLES
Main.ForrestChristian - 25 Mar 2007 - Changed with Michal and Bob's new materials
-->
