%META:TOPICINFO{author="BenClifford" date="1172850460" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="LectureFiveTutorial"}%
<link rel="stylesheet" type="text/css" href="%PUBURL%/%WEB%/WorkshopTutorialModules/exercises.css">
d170 14
---+!! A Simple DAG

d45 5
<!-- put all text for the tutorial beneath this point and above the STOPINCLUDE -->
%STARTINCLUDE%
%EDITTHIS%

---+++ Create a script to monitor the queue
Create a small shell script to monitor the Condor-G queue. We will use this throughout the rest of the tutorial:

<pre class="screen">
$ <userinput>cat &gt; watch_condor_q
#! /bin/sh
while true; do
     condor_q
     condor_q -globus
     sleep 10
done
<em>[Ctrl+D]</em></userinput>
$ <userinput>cat watch_condor_q</userinput>
#! /bin/sh
while true; do
     condor_q
     condor_q -globus
     sleep 10
done
$ <userinput>chmod a+x watch_condor_q </userinput>
</pre>


---+++ Create a minimal DAG
Create a minimal DAG for DAGMan. This DAG will have a single node.

<pre class="screen">
$ <userinput>cat &gt; mydag.dag
Job HelloWorld myjob.submit
<em>[Ctrl+D]</em></userinput>
$ <userinput>cat mydag.dag</userinput>
Job HelloWorld myjob.submit
</pre>

---+++ Submit the DAG
This section requires you to have three windows open. We will submit the DAG in the first window and watch the progress of it and the job in the other two. We will do these in the following order:
   1. In the first window, submit the DAG.
   1. In the second window, tail the results log.
   1. In the third window, tail the DAGMan log.
   1. In the first window, watch condor with =watch_condor_q=.
This section requires you to have three windows open. We will submit the DAG in the first window and watch the progress of it and the job in the other two.


In the first log window, run <tt><userinput>tail -f --lines=500 results.log</userinput></tt> to watch the job log file as your job runs.  

In a third window, watch DAGMan's log file by runnning <tt><userinput>tail -f --lines=500 mydag.dag.dagman.out</userinput></tt>. We suggest that you re-run this command whenever you submit a DAG during the remainder of this tutorial. This will show you how a typical DAG progresses.  Use *<userinput>[Ctrl+C]</userinput>* to stop watching the file. An example is shown below:

<pre class="screen">
7/10 10:36:43 ** $CondorVersion: 6.5.1 Apr 22 2003 $
$ <userinput>tail -f --lines=500 mydag.dag.dagman.out</userinput>

7/10 10:36:43 ******************************************************
7/10 10:36:44 DaemonCore: Command Socket at &lt;128.105.185.14:34571&gt;
7/10 10:36:43 ** $CondorVersion: 6.8.4 Apr 22 2006 $
7/10 10:36:43 ** $CondorPlatform: INTEL-LINUX-GLIBC22 $
7/10 10:36:43 ** PID = 26844
7/10 10:36:43 ******************************************************
7/10 10:36:44 DaemonCore: Command Socket at &lt;%LOGINIP%:34571&gt;
7/10 10:36:44 argv[0] == "condor_scheduniv_exec.6.0"
7/10 10:36:44 argv[1] == "-Debug"
7/10 10:36:44 argv[2] == "3"
7/10 10:36:44 argv[3] == "-Lockfile"
7/10 10:36:44 argv[4] == "mydag.dag.lock"
7/10 10:36:44 argv[5] == "-Condorlog"
7/10 10:36:44 argv[6] == "results.log"
7/10 10:36:44 argv[7] == "-Dag"
7/10 10:36:44 argv[8] == "mydag.dag"
7/10 10:36:44 argv[9] == "-Rescue"
7/10 10:36:44 argv[10] == "mydag.dag.rescue"
7/10 10:36:44 Condor log will be written to results.log
7/10 10:36:44 DAG Lockfile will be written to mydag.dag.lock
7/10 10:36:44 DAG Input file is mydag.dag
7/10 10:36:44 Rescue DAG will be written to mydag.dag.rescue
7/10 10:36:44 Parsing mydag.dag ...
7/10 10:36:44 Dag contains 1 total jobs
7/10 10:36:44 Bootstrapping...
7/10 10:36:44 Number of pre-completed jobs: 0
7/10 10:36:44 Submitting Job HelloWorld ...
7/10 10:36:44    assigned Condor ID (7.0.0)
7/10 10:36:45 Event: ULOG_SUBMIT for Job HelloWorld (7.0.0)
7/10 10:36:45 0/1 done, 0 failed, 1 submitted, 0 ready, 0 pre, 0 post
7/10 10:37:05 Event: ULOG_GLOBUS_SUBMIT for Job HelloWorld (7.0.0)
7/10 10:37:05 Event: ULOG_EXECUTE for Job HelloWorld (7.0.0)
7/10 10:38:10 Event: ULOG_JOB_TERMINATED for Job HelloWorld (7.0.0)
7/10 10:38:10 Job HelloWorld completed successfully.
Submit the DAG with =[[http://www.cs.wisc.edu/condor/manual/v6.7/condor_submit_dag.html][condor_submit_dag]]= and watch the run with =watch_condor_q=. =condor_dagman= is running as a job and submits your real job on your behalf, without your direct intervention. You might see the =C= (completed) state as your job finishes, but that often goes by too quickly to notice.
7/10 10:38:10 All jobs Completed!
7/10 10:38:10 **** condor_scheduniv_exec.6.0 (condor_DAGMAN) EXITING WITH STATUS 0
</pre>

Submit the DAG with =[[http://www.cs.wisc.edu/condor/manual/v6.9/condor_submit_dag.html][condor_submit_dag]]= and watch the run with =watch_condor_q=. =condor_dagman= is running as a job and submits your real job on your behalf, without your direct intervention. You might see the =C= (completed) state as your job finishes, but that often goes by too quickly to notice.

<pre class="screen">
$ <userinput>condor_submit_dag mydag.dag</userinput>

Checking your DAG input file and all submit files it references.
This might take a while... 
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor   : mydag.dag.condor.sub
Log of DAGMan debugging messages         : mydag.dag.dagman.out
Log of Condor library debug messages     : mydag.dag.lib.out
Log of the life of condor_dagman itself  : mydag.dag.dagman.log

Condor Log file for all jobs of this DAG : results.log
Submitting job(s).
Logging submit event(s).
-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu
-----------------------------------------------------------------------
   2.0   adesmet         7/10 17:33   0+00:00:03 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:00 I  0   0.0  myscript.sh TestJo

-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   %LOGINNAME%         7/10 17:33   0+00:00:03 R  0   2.6  condor_dagman -f -
-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu

   3.0   adesmet       UNSUBMITTED fork     gk2   /tmp/<em>username</em>-cond


-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   2.0   adesmet         7/10 17:33   0+00:00:33 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:15 R  0   0.0  myscript.sh TestJo
%STARTMore%
-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   %LOGINNAME%         7/10 17:33   0+00:00:33 R  0   2.6  condor_dagman -f -
-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu

   3.0   adesmet       ACTIVE fork     gk2   /tmp/<em>username</em>-cond


-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   2.0   adesmet         7/10 17:33   0+00:01:03 R  0   2.6  condor_dagman -f -
   3.0   adesmet         7/10 17:33   0+00:00:45 R  0   0.0  myscript.sh TestJo

-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   2.0   %LOGINNAME%         7/10 17:33   0+00:01:03 R  0   2.6  condor_dagman -f -
-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu

   3.0   adesmet       ACTIVE fork     gk2   /tmp/<em>username</em>-cond


-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   3.0   %LOGINNAME%       ACTIVE fork     %LOGINHOST%   /tmp/<em>username</em>-cond


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
-- Submitter: puffin.cs.wisc.edu : &lt;128.105.185.14:35688&gt; : puffin.cs.wisc.edu

0 jobs; 0 idle, 0 running, 0 held


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        


<em><userinput>[Ctrl+C]</userinput></em>
%ENDMore%
</pre>

-rw-r--r--    1 adesmet  adesmet        28 Jul 10 10:35 mydag.dag
-rw-r--r--    1 adesmet  adesmet       523 Jul 10 10:36 mydag.dag.condor.sub
-rw-r--r--    1 adesmet  adesmet       608 Jul 10 10:38 mydag.dag.dagman.log
-rw-r--r--    1 adesmet  adesmet      1860 Jul 10 10:38 mydag.dag.dagman.out
-rw-r--r--    1 adesmet  adesmet        29 Jul 10 10:38 mydag.dag.lib.out
-rw-------    1 adesmet  adesmet         0 Jul 10 10:36 mydag.dag.lock
-rw-r--r--    1 adesmet  adesmet       175 Jul  9 18:13 myjob.submit
-rwxr-xr-x    1 adesmet  adesmet       194 Jul 10 10:36 myscript.sh
-rw-r--r--    1 adesmet  adesmet        31 Jul 10 10:37 results.error
-rw-------    1 adesmet  adesmet       833 Jul 10 10:38 results.log
-rw-r--r--    1 adesmet  adesmet       261 Jul 10 10:37 results.output
-rwxr-xr-x    1 adesmet  adesmet        81 Jul 10 10:35 watch_condor_q


---+++ Verify your results:
I'm process id 29149 on %OTHERHOST%
<pre class="screen">
$ <userinput>ls -l</userinput>
Running as binary /n/uscms_share/home/adesmet2/.globus/.gass_cache/local/md5/aa/ceb9e04077256aaa2acf4dff670897/md5/27/2f50da149fc049d07b1c27f30b67df/data TEST 1
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%        28 Jul 10 10:35 mydag.dag
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%       523 Jul 10 10:36 mydag.dag.condor.sub
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%       608 Jul 10 10:38 mydag.dag.dagman.log
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%      1860 Jul 10 10:38 mydag.dag.dagman.out
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%        29 Jul 10 10:38 mydag.dag.lib.out
-rw-------    1 %LOGINNAME%  %LOGINNAME%         0 Jul 10 10:36 mydag.dag.lock
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%       175 Jul  9 18:13 myjob.submit
-rwxr-xr-x    1 %LOGINNAME%  %LOGINNAME%       194 Jul 10 10:36 myscript.sh
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%        31 Jul 10 10:37 results.error
-rw-------    1 %LOGINNAME%  %LOGINNAME%       833 Jul 10 10:38 results.log
-rw-r--r--    1 %LOGINNAME%  %LOGINNAME%       261 Jul 10 10:37 results.output
-rwxr-xr-x    1 %LOGINNAME%  %LOGINNAME%        81 Jul 10 10:35 watch_condor_q
$ <userinput>cat results.error </userinput>
This is sent to standard error
$ <userinput>cat results.output </userinput>
I'm process id 29149 on %LOGINHOST%
executable   = /afs/cs.wisc.edu/u/a/d/adesmet/miron-condor-g-dagman-talk/vdt/condor/bin/condor_dagman
Thu Jul 10 10:38:44 CDT 2003
Running as binary %HOMEDIR%/.globus/.gass_cache/local/md5/aa/ceb9e04077256aaa2acf4dff670897/md5/27/2f50da149fc049d07b1c27f30b67df/data TEST 1
My name (argument 1) is TEST
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting
RESULT: 0 SUCCESS
</pre>

Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job):
000 (006.000.000) 07/10 10:36:43 Job submitted from host: &lt;128.105.185.14:33785&gt;
<pre class="screen">
001 (006.000.000) 07/10 10:36:44 Job executing on host: &lt;128.105.185.14:33785&gt;
mydag.dag         mydag.dag.dagman.log  mydag.dag.lib.out  myjob.submit  results.error  results.output
mydag.dag.condor.sub  mydag.dag.dagman.out  mydag.dag.lock     myscript.sh   results.log    watch_condor_q
$ <userinput>cat mydag.dag.condor.sub</userinput>
# Filename: mydag.dag.condor.sub
# Generated by condor_submit_dag mydag.dag
universe   = scheduler
executable   = /path/to/condor/bin/condor_dagman
getenv      = True
output      = mydag.dag.lib.out
error      = mydag.dag.lib.out
log      = mydag.dag.dagman.log
remove_kill_sig   = SIGUSR1
arguments   = -f -l . -Debug 3 -Lockfile mydag.dag.lock -Condorlog results.log -Dag mydag.dag -Rescue mydag.dag.rescue
environment   = _CONDOR_DAGMAN_LOG=mydag.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0
queue
$ <userinput>cat mydag.dag.dagman.log</userinput>
000 (006.000.000) 07/10 10:36:43 Job submitted from host: &lt;%LOGINIP%:33785&gt;
...
001 (006.000.000) 07/10 10:36:44 Job executing on host: &lt;%LOGINIP%:33785&gt;

...
005 (006.000.000) 07/10 10:38:10 Job terminated.
   (1) Normal termination (return value 0)
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
      Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
   0  -  Run Bytes Sent By Job
   0  -  Run Bytes Received By Job
   0  -  Total Bytes Sent By Job
   0  -  Total Bytes Received By Job
...
</pre>

If you weren't watching the DAGMan output file with tail -f, you can examine the file with the following command:

<pre class="screen">
$ <userinput>cat mydag.dag.dagman.out</userinput>

---+++ Cleaning Up
Clean up your results.  Be careful when deleting the =mydag.dag.*= to _not_ delete the =mydag.dag= file. Note the =.*=!
<!-- ***  End Comment                                    ***********    -->




%STOPINCLUDE%

<!--                                                                            
      * Set LOGINHOST = gridlab1
      * Set GRIDHOST = tg-login.ncsa.teragrid.org

      * Set OTHERHOST = gridlab2
-->

%BOTTOMMATTER%
-- Main.ForrestChristian - 25 Jan 2007 (from original Lecture 5) %BR% 
