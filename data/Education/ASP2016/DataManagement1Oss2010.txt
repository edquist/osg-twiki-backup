%META:TOPICINFO{author="TanyaLevshina" date="1278975286" format="1.1" reprev="1.7" version="1.7"}%
%META:TOPICPARENT{name="MaterialsOSS2010"}%
---+!!Data Management Exercises (Part I) 
%TOC{depth="2"}%

---++ Introduction
The goals of these exercises are...
---++ Exercises 
---+++ Prerequisite 
   * Login on submission node <pre class="screen">
<verbatim>
ssh <username>@vdt-itb.cs.wisc.edu
</verbatim>
</pre>
   * Setup grid enviroment<pre class="screen">
 . /opt/osg-client/setup.sh
</pre>
   * Obtain proxy certificate<pre class="screen">
voms-proxy-init -voms osgedu:/osgedu
Enter GRID pass phrase: 
</pre>

---+++ Classic Storage
A Classic Storage is represented by a stand-alone [[http://www.globus.org/toolkit/data/gridftp][!GridFTP]] server installed  on a Compute Element. It allows access to several specific data directories available on all OSG sites.    The storage space allocated under ==OSG_DATA== directory is intended as the space for applications to write input and output relatively small data files with persistency that must exceed the lifetime of the job which created it.

In order to upload data to ==OSG_DATA== you will need to use ==globus-url-copy== command. The basic syntax for globus-url-copy is:

==globus-url-copy [optional command line switches] Source_URL Destination_URL==

You will need to use the following URL prefixes to specify local file and file on the remote node:
   * file:// (on a local machine only)
   * gsiftp://

   * What is the path to OSG_DATA?<pre class="screen">
globus-job-run osg-edu.cs.wisc.edu:/jobmanager-fork /usr/bin/env|grep OSG_DATA
OSG_DATA=/nfs/osg-data
</pre>
   * Let's create your directory there<pre class="screen">
globus-job-run osg-edu.cs.wisc.edu:/jobmanager-fork /bin/mkdir /nfs/osg-data/osgedu/${USER}
</pre>
   * We will use ==Complete-Prose-Works-by-Walt-Whitman.txt== located in ~/tanya. The file size is 1.4 MB and contain works by Walt Whitman.  The file is downloaded from [[http://fliiby.com/file/221825/jodjt2p6sb.html][this site]], see also [[http://gutenberg.net][information about Gutenberg Project ]].  Let's use ==globus-url-copy== to upload the file<pre class="screen">
globus-url-copy file:///home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt gsiftp://osg-edu.cs.wisc.edu:2811/nfs/osg-data/osgedu/${USER}/testfile
 globus-job-run osg-edu.cs.wisc.edu:/jobmanager-fork /bin/ls -l /nfs/osg-data/osgedu/${USER}
total 1416
-rw-r--r--  1 osgedu users 1445762 Jul 12 13:17 Complete-Prose-Works-by-Walt-Whitman.txt
</pre>
   * Run simple condor-g jobs that handle data file differently:
    1. Transfer data with condor job. First, create a submission file ==test1==  <pre class="file">
universe=grid
grid_resource = gt2 red.unl.edu:/jobmanager-fork
Executable=word_counter
Log = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).log
Output = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).out
Error = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).err
WhenToTransferOutput = ON_EXIT
transfer_input_files =  Complete-Prose-Works-by-Walt-Whitman.txt
transfer_output_files = wordcounter.out.$(Cluster).$(Process)
Arguments =  Complete-Prose-Works-by-Walt-Whitman.txt grass
queue
</pre> It submits the python script ==word_counter== that counts the number of  encounters of the specified word in the text. It transfer the file with condor job. Now, we can submit job that is using file from ==$OSG_DATA/osgedu/${USER}== directory: <pre class="file">
universe=grid
grid_resource = gt2 red.unl.edu:/jobmanager-fork 
Executable=word_counter
Log = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).log
Output = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).out
Error = /home/tanya/whitman_logs/wordcounter.$(Cluster).$(Process).err
Arguments =  /nfs/osg-data/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt grass
queue
</pre>

  

---++++Challenges 
   *  !GridFTP allows to do  third-party transfer. Try to initiate a data transfer from a client running on vtb-itb to transfer data from Wisconsin to Nebraska
   * You can speed !GridFTP transfers: try to use bigger tcp window, memory buffer and parallel streams
---+++ Storage Element 
What you have to know before you start:
---++++ First steps
   * Verify that you can access storage<pre class="screen">
srm-ping srm://osg-edu.cs.wisc.edu:10443
srm-ping   2.2.1.3.12  Tue Apr 27 13:13:24 PDT 2010
BeStMan and SRM-Clients Copyright(c) 2007-2010,
Lawrence Berkeley National Laboratory. All rights reserved.
Support at SRM@LBL.GOV and documents at http://sdm.lbl.gov/bestman

SRM-CLIENT: SURL does not contains ?SFN 
SRM-CLIENT: serviceHandle /srm/v2/server is taken from the srmclient.conf 
SRM-CLIENT: SFN is assumed as 
SRM-CLIENT: Connecting to serviceurl httpg://osg-edu.cs.wisc.edu:10443/srm/v2/server

SRM-PING: Mon Jul 12 16:45:54 CDT 2010  Calling SrmPing Request...
versionInfo=v2.2

Extra information (Key=Value)
backend_type=BeStMan
backend_version=2.2.1.3.13
backend_build_date=2010-04-28T18:55:52.000Z 
gsiftpTxfServers[0]=gsiftp://osg-edu.cs.wisc.edu
clientDN=/DC=org/DC=doegrids/OU=People/CN=Tanya Levshina 508821
localIDMapped=osgedu

</pre>
   * Verify that you can upload file. First, create a directory <pre class="screen">
 srm-mkdir  srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/osgedu/tanya
srm-mkdir   2.2.1.3.12  Tue Apr 27 13:13:24 PDT 2010
BeStMan and SRM-Clients Copyright(c) 2007-2010,
Lawrence Berkeley National Laboratory. All rights reserved.
Support at SRM@LBL.GOV and documents at http://sdm.lbl.gov/bestman
SRM-CLIENT: Connecting to serviceurl httpg://red-srm1.unl.edu:8443/srm/v2/server

SRM-DIR: Mon Jul 12 17:26:02 CDT 2010 Calling SrmMkdir
SRM-DIR: DirectoryPath(0)=srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/osgedu/tanya
        status=SRM_SUCCESS
        explanation=null
</pre>

Copy file to this SE directory: <pre class="screen">
srm-copy file:///home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt 
srm-copy   2.2.1.3.12  Tue Apr 27 13:13:24 PDT 2010
BeStMan and SRM-Clients Copyright(c) 2007-2010,
Lawrence Berkeley National Laboratory. All rights reserved.
Support at SRM@LBL.GOV and documents at http://sdm.lbl.gov/bestman
SRM-CLIENT: Mon Jul 12 17:47:28 CDT 2010 Connecting to httpg://red-srm1.unl.edu:8443/srm/v2/server

SRM-CLIENT: Mon Jul 12 17:47:29 CDT 2010 Calling SrmPrepareToPutRequest now ...
request.token=put:754867
Request.status=SRM_SUCCESS
explanation=null

SRM-CLIENT: RequestFileStatus for SURL=file:///home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt is Ready.
SRM-CLIENT: received TURL=gsiftp://red-gridftp9.unl.edu:2811//mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt

SRM-CLIENT: Mon Jul 12 17:47:38 CDT 2010 start file transfer
SRM-CLIENT:Source=file:////home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt
SRM-CLIENT:Target=gsiftp://red-gridftp9.unl.edu:2811//mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt

SRM-CLIENT: Mon Jul 12 17:47:43 CDT 2010 end file transfer for file:///home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt

SRM-CLIENT: Mon Jul 12 17:47:43 CDT 2010 Calling putDone for srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt
Result.status=SRM_SUCCESS
Result.Explanation=null

SRM-CLIENT: Request completed with success

SRM-CLIENT: Printing text report now ...

SRM-CLIENT*REQUESTTYPE=put
SRM-CLIENT*TOTALFILES=1
SRM-CLIENT*TOTAL_SUCCESS=1
SRM-CLIENT*TOTAL_FAILED=0
SRM-CLIENT*REQUEST_TOKEN=put:754867
SRM-CLIENT*REQUEST_STATUS=SRM_SUCCESS
SRM-CLIENT*SOURCEURL[0]=file:///home/tanya/Complete-Prose-Works-by-Walt-Whitman.txt
SRM-CLIENT*TARGETURL[0]=srm://red-srm1.unl.edu:8443/srm/v2/server?SFN=/mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt
SRM-CLIENT*TRANSFERURL[0]=gsiftp://red-gridftp9.unl.edu:2811//mnt/hadoop/public/osgedu/tanya/Complete-Prose-Works-by-Walt-Whitman.txt
SRM-CLIENT*ACTUALSIZE[0]=0
SRM-CLIENT*FILE_STATUS[0]=SRM_SUCCESS
SRM-CLIENT*EXPLANATION[0]=SRM-CLIENT: PutDone is called successfully

</pre>
   * Run simple condor-g job that access this data from the worker node: <pre class="file">
</pre>
---+++ Running job on the Grid with SE







-- Main.TanyaLevshina - 09 Jul 2010
