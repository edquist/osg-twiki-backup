%META:TOPICINFO{author="CraigPrescott" date="1172251506" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="Sandbox.TierThreePrerequisiteDocument"}%
---++Introduction
PBS stands for "Portable  Batch System".  It is a popular networked subsystem for submitting, monitoring, and controlling a work load of batch jobs on one or more systems.  PBS has a long history, and is available nowadays in three flavors: 
   * <nop>OpenPBS - the original PBS developed for NASA in the early to mid-1990s, available as open source
   * PBS Pro - a commercial version of PBS from Altair Engineering
   * Torque - the open source successor to <nop>OpenPBS - http://clusterresources.com/downloads/torque

<nop>OpenPBS still works, and you can use it, but you should be aware that the focus of the open source development effort has moved on to Torque for some time now.  If you want to run <nop>OpenPBS and support is important to you, you may purchase it from Altair Engineering.  Personally, I am not aware of a technical argument to prefer <nop>OpenPBS to Torque.  

PBS Pro is a fine product.  It is reasonably priced compared to competing commercial batch systems and has dedicated support from Altair Engineering should you desire it.  We use it where I work, and I have no serious complaints.  The most recent releases of PBS Pro (versions 7.x and later) have features that <nop>OpenPBS and Torque do not, as well as a more flexible resource specification than its open source counterparts.  If I recall correctly, you should be able to get a trial version of PBS Pro before you buy - contact Altair for info.  Prior to versions 7.x, you could plug in the MAUI scheduler in place of the FIFO scheduler that is included with PBS Pro.  With versions 7.x and later, this no longer seems to work.

As mentioned previously, Torque is the open source PBS project which is being actively developed, and the user community has followed suit.  Iin its current 2.x versions, it has also matured into a quality product and should be more than capable of scaling to typical Tier3 cluster sizes.  In addition, you have the option of using the more flexible and open source MAUI scheduler in place of the FIFO scheduler included with Torque, should you wish it.

The above are my personal opinions.  In this document, I will specifically describe deployment and configuration of Torque and its FIFO scheduler as the batch system backing an an OSG computing element.  The configuration steps outlined below, however, should apply nearly equally well to <nop>OpenPBS or PBS Pro.

---+++Useful Links
It is always handy to have a few reference links at your fingertips, so I enclose a few here.  As always, Google is your friend and a wealth of information.

   * Cluster Resource's Torque page page: http://www.clusterresources.com/pages/products/torque-resource-manager.php
   * Altair's PBS Pro front page: http://www.altair.com/software/pbspro.htm
   * <nop>OpenPBS versus PBS Pro - "Which PBS is for you?": http://www.openpbs.org/which_pbs.html
   * <nop>OpenPBS Mini-HOWTO: http://dcwww.camp.dtu.dk/pbs.html

---++Obtaining Torque

Point your web browser to http://clusterresources.com/downloads/torque.  Select the most recent version and save it to a file (at the time of this writing, version 2.1.7 is just released).  Copy the tarball to the node you intend to use as your OSG computing element headnode.

---++Building Torque

If your head node runs an RPM-based Linux distribution and you don't want to build Torque yourself, you can skip this section and use some pre-built RPM packages I have prepared.  They should work on any RHEL4-compatible machine.  They may also work on other distributions - your mileage may vary.  The RPM packages for i386 and x86_64 are available here:
d34 1
   * http://lorien.phys.ufl.edu/~prescott/centos/4.3/contrib/i386/
   * http://lorien.phys.ufl.edu/~prescott/centos/4.4/contrib/x86_64/

As implied by the URLs, I built the i386 packages on a <nop>CentOS 4.3 machine, and the x86_64 packages on <nop>CentOS 4.4.  Download the packages appropriate for your architecture, copy them to your head node, and skip to "Installing Torque on the Head Node".
As implied by the URLs, I built the i386 packages on a <nop>CentOS 4.3 machine, and the x86_64 packages on <nop>CentOS 4.4.  Download the packages appropriate for your architecture, copy them to your head node, and skip to the next secction.

We have a couple of options in ways to build Torque: you can either build binaries as one normally does from a tarball, or you can build RPMs.  In my opinion, if you are running a  Red Hat/RPM-based Linux distribution, you should build the RPMs - having RPMs makes deploying Torque components on multiple machines easy, and the RPMs can likely be integrated into a cluster management tool like Rocks very trivially.

Unpack the tarball and change directory to the top level of the source tree.  You may do this as a normal user, if you like.    If you are *not* going to make RPMs, you will find it convenient later to do this in an NFS-shared area.  If you are going to make RPMs, it doesn't matter where you do this.

<verbatim>
tar xvzf torque-2.1.7.tar.gz
cd torque-2.1.7
</verbatim>

You may wish to glance at the <code>README.torque</code> and <code>README.configure</code> files at this point.  If you have built open source software before, you may also wish to examine the myriad of configuration options of the build by executing 

<verbatim>
./configure --help
</verbatim>

But you don't have to if you aren't too curious.  :-)

Some words about build dependencies: you will need a basic development environment installed in order to build Torque.  The GNU compiler collection is just what the doctor ordered; you need the C, C++, and Fortran 77 compilers installed to build everything.  Specifically, make sure you have the programs <code>gcc</code>, <code>g++</code>, and <code>g77</code> installed.  In addition, you should have SSH clients installed (does anyone use <code>rsh</code> anymore?); the configuration step prior to compilation will check to see if <code>scp</code> is available - if so, it will be used as the "remote copy program" used to relay job stdout and stderr back to the user from the compute nodes.  In addition, Torque comes with Tcl/Tk-based GUI programs monitor jobs and batch system status.  To build these, you will need a Tcl/Tk development environment, as well.

On an Red Hat-based machine, everything will be built if you have these packages (and their associated dependencies) installed:
   * make
   * gcc
   * gcc-g++
   * gcc-g77
   * gawk
   * glibc-devel
   * bison
   * flex
   * groff
   * openssh-clients
   * tcl
   * tcl-devel
   * tclx
   * tclx-devel
   * tk
   * tk-devel
   * xorg-x11-xauth

Probably you have (almost?) all this stuff installed already.

Ok, we have all of that installed and available.  Now we configure the source tree to prepare for building the package.  I am happy to accept the default installation path (you can control it with the <code>--prefix=...</code> option if you like), so I just say:

<verbatim>
./configure
</verbatim>

A bunch of test output will spew forth.  Ultimately, this step generates all the <code>Makefile</code>s needed to build the package.  Now we just say

<verbatim>
make
</verbatim>

or, if we are on a Red Hat-based machine, you should do *as root*:

<verbatim>
make rpm
</verbatim>

The former command will simply build binaries.  The latter command will build binary RPM packages.  I am on a Red Hat-based machine, so I use <code>make rpm</code>.  On any modern machine, this will take a few minutes at most.  In the end, I am left with the following files:

<verbatim>
/usr/src/redhat/RPMS/i386/torque-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-debuginfo-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-docs-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-scheduler-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-server-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-mom-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-client-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-gui-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-localhost-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-devel-2.1.7-1cri.i386.rpm
/usr/src/redhat/RPMS/i386/torque-pam-2.1.7-1cri.i386.rpm
</verbatim>

---++Installing Torque on the Head Node

Now we are ready to deploy some packages on our head node.  If you didn't create RPMs in the steps above, you need to *become root* on the head node and type <code>make install</code> from the top level of the Torque source tree.  Everything will be installed into <code>/usr/local</code> unless you specified an alternate installation path at <code>configure</code> time.

On the other hand, if you have RPMs read on.  On our head node, we at least want to install the Torque server and scheduler, and have commands available to submit, monitor, and remove jobs.  If you want your head node to be able to run batch jobs, you will also need to install the "MOM" package here.  You may also like to install the documentation and GUI tools.  To do all this, *become root*, go to the directory where the RPMs reside, and type:

<verbatim>
rpm -Uvh  torque-server-2.1.7-1cri.i386.rpm  torque-scheduler-2.1.7-1cri.i386.rpm torque-mom-2.1.7-1cri.i386.rpm \
torque-client-2.1.7-1cri.i386.rpm torque-docs-2.1.7-1cri.i386.rpm torque-gui-2.1.7-1cri.i386.rpm torque-2.1.7-1cri.i386.rpm
</verbatim>

---++ Installing Torque on the Compute Nodes
PBS_DAEMON="/usr/sbin/pbs_server -h <hostname> -S <hostname> -M <hostname>"
where <hostname> is the same hostname you used when you created the server instance.  Once this is done, you can restart the <code>pbs_server</code> from the <code>init.d</code> script:

<verbatim>
/etc/init.d/pbs_server restart
</verbatim>
d189 2
where <code>&lt;hostname&gt;</code> is the same hostname you used when you created the server instance.

We can now configure the server.  This is done with the <code>qmgr</code> command.  If <code>qmgr</code> is executed without any options, it will put you in an interactive shell from which you can just type in PBS commands.  But you can also feed commands to <code>qmgr</code> with the <code>-c</code> option.  Let's turn on scheduling, create a routing queue and an execution queue, and take care of some defaults:

<verbatim>
qmgr -c 'set server scheduling=true'
qmgr -c 'create queue defaultq'
qmgr -c 'set queue defaultq queue_type = route'
qmgr -c 'create queue batchq'
qmgr -c 'set queue batchq queue_type = execution'
qmgr -c 'set queue defaultq started = true'
qmgr -c 'set queue defaultq route_destinations = batchq'
qmgr -c 'set queue batchq queue_type = execution'
qmgr -c 'set queue defaultq enabled = true'
qmgr -c 'set server resources_default.nodes = 1
qmgr -c 'set queue batchq started = true'
qmgr -c 'set queue batchq enabled = true'
qmgr -c 'set server resources_default.nodes = 1'


where <code>&lt;hostname&gt;</code> is the same hostname you used for the <code>pbs_server</code> setup.  This is a dirty trick.  But the fact is that the built-in Torque scheduler will only listen on the interface that corresponds to the output of <code>gethostname</code>, so the gloves may have to come off.
/etc/init.d/pbs_mom start
</verbatim>
qmgr -c 'create node <fqdn> np=<ncpus>' 
set server node_check_rate = 150
The <code>qstat</code> command is used to display queue and job status.  By itself, if will print out the list of jobs that are in the queue, and their status.  <code>qstat</code> has a number of interesting options - see <code>man qstat</code> for more info.  Since we don't have any jobs running yet, <code>qstat</code> won't show us anything too interesting.
---+++ Submitting a Job

You use the <code>qsub</code> command to submit the job to the batch system; just give qsub the name of your job script, like so:

<verbatim>
---+++ Querying a Job
---++ Final Words

Hopefully this note will help you get your Torque batch system up and running, and give you a bit of familiarity with the typical procedures and tools available.  If you have further questions, I highly recommend to look at the Admin Manual and numerous man pages included with the Torque packages, and to consult the <code>torqueuser</code> mailing list (archives at http://www.supercluster.org/pipermail/torqueusers/).  Torque is highly configurable; in this short tutorial, we have only done enough to get you started.  While what we've done so far may be perfectly adequate for many environments, you should be aware that configuration options exist to add user and group ACLs, resource attributes handy for heterogenous environments, optimizations for job output relay, multiple execution queues with their own scheduling priorities, considerations for running parallel jobs, dropping in of powerful third party schedulers such as Maui, etcetera.  Good luck!

%META:TOPICMOVED{by="CraigPrescott" date="1172188993" from="Education.PBSSetup" to="Education.PbsSetup"}%
