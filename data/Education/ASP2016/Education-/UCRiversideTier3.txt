%META:TOPICINFO{author="FkW" date="1171658468" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="TierThreePrerequisiteDocument"}%
%TOC%

---+++ Hardware Deployed
   * 4 x 7TB Apple RAID arrays, served by Dual Opteron single core with 4GB RAM each. Call this AS1-4.
   * 1 x Warewulf headnode. Call this WH1.
   * 10 x Warewulf worker nodes. Dual Dual Opteron with 4GB RAM each. Call this WW1-10.
   * Private Gbit LAN connecting all nodes.
   * Public Gbit LAN connecting WH1 and AS1-4. 

---+++ Services Deployed at hosts
   * Condor batch system hosted by WH1. I.e. collector and negotiator are run on WH1.
   * PhEDEx for CMS data transfers hosted by AS1.
   * OSG CE hosted by AS2. This includes CEMon, pre-WS GRAM (incl. gftp that comes with it), schedd for condor batch system, etc..
   * GUMS hosted by AS2.
   * GT4 gridftp server for filling data disks from PhEDEx hosted by AS3.
   * CMS data areas hosted by AS1-3.
   * local user data hosted by AS4.
   * 4 batch slots on each WW1-10.
   * NFS exported from each of AS1-4. Read only to WW1-10. Read/write to AS1-4.
   * OSG_APP exported read only from AS3 to all WW1-10, and read/write to AS1-4.
   * No OSG_DATA is implemented.

---+++ Policies implemented
   * A UCR [[http://hepuser.ucsd.edu/twiki/bin/view/UCSDTier2/UCSDCondorGroupConfig][group in condor]] that has first access to all batch slots that open up.
   * Other than that, all are allowed by default but may be halted or killed at any time.
   * A maximum wall clock time of 48 hours is strictly enforced.


-- Main.FkW - 16 Feb 2007

