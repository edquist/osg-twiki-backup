%META:TOPICINFO{author="X509_2fDC_3dorg_2fDC_3ddoegrids_2fOU_3dPeople_2fCN_3dMats_20Rynge_20274484" date="1291731645" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="Brazil2011"}%
---+ Glidein for the users - Hands-on Session, Wednesday July 21st, 2010 

This session will give you hands-on experience in using the glideinWMS as a user.

---++ Local setup

As with all the other hands-on exercises, we will be using *submit03.ncc.unesp.br* to submit and monitor our jobs.

However, we will use *a different Condor instance running on that same machine*.
The reason for having a different instance is due to the security requirements
of having a Condor pool spread across the WAN.
Interested students can talk to me to get a more detailed information,
for all the others, you just need to make sure you point to the right Condor installation.

To use the proper Condor instance, please run:
<pre class="screen">
$ source /opt/glidecondor/condor.sh 
</pre>

At any given point, you can check you are using the right one by using:
<pre class="screen">
$ which condor_submit
/opt/glidecondor/bin/condor_submit
$ echo $CONDOR_CONFIG
/opt/glidecondor/etc/condor_config
</pre>

---++ The work environment

As mentioned in the lecture, the glideinWMS environment looks almost exactly like a regular, local Condor pool.

It just does not have any resources attached unless you ask for them;
try
<pre class="screen">
condor_status
</pre>

The glideinWMS will submit glideins on your behalf when you will need them.
But you may need to tell it what are your needs (but more on this later on).

---++ Generic jobs

Let's start with a very generic job;<br>
a variation of the basic Condor jobs Alain introduced you to.

Let us [[http://www.stealthcopter.com/blog/2009/09/python-calculating-pi-using-random-numbers/][calculate Pi using the monte carlo method]];<br>
create a file called *pi.py* containing:
<pre class="file">
#!/bin/env python
from random import *  
from math import sqrt,pi  
from sys import argv
inside=0  
n=int(argv[1])
for i in range(0,n):  
    x=random()  
    y=random()  
    if sqrt(x*x+y*y)<=1:  
        inside+=1  
pi_prime=4.0*inside/n  
print pi_prime, pi-pi_prime
</pre>
and make it executable.

Try to run it:
<pre class="screen">
./pi.py 1000000
</pre>
The first number is the approximation of pi, while the second one is how far from the real Pi it is.

Repeat a couple of times.
As you can see, the result changes every time.

Now, lets submit it as a Condor job;
or better to say, as a bunch of Condor jobs.

These jobs should run everywhere, so no need to specify any requirement:
<pre class="file">
Universe   = vanilla
Executable = pi.py
Arguments  = 10000000
Requirements = (Arch=!="")
Log        = job.$(Cluster).log
Output   = job.$(Cluster).$(Process).out
Error      = job.$(Cluster).$(Process).err
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT
Queue 100
</pre>

Submit the above.

Now sit back and relax... Condor+glideinWMS will take care of everything else... 
after a few minutes we just look at the outputs and see the different results.

So run <tt>condor_q</tt> and <tt>condor_status</tt> from time to time, until the jobs are done.

What do you see?

---++ Understanding where jobs are running

While your jobs can run everywhere, you may still want to know where they actually ran;
either becuase you want to know who to thank for the CPUs you were consuming,
or to debug problems you had with your program (unlikely in this case... but one never knows).

So let us add a couple additional attributes to the submit file:
<pre class="file">
Universe   = vanilla
Executable = pi.py
Arguments  = 50000000
Requirements = (Arch=!="")
Log        = job.$(Cluster).log
Output   = job.$(Cluster).$(Process).out
Error      = job.$(Cluster).$(Process).err
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT
+JOB_Site = "$$(GLIDEIN_Site:Unknown)"
+JOB_Gatekeeper = "$$(GLIDEIN_Gatekeeper:Unknown)"
Queue 100
</pre>

Now submit the job cluster.

Now monitor the running jobs with
<pre class="screen">
condor_q `id -un` -const 'JobStatus==2' -format '%d.' ClusterId -format '%d ' ProcId -format '%s\n' MATCH_EXP_JOB_Site
</pre>

What do you see?

For completed jobs, you can use
<pre class="screen">
condor_history JOB_ID -format '%d.' ClusterId -format '%d ' ProcId -format '%s\n' MATCH_EXP_JOB_Site
</pre>

Similarly, you can monitor where the available resources are by using:
<pre class="screen">
condor_status -format "%-40s\t" Name -format "%s\t" GLIDEIN_Site -format "%s\n" State
</pre>

---++ BLAST jobs

Let us now run a set of BLAST jobs, similarly the way you did yesterday.

BLAST jobs will however not run everywhere... only a subset of Grid site have BLAST installed.

So we need to tell the glideinWMS backend we want to run only on sites with BLAS installed.
In this particular glideinWMS installation, we do this by adding this attribute:
<pre class="file">
+NeedBLAST=True
</pre>

Moreover, we have to tell the Negotiator that we want the jobs only to run on sites that support BLAST;
while we may not be requesting glideins on sites without BLAST, someone else might, and we do not want to run there.

The glideins in this setup will be publishing the HasBLAST attribute.
So let's add these requirements to your submit file:
<pre class="file">
Requirements=(Arch=!="") && (HasBLAST=?=True)
</pre>


*FIXME*


Now submit a job cluster and monitor it as before.

Where did they run?

---++ Mixing them up

Let's now consider what happens when you have jobs with different requirements.

Submit a few clusters of BLAST jobs, and a few of the Pi jobs, possibly intermixing them.

Where did they run?

In which order?


-- Main.MatsRynge - 03 Dec 2010
