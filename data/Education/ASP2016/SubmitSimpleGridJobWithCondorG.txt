%META:TOPICINFO{author="ForrestChristian" date="1174756907" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="LectureFiveTutorial"}%
<link rel="stylesheet" type="text/css" href="%PUBURL%/%WEB%/WorkshopTutorialModules/exercises.css">

---+!! %SPACEOUT{ "%TOPIC%" }%


%STARTINCLUDE%
%EDITTHIS%

Now we are ready to submit our first job with Condor-G.  The basic procedure is to create a Condor job submit description file.  This file can tell Condor what executable to run, what resources to use, how to handle failures, where to store the job's output, and many other characteristics of the job submission.  Then this file is given to condor_submit.

There are many options that can be specified in a Condor-G submit description file.  We will start out with just a few. We'll be sending the job to the computer "%LOGINHOST%" and running under the "jobmanager-fork" job manager.  We're setting notification to never to avoid getting email messages about the completion of our job, and redirecting the stdout/err of the job back to the submission computer. 

For more information, see the [[http://www.cs.wisc.edu/condor/manual/v6.9/condor_submit.html][condor_submit manual]].


%NOTE% Feel free to use your favorite editor, but we will demonstrate with *cat* in the example below.  When using cat to create files, press *Ctrl+D* to close the file &mdash; don't actually type =[Ctrl+D]= into the file.  Whenever you create a file using cat, we suggest you use cat to display the file and confirm that it contains the expected text.

---+++ Create the Submit File
Move to our scratch submission directory and create the submit file. Verify that it was entered correctly:

<pre class="screen">
$ <userinput>cd ~/condor-tutorial/submit</userinput>
$ <userinput>cat &gt; myjob.submit
executable=myscript.sh
arguments=TestJob 10
output=results.output
error=results.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 %LOGINHOST%/jobmanager-fork
queue
<em>[Ctrl+D]</em></userinput>
$ <userinput>cat myjob.submit</userinput>

executable=myscript.sh
arguments=TestJob 10
output=results.output
error=results.error
log=results.log
notification=never
universe=grid
grid_resource=gt2 %LOGINHOST%/jobmanager-fork
queue
</pre>


---+++ Create the Program
Create a small program to run on the grid:

<pre class="screen">
$ <userinput>cat &gt; myscript.sh
#! /bin/sh

echo "I'm process id $$ on" `hostname`
echo "This is sent to standard error" 1&gt;&2
date
echo "Running as binary $0" "$@"
echo "My name (argument 1) is $1"
echo "My sleep duration (argument 2) is $2"
sleep $2
echo "Sleep of $2 seconds finished.  Exiting"
echo "RESULT: 0 SUCCESS"
<em>[Ctrl+D]</em> </userinput>
$ <userinput>cat myscript.sh</userinput>
#! /bin/sh

echo "I'm process id $$ on" `hostname`
echo "This is sent to standard error" 1&gt;&2
date
echo "Running as binary $0" "$@"
echo "My name (argument 1) is $1"
echo "My sleep duration (argument 2) is $2"
sleep $2
echo "Sleep of $2 seconds finished.  Exiting"
echo "RESULT: 0 SUCCESS"
</pre>

---+++ Test the program
Make the program executable and test it.

<pre class="screen">
$ <userinput>chmod a+x myscript.sh</userinput>
$ <userinput>./myscript.sh TEST 1</userinput>
I'm process id 3428 on %LOGINHOST%
This is sent to standard error
Thu Jul 10 12:21:11 CDT 2006
Running as binary ./myscript.sh TEST 1
My name (argument 1) is TEST
My sleep duration (argument 2) is 1
Sleep of 1 seconds finished.  Exiting
RESULT: 0 SUCCESS
</pre>

---+++  Submit your test job to Condor-G.

<pre class="screen">
$ <userinput>condor_submit myjob.submit</userinput>

Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 1.
</pre>

Run [[http://www.cs.wisc.edu/condor/manual/v6.9/condor_q.html][condor_q]] to see the progress of your job.  You may also want to run =condor_q&nbsp;-globus= at regular intervals to see Globus-specific status information. (See the [[http://www.cs.wisc.edu/condor/manual/v6.9/condor_q.html][condor_q manual]] for more information.)

<pre class="screen">
$ <userinput>condor_q</userinput>


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   %LOGINNAME%         7/10 17:28   0+00:00:00 I  0   0.0  myscript.sh TestJo

1 jobs; 1 idle, 0 running, 0 held
$ <userinput>condor_q -globus</userinput>


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   %LOGINNAME%       UNSUBMITTED fork     %OTHERHOST%   %HOMEDIR%/cond
$ <userinput>condor_q</userinput>

-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   %LOGINNAME%         7/10 17:28   0+00:00:27 R  0   0.0  myscript.sh TestJo

1 jobs; 0 idle, 1 running, 0 held
$ <userinput>condor_q -globus</userinput>


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:35688&gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   %LOGINNAME%       ACTIVE fork     %OTHERHOST%   %HOMEDIR%/cond
$ <userinput>condor_q</userinput>

-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:33785&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
   1.0   %LOGINNAME%         7/10 17:28   0+00:00:40 C  0   0.0  myscript.sh       

0 jobs; 0 idle, 0 running, 0 held
$ <userinput>condor_q -globus</userinput>

-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:33785&gt; : %LOGINHOST%
 ID      OWNER          STATUS  MANAGER  HOST                EXECUTABLE        
   1.0   adesmet       DONE fork     %OTHERHOST%   %HOMEDIR%/cond

$ <userinput>condor_q</userinput>


-- Submitter: %LOGINHOST% : &lt;%LOGINIP%:33785&gt; : %LOGINHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               

0 jobs; 0 idle, 0 running, 0 held
</pre>



---+++ Monitoring Progress with =tail=
In another window, run =tail -f= on the log file for your job to monitor progress. Re-run =tail= when you submit one or more jobs throughout this tutorial. You will see how typical Condor-G jobs progress. Use *<userinput>Ctrl+C</userinput>* to stop watching the file.

<pre class="screen">
$ <userinput>cd ~/condor-tutorial/submit</userinput>
$ <userinput>tail -f --lines=500 results.log</userinput>
000 (001.000.000) 07/10 17:28:48 Job submitted from host: <%LOGINIP%:35688>
...
017 (001.000.000) 07/10 17:29:01 Job submitted to grid resource
    GridResource: gt2 %LOGINHOST%/jobmanager-fork
    GridJobId: gt2 %LOGINHOST%/jobmanager-fork https://%LOGINHOST%:51277/31413/1174756212/
...
001 (001.000.000) 07/10 17:29:01 Job executing on host: gt2 %LOGINHOST%/jobmanager-fork
...
005 (001.000.000) 07/10 17:30:08 Job terminated.
        (1) Normal termination (return value 0)
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage
                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage
        0  -  Run Bytes Sent By Job
        0  -  Run Bytes Received By Job
        0  -  Total Bytes Sent By Job
        0  -  Total Bytes Received By Job
...

</pre>



---+++ Verifying completed jobs
When the job is no longer listed in condor_q, or when the log file reports "Job terminated," the results can be viewed using the [[http://www.cs.wisc.edu/condor/manual/v6.9/condor_history.html][condor_history]] command:

<pre class="screen">
$ <userinput>condor_history</userinput>
 ID      OWNER            SUBMITTED     RUN_TIME ST   COMPLETED CMD
   1.0   %LOGINNAME%         7/10 10:28   0+00:00:00 C   ???        %HOMEDIR%/cond
</pre>

When the job completes, verify that the output is as expected. The binary name is different from what you created because of how Globus and Condor-G cooperate to stage your file to execute computer.

<pre class="screen">
$ <userinput>ls</userinput>
myjob.submit  myscript.sh*  results.error  results.log   results.output
$ <userinput>cat results.error</userinput>
This is sent to standard error
$ <userinput>cat results.output </userinput>
$I'm process id 733 on %LOGINHOST%
Thu Jul 10 17:28:57 CDT 2003
Running as binary %HOMEDIR%/.globus/.gass_cache/local/md5/28/fcae5001dbcd99cc476984b4151284/md5/af/355c4959dc83a74b18b7c03eb27201/data TestJob 10
My name (argument 1) is TestJob
My sleep duration (argument 2) is 10
Sleep of 10 seconds finished.  Exiting
RESULT: 0 SUCCESS
</pre>

If you didn't watch the results.log file with =tail -f=, you will want to examine the logged information:

<pre class="screen">
$ <userinput>cat results.log </userinput>
</pre>



---+++ Submitting a job to other hosts
%OTHERHOST% is also running a little Condor pool. Try submitting a job to it through Condor-G and Globus. 

Create a new submit file:

<pre class="screen">
$ <userinput>cat &gt; myjob2.submit
executable=myscript.sh
arguments=TestJob 10
output=results2.output
error=results2.error
log=results2.log
notification=never
universe=grid
grid_resource=gt2 %OTHERHOST%/jobmanager-condor
queue
<em>[Ctrl+D]</em></userinput>
$ <userinput>cat myjob2.submit</userinput>
executable=myscript.sh
arguments=TestJob 10
output=results2.output
error=results2.error
log=results2.log
notification=never
universe=grid
grid_resource=gt2 %OTHERHOST%/jobmanager-condor
queue
</pre>

Notice that the setting for the =grid_resource= now refers to =condor= instead of =fork=. Globus will submit the job to Condor on %OTHERHOST% instead of running the job directly.

Submit the job to Condor-G:

<pre class="screen">
$ <userinput>condor_submit myjob2.submit</userinput>
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 2.
</pre>

You can monitor the job's progress just like the first job. If you log into %OTHERHOST% in another window, you can see your job in the Condor queue there. Be quick, or the job will finish before you look!

<pre class="screen">
$ <userinput>ssh %OTHERHOST%</userinput>
%LOGINNAME%@%OTHERHOST%'s password: 
$ <userinput>condor_status</userinput>

Name          OpSys       Arch   State      Activity   LoadAv Mem   ActvtyTime

vm100@clu1.ph LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:35
vm10@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:33
vm11@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:34
vm12@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:35
...
vm99@clu1.phy LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:34
vm9@clu1.phys LINUX       INTEL  Unclaimed  Idle       0.000     9  0+00:03:32

                     Machines Owner Claimed Unclaimed Matched Preempting

         INTEL/LINUX      100     0       0       100       0          0

               Total      100     0       0       100       0          0
$ <userinput>condor_q</userinput>

-- Submitter: %OTHERHOST% : <%LOGINIP%:36311> : %OTHERHOST%
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  11.0   %LOGINNAME%         7/10 23:04   0+00:00:00 I  0   0.0  data TestJob 10

1 jobs; 1 idle, 0 running, 0 held
</pre>

Clean up the results after the second job has finished running:

<pre class="screen">
$ <userinput>rm results.* results2.*</userinput>
</pre>

%STOPINCLUDE%

<!--                                                                            
      * Set LOGINHOST = gridlab1
      * Set GRIDHOST = tg-login.ncsa.teragrid.org
      * Set OTHERHOST = gridlab2
-->

%BOTTOMMATTER%
-- Main.ForrestChristian editing original Lecture 5 %BR%
