%META:TOPICINFO{author="BrianLin" date="1437920080" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="UserSchool15Materials"}%
---+ Tuesday Exercise 2.1: Hardware Differences in the OSG

The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW–Madison) and an OSG glidein pool. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different.

In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one requests; we call this a _parameter sweep_, in that we are testing many possible values of a parameter. We will request memory from 1&ndash;8 GB, in increments of 1 GB. One set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run.

---++ Part 1: Checking on the Availability of Memory, Locally

In this first part, you will create the submit files for both the local and OSG jobs, then submit the local set.

---+++ Create the submit files

To create our parameter sweep, we will create 8 submit files, each differing only in the value of our parameter, =request_memory=.

   1. If not already, log on to =osg-ss-submit=
   1. Create and change into a new subdirectory called =tuesday-2.1= &ndash; doing things this way will make Part 2 much easier
   1. Create a submit file that is named =sleep-1gb.sub= and that executes the command =/bin/sleep 10=\
       <p>If you do not remember all of the submit statements to write this file, or just to go faster, find a similar submit file from yesterday, copy and rename it here, and make sure the argument to =sleep= is =10= (although the exact value does not matter for the test).</p>
   1. Request exactly 1 gigabyte (GB) of memory:\
       <pre class="file">request_memory = 1GB</pre>
   1. Set the submit file to run 5 instances of this job:\
       <pre class="file">queue 5</pre>
   1. Save the submit file and exit your editor
   1. Make seven copies of =sleep-1gb.sub=: =sleep-2gb.sub=, =sleep-3gb.sub=, &hellip;, =sleep-8gb.sub=
   1. Edit each copy so that the =request_memory= setting matches the filename: =2GB=, =3GB=, &hellip;, =8GB=

<strong>Note:</strong> It is also possible to set up this parameter sweep using DAGMan, VARS, and a single submit file. If you would like the extra challenge, do this version instead of or in addition to the one above!

---+++ Submitting locally

We want to watch how long it takes our different =request_memory= jobs to run, so we would like to submit all 8 submit files as close to each other as possible.

   1. Submit all 8 sets of jobs quickly:\
       <pre class="screen">%UCL_PROMPT_SHORT% <strong>for i in 1 2 3 4 5 6 7 8; do condor_submit sleep-${i}gb.sub; done</strong></pre>

---+++ Monitoring the local jobs

Every few minutes, run =condor_q= and see how your sleep jobs are doing. Consider making a little table like the one below to track progress.

| *Memory* | *Cluster* | *Done #1* | *Done #2* | *Done #3* |
| 1 GB | 11701 | 0 | 4 | |
| 2 GB | 11702 | 0 | 3 | |
| 3 GB | 11703 | 1 | 4 | |
| 4 GB | 11704 | 0 | 5 | |
| 5 GB | 11705 | 1 | 4 | |
| 6 GB | 11706 | 0 | 4 | |
| 7 GB | 11707 | 0 | 4 | |
| 8 GB | 11708 | 0 | 3 | |

In the meantime, between checking on your local jobs, start Part 2 &ndash; taking a break every few minutes to record progress on your local jobs.

---++ Part 2: Checking on the Availability of Memory, Remotely

For the second part of the exercise, you will just copy over the directory from part 1 on =osg-ss-submit= to =osg-ss-glidein-submit= and resubmit your jobs to the OSG. You should have plenty of experience copying over files with =scp= and submitting them to the OSG so I won't provide instructions here. If you get stuck at any point in this process, refer to [[Education.UserSchool15Tue12LoginScp][exercise 1.2]].

---+++ Monitoring the remote jobs

As you did in part 1, use =condor_q= to track how your sleep jobs are doing. You can move onto the next exercise but keep tracking the status of your jobs. After you are done with the rest of the exercises, come back to this exercise, and move onto part 3

---++ Part 3: Analyzing the Results


%META:TOPICMOVED{by="BrianLin" date="1437683231" from="Education.UserSchool15Wed21GlideinDiff" to="Education.UserSchool15Tue21HardwareDiff"}%
