%META:TOPICINFO{author="BrianBockelman" date="1266808191" format="1.1" version="1.11"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---++ Introduction
The OSG compute element provides a mechanism for launching job on remote clusters; OSG also provides a common job environment and software tools these jobs will be able to access when running on the worker node.  These job-level tools are often essential for developers building higher level services on the OSG.

This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.

The OSG provides no scientific software dependencies or software build tools on the worker node; you will be expected to package all your application-level dependencies yourself.  Sites are not required to provide any specific tools (gcc, svn, lapack, blas, etc) beyond the ones in the OSG worker node client.  Specific VOs which own their own resources may require their sites to install additional software, but opportunistic VOs cannot do this as a rule.

---++ Common software available on worker nodes.

The OSG worker node client (referred to as the =wn-client= package) has the following software:

   1 The site's supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up).
   1 Basic VDT underlying infrastructure.
   1 Proxy management tools:
      * Create proxies: =voms-proxy-init= and =grid-proxy-init=
      * Show proxy info: =voms-proxy-info= and =grid-proxy-info=
      * Destroy the current proxy: =voms-proxy-destroy= and =grid-proxy-destroy=
   1 Data transfer tools:
      * HTTP/plain FTP protocol tools:
         * =wget= and =curl=: standard tools for downloading files with HTTP and FTP.
      * SRM clients
         * LCG SRM Client (=lcg-cp= and others)
         * LBNL SRM Client (=srm-copy= and others)
         * FNAL SRM Client (=srmcp= and others)
      * !GridFTP client
         * Globus !GridFTP client (=globus-url-copy=)
         * !UberFTP, another command-line client for !GridFTP.  Covers a wider variety of the !GridFTP protocol than just copying.
      * Site-specific protocols.
         * DCache client: =dccp=.  This client is specifically for sites running !dCache, about 5-10 out of 80 OSG sites.
   1 The Pegasus worker node software
   1 !MyProxy. The full set of !MyProxy client tools.

Advanced users can read the Pacman source code to see directly whats in the =wn-client= package for this release, OSG-%VERSION%:
   * http://software.grid.iu.edu/pacman/wn-client.pacman

---++ Directories in the Worker Node Environment
The following table outlines the various important directories for the worker node environment.  You can find the directory by referring to the corresponding environment variable.

%EDITTABLE{ header="| *Environment Variable* | *What you can do* | *Notes* |" format="| text | textarea, 2x30 | textarea, 2x30 |" changerows="on" }%
| *Environment Variable* | *What you can do* | *Notes* |
| =$OSG_APP= | Install application software releases into via the CE. | Access to this area varies from site-to-site.  Most sites allow writes only from the head node, while others require you to access it via a special job description of VOMS role. |
| =$OSG_DATA= | Data files that are accessible via NFS from all batch slots, read-write. | %RED% Not all OSG sites have deployed this area %ENDCOLOR% |
| =$OSG_GRID= | Set of "client tools" that are part of the OSG software stack. | The software tools referenced above are found here. |
| =$OSG_WN_TMP= | Temporary storage area in which your job(s) run | Local to each batch slot |

%NOTE% Be careful with using $OSG_WN_TMP.  This is the top-level directory, and might be shared with other VOs.  We recommend the following code (suppose =gpn= is your VO's name):

<pre class="screen">
<userlisting>
mkdir -p $OSG_WN_TMP/gpn
export mydir=`mktemp -d -t gpn`
cd $mydir
# Run the rest of your application
rm -rf $mydir
</userlisting>
</pre>

A significant number of sites use the batch system to make an independent directory for each user job, and change $OSG_WN_TMP on the fly to point to this directory.

There is no way to know in advance how much scratch disk space any given worker node has available, as OSG information systems don't advertise this.  Most of the times, it is shared among a number of job slots.

---++ Input and Output Files Specified via Condor-G
Condor-G has the following attributes related to file transfers and your jobs:

| =executable= | one file |
| =output= | one file - stdout | 
| =error= | one file - stderr |
| =transfer_input_Files= | comma separated list of files |
| =transfer_output_files= | comma separated list of files |

Do not use =transfer_input_files= or =transfer_output_files= to transfer more than a few megabytes: these files are transfered via the CE headnode and can cause serious loads, which can bring down the cluster.  Space on the headnode also tends to be limited, especially as some sites severely quota your home directory.  For large amount of file movement, we recommend pre-staging as covered by [[Storage/WebHome#Storage_for_the_End_User][the OSG-Storage documentation]].

---++ Finding your way around
%NOTE% A definition of the concepts used in this document can be found in [[LocalStorageConfiguration][Local Storage Configuration]], including a section describing minimal requirements and some sample configurations.

Here we describe how to find the various storage locations. We start by describing how to find things after your job starts, and then complete the discussion by describing what you can determine from the outside, before you submit a job to the site. We deliberately do not describe what these various storage implementations are. That is done in the [[LocalStorageConfiguration][Local Storage Configuration]] document.

---++++ Setting Up $OSG_GRID
All OSG sites should be configured so that the $OSG_GRID environment variable is already defined when your job starts running.  The first command any job should execute after it is started in a batch slot at an OSG site is:

<pre class="screen">
$ <userinput>source $OSG_GRID/setup.sh</userinput>
</pre>
=setup.sh= sets up several environment variables and configures certain client tools, such as srmcp, and adds them to the job's path. 
The following is output from a test run of a job with the scripts configured correctly:
<br/>
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="screen">
<userinput>$ globus-job-run fgitb-gk/jobmanager-condor /usr/bin/printenv</userinput>
_CONDOR_ANCESTOR_28668=28669:1190819791:495265920
_CONDOR_ANCESTOR_28669=25909:1195590358:2044699779
_CONDOR_ANCESTOR_25909=25915:1195590361:2072113792
HOME=/grid/home/fnalgrid
OSG_WN_TMP=/local/stage1
OSG_DATA=/grid/data
LD_LIBRARY_PATH=/usr/local/vdt-1.8.1/apache/lib:/usr/local/vdt-1.8.1/MonaLisa/Service/VDTFarm/pgsql/lib:/usr/local/vdt-1.8.1/glite/lib:/usr/local/vdt-1.8.1/prima/lib:/usr/local/vdt-1.8.1/mysql/lib/mysql:/usr/local/vdt-1.8.1/jdk1.5/jre/lib/i386:/usr/local/vdt-1.8.1/jdk1.5/jre/lib/i386/server:/usr/local/vdt-1.8.1/jdk1.5/jre/lib/i386/client:/usr/local/vdt-1.8.1/berkeley-db/lib:/usr/local/vdt-1.8.1/expat/lib:/usr/local/vdt-1.8.1/globus/lib:
GRID3_SITE_NAME=FNAL_FERMIGRID_ITB
GRID3_TMP_WN_DIR=/local/stage1
GRID3_TMP_DIR=/grid/data
OSG_LOCATION=/usr/local/vdt-1.8.1
OSG_HOSTNAME=fgitb-gk.fnal.gov
OSG_STORAGE_ELEMENT=y
OSG_JOB_CONTACT=fgitb-gk.fnal.gov/jobmanager-condor
OSG_APP=/grid/app
GRID3_DATA_DIR=/grid/data
GRID3_BASE_DIR=/usr/local/vdt-1.8.1
OSG_DEFAULT_SE=FNAL_FAPL_ITB_SE
OSG_GRID=/usr/local/grid
LOGNAME=fnalgrid
OSG_SQUID_LOCATION=fermigrid4.fnal.gov
GLOBUS_GRAM_JOB_CONTACT=https://fgitb-gk.fnal.gov:49005/11503/1195590354/
GLOBUS_LOCATION=/usr/local/grid/globus
OSG_SITE_NAME=FNAL_FERMIGRID_ITB
GLOBUS_GRAM_MYJOB_CONTACT=URLx-nexus://fgitb-gk.fnal.gov:49006/
OSG_SITE_WRITE=srm://fapl032.fnal.gov:8443
GRID3_APP_DIR=/grid/app
X509_USER_PROXY=/grid/home/fnalgrid/.globus/job/fgitb-gk.fnal.gov/11503.1195590354/x509_up
OSG_SITE_READ=dcap://fapl035.fnal.gov:24525
_CONDOR_SCRATCH_DIR=/local/stage1/condor/execute/dir_25909
_CONDOR_SLOT=1
_CONDOR_HIGHPORT=65535
_CONDOR_LOWPORT=61440
</pre>
%ENDTWISTY%
<br/>
See the [[ComputeElementInstall][CE Install Guide]] and [[LocalStorageConfiguration][Local Storage Configuration]] for more information. 

---++++ !!Getting Information About CE Storage Before You Submit a Job
You can retrieve the storage information for some sites by querying the OSG information services.  We do not recommend using this method - instead, have the job discover the correct directory through its environment.  This will not work reliably at all sites.

We will illustrate this below using !ReSS.

%TWISTY{
mode="div"
showlink="Show unrecommended query method..."
hidelink="Hide queries"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
The following query looks for storage directories for sites that support CMS (change "VO:cms" with the name of your VO):
<pre class="screen">
<userinput>condor_status -pool osg-ress-1.fnal.gov -const 'stringListIMember("VO:cms", GlueCEAccessControlBaseRule)' -format 'CE: %s ' GlueCEUniqueID -format 'OSG_WN_SCRATCH: %s ' GlueSubClusterWNTmpDir -format 'OSG_DATA: %s ' GlueCEInfoDataDir -format 'OSG_APP: %s\n' GlueCEInfoApplicationDir | sort | uniq</userinput>
</pre>
The reason the query is so long is due to the extensive format strings.  The above query shows the scratch, app, and data directories - one line per CE.  Here's a few lines of output:
<pre class="screen">
CE: antaeus.hpcc.ttu.edu:2119/jobmanager-sge-cms Scratch: /state/partition1 OSG_DATA: /lustre/hep/osg OSG_APP: /lustre/home/antaeus/apps
CE: antaeus.hpcc.ttu.edu:2119/jobmanager-sge-serial Scratch: /state/partition1 OSG_DATA: /lustre/hep/osg OSG_APP: /lustre/home/antaeus/apps
CE: belhaven-1.renci.org:2119/jobmanager-condor-default Scratch: /condor/osg_wn_tmp OSG_DATA: /nfs/osg-data OSG_APP: /nfs/osg-app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmshi Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmsprod Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmsuser Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_monitor Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_monitor Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_priority Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_production Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_user Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_monitor Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_priority Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_production Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_user Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cms-0.mps.ohio-state.edu:2119/jobmanager-condor-default Scratch: /hadoop/tmp OSG_DATA: /data/se/osg OSG_APP: /sharesoft/osg/app
</pre>

%ENDTWISTY%

%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.StevenTimm - 18 Oct 2007 %BR%
%REVIEW% Main.RobGardner - 09 Nov 2007 %BR%
%REVCOM% What can be done about the red warnings?  (pound on each site individually where the inconsistency is discovered.)