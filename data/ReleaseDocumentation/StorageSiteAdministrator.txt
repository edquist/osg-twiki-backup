%META:TOPICINFO{author="TedHesselroth" date="1266526290" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<noop>%SPACEOUT{ "Storage for the Site Administrator" }%*
https://twiki.grid.iu.edu/twiki/pub/Storage/WebHome/images.jpg
%TOC{depth="2"}%


---++Planning and Configuration

For descriptions of the specific software discussed below, please see the [[Storage.WebHome][parent topic]] of this page. 

---+++Choosing the storage software

Each site may chose what storage software to deploy. If you can anticipate usage patterns, you can more effectively plan your system. To see a summary of data movement patterns, see [[Storage.WebHome#IntroStorageEndUser][Storage for the End User]]. Based on expected job runtime and data movement profiles, the following are rules of thumb for addressing this issue. Further information and installation instructions are provided in succeeding sections. 

   * *Shared file system.* If the jobs running on your site need less than a few terabytes of storage and have lightweight, local data transfer requirements, a NFS filesystem shared among worker nodes would be sufficient. In further documentation, this option is covered in the "Local Storage" section, since it must be configured along with additional required storage that resides on the worker node itself.

   * *gridftp server.* If access to authorized off-site entities is required and storage size and data-handling capacities are moderate, a gridFTP server should be deployed. Since Storage Elements also include bundled gripFTP servers, consider the cost/benefit of installing a Storage Element in this case. A Storage Element may be somewhat more complex to set up, but offers benefits in throughput and expandability.

   * *xrootd.* This storage software would be installed if it is known that user applications require it. Some Storage Element implementations are compatible with xrootd; Bestman-Gateway can use xrootd as the file system, and dCache supports the protocol that xrootd uses for file transfer.

   * *Storage Element.* If your site will provide storage for many terabytes of data, or will provide extensive storage access to off-site entities, you will need to install a Storage Element. There are choices among Storage Element implementations that present a range of complexity and capability; see the following sections.

Sites often employ some combination of the above, for example, allowing jobs with light storage requirements to use a shared file system, but making a Storage Element available for greater storage needs. In some cases the Storage Element and the shared file system may use the same underlying distributed file system, providing local access from the worker nodes and external access through the gsiftp protocol, or the SRM specification.

---+++ Other Required Software

In addition to the above, to operate on the Open Science Grid other software for grid-wide monitoring and accounting of storage must be installed.

   * *Gratia*

   * *GIP*
[[InformationServices/DcacheGip]]
   * *RSV*


---+++Hardware Requirements for Storage Elements

For a storage size of from several to a few tens of terabytes, a disk array is recommended. If data redundancy through RAID5 is desired, the usable storage will be about half of the installed disk space.  It is recommended that any Storage Element be installed on at least one dedicated server, several if higher performance is needed, up to dozens if a large distributed storage system is employed. For very high transfer rates, 10 Gbs network connections are used if available through the campus network infrastructure. Except for the largest sites, processor requirements are not more severe than for computational nodes, though namespace nodes for systems storing a large number for files may need 8 or even 16 GB memory. 

To determine the number of nodes needed for a Storage Element, we provide the following information for the various implementations. Also see the discussion in [[ReleaseDocumentation/SitePlanning][Release Site Planning page]] on the kind of nodes needed for sites of various sizes.

#BestmanStorageHardwareRequirements
---++++Bestman and Bestman-gateway

A !BestMan-based SE consists of one !BeStMan installation and one or more !GridFTP installations. Usually one would prefer to separate !GridFTP from !BeStMan for better 
performance.

   * If your site will be allowing VOs to do heavy data movement (dedicated or opportunistic), you should have a node dedicated to !BeStMan.  At the time of this writing, the VOs that use storage heavily are CMS, ATLAS, CDF, and D0.
   * If you serve one of the above VOs and have more than 250 cores, you should *definitely* have a dedicated !BeStMan node.
   * If your site will be managing more than 50 TB of disk space, you should have a node dedicated to !BeStMan.
   * If none of the above apply, you can co-locate !BeStMan and your !GridFTP server on the same node.
   * If you have less than 1Gbps bandwidth to your site, then you need only one !GridFTP server - at most, 2 servers will be needed.
   * If you have more than 1Gbps bandwidth, then you should plan on at least one !GridFTP server per 4Gbps of available bandwidth (assuming you have 10Gbps interfaces on the server) if you want to maximize throughput.  If you do not have 10Gbps interfaces, make sure you have roughly 1.5 Gbps of network interface cards per 1 Gbps of WAN bandwidth.  In this case, you should not co-locate your !GridFTP and !BeStMan server.

If you have absolutely no idea what your site's usage will be, we recommend 1 server (!BeStMan + !GridFTP combined) for 0-200 cores, one to two dedicated !GridFTP servers for 200 - 500 cores, and 2 to 10 dedicated !GridFTP servers for over 500 cores, depending on your site's load.


---++++Bestman-xrootd
 
Additional nodes are needed beyond the standard Bestman installation, in order to serve files through the xroot protocol. The absolute minimum number of nodes is 3: 
   1. !BeStMan, !XrootdFS, FUSE, !GridFTP  
   1. Xrootd redirector 
   1. Xrootd data server node

Please see the above [[#BestmanStorageHardwareRequirements][Bestman and Bestman-gateway section]] for guidance on additional nodes that may be needed according to usage. Added file servers could be Xrootd data server nodes or 
!GridFTP servers -the decision should be based on your application requirements. 


---++++Bestman-hadoop

The minimal Bestman-hadoop deployment will have two nameservice nodes, datanodes, and a node for Bestman and gridftp. Expansion to hundreds of terabytes is possible by the addition of additional gridftp servers and storage nodes. For details, see [[Storage.HadoopUnderstanding#HadoopStorageRecommendedHardware]]


---++++dCache

The architecture for medium and large sites is similar among the Storage Element implementations: a node to handle incoming SRM requests, an node for the namespace server, nodes for gridftp servers, and storage nodes.  The general guidelines given in the Bestman planning material (see above) for the numbers needed are applicable for dCache installations as well. The [[ReleaseDocumentation/SitePlanning][Release Site Planning page]]  has examples of how the services are distributed among nodes in medium and large dCache deployments. More detailed diagrams of dCache setups for small, medium, and large sites may be found at [[Documentation.StorageDcacheOverviewHardwareLayout]]. [[https://indico.desy.de/getFile.py/access?contribId=19&sessionId=5&resId=0&materialId=slides&confId=138][dCache hardware layouts]] provides a good guide for the layout of dCache hardware in a variety of scales.


---++Installation

Installation of the software listed above is provided in the ReleaseDocumentation.WebHome. Specific links are shown below.

---+++Shared file system

OSG does not provide instructions for creating a shared NFS file system.  Assuming such has be set up, instructions for configuring OSG software to make use of it and other needed storage types are found in [[ReleaseDocumentation.LocalStorageConfiguration]] Some Storage Elements allow sharing of files through mount points. See the documentation for each storage element to determine its local access capability.

---+++gridftp server

A stand-alone gridftp server was once called a storage element, and so is still sometimes referred to as a "classic SE". Installation of a gridftp server is included in the OSG distribution, see [[ReleaseDocumentation.GsiFtpStandAlone]]. Configuration of worker nodes to use a gridftp server is explained in [[ReleaseDocumentation.StorageElementAdmins]]. For an overview of the general features and use of gridftp, please see https://twiki.grid.iu.edu/bin/view/Documentation/StorageGridFTP.

---+++xrootd

xrootd provides posix-like file access in the context of physics analysis programs written with "root". It is included in the OSG distribution. For installation instructions, see [[ReleaseDocumentation.XrootdStandAlone]]. Note that xrootd may also be installed with Bestman for serving files through gridftp and the SRM specification, see below.

---+++Storage Element

All Storage Element implementations in the Open Science Grid support the gsiftp protocol and the SRM specification (with the exception of the Bestman gateway, which supports key elements of the SRM specification, not including space reservation). Implementations vary in other features, such as the choice of underlying distributed storage, support for additional file transfer protocols, and support for a tape-archival backend. Because it allows for a choice among various distributed storage implementations, there are several flavors of Bestman in particular.

---++++Bestman-filesystem

Berkeley Storage Manager (BeStMan) is a full implementation of SRM v2.2 that works on top of existing disk-based unix file systems. Developed by Lawrence Berkeley National Laboratory, for a disk based storage and mass storage systems such as HPSS, it has been reported so far to work on file systems such as NFS, GPFS, PVFS, GFS, Ibrix, HFS+, Hadoop, <nop>XrootdFS and Lustre. It also works with any existing file transfer service, such as gsiftp, http, https, bbftp and ftp.

The full version of Bestman is included in the OSG software distribution. For installation instructions, see [[ReleaseDocumentation.Bestman]].

Bestman-gateway is a variant of Bestman that supports a subset of the SRM v2.2 specification. Space reservation via client callout is not supported, but can be done by site administrators. Bestman-gateway is specifically designed to work with gLite File Transfer Service (FTS), glite-url-copy and LCG-utils, componments of gLite software used by the ATLAS experiment on the Large Hadron Collider. Bestman-gateway is included in the OSG software distribution. If installing Bestman-gateway specifically on an xrootd or hadoop filesystem, see the corresponding sections below. For other filesystems, see [[ReleaseDocumentation.BestmanGateway]].

If you choose to install either the Bestman full version or Bestman-gateway on a server that is a Compute Element, please follow the instructions at [[ReleaseDocumentation.BestmanOnCE]].

---++++Bestman-xrootd

If some applications need to access the storage through an xrootd server, and other applications need access the gsiftp protocol or SRM specification, then Bestman-gateway over an xrootd filesystem should be installed. For installation instructions, see [[ReleaseDocumentation.BestmanGatewayXrootd]].

---++++Bestman-hadoop

HPFS is a distributed filesystem that used the hadoop computing paradigm and implementation. To allow additional access to a hadoop filesystem through gridftp SRM, install Bestman-gateway over the "hadoop" filesystem. For installation instructions, see [[Storage.Hadoop]].

---++++dCache

The OSG makes available a [[http://vdt.cs.wisc.edu/software/dcache/server][dCache installation package]] through the [[http://vdt.cs.wisc.edu][Virtual Data Toolkit]]. The package bundles the needed components and their dependencies and provides scripts for configuration and installation. VDT has bundled the dCache server rpm into a package which includes a configuration dialog and installation scripts for postgres and pnfs, which are services used by dCache. In addition, OSG provides Gratia probes for site and grid-wide monitoring, Generic Information Providers for the support of information services, and a suite of test scripts to validate a dCache installation. Information and links for installing dCache may be found at [[ReleaseDocumentation.dCache][Preparing, Installing and Validating dCache]] 


---+++RSV

[[Storage.OpportunisticStorageForSCEC]]
---++Tuning

The article [[http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=1837&version=1&filename=dcache_tuning.pdf"][ _dCache Tuning_ ]] gives recommended Linux TCP/IP parameter tuning for dCache. Since the tuning was done at the operating system level, it would also apply to other Storage Element implementations.


---++Troubleshooting
---+++OSG Support Process
---++Tools
---+++ OSG Storage Operations Toolkit
 [[http://datagrid.ucsd.edu/toolkit/][OSG Storage Operations Toolkit ]] 
-- Main.TedHesselroth - 13 Jan 2010
