%META:TOPICINFO{author="ForrestChristian" date="1162923416" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="DocumentationTable050"}%
---+!! Local Storage Requirements

%TOC%

---++ Introduction
This section provides recommendations for the definitions and environment variables for the Local Storage accessible to Compute Elements on OSG. It covers both the definition of these spaces and and their correspondences to external schemas (e.g. GLUE, Grid3/OSG).

The paths defined with the names below are sometimes also called _CE storages_. They are disk spaces or SEs accessible from within the CE. 
<!-- (the same from each jobmanager of that CE: jobs on the headnode using _fork_, jobs on the worker nodes usong _pbs/condor/lsf/..._, multiple gatekeeper added to the same CE for reliability or scalability reasons will have to have the same values). -->

If your CE does not support a particular CE storage, provide the keyword =UNAVAILABLE= instead of the path. This will distinguish your site, which provides support for some CE storages and not others, from a site that is not configured. This only deals with CE storages; it does not refer to any disk space as viewed from outside CE, such as an SE, !GridFTP server, etc.

<!-- In this document there are some suggestions on how to encourage proper use and deployment (how to make this information available), but in general this is the responsibility of other groups in OSG.
-->
Possible technologies to deploy CE storage include, but are not limited to:
   1. Variables defined in the environment that resolve to the correct path or URL
   2. Path or URLs consistent across the CE (headnodes and WN), published using the information provider (e.g., GIP or BDII)
   3. *%RED%[DEPRECATED]%ENDCOLOR%* !GridCat

Solutions 2 and 3 may involve a lookup to the information system that can be done before submitting the job. In this case, the information needed to use the CE will be sent along with the job.

<!-- This page has a sister page, LocalStorageRequirementsDiscussion, that includes the discussion that led to this document. Please read it and feel free to add your ideas to that page if you are interested in proposing changes.
-->

[[LocalStorageConfiguration050][Local Storage Configuration]] provides installation/configuration notes and examples.
[[LocalStorageUse050][Local Storage Use]] is a document for users containing best practices, notes and examples.

In the following table there is a name matrix:
   * CE Storage corresponds to the names in this document and the variables in =osg-attributes.conf=
   * GLUE is the attribute name as in GLUE Schema 1.2 (The same attribute may appear in more than one place in the Schema)
   * Grid3/OSG is the LDAP attribute name as in Grid3 Schema
%EDITTABLE{ format="| text, 25, | text, 50, | text, 30, | textarea, 2x50 |" header="| *CE Storage* |*GLUE*|*Grid3/OSG*|*Description*|" }% 
| *CE Storage* |*GLUE*|*Grid3/OSG*|*Description*| 
| [[#OSG_GRID][OSG_GRID]]  | Location.Path <sup>[[#Foot2Table1][(2)]]</sup> | Gri3Dir | Read-only area for OSG worker node's client software |
| [[#OSG_APP][OSG_APP]] | <nop>CE.Info.ApplicationDir (!CE.Info.ApplicationDir) <sup>[[#Foot1Table1][(1)]]</sup>  | Gri3AppDir | VO-wide software installations |
| [[#OSG_DATA][OSG_DATA]] | <nop>CE.Info.DataDir (!CE.VOView.DataDir) <sup>[[#Foot1Table1][(1)]]</sup> | Gri3DataDir | Transient storage shared between jobs executing on the worker nodes |
| [[#OSG_SITE_READ][OSG_SITE_READ]]  | Location.Path <sup>[[#Foot2Table1][(2)]]</sup> | | Transient storage visible from all worker nodes and optimized for high-performance read operations |
| [[#OSG_SITE_WRITE][OSG_SITE_WRITE]] | Location.Path <sup>[[#Foot2Table1][(2)]]</sup> |   | Transient storage visible from all worker nodes and optimized for high-performance write operations |
| [[#OSG_WN_TMP][OSG_WN_TMP]] | CE.Cluster.<nop>WNTmpDir (!CE.SubCluster.WNTmpDir) | | Temporary work area that may be purged when the job completes |
| [[#OSG_DEFAULT_SE][OSG_DEFAULT_SE]] | CE.Info.<nop>DefaultSE (!CE.VOView.DefaultSE) |  | Storage Element closely related to the CE, only accessible using SE access methods |

   1. <a name="Foot1Table1"></a>GLUE provides the possibility to have multiple values for some of the CE storages, depending on the VO and the Role (VOMS FQAN). In OSG currently, these are site-wide information.<br>
   1. <a name="Foot2Table1"></a>The GLUE Schema does not have an specific attribute for SITE_WRITE or SITE_READ, but it provides the location entity (Name/Version/Path sets) to accommodate additional CE local storage. In order to accommodate that, two locations will have to be defined through the GIP:
      1. !LocalID: GRID+OSG, Name:GRID, Version: OSG, Path: <value of GRID>
      1. !LocalID: SITE_WRITE+OSG, Name:SITE_WRITE, Version: OSG, Path: <value of SITE_WRITE>
      1. !LocalID: SITE_READ+OSG, Name:SITE_READ, Version: OSG, Path: <value of SITE_READ>

Each CE administrator will provide for the correct functioning of the client software for all the jobs running on the CE, including providing Globus and SRM, access to the users proxy, and the gass_cache mechanism. Most administrators use a shared =$HOME= directory, but the administrator is free to use other mechanisms transparent to the users. Users should have no other assumption about the CE different from what is stated in this document. It is unsafe to make assumptions about the existence and characteristics (size, being shared, ...) of the $HOME directory. In particular, site admins are free to deploy configurations that do not include any NFS exports from the CE, e.g. as described in [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382][OSG document 382]].

The following sections describe the different storage areas that may be declared local to a CE. Each section includes:
   * brief description
   * detailed description
   * use cases (informal)
   * notes

---++ Known Problems
_(intentionally left blank)_



---+ Local Storage Areas
d62 1
---++ OSG_GRID
%INCLUDE{ "AboutStorageAreaOsgGrid" }%

The OSG_GRID is read-only storage area where CE administrators install the [[WorkerNodeClient050][OSG-WN-Client package]].

Relative paths and content must be consistent between the gatekeeper and worker nodes, even if the base OSG_GRID directory is different. If OSG_GRID differs between the CE/gatekeeper and worker nodes, create softlinks on the worker node that will recreate the gatekeeper's base path for OSG_GRID.

Site administrators must provide a shared directory or locally install it on each machine. 

OSG_GRID may be included directly into the PATH defined for jobs running at the CE.

---+++ Typical uses of OSG_GRID
   * Provide a common set of OSG client software (_globus-job-run_, _globus-url-copy_, _srmcp_, etc.)

---+++ Notes
None.

d66 1
---++ OSG_APP
%INCLUDE{ "AboutStorageAreaOsgApp" }%

The OSG_APP storage agea holds VO-wide software installations.

Relative paths must be consistent between gatekeeper and worker nodes, even if the base OSG_APP directory differ. However, we strongly recommend making the base directory the same in both. Failing to do so may mean that some legacy software<sup>[[#Foot1Table2][1]]</sup> will not function properly.  

This area may be writable only by a subset of users and read-only for all the others: there is no guarantee that every user will have write access.  OSG_APP must point to a POSIX-compliant filesystem for software installation. 

To improve basic security and robustness, we recommend that only specific users and/or VO roles have write access to OSG_APP. <sup>[[#Foot2Table2][2]]</sup>

---+++ Typical uses of OSG_APP
   * Install and run VO applications

---+++ Notes:
   1. <a name="Foot1Table2"></a>Pacman resolves variables an symbolic links saving the full real path. The following may help you avoid problems with it:
      * Choose any absolute location on your gatekeeper file system for OSG_APP.
      * On each worker node arrange by mount or symlink that OSG_APP has the same path as on the gatekeeper node.
      * Install applications only using the fork jobmanager.
   1. <a name="Foot2Table2"></a>We recommend enabling the 'sticky bit' on OSG_APP for all shared CE storages.

<!-- Furthermore, many shared filesystems trust the integrity of the clients; a compromised client may compromise all of OSG_APP if NFS write access is allowed. The mechanisms to recognize a sw manager role (FQAN) and send only it to the APP server with the possibility of installing software are currently missing in OSG and will require some software development.
-->

d70 1
---++ OSG_DATA
%INCLUDE{ "AboutStorageAreaOsgData" }%
OSG_DATA is a storage area for transient / volatile files that are shared between jobs that are executing on the worker nodes. It provides similar functionality to the comibination of OSG_SITE_READ and OSG_SITE_WRITE and may be easier for small sites to deploy. However, OSG_SITE_READ and OSG_SITE_WRITE are usually efficient for big production sites with specialized hardware, because it forces the separation between input and output.

Currently, we discourage both users from using OSG_DATA and site administrators from removing it. Users may need time to ween themselves from using OSG_DATA.

Relative paths must resolve consistently between gatekeeper and worker nodes.  

This area is intended to hold:
   * data shared among different jobs running on different worker nodes
   * data that has to outlive the execution of the jobs on the worker nodes

Regular programs should have open/read/write permissions on OSG_DATA, because they may use it transparently as a local disk space --  NFS, <!-- dcap, drm, --> AFS, CIFS are OK). This is a subset of a POSIX-compatible filesystem that excludes features such as creating special files (e.g., pipes, sockets, locks, links) and modifying file metadata (permissions)<sup>[[#Foot1Table3][1]]</sup>.

Gridftp or "SE-like" access from outside of the cluster is required to allow an efficient use of OSG_DATA as a staging area.<sup>[[#Foot2Table3][2]]</sup>

Since the allocation of this space is transient, it is important that users remove unused data and/or that a simple mechanism to allow cleanups<sup>[[#Foot3Table3][3]]</sup> is added.

The use of OSG_DATA as a writable area from compute nodes is one of the most significant performance bottlenecks in an OSG cluster. Use of OSG_DATA as the "hold all read/write area" (working area for the jobs) is discouraged.

---+++ Using OSG_DATA to provide more scalable and reliable data access
   * Write data to OSG_DATA via gridftp or, if necessary, fork jobs (unpack tarballs, etc.)
   * Stage the job into the cluster
   * The job copies its data to the compute node (OSG_WN_TMP) or reads data sequentially from OSG_DATA, if the data is read once. 
      $ %X% *WARNING*: Reading data sequentially should be avoided. It significantly affect performance on typical network files systems if many random reads are necessary. Random data access over large data sets is where grid storage shows its potential &emdash; its distributed nature is better suited for handling that type of data access scalably and reliably.
   * Job output is placed in OSG_WN_TMP
   * At the end of job, the results from OSG_WN_TMP are packaged, staged to OSG_DATA and picked up via gridftp or staged out via other mechanisms (such as srmcp).
   
   $ %X% *WARNING*: If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another OSG_DATA de facto.<sup>[[#Foot4Table3][4]]</sup> Again, at present we discourage site administrators from removing OSG_DATA and discourage users from using OSG_DATA.
   
---+++ Typical uses of OSG_DATA
   * Input datasets for jobs executing on the worker nodes
   * Datasets produced by the jobs executing on the worker nodes
   * Shared data for MPI applications
   * Data staged in or waiting to be staged out

---+++ Notes:
   1. <a name="Foot1Table3"></a>The ability to modify file permissions may be required in future revisions -- it is available in SRMv2
   1. <a name="Foot2Table3"></a>The OSG_DEFAULT_SE entry may be used to publish the base GSIftp URL of the SE viewing that space.
   1. <a name="Foot3Table3"></a>An example of a simple space management solution could be a file in each directory ( _.keep_) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested), all files in that directory and subdirectories may be removed. This is a "gentleman's agreement"; keep in mind that none of the data in a transient / volatile storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
   1. <a name="Foot4Table3"></a>For example, the current Condor jobmanager uses a shared space (=gass_cache= by default) when it processes requests to transfer the executable or some data. You should not assume that =$HOME= is shared between gatekeeper and worker nodes. Furthermore, removing a shared space like OSG_DATA for performance issues is useless if you only reintroduce it as =$HOME=, as that will probably also adversely affect performance because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate =$HOME= at the same time. See [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382][OSG document 382]] for more information.

d74 1
---++ OSG_SITE_READ
%INCLUDE{ "AboutStorageAreaOsgSiteRead" }%
OSG_SITE_READ is a transient storage area (storage area for volatile storage?) visible from all worker nodes and optimized for high-performance read operations that holds input data shared among different applications running on different worker nodes.

Relative paths must resolve consistently between gatekeeper and worker nodes.

OSG_SITE_READ allows open, seek, read, and close by regular programs, which may use it transparently as a local disk space. It is provided through a grid file access library, and the users do not have to know the storage location and its underlying implementation. The usage is uniform across OSG.

Users have no write access to this area from the worker node. Users can write from the grid side of the Storage Element. However, site administrators can write files to the area. <sup>[[#Note1Sec4][1]]</sup>.

If the gatekeeper cannot write to this area, jobmanagers may have problems using a shared directory to transfer the executable or some data.<sup>[[#Foot4Table3][4]]</sup>

<!-- This follows the LCG model. This last option may cause problems to many current applications that count on normal file access to a shared space and it may require non-trivial changes to them to use special client programs to access the DEFAULT_SE. Furthermore it may cause problems with jobmanagers using a shared directory to transfer the executable or some data. A dataflow example could be: 

Its features cover the read part of DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.
-->

---+++ Typical uses of OSG_SITE_READ
   * Input datasets for jobs executing on the worker nodes
   * Data staged in 

---+++ Notes (*#):
   1. <a name="Note1Sec4"></a>In some cases, GSIftp can put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes. If so, the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported.

d78 1
---++ OSG_SITE_WRITE
%INCLUDE{ "AboutStorageAreaOsgSiteWrite" }%
OSG_SITE_WRITE is a write-only (or "mostly write") transient / volatile storage area that is visible from all worker nodes and optimized for high-performance write operations and holds the output from jobs running on the CE that is required to persist beyond the originating job's lifetime. 

OSG_SITE_WRITE allows open, set flags, write (sequential, with multiple write operations), and close, although it may not be possible to modify a file once it is closed. 

OSG_SITE_WRITE is provided through a grid file access library specific for the underlying storage, and programs may use it transparently <sup>[[#Note1Sec5][1]]</sup> as a local disk space.

Users shall not have read access to this area. In cases where read access is necessary, the site administrator must facilitate access, or provide mechanisms external to the CE, such as supporting Storage Element reading that area. <sup>[[#Note1Sec4][4]]</sup>). <!-- Its features cover the write part of OSG_DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.-->

---+++ Typical uses of OSG_SITE_WRITE
   * Storage of datasets produced by the jobs executing on the worker nodes
   * Data waiting to be staged out

---+++ Notes
   1. <a name="Note1Sec5"></a>Limitations should be avoided when possible, and be clearly stated.

---++ TMP *%RED%[OBSOLETE!]%ENDCOLOR%*
This area was intended as a shared temporary work area. Because of its similarities with OSG_DATA, the reduced interest in multimode applications (that anyway can use OSG_DATA), and the possibility of the application abusing of a shared space, this *has been removed* in the most recent OSG version.
d86 1
---++ OSG_WN_TMP
%INCLUDE{ "AboutStorageAreaOsgWnTmp" }%
OSG_WN_TMP is a temporary storage area specific to a single job that may be purged when the job completes. Two jobs running on a cluster cannot (generally) read from or write to each other's OSG_WN_TMP areas. The OSG_WN_TMP area is thus fundamentally different from the others in that it is job-specific.

OSG_WN_TMP must point to a POSIX-compliant filesystem that supports links, sockets, locks, pipes and other special files as required. It requires good I/O job performance, so we recommend a local filesystem. Ideally, OSG_WN_TMP should be a temporary directory (or partition) assigned empty for the job, with a well-defined amount of space (many jobs require at least 1-2 GB of disk space), that is purged when the job completes. The space provided should be dedicated and isolated: jobs reusing their own OSG_WN_TMP areas should not affect each other or affect the OS.

In most cases, OSG_WN_TMP points to a space shared among all jobs that are currently executing at the worker node. Therefore, we recommend each job create a unique subdirectory of OSG_WN_TMP (e.g., =OSG_WN_TMP/somestring$JOBSLOT=) that becomes its working area. The job should then remove this subdirectory before it completes,

Sites using _condor_ as their batch system typically implement this by letting condor create the jobs directory prior to launching the job. In those cases, OSG_WN_TMP may be defined in the GIP implicitly by pointing to another environment variable, rather than explicitly by pointing to a full path.

---+++ Typical uses of OSG_WN_TMP
   * Working directory for jobs (running on the worker nodes)


---+++ Notes
None. 
<!--
1. A mechanism to help automatic cleanup would be the introduction of a 'lock' directories with files named according the temporary directory, containing the PID of the job. If that process is terminated, the directory can be removed
-->

d90 1
---++ OSG_DEFAULT_SE
%INCLUDE{ "AboutStorageAreaOsgDefaultSe" }%
The OSG_DEFAULT_SE variable represents a Storage Element closely related to the CE, accessible from within the cluster (visible to worker nodes) only via SE access methods such as Gridftp or SRM. If OSG_DEFAULT_SE is accessible from outside, it is the preferred SE for the CE and should be used when doing a two- or more step copy of input datasets to the CE. For example, doing a 3rd-party transfer to OSG_DEFAULT_SE, then copying to OSG_WN_TMP using a client program.

In a simple cluster, OSG_DEFAULT_SE could be an SE visible from both inside and outside, and serving an internally shared space like OSG_DATA. This would allow I/O via grid tools in sites that had no shared space,

---+++ Typical uses of OSG_DEFAULT_SE
   * Staging in of large input files (e.g. datasets)
   * Staging out of large output files (e.g. datasets)

---+++ Notes
None. 
<!--
  1. This is not guaranteed. To describe better the SE connected to CE storages like DATA, SITE_READ, SITE_WRITE a better mechanism will have to be defined. The GLUE CE-SE binding schema could be a starting point.
-->

---++ Mandatory set options
An OSG site is not required to provide all of the areas described above. However, the site must implement at least one of the options below. Providing a wider selection of CE storages would allow the jobs to select the most proper for their needs, but may also allow the jobs to adopt inefficient execution models that could reduce the performance of the whole cluster.

See LocalStorageConfiguration050 for installation/configuration notes and examples.

Areas that are not provided should be labelled as "UNAVAILABLE".


---+++<a name="Grid3Model"></a> OSG_GRID, OSG_APP, OSG_DATA, OSG_WN_TMP  (Grid3 Model)
A simple CE (one or few nodes, not a production CE) may even decide to use a single shared disk, pointing both OSG_DATA and OSG_WN_TMP to the same directory, but this is not recommended.

---+++<a name="Grid3ModelWithReadWriteInsteadOfData"></a> OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_SITE_WRITE, OSG_WN_TMP
A dataflow example may be:
   * site admin intervention or external transfer 
   * cp: OSG_SITE_READ->OSG_WN_TMP 
   * job execution 
   * cp: OSG_WN_TMP->OSG_SITE_WRITE 
   * external stage-out

#LcgModel
---+++ OSG_GRID, OSG_APP, OSG_DEFAULT_SE, OSG_WN_TMP  (LCG model)
Current applications depending on normal file access to a shared space may require non-trivial changes -- there may also be issues with jobmanagers that use a shared directory to transfer the executable or data.

A dataflow example may be: 
   * site admin intervention or external transfer 
   * srmcp: OSG_DEFAULT_SE->OSG_WN_TMP
   * job execution
   * srmcp: OSG_WN_TMP->OSG_DEFAULT_SE
   * external stage-out

#SrmDacheModel
---+++ OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_DEFAULT_SE, OSG_WN_TMP  (SRM/dCache model)
The SRM/dCache model is a superset of the LCG Model, so this CE satisfies that one.  

In the SRM/dCache model, read access is via dcap, while write access is via srmcp only. DCache does not allow modification of a file once it is closed, although write access via dcap is possible in principle and could provide OSG_SITE_WRITE. 

A typical deployment allows writes only via srmcp (i.e., OSG_DEFAULT_SE) while reads may use either srmcp (OSG_DEFAULT_SE) or dcap (OSG_SITE_READ). 

The current SRM implementation in dCache does not allow overwriting of logical files, This guarantees that all physical file replicas of the same logical file remain the same. 

A dataflow example may be: 
   * site admin intervention or external transfer 
   * job execution (open/seek/read from OSG_SITE_READ using dcap)
   * srmcp: OSG_WN_TMP->OSG_DEFAULT_SE
   * external stage-out

%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
 -- Main.BurtHolzman - 04 Aug 2005 %BR%
 -- Main.MarcoMambelli - 23 Sep 2005 %BR%
 -- Main.FkW - 25 Apr 2006 %BR%
 -- Main.RobQ - 01 May 2006 %BR%
 -- Main.Forrest.Christian - 06 Nov 2006 (editing only) %BR%
 
