%META:TOPICINFO{author="ForrestChristian" date="1162923492" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="DocumentationTable050"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->
d7 1
---+!!<nop>Local Storage Configuration
%TOC%



---++Introduction
%STARTINCLUDE%
This document describes how a CE administrator can configure the OSG attributes, including ones referencing "CE storage", during the installation and afterwards (if he needs to change the layout of his CE). 
[[LocalStorageRequirements050][Local Storage Requirements]] describes what the different CE storage are and [[LocalStorageRequirements050#MandatorySetOptions][Mandatory Set Options]] specifies the minimum requirements for being OSG-compliant. It is not mandatory to provide all the CE storages.  Also, see [[LocalStorageUse050][Local Storage Use]] for a very draft idea on how these should be used.
[[LocalStorageRequirements][Local Storage Requirements]] describes what the different CE storage are and [[LocalStorageRequirements#MandatorySetOptions][Mandatory Set Options]] specifies the minimum requirements for being OSG-compliant. It is not mandatory to provide all the CE storages.  Also, see [[UsingLocalStorage][Local Storage Use]] for a very draft idea on how these should be used.


---++Configuring the OSG Attributes
This section covers the configuration of information about your CE that you must make known to OSG. 

These will be published as pat of the GLUE schema using the GIP [or the Grid3 schema] and used directly or indirectly by other OSG applications and users submitting jobs. <!-- The current version also provides backward compatibility with applications using Grid3 conventions. -->

At the core is =$VDT_LOCATION/monitoring/osg-attributes.conf=, a standard configuration file that can be edited or reviewed directly. The configuration script =$VDT_LOCATION/monitoring/configure-osg.sh=  automates much of the configuration.
The meaning and purpose of the various elements of the configuration attributes are documented further in [[LocalStorageRequirements050][Local Storage Requirements]] and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. 
The meaning and purpose of the various elements of the configuration attributes are documented further in [[LocalStorageRequirements][Local Storage Requirements]] and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. 

New resource administrators may want to read that information carefully and determine how to map those elements onto their Resource before proceeding. 

Guidance on the basic elements and common defaults is provided below. 


---+++!!Gather configuration information
OSG strives to make resources available with minimal requirements; however, the grid requires certain information about files and filesystem mount points to provide a basic execution environment. 

For applications to be installed and to be executed correctly, filesystem sharing and the filesystem mount points available for a cluster must be specifically coordination. For this purpose, administrators must define special directory hierarchies (mount points) and allocate them in the OSG environment. Many of these mount points should be available on the head / gatekeeper node and, using the exact path, on each of the worker nodes. Generally, they do _not_ have to be made available in form of a shared filesystem across the whole cluster. 

Read-only spaces can generally be provisioned with or without a shared filesystem as long as you provide consistent paths.
---+++ OSG Location ($OSG_LOCATION)
d50 1
Where OSG software will be installed and must be writable by root.
Where OSG software will be installed. It must be writable by root.  This attribute is automatically setup by the configure_osg.sh script and users are not asked for this value.  

$OSG_LOCATION contains the OSG-specific software as well as the Globus middleware and other middleware applications, including server and client utilities used by the system itself.  Users will use a different client installation.  This
directory should not be exported to the worker nodes.

---+++ OSG Grid ($OSG_GRID)
Where OSG Client Software will be installed &mdash; see [[WorkerNodeClient][Worker Node Client]] for description.  
Where OSG Client Software will be installed &emdash; see [[WorkerNodeClient050][Worker Node Client]] for description.  $OSG_GRID includes client utilities for Grid middleware, such as VDS and srmcp.  It should be writable by root and readable by all users. It must be accessible by both gatekeeper and worker nodes via a shared filesystem, or different installations on local disks using a consistent pathname.
Where OSG Client Software will be installed &mdash; see [[WorkerNodeClient][Worker Node Client]] for description.  
$OSG_GRID includes client utilities for Grid middleware, such as VDS and srmcp.  It should be writable by root and readable by all users. It must be accessible by both gatekeeper and worker nodes via a shared filesystem, or different installations on local disks using a consistent pathname.
---+++ Application Directory ($OSG_APP)
Base location for VO-specific application software. Only users with software installation privileges in their VO should have write privileges to this directories. At least 10 GB of space should be allocated per VO.
Base location for VO-specific application software. 
$OSG_APP is read-only mounted on all worker nodes in the cluster.
$OSG_APP is read-only mounted on all worker nodes in the cluster. Only users with software installation privileges in their VO should have write privileges to this directories. At least 10 GB of space should be allocated per VO.

---+++ Data Directories ($OSG_DATA or $OSG_SITE_READ & $OSG_SITE_WRITE)
These directories must be accessible from the head node as well as each of the worker nodes. 
d72 1
---+++ Data Directories: =$OSG_DATA= __or__ =$OSG_SITE_READ= & =$OSG_SITE_WRITE=
The data directories are intended as the spaces for applications to write input and output data files with persistency that must exceed the lifetime of the job which created it. 
   * These directories should be writable by all users.  
   * Users will be able to create sub-directories which are private, as provided by the filesystem. 
   * At least 10 GB of space should be allocated per worker node; some VOs require much larger allocations.

The following different options are possible: 
   * $OSG_DATA: shared directory with read-write access for all users
   * $OSG_SITE_READ: shared directory with read-only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
A CE can provide $OSG_DATA, both $OSG_SITE_READ and $OSG_SITE_WRITE, or none of them if it has a local SE specified in $OSG_DEFAULT_SE. The keyword to say that one hierarchy is not provided is UNAVAILABLE.
For additional details please refer to [[RequirementsForLocalStorage][Local Storage Requirements]].
For additional details please refer to [[LocalStorageRequirements050][Local Storage Requirements]] .
For additional details please refer to [[LocalStorageRequirements][Local Storage Requirements]].
%IMPORTANT% The $OSG_DATA, $OSG_SITE_READ and $OSG_SITE_WRITE directories must be accessible from the head node as well as each of the worker nodes. 
---+++ Temporary Directory ($OSG_WN_TMP)
---+++ Temporary Directory: =$OSG_WN_TMP=
A temporary directory local to the worker node, used as a working directory.  
   * At least 10 GB per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs that can run up to 4 jobs, should have 40GB). 
   * Files placed in this area by a job may be deleted upon completion of the job.

---+++ Default Storage Element ($OSG_DEFAULT_SE)
---+++ Default Storage Element: =$OSG_DEFAULT_SE=
A storage element that is close and visible from all the nodes of the CE, both worker and head node. <!-- Usually, it is local to the CE and accessible from outside with the same or a different URL. --> 
The value to be specfied in $OSG_DEFAULT_SE is the full URL, including method, host/port and path of the base dir. This full URL must be reachable from inside as well as outside the cluster. The $OSG_DEFAULT_SE generally supports only put and get, rather than open/read/write/close as discussed in [[LocalStorageRequirements050][Local Storage Requirements]] . 
The value to be specfied in $OSG_DEFAULT_SE is the full URL, including method, host/port and path of the base dir. This full URL must be reachable from inside as well as outside the cluster. The $OSG_DEFAULT_SE generally supports only put and get, rather than open/read/write/close as discussed in [[LocalStorageRequirements][Local Storage Requirements]]. 
At present, the supported methods for the $OSG_DEFAULT_SE are SRM and gftp.
If the CE has no default SE it can use the value UNAVAILABLE for $OSG_DEFAULT_SE. 
If the CE has no default SE it can use the value UNAVAILABLE for $OSG_DEFAULT_SE. 
%NOTE% The current release supports SRM and gftp for $OSG_DEFAULT_SE.


The final question asks about the VO sponsor of your site. This attempts to determine the VOs paying for the resources of the cluster. The notation incorporates a VO name followed by a percentage, so that CEs are able to denote multiple VO partners. 
To determine what Virtual Organization(s) are paying for the resources of this cluster. The notation incorporates a VO name followed by a percentage, so that CEs are able to denote multiple VO partners. 


---+++!!Execute the configuration script
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
  #  cd $VDT_LOCATION/monitoring
  # ./configure-osg.sh
</verbatim>
%ENDMore%
---++++ Sample from a typical installation
d149 1
<verbatim>
***********************************************************************
################# Configuration for the OSG CE Node ###################
***********************************************************************
The =configure-osg.sh= script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file and =grid3-info.conf= that is a link to it.  The script also creates =$VDT_LOCATION/monitoring/osg-job-environment.conf= which duplicates some of the attributes from =$VDT_LOCATION/monitoring/osg-attributes.conf= so that they can be palced in the job environment as well as =$VDT_LOCATION/monitoring/osg-local-job-environment.conf=. The =$VDT_LOCATION/monitoring/osg-local-job-environment.conf= file is to allow site admins to set site specific environment variables that should be present for jobs that are running on the cluster. The This file is used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CEs.
This script collects the necessary information required by the various
monitoring and discovery systems for operating for the OSG.

A definition of the attributes that you will have to enter below is in:
http://osg.ivdgl.org/twiki/bin/edit/Integration/LocalStorageRequirements050
Instructions on how to use this script are in:
http://osg.ivdgl.org/twiki/bin/edit/Integration/LocalStorageConfiguration050

Your CE may not provide some of the CE-Storages (DATA, SITE_READ, SITE_WRITE,
DEFAULT_SE). In those instances, the value to enter is UNAVAILABLE

At any time, you can <CNTL-C> out of the script and no updates will be applied.


Preset information you are not prompted for
--------------------------------------------

These variables are preset at installation and cannot be changed:
OSG location
Globus location
User-VO map file
gridftp.log location


Information about your site in general
--------------------------------------
Group:      The monitoring group your site is participating in.
             - for the integration testbed, use OSG-ITB.
             - for production, use OSG.

Site name:  The name by which the monitoring infrastructure
            will refer to this resource.

Sponsors:   The VO sponsors for your site.
            For example: usatlas, ivdgl, ligo, uscms, sdss...
            You must express the percentage of sponsorship using
            the following notation.
              myvo:50 yourvo:10 anothervo:20 local:20

Policy URL: This is the URL for the document describing the usage policy /
            agreement for this resource

Specify your OSG GROUP [OSG-ITB]:
Specify your OSG SITE NAME [UNAVAILABLE]: FNAL_64_TEST
Specify your VO sponsors [UNAVAILABLE]: fermilab
Specify your policy url [UNAVAILABLE]: http://fermigrid.fnal.gov/policy.html

Information about your site administrator
-------------------------------------------
Contact name:  The site administrator's full name.
Contact email: The site administrator's email address.

Specify a contact for your server (full name) [UNAVAILABLE]: Steven Timm
Specify the contact's email address [UNAVAILABLE]: timm@fnal.gov

Information about your servers location
----------------------------------------
City:    The city your server is located in or near.
Country: The country your server is located in.

Logitude/Latitude: For your city. This  will determine your placement on any
         world maps used for monitoring.  You can find some approximate values
         for your geographic location from:
            http://geotags.com/
         or you can search your location on Google

         For USA: LAT  is about   29 (South)       ...  48 (North)
                   LONG is about -123 (West coast) ... -71 (East coast)

Specify your server's city [UNAVAILABLE]: Batavia
Specify your server's country [UNAVAILABLE]: USA
Specify your server's longitude [UNAVAILABLE]: 41.8412
Specify your server's latitude [UNAVAILABLE]: -88.2546

Information about the available storage on your server
------------------------------------------------------
GRID:       Location where the OSG WN Client (wn-client.pacman) has
            been installed.
APP:        Typically used to store the applications which will run on
            this gatekeeper.  As a rule of thumb, the OSG APP should be on
                - dedicated partition
                - size: at least 10 GB.
DATA:       Typically used to hold output from jobs while it is staged out to a
            Storage Element.
            - dedicated partition
            - size: at least 2 GB times the maximum number of simultaneously
                    running jobs that your cluster's batch system can support.
WN_TMP:     Used to hold input and output from jobs on a worker node where the
            application is executing.
            - local partition
            - size: at least 2 GB
SITE_READ:  Used to stage-in input for jobs using a Storage Element or for
            persistent storage between jobs.  It may be the mount point of a
            dCache SE accessed read-only using dcap.
SITE_WRITE: Used to store to a Storage Element output from jobs or for
            persistent storage between jobs.  It may be the mount point of a
            dCache SE accessed write-only using dcap.

Specify your OSG GRID path [UNAVAILABLE]: /usr/local/grid
Specify your OSG APP path [UNAVAILABLE]: /usr/local/app
Specify your OSG DATA path [UNAVAILABLE]: /usr/local/data
Specify your OSG WN_TMP path [UNAVAILABLE]: /local/stage1
Specify your OSG SITE_READ path [UNAVAILABLE]: dcap://fndca1.fnal.gov:24125//pnfs/fnal.gov/usr/
Specify your OSG SITE_WRITE path [UNAVAILABLE]: srm://fndca1.fnal.gov:8443/

Information about the Storage Element available from your server
----------------------------------------------------------------
A storage element does NOT exist for this node.

This is the Storage Element (SE) that is visible from all the nodes of this
server (CE). It may be a SE local or close to the CE that is preferred as
destination SE if the job does not have other preferences.

Is a storage element (SE) available [n] (y/n): y
Specify your default SE [UNAVAILABLE]: gsiftp://fndca.fnal.gov:2811

Information needed for the MonALISA monitoring.
-----------------------------------------------
MonALISA services are NOT being used.

If you do not intend to run MonALISA for monitoring purposes, you can
skip this section.

Ganglia host: The host machine ganglia is running on.
Ganglia port: The host machine's port ganglia is using.
VO Modules:   (y or n) If 'y', this will activate the VO Modules module
              in the MonALISA configuration file.

Are you running the MonALISA monitoring services [n] (y/n): y
Are you using Ganglia [y] (y/n): y
Specify your Ganglia host [fermigrid0.fnal.gov]:
Specify your Ganglia port [UNAVAILABLE]: 8649
Do you want to run the OSG VO Modules [y] (y/n): y

Information about the batch queue manager used on your server
-------------------------------------------------------------
The supported batch managers are:
  condor pbs fbs lsf sge

For condor: The CONDOR_CONFIG variable value is needed.
For sge:    The SGE_ROOT variable value is needed

Specify your batch queue manager OSG_JOB_MANAGER [UNAVAILABLE]: condor
Specify installation directory for condor [UNAVAILABLE]: /opt/condor
Specify the Condor config location []: /opt/condor/etc/condor_config

#####  #####  ##### #####  #####  #####  ##### #####
Please review the information below:

***********************************************************************
################# Configuration for the OSG CE Node ###################
***********************************************************************

Preset information you are not prompted for
--------------------------------------------
OSG location:     /usr/local/vdt-1.3.10
Globus location:  /usr/local/vdt-1.3.10/globus
User-VO map file: /usr/local/vdt-1.3.10/monitoring/grid3-user-vo-map.txt
gridftp.log file: /usr/local/vdt-1.3.10/globus/var/gridftp.log

Information about your site in general
--------------------------------------
Group:       OSG-ITB
Site name:   FNAL_64_TEST
Sponsors:    fermilab
Policy URL:  http://fermigrid.fnal.gov/policy.html

Information about your site admininistrator
-------------------------------------------
Contact name:   Steven Timm
Contact email:  timm@fnal.gov

Information about your servers location
----------------------------------------
City:       Batavia
Country:    USA
Longitude:  41.8412
Latitude:   -88.2546

Information about the available storage on your server
------------------------------------------------------
WN client: /usr/local/grid

Directories:
  Application: /usr/local/app
  Data:        /usr/local/data
  WN tmp:      /local/stage1
  Site read:   dcap://fndca1.fnal.gov:24125//pnfs/fnal.gov/usr/
  Site write:  srm://fndca1.fnal.gov:8443/

Information about the Storage Element available from your server
----------------------------------------------------------------
A storage element exists for this node.

Storage Element: gsiftp://fndca.fnal.gov:2811

Information needed for the MonALISA monitoring.
-----------------------------------------------
MonALISA services are being used.

Ganglia host: fermigrid0.fnal.gov
Ganglia port: 8649
VO Modules:   y

Information about the batch queue manager used on your server
-------------------------------------------------------------
Batch queue:     condor

Job queue:       fermigrid0.fnal.gov/jobmanager-condor
Utility queue:   fermigrid0.fnal.gov/jobmanager

Condor location: /opt/condor
  Condor config: /opt/condor/etc/condor_config
PBS location:
FBS location:
SGE location:
    SGE_ROOT:
LSF location:


##################################################
##################################################
Is the above information correct (y/n)?: y

</verbatim>


This configure script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file, and =grid3-info.conf= that is a link to it. This file is used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CEs.
d112 1

The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.


---++Reconfiguration
---++Reconfiguring an Existing OSG Site
An already installed OSG site can be reconfigured by editing =osg-attributes.conf=, or by rerunning the =configure-osg= script.


---++Examples

Your configuration may differ considerably.
   * [[InstallingOSGWithSRMbasedCE][Installing OSG with an SRM-based CE]]
---+++ Single Computer OSG site 
You want to install a complete OSG site onto a single computer as a testbed. You thus have CE, SE, and batch system on the same piece of hardware &emdash; no clustering or multiple pieces of hardware.

For this example, let's assume we are using a partition mounted as  =/osg=. Users are mapped to different unix accounts but all belong to the same =griduser= unix group. 
   $ %H% *TIP*: If you prefer, you can mount each subdirectory of =/osg= &emdash; =/osg/grid=, =/osg/wntmp=, =/osg/app=, =/osg/data=, =/osg/wngrid= &emdash; on a different partition. Mixing these so that some are on separate partitions and some share a partition also works.
 
   1. Create the subdirectories =/osg/grid=, =/osg/app=, =/osg/data=, =/osg/wngrid=, =/osg/wntmp=
   1. Change their group ownership to =griduser=
   1. Change their permissions (1770 on /osg/data, /osg/wngrid, /osg/app, /osg/wntmp)
   1. Install the server (OSG) in =/osg/grid= and the user client (OSG-WN-Client) in =/osg/wngrid=
   1. Configure variables (=GRID=/osg/wngrid=, =APP=/osg/app=, =DATA=/osg/data=, =WN_TMP=/osg/wntmp=)
<!--   * Point the gridftp server to =/osg/data= (gsiftp://myserver.domain/mydir/ -> =/osg/data=) -->

Here you will have a summary of =osg-configure.sh= (=osg-attributes.conf=) that looks like:  
<pre>
Please review the information:
Grid Site Name:  <i>UNIQUE_NAME</i>
OSG Location:    <i>/osg/grid</i>
OSG WN Client:   <i>/osg/wngrid</i>
Application:     <i>/osg/app</i>
Data:            <i>/osg/data</i>
Site read:       UNAVAILABLE
Site write:      UNAVAILABLE
WorkerNode Temp: <i>/osg/wntmp</i>
Default SE:      <i>gsiftp://myheadnode.athome.edu:2811</i>
JOB Manager:     <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

The specification for the =Default SE= should be such that one can concatenate it with the setting for =Data= to arrive at a valid location SURL without any additional or extraneous slashes.

=myheadnode.athome.edu= is the nodename of the single headnode. The port number is that at which the gsiftp server is listening.

In addition, the OSG CE expects a home directory (=/osg/users/$USER=) for every user to which the proxy and gass-cache are written as jobs are submitted.

This installation implements [[LocalStorageRequirements050#Grid3Model][Grid3 Model configuration]] and is thus OSG compliant.

---+++Single Headnode Cluster
You have a cluster with a batch system, and you want to add a single OSG headnode to it that instantiates a CE and SE on one piece of hardware in the simplest possible way. We do not suggest this as a production installation as it couples CE and SE and thus does not provide a very reliable OSG site.

   1. Start by following the "Single Computer OSG Site" instructions above.
   1. Mount the file systems mentioned on the single headnode as follows:
      * _/osg/grid_ :   read/write
      * _/osg/wngrid_ :  read/write
      * _/osg/app_ :     read/write
      * _/osg/data_ :    read/write
      * _/osg/users_ :   read/write
      * _/osg/wntmp_ :   read/write
   1. Make sure that the following file systems are read/write accessible via the gridftp server:
      * /osg/app
      * /osg/data
   1. Mount the file systems mentioned on all worker nodes on the cluster as follows:
      * _/osg/wngrid_ :  read only
      * _/osg/app_ :     read only
      * _/osg/data_ :    read/write
      * _/osg/users_ :   read/write
      * _/osg/wntmp_ :   read/write

In this configuration, WN_TMP (/osg/wntmp) is not a shared file system. It should be local to each and every worker node. In fact, it could be local to each and every batch slot.

This installation implements the [[LocalStorageRequirements050#Grid3Model][Grid3 Model configuration]] and is thus OSG compliant. However, we do not suggest this configuration unless the cluster is quite small.

---+++Suggested Minimal Configuration
The minimal set of headnodes for an OSG site (site = cluster & headnodes) to be reliable is two:
one for the CE and one for the minimal SE configuration.

To deploy such a configuration, follow the "Single Headnode Cluster" configuration above but separate
components as follows:
<verbatim>
Headnode 1:
Install the CE

Headnode 2:
Install the gridftp server
Export all file systems
</verbatim>

In this configuration you must point "myheadnode.athome.edu" in the specification of "Default SE" to the nodename of  Headnode 2, the one that instantiates the gsiftp server. The CE installation will also install a gridftp server. However, that gftp server will not be advertised in any way shape or form. It is required because the condor-g client uses it to retrieve those files that are specified in the jdl as output files. Condor-g implicitly assumes that the host that runs the GRAM also runs gsiftp.

The intended use case here is for large scale data movement to go via the "Default SE" while small output files (O(a few MB)) that are retrieved by condor-g directly may be transfered via the CE.

This installation implements [[LocalStorageRequirements050#Grid3Model][Grid3 Model configuration]] and is thus OSG compliant.

---+++SRM style CE
This section covers only two SRM versions, SRM/DRM and SRM/dCache, and provides an overview of how each is incorporated into the OSG infrastructure. For a comprehensive description of the configuration of these products, see the SRM/DRM or SRM/dCache documentation, or the [[http://srm.fnal.gov/][Fermilab SRM site]]. 

In principle, replacing =$OSG_DATA= in the "Minimal Suggested Configuration" with either one of these SRM solutions is an OSG-compliant configuration as specified in the [[LocalStorageRequirements050#LcgModel][LCG model]] description. 

However, in practice, too many applications are not yet ready to benefit from SRM, and we thus suggest
sites to deploy SRM in addition to the "Minimal Suggested Configuration" for the time being. A reasonable configuration might provide a small =$OSG_DATA= area (~100GB) in addition to a large multi-TB SRM space.

For applications that are able to use dcap, SRM/dCache replaces /osg/data.  [[LocalStorageRequirements050#SrmDcacheModel][The SRM/dCache Model discussion]] showcases an ideal workflow. 

For applications that are not yet able to use dcap, the [[LocalStorageRequirements050#LcgModel][LCG Model]] may be suitable.  However, in that case the data a job needs must fit into WN_TMP which often is quite small (~10GB). As a result, it is highly advisable for applications to learn about dcap if they want to benefit from the large storage available in SRM/dCache.

If you choose this configuration configure "Site Read", "Site Write", and "Default SE" as follows:

| *Site read:* | _dcap://mydcapnode.athome.edu:22136//pnfs/athome.edu_ |
| *Site write:* | _srm://mysrmnode.athome.edu:8443/_ | 
| *Default SE:* | _gsiftp://myheadnode2.athome.edu:2811_ | 

   $ %X% *%RED%WARNING%ENDCOLOR%*: A file that is copied into the Default SE can not be read from "Site read" as these are two entirely different storage elements in this site configuration.

The specifications for "Site read" and "Site write" are chosen such that a user who knows their path inside the dCache pnfs logical namespace can add that path behind "Site read" or "Site write" without any additional or extraneous forward slashes.


---++ GIP configuration
See [[GenericInformationProviders050][Generic Information Providers]].

<!-- ++Contributed Examples: Here administrators can add tips or links to descriptions of their installation. These may include specific references that may not apply to any other CE. -->
d131 1
See [[GenericInformationProviders][Generic Information Providers]].
%STOPINCLUDE%

%INCLUDE{ "Documentation.ToolsBottomMatter" }%
<!-- MAJOR UPDATES: For significant updates to the topic, consider adding your 'signature' (beneath this editing box) -->
 %BR%
*Major updates*:
%META:TOPICMOVED{by="ForrestChristian" date="1163712765" from="Integration.LocalStorageConfiguration050" to="Integration/ITB_0_5.ConfiguringLocalStorage"}%
<!-- Future editors should add their signatures beneath yours! -->

   * -- Main.MarcoMambelli - 11 Nov 2005
   * -- Main.FkW - 25 Apr 2006
   * -- Main.RobQ - 01 May 2006
   * -- Main.ForrestChristian - 06 Nov 2006 (editing only)
%META:TOPICMOVED{by="ForrestChristian" date="1164649583" from="Integration/ITB_0_5.ConfiguringLocalStorage" to="Integration/ITB_0_5.LocalStorageConfiguration"}%
