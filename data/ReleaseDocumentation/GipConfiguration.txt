%META:TOPICINFO{author="RobertEngel" date="1285280681" format="1.1" reprev="1.23" version="1.23"}%
%DOC_STATUS_TABLE%

---+!! Configuration of the Generic Information Provider
%TOC%

---+ About this Document
<!-- conventions used in this document
   * Local UCL_CWD  = /opt/osg-%VERSION%
   * Local UCL_HOST = ce
-->

%ICON{hand}% This document is for System Administrators. It contains the procedure to configure the %LINK_GIP%.

%STARTINCLUDE%

---+ Configuration of the Generic Information Provider

One important aspect of a site being part of the %LINK_OSG% is the ability of the %LINK_GLOSSARY_CE% to describe the site to external users. The %LINK_GIP% generates site information and provides it in the [[http://glueschema.forge.cnaf.infn.it/][GLUE]] schema.

The GIP is collecting information about some aspects of your site including its hardware composition, the batch system and associated storage. It relies on the resource configuration file =%UCL_CWD%/osg/etc/config.ini= to collect the information.

The following sections detail information about the options in the =[GIP]=, =[Subcluster]=, and =[SE]= sections of the configuration file.

<!--
Do we really still need the information below?
 
_Note for seasoned admins_:  This section has undergone a *significant amount of revision*; the latest update has introduced a new way to configure subclusters and SEs.  The OSG 0.8.0 style configuration should still work, but is deprecated.  *We strongly recommend updating your config to the current style.*
-->

---++ For Sites with Multiple Compute Elements

If you would like to properly advertise multiple %LINK_GLOSSARY_CE% per resource, make sure to:

   * set the value of =cluster_name= in the =[GIP]= section to be the same for each CE
   * set the value of =other_ces= in the [GIP] section to be the hostname of the other CEs at your site; this should be comma separated.  So, if you have two CEs, ce1.example.com and ce2.example.com, the value of other_ces on ce1.example.com should be "ce2.example.com.  This assumes that the same queues are visible on each CE.
   * set the value of =resource_group= in the "Site Information" section to be the same for each CE. 
   * the value of =resource= in the "Site Information" section should be CE unique.
   * Have the *exact* same configuration values for the GIP, SE*, and Subcluster* sections in each CE.

It is good practice to run "diff" between the config.ini of the different CEs.  The only changes should be the value of localhost in the [DEFAULT] section and the value of other_ces in the [GIP] section.

---++ Subcluster Configuration

Another aspect of the %LINK_GIP% is to advertise the physical hardware a job will encounter when submitted to your site.  This is information is provided to GIP in the =[Subcluster &lt;name&gt;]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini=.

A _sub-cluster_ is defined to be a homogeneous set of worker node hardware. At least one Subcluster section _is required_. For WLCG sites, information filled in here will be advertised as part of your !MoU commitment, so please strive to make sure it is correct.  

For each sub-cluster constituting your site, fill in the information about the worker node hardware by creating a new section choosing a unique name using the following format:  =[Subcluster &lt;name&gt;]= where &lt;name&gt; is the sub-cluster name.

%IMPORTANT% for OSG 1.2.4 and below, this sub-cluster name must be unique for the entire OSG.  Do not pick something generic like =[Subcluster Opterons]=!

Examples can be found [[ReleaseDocumentation/ConfigurationFileGIPExamples][here]]. [[InformationServices/SubclusterMapping][This page]] shows the mapping from attribute names in =config.ini= to [[http://glueschema.forge.cnaf.infn.it/][GLUE]] attribute names.

The values for this section are relatively well-documented and self-explanatory and are given below:

%TWISTY{%TWISTY_OPTS_MORE%}%
%TABLE{ cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150," }%
|*Option* | *Valid Values* | *Explanation* |
|name|_String_|This is the same name that is in the Section label.  It should be globally unique!|
|node_count|_Positive Integer_|This is the number of worker nodes in the sub-cluster|
|ram_mb|_Positive Integer_|Megabytes of RAM per node.|
|cpu_model|_String_|CPU model, as taken from =/proc/cpuinfo=.  Please, no abbreviations!|
|cpu_vendor|_String_|Vendor's name: AMD, Intel, or any other.|
|cpu_speed_mhz|_Positive Integer_|Approximate speed in MHZ of the CPU as taken from =/proc/cpuinfo=|
|cpus_per_node|_Positive Integer_|Number of CPUs (physical chips) per node.|
|cores_per_node|_Positive Integer_|Number of cores per node.|
|inbound_network|_True_ or _False_|Set to true or false depending on inbound connectivity.  That is, external hosts can contact the worker nodes in this sub-cluster based on their hostname.|
|outbound_network|_True_ or _False_|Set to true or false depending on outbound connectivity.  Set to true if the worker nodes in this sub-cluster can communicate with the internet.|
|cpu_platform|_x86_64_ or _i686_|*NEW for OSG 1.2*.  Set according to your sub-cluster's processor architecture. |
%ENDTWISTY%

---++ For Compute Elements

The %LINK_GIP% queries the batch system specified and enabled in =%UCL_CWD%/osg/etc/config.ini=. Alternatively you may manually specify which batch system to query by setting the =batch= attribute in the =[GIP]= section.

---+++ Information on PBS

By default any authorized grid user may submit to every PBS queue listed in =$VDT_LOCATION/globus/share/globus_gram_job_manager/pbs.rvf=. In this case GIP automatically advertises all listed queues. 

GIP will also detect if queues restrict access to certain users (=acl_users=) or certain groups (=acl_groups=). A reverse mapping from these users and groups to their associated %LINK_GLOSSARY_VO% provides access information of VOs to certain queues which will be advertised by the GIP instead.

This process is not perfect and sometimes fails to generate the right information. In this case you may _manually_ blacklist and whitelist PBS queues for listed VOs in the =[PBS]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini=:

%CODE{"scheme"}%
[PBS]

; deny access to all VOs to the queue identified by <queuename>
<queuename>_blacklist = *

; then allow access for one or more VOs to the same queue
<queuename>_whitelist = <a VO name>, <another VO name>

%ENDCODE%

%NOTE% To generate a list of supported VOs for a queue, the blacklist is evaluated before the whitelist.

To _exclude_ queues from being advertised to the grid provide a comma-separated list of queue names to the =queue_exclude= attribute:

%CODE{"scheme"}%
[PBS]

; exclude the queue specified by their names from being advertised to the grid
queue_exclude = <a queue name>, <another queue name>
%ENDCODE%

---+++ Information on Condor

By default, owner VMs are not accounted for the total CPU numbers published by GIP. According to Condor definition an Owner state implies <i>"The machine is 
being used by the machine owner, and/or is not available to run Condor jobs"</i>.  In order to tell the GIP to count the CPUs in _"Owner"_ state for the total CPU count a site administrator should add the following line to the =[Condor]= section in the resource configuration file =%UCL_CWD%/osg/etc/config.ini=:

%CODE{"scheme"}%
[Condor]

; Do no subtract CPUs provided by owner VMs:
subtract_owner = False
%ENDCODE%

---+++ Information on SGE

The =home= attribute in the =[SGE]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini= has been replaced by =sge_root= and =sge_cell=, which should be set to the value of  of =$SGE_ROOT= and =$SGE_CELL=, respectively.  The GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.


#StorageElementConfiguration
---++ For Storage Elements

For each %LINK_GLOSSARY_SE% add a new section to the resource configuration file =%UCL_CWD%/osg/etc/config.ini= named =[SE_&lt;name&gt;]= where &lt;name&gt; is the unique name of your SE. The following sections illustrate the configuration process for a:

   1 Generic Storage Element
   1 !BeStMan Storage Element
   1 dCache Storage Element

See following [[ConfigurationFileGIPExamples][examples]] for working SE configuration sections.

%NOTE% WLCG sites _must_ use the =betsman= or =dcache19= provider in order to be advertised correctly.

---+++ Generic Storage Element

Following attributes are available to configure the information provided by GIP for a generic storage element:

%TABLE{ cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150," }%
|*Option* | *Valid Values* | *Explanation* |
|enabled |_True_ or _False_|Indicates whether or not this SE section is enabled for the GIP.|
|name |_String_|Name of the Storage Element as registered in %LINK_OIM%.|
|srm_endpoint |_String_|The endpoint of the SE.  It MUST contain the _hostname_, _port_, and the _server location_ (such as /srm/v2/server).  It MUST NOT have the ?SFN= string.  Example:  srm://srm.example.com:8443/srm/v2/server |
|provider_implementation |String|Set to =static= for a generic SE.%BR%Set to =bestman= for !BeStMan SE.%BR%Set to =dcache= or =dcache19= for dCache SE.|
|implementation |_String_|Name of the SE implementation.|
|version |_String_|Version number of the SE implementation.|
|default_path |_String_|Default storage path.|
|use_df |_True_ or _False_|Set to _True_ if the SE provider may use =df= on the directory referenced in =default_path= to obtain the size of the SE.|
|vo_dirs |_Comma-separated List_|A comma-separated list of &lt;VONAME&gt;:&lt;PATH&gt; pairs. The PATH will override the =default_path= attribute for VONAME.|
|allowed_vos|_Comma-separated List_|By default all VOs are advertised to have access to the SE. To advertise only a subset of VOs, provide a comma-separated list here.|

---+++ !BeStMan Storage Element

For !BeStMan storage elements, there are a few notes to consider for the SE attributes:

%TABLE{ cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150," }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/v2/server.|
|provider_implementation|_String_|Set to =bestman=.|
|implementation|_String_|Set to =bestman=.|

The !BeStMan provider may use =srm-ping= to query a !BeStMan storage element for information.  This means that the Unix =daemon= user will need to access to a valid =http= service certificate in =/etc/grid-security/http/httpcert.pem= and its associated key. Instructions on how to request and install a service certificate can be found [[ReleaseDocumentation/GetGridCertificates#Retrieve_and_Install_the_Service][here]].

---+++ dCache Storage Element

%TABLE{ cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150," }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/managerv2.|
|provider_implementation|_String_|Set to =dcache19= for dCache 1.9 sites.%BR%Set to =static= for default values.%BR%If you use the dcache provider with dCache 1.8, see [[InformationServices/DcacheGip][this page to complete installation]].  If you use the dcache19 provider, you must fill in the location of your dCache's information provider|
|infoprovider_endpoint|_String_|Url to the dcache information provider.  Only required for the dcache19 provider.|
|implementation|_String_|Set to =dcache=.|
|version|_String_|The dCache version.%BR%Will be _auto-detected_ in the case of non-static providers.|

The implementation of dCache version 1.9 added a new read-only XML information service which feeds the dcache19 provider.  The location of the info provider is most likely =http://dcache_head_node.example.com:2288/info=. It runs on the same node that the dCache web interface. Also consult your dCache documentation on how to enable the information service.

---+++ Advertise Available Storage Space

The %LINK_GIP% has a concept of _space_ as the logical grouping of available storage space. The =static= provider just advertises the SE as one homogeneous storage space. The =dcache= and =bestman= providers can break the storage space into several spaces.

You can control how the space access is advertised through adding options to the SE's section, as documented below.
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%

For dCache, a space may be one of several dCache concepts:
   * A link group is a space, if link groups are configured.
   * A pool group is a space.
If a pool group is inside a link group, only the link group will be advertised.  The space name is the name of the link group or pool group.

For !BestMan, a space is a SRM space token.  The name of the space is the space token description.

The following space-related attributes are valid.  Add these to your SE's section.  Replace NAME with the space's name:
|Option|Values Accepted|Explanation|
|spaces|Comma-delimited list|Comma-delimited list of all the spaces that should be restricted by VO|
|space_NAME_vos|Comma-delimited list of VOs|By default, all VOs are allowed to access all spaces.  If you would like to change this for a specific space, "NAME", this should be a comma-delimited list of VOs allowed to access NAME.|
|space_NAME_default_path|String|The default storage path for VOs for the space NAME.  As in the "default_path" option, the word VONAME is evaluated to be the VO's name.|
|space_NAME_path|Comma-delimited list|A list of VONAME:PATH pairs for this space; works like the SE's vo_dirs variable.|

Both dCache and !BeStMan providers will attempt to guess the correct values, but are not always perfect.  We recommend you fill these in.
%ENDTWISTY%

---+++ No SE - Publish CE to SE Bindings for external SE
For a small number of sites, the SE may be hosted externally by another site and shared.  For this use case, the edit gip.conf in $VDT_LOCATION/gip/etc (create if not present).  Make sure the following lines are present:
<verbatim>
[cesebind]
simple=False
se_list=SE1.example.com,SE2.example.com,...
</verbatim>
The se_list is a comma delimited list of SE Unique ID's as configured at the external site.  Usually, the SE unique ID is set to the full hostname of the SRM endpoint.

---++ GIP Section Options

The [GIP] section of the config.ini covers the advertising of extra services that do not fit naturally elsewhere.  These are relatively well-documented in the config.ini file; the majority of sites do not need to edit anything.  However, Multi-CE sites *MUST* edit both =cluster_name= and =other_ces=.

All options are given in the table below:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
| Option | Values Accepted | Explanation |
| advertise_gums | =True=, =False= | Defaults to False. If you want GIP to query and advertise your gums server set this to True. |
| advertise_gsiftp | =True=, =False= | Defaults to True.  If you don't want GIP to advertise your gridftp server set this to False. |
| gsiftp_host  | String | This should be set to the name of the gridftp server GIP will advertise if the advertise_gridftp setting is set to True. |
| cluster_name | String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the FQDN of the head node of the cluster. |
| other_ces| String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the comma-separate list of FQDNs for the other CEs at this site. |
%ENDTWISTY%

%STOPINCLUDE%
%BR%

---+ Comments

| See my note in the multi-CE section:  you say for a multi_ce section that site_name should  be the same for all but for OSG 1.2.4 and greater site_name is deprecated, shouldn&#39;t it be&#60;br /&#62;resource_group instead? | Main.StevenTimm | 30 Aug 2010 - 18:00 |
| You are correct.  It is fixed. | Main.AnthonyTiradani | 23 Sep 2010 - 09:30 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani 

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = HowTo

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman

 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->
