%META:TOPICINFO{author="RobertEngel" date="1287675871" format="1.1" version="1.28"}%
%DOC_STATUS_TABLE%

---+!! Configuration of the Generic Information Provider
%TOC%

---+ About this Document
<!-- conventions used in this document
   * Local UCL_CWD  = /opt/osg-%VERSION%
   * Local UCL_HOST = ce
   * Local TABLE_OPTS = cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150,"
   * Local UCL_CONFIG = =%UCL_CWD%/osg/etc/config.ini=
-->

%ICON{hand}% This document is for System Administrators. It contains the procedure to configure the %LINK_GIP%.

%STARTINCLUDE%

%STARTSECTION{"All"}%

%STARTSECTION{"ConfigurationStart"}%
---%SHIFT%+ Configuration of the Generic Information Provider (GIP)

One important aspect of a site being part of the %LINK_OSG% is the ability of the %LINK_GLOSSARY_CE% to describe the site to external users. The %LINK_GIP% generates site information and provides it in the [[http://glueschema.forge.cnaf.infn.it/][GLUE]] schema.

The GIP is collecting information about some aspects of your site including its hardware composition, the batch system and associated storage. It relies on the resource configuration file %UCL_CONFIG% to collect the information.

The following sections detail information about the options in the =[GIP]=, =[Subcluster]=, and =[SE]= sections of the configuration file.

<!--
Do we really still need the information below?
 
_Note for seasoned admins_:  This section has undergone a *significant amount of revision*; the latest update has introduced a new way to configure subclusters and SEs.  The OSG 0.8.0 style configuration should still work, but is deprecated.  *We strongly recommend updating your config to the current style.*
-->
%ENDSECTION{"ConfigurationStart"}%

%STARTSECTION{"MultiCE"}%
---%SHIFT%++ For Sites with Multiple Compute Elements

If you would like to properly advertise multiple %LINK_GLOSSARY_CE% per resource, make sure to:

   * set the value of =cluster_name= in the =[GIP]= section to be the same for each CE
   * set the value of =other_ces= in the [GIP] section to be the hostname of the other CEs at your site; this should be comma separated.  So, if you have two CEs, ce1.example.com and ce2.example.com, the value of other_ces on ce1.example.com should be "ce2.example.com.  This assumes that the same queues are visible on each CE.
   * set the value of =resource_group= in the "Site Information" section to be the same for each CE. 
   * the value of =resource= in the "Site Information" section should be CE unique.
   * Have the *exact* same configuration values for the GIP, SE*, and Subcluster* sections in each CE.

It is good practice to run "diff" between the %UCL_CONFIG% of the different CEs.  The only changes should be the value of localhost in the [DEFAULT] section and the value of other_ces in the [GIP] section.

%ENDSECTION{"MultiCE"}%

%STARTSECTION{"SubCluster"}%
---%SHIFT%++ Subcluster Configuration

Another aspect of the %LINK_GIP% is to advertise the physical hardware a job will encounter when submitted to your site.  This is information is provided to GIP in the =[Subcluster &lt;name&gt;]= section of the resource configuration file %UCL_CONFIG%.

A _sub-cluster_ is defined to be a homogeneous set of worker node hardware. At least one Subcluster section _is required_. For WLCG sites, information filled in here will be advertised as part of your !MoU commitment, so please strive to make sure it is correct.  

For each sub-cluster constituting your site, fill in the information about the worker node hardware by creating a new section choosing a unique name using the following format:  =[Subcluster &lt;name&gt;]= where &lt;name&gt; is the sub-cluster name.

%IMPORTANT% for OSG 1.2.4 and below, this sub-cluster name must be unique for the entire OSG.  Do not pick something generic like =[Subcluster Opterons]=!

Examples can be found [[ReleaseDocumentation/ConfigurationFileGIPExamples][here]]. [[InformationServices/SubclusterMapping][This page]] shows the mapping from attribute names in %UCL_CONFIG% to [[http://glueschema.forge.cnaf.infn.it/][GLUE]] attribute names.

The values for this section are relatively well-documented and self-explanatory and are given below:

%TWISTY{%TWISTY_OPTS_MORE%}%
%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|name|_String_|This is the same name that is in the Section label.  It should be globally unique!|
|node_count|_Positive Integer_|This is the number of worker nodes in the sub-cluster|
|ram_mb|_Positive Integer_|Megabytes of RAM per node.|
|cpu_model|_String_|CPU model, as taken from =/proc/cpuinfo=.  Please, no abbreviations!|
|cpu_vendor|_String_|Vendor's name: AMD, Intel, or any other.|
|cpu_speed_mhz|_Positive Integer_|Approximate speed in MHZ of the CPU as taken from =/proc/cpuinfo=|
|cpus_per_node|_Positive Integer_|Number of CPUs (physical chips) per node.|
|cores_per_node|_Positive Integer_|Number of cores per node.|
|inbound_network|_True_ or _False_|Set to true or false depending on inbound connectivity.  That is, external hosts can contact the worker nodes in this sub-cluster based on their hostname.|
|outbound_network|_True_ or _False_|Set to true or false depending on outbound connectivity.  Set to true if the worker nodes in this sub-cluster can communicate with the internet.|
|cpu_platform|_x86_64_ or _i686_|*NEW for OSG 1.2*.  Set according to your sub-cluster's processor architecture. |
%ENDTWISTY%

%ENDSECTION{"SubCluster"}%

%STARTSECTION{"ForComputeElements"}%
---%SHIFT%++ For Compute Elements

The %LINK_GIP% queries the batch system specified and enabled in %UCL_CONFIG%. Alternatively you may manually specify which batch system to query by setting the =batch= attribute in the =[GIP]= section. 

%AH_CONDOR_BEGIN%---%SHIFT%+++ Information on Condor

By default, owner VMs are not accounted for the total CPU numbers published by GIP. According to Condor definition an Owner state implies <i>"The machine is 
being used by the machine owner, and/or is not available to run Condor jobs"</i>.  In order to tell the GIP to count the CPUs in _"Owner"_ state for the total CPU count a site administrator should add the following line to the =[Condor]= section in the resource configuration file %UCL_CONFIG%:

%CODE{"scheme"}%
[Condor]

; Do no subtract CPUs provided by owner VMs:
subtract_owner = False
%ENDCODE%

%AH_CONDOR_END%

%AH_PBS_BEGIN%---%SHIFT%+++ Information on PBS

By default any authorized grid user may submit to every PBS queue listed in =$VDT_LOCATION/globus/share/globus_gram_job_manager/pbs.rvf=. In this case GIP automatically advertises all listed queues. 

GIP will also detect if queues restrict access to certain users (=acl_users=) or certain groups (=acl_groups=). A reverse mapping from these users and groups to their associated %LINK_GLOSSARY_VO% provides access information of VOs to certain queues which will be advertised by the GIP instead.

This process is not perfect and sometimes fails to generate the right information. In this case you may _manually_ blacklist and whitelist PBS queues for listed VOs in the =[PBS]= section of the resource configuration file %UCL_CONFIG%:

%CODE{"scheme"}%
[PBS]

; deny access to all VOs to the queue identified by <queuename>
<queuename>_blacklist = *

; then allow access for one or more VOs to the same queue
<queuename>_whitelist = <a VO name>, <another VO name>

%ENDCODE%

%NOTE% To generate a list of supported VOs for a queue, the blacklist is evaluated before the whitelist.

To _exclude_ queues from being advertised to the grid provide a comma-separated list of queue names to the =queue_exclude= attribute:

%CODE{"scheme"}%
[PBS]

; exclude the queue specified by their names from being advertised to the grid
queue_exclude = <a queue name>, <another queue name>
%ENDCODE%

%AH_PBS_END%

%AH_SGE_BEGIN%---%SHIFT%+++ Information on SGE

The =home= attribute in the =[SGE]= section of the resource configuration file %UCL_CONFIG% has been replaced by =sge_root= and =sge_cell=, which should be set to the value of  of =$SGE_ROOT= and =$SGE_CELL=, respectively.  The GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.

%AH_SGE_END%

---%SHIFT%+++ Link the Compute Element to an external Storage Element

A compute element may provide mass storage to grid users on an _external_ storage element. To advertise the available storage space on the external storage element create the file =%UCL_CWD%/gip/etc/gip.conf= with following content:

%CODE{"scheme"}%
[cesebind]
; Advertise the availability of an external storage element for mass storage use on the compute element.
simple = False
se_list = <coma-separated list of the SRM endpoint of storage elements>
%ENDCODE%

%ENDSECTION{"ForComputeElements"}%


%STARTSECTION{"ForStorageElements"}%

#StorageElementConfiguration
---%SHIFT%++ For Storage Elements

For each %LINK_GLOSSARY_SE% add a new section to the resource configuration file %UCL_CONFIG% named =[SE_&lt;name&gt;]= where &lt;name&gt; is the unique name of your SE. The following sections illustrate the configuration process for a:

   1 Generic Storage Element
   1 %BESTMAN% Storage Element
   1 dCache Storage Element

See following [[ConfigurationFileGIPExamples][examples]] for working SE configuration sections.

%NOTE% WLCG sites _must_ use the =betsman= or =dcache19= provider in order to be advertised correctly.

---%SHIFT%+++ Generic Storage Element

Following attributes are available to configure the information provided by GIP for a generic storage element:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|enabled |_True_ or _False_|Indicates whether or not this SE section is enabled for the GIP.|
|name |_String_|Name of the Storage Element as registered in %LINK_OIM%.|
|srm_endpoint |_String_|The endpoint of the SE.  It MUST contain the _hostname_, _port_, and the _server location_ (such as /srm/v2/server).  It MUST NOT have the ?SFN= string.  Example:  srm://srm.example.com:8443/srm/v2/server |
|provider_implementation |String|Set to =static= for a generic SE.%BR%Set to =bestman= for %BESTMAN% SE.%BR%Set to =dcache= or =dcache19= for dCache SE.|
|implementation |_String_|Name of the SE implementation.|
|version |_String_|Version number of the SE implementation.|
|default_path |_String_|Default storage path.|
|use_df |_True_ or _False_|Set to _True_ if the SE provider may use =df= on the directory referenced in =default_path= to obtain the size of the SE.|
|vo_dirs |_Comma-separated List_|A comma-separated list of &lt;VONAME&gt;:&lt;PATH&gt; pairs. The PATH will override the =default_path= attribute for VONAME.|
|allowed_vos|_Comma-separated List_|By default all VOs are advertised to have access to the SE. To advertise only a subset of VOs, provide a comma-separated list here.|

---%SHIFT%+++ %BESTMAN% Storage Element

For %BESTMAN% storage elements, there are a few notes to consider for the SE attributes:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/v2/server.|
|provider_implementation|_String_|Set to =bestman=.|
|implementation|_String_|Set to =bestman=.|

The %BESTMAN% provider may use =srm-ping= to query a %BESTMAN% storage element for information.  This means that the Unix =daemon= user will need to access to a valid =http= service certificate in =/etc/grid-security/http/httpcert.pem= and its associated key. Instructions on how to request and install a service certificate can be found [[ReleaseDocumentation/GetGridCertificates#Retrieve_and_Install_the_Service][here]].

---%SHIFT%+++ dCache Storage Element

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/managerv2.|
|provider_implementation|_String_|Set to =dcache19= for dCache 1.9 sites.%BR%Set to =static= for default values.%BR%If you use the dcache provider with dCache 1.8, see [[InformationServices/DcacheGip][this page to complete installation]].  If you use the dcache19 provider, you must fill in the location of your dCache's information provider|
|infoprovider_endpoint|_String_|Url to the dcache information provider.  Only required for the dcache19 provider.|
|implementation|_String_|Set to =dcache=.|
|version|_String_|The dCache version.%BR%Will be _auto-detected_ in the case of non-static providers.|

The implementation of dCache version 1.9 added a new read-only XML information service which feeds the dcache19 provider.  The location of the info provider is most likely =http://dcache_head_node.example.com:2288/info=. It runs on the same node that the dCache web interface. Also consult your dCache documentation on how to enable the information service.

---%SHIFT%+++ Advertise Available Storage Space

The %LINK_GIP% has a concept of _space_ as the logical grouping of available storage space. The =static= provider just advertises the storage element as one homogeneous storage space. The dCache and %BESTMAN% can also partition the storage space into several sub-spaces.

   * For dCache a space may be either a _link group_ or a _pool_ group. If a link group contains one or more pool groups, only the link group will be advertised. The space name is the name of the link group or pool group.

   * For %BESTMAN% a space is a SRM _space token_. The name of the space is the description of the space token.


Both dCache and %BESTMAN% storage elements will try to detect the correct values automatically. To _manually_ change attributes in the SE section of the resource configuration file %UCL_CONFIG% consult the table of available space-related attributes:


%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|spaces|_Comma-separated List_|List of all the spaces that should be restricted by VO.|
|space_&lt;NAME&gt;_vos|_Comma-separated List_|By default, all VOs are allowed to access all spaces. To restrict access by VO provide a list of VOs that may access the storage space &lt;NAME&gt;.|
|space_&lt;NAME&gt;_default_path|_String_|The default storage path for VOs for the space named &lt;NAME&gt;.  As in the "default_path" option, the word VONAME is evaluated to be the VO's name.|
|space_&lt;NAME&gt;_path|_Comma-separated List_|A list of &lt;VONAME&gt;:&lt;PATH&gt; pairs for this space.|

%ENDSECTION{"ForStorageElements"}%


%STARTSECTION{"AdvertiseAvailableServices"}%
---%SHIFT%++ Advertise Available Services

The =[GIP]= section of the resource configuration file %UCL_CONFIG% provides the possibility to advertise services available at your site.

However, Multi-CE sites *MUST* edit both =cluster_name= and =other_ces=.

All options are given in the table below:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
| advertise_gums  | _True_ or _False_  | Defaults to False. If you want GIP to query and advertise your gums server set this to True. |
| advertise_gsiftp  | _True_ or _False_  | Defaults to True.  If you don't want GIP to advertise your gridftp server set this to False. |
| gsiftp_host  | _String_  | This should be set to the name of the gridftp server GIP will advertise if the advertise_gridftp setting is set to True. |
| cluster_name | _String_  | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the FQDN of the head node of the cluster. |
| other_ces| _String_  | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the comma-separate list of FQDNs for the other CEs at this site. |

%ENDSECTION{"AdvertiseAvailableServices"}%

%ENDSECTION{"All"}%
%STOPINCLUDE%

---+ Comments

%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani 

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman

 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->
