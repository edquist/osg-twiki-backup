%META:TOPICINFO{author="StevenTimm" date="1283191226" format="1.1" reprev="1.20" version="1.20"}%
---+!! *%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%

%BR%
---+ _%INCLUDEHEADING%  %SPACEOUT{ "%TOPIC%" }%_
%BR%

%STARTINCLUDE%
---++ Information services in the OSG.
%EDITTHIS%

One important aspect of your site being in the larger grid is the CE's ability to describe the site to external users.  In the OSG, the piece of software that generates the site information is called the "Generic Information Provider" (GIP); the GIP generates information in a schema called GLUE.

Because the GIP advertises information about all aspects of your site including the batch system, cluster hardware composition, and associated storage, it is sensitive to config.ini errors.  The GIP reads config.ini directly; there is never any need to re-run configure-osg when making GIP-related changes.

This section gives information about the options in the [GIP], [Subcluster*], and [SE*] sections of the configuration file.

_Note for seasoned admins_:  This section has undergone a *significant amount of revision*; the latest update has introduced a new way to configure subclusters and SEs.  The OSG 0.8.0 style configuration should still work, but is deprecated.  *We strongly recommend updating your config to the current style.*

---+++ Batch System Information

The GIP will query your batch system for many pieces of information; it will determine which batch system to query by examining the [Condor], [LSF], [SGE], and [PBS] sections and look to see which is enabled.  You can manually specify the batch system by setting the _batch_ attribute in the [GIP] section to one of the batch system names.

---++++ Batch System Specific Notes

   * *PBS*:  Globus allows users to submit to any queue listed in $VDT_LOCATION/globus/share/globus_gram_job_manager/pbs.rvf.  The GIP automatically reads this file and advertises all listed queues.  If any of the queues have acl_users or acl_groups set, the GIP will also try to map the user/group names to VOs and correctly advertise which VOs are allowed access to which queues.  This process is not perfect and sometimes fails to generate the right information; we have provided a manual override.  PBS sites may add the following attributes to the [PBS] section of config.ini.
      * =&lt;queuename&gt;_blacklist= - Comma separated list of VO names to exclude from queue <queuename>; * to exclude all.
         * To generate a list of supported VOs for a queue, first the queue's blacklist is generated, then the whitelist is applied.  This way, even if the blacklist is *, some VO might be allowed in it.
      * =&lt;queuename&gt;_whitelist= - Comma separated list of VO names which are always allowed in this queue.
      * =queue_exclude= - Local-user only queues which should not be advertised to the grid.
   * *Condor*:  By default, owner VMs are not counted for the total CPU numbers published by GIP. According to Condor definition an Owner state implies "The machine is being used by the machine owner, and/or is not available to run Condor jobs".  In order to tell the GIP to count the CPUs in "Owner" state for the total CPU count a site administrator should add the following line
   <pre>subtract_owner=False</pre> to config.ini in the [CONDOR] section.
   * *SGE* (This note only applies to OSG 1.2.2 and above.).  The =home= attribute in the [SGE] section has been replaced with with =sge_root= and =sge_cell=, which should be set to the value of  of $SGE_ROOT and $SGE_CELL, respectively.  The GIP will assume that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= and end up with a working SGE tools (such as qstat) in the environment.

---++++ Notes for multi-CE sites.

If you would like to properly advertise multiple CEs per cluster, make sure that you:
   * Set the value of cluster_name in the [GIP] section to be the same for each CE.
   * Set the value of other_ces in the [GIP] section to be the hostname of the other CEs at your site; this should be comma separated.  So, if you have two CEs, ce1.example.com and ce2.example.com, the value of other_ces on ce1.example.com should be "ce2.example.com.  This assumes that the same queues are visible on each CE.
   * Set the value of site_name in the "Site Information" section to be the same for each CE.  (for OSG >=1.2.4 shouldn't that be resource_group instead?! ST)
   * Have the *exact* same configuration values for the GIP, SE*, and Subcluster* sections in each CE.

It is good practice to run "diff" between the config.ini of the different CEs.  The only changes should be the value of localhost in the [DEFAULT] section and the value of other_ces in the [GIP] section.

---+++ Subcluster Configuration

Another important aspect of the GIP is advertising the physical hardware jobs will encounter if they are submitted to your cluster.  This is done by describing the *subclusters* of your cluster.  A subcluster is defined to be a homogeneous set of worker node hardware.  For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format:  [Subcluster CHANGEME] where CHANGEME is the unique subcluster name.  *IMPORTANT*: for OSG 1.2.4 and below, this subcluster name must be unique for the entire OSG.  Do not pick something generic like *Subcluster Opterons*.

At least *one* subcluster section *is required*; please populate the information for all your subclusters.  For WLCG sites, information filled in here will be advertised as part of your !MoU commitment, so please strive to make sure it is correct.  

There is a lot of information needed for a subcluster; examples of working sections can be found at ConfigurationFileGIPExamples.  [[https://twiki.grid.iu.edu/bin/view/InformationServices/SubclusterMapping][This page shows the mapping from these attribute names to GLUE attribute names]].

The values for this section are relatively well-documented and self-explanatory and are given below:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
|Option|Values Accepted|Explanation |
|*name*|string|This is the same name that is in the Section label.  It should be *globally unique*|
|*node_count*|Positive Integer|This is the number of worker nodes in the subcluster|
|*ram_mb*|Positive Integer|Megabytes of RAM per node.|
|*cpu_model*|String|CPU model, as taken from /proc/cpuinfo.  Please, no abbreviations!|
|*cpu_vendor*|String|Vendor's name: AMD, Intel, or ??|
|*cpu_speed_mhz*|Positive Integer|Approximate speed, in MHZ, of the chips|
|*cpus_per_node*|Positive Integer|Number of CPUs (physical chips) per node.  Usually 1 or 2|
|*cores_per_node*|Positive Integer|Number of cores per node.  Usually 4 or 8|
|*inbound_network*|True, False|Set to true or false depending on inbound connectivity.  That is, external hosts can contact the worker nodes in this subcluster based on their hostname.|
|*outbound_network*|True, False|Set to true or false depending on outbound connectivity.  Set to true if the worker nodes in this subcluster can communicate with the external internet.|
|*cpu_platform*| x86_64 or i686 | *NEW for OSG 1.2*.  Set according to your subcluster's processor architecture. |
%ENDTWISTY%

#StorageElementConfiguration
---+++ Storage Element (SE) Configuration

For each storage element, add a new section called [SE_CHANGEME] where CHANGEME is the name of your SE.  Each SE name must be unique for the entire grid, so make sure to not
pick anything generic like "MAIN".  Each SE section must start with the words "SE", and cannot actually be named "CHANGEME".

SE configuration can be a bit tricky.  We provide examples of working SE sections at ConfigurationFileGIPExamples.

---++++ The SE Section

We first outline the configuration for a generic SE, then have additional comments for !BestMan and dCache SEs.  *For WLCG sites to be advertised correctly, they _must_ use the bestman or dcache19 provider.*.  Because this section is relatively difficult for most people, we document it more thoroughly here:

|Option|Values Accepted|Explanation|
|*enabled*|Boolean|True/False Indicates whether or not this SE section is enabled for the GIP.|
|*name*|String|Name of the SE as registered in OIM.|
|*srm_endpoint*|String|The endpoint of the SE.  It MUST have the hostname, port, and the server location (such as /srm/v2/server).  It MUST NOT have the ?SFN= string.  Example:  srm://srm.example.com:8443/srm/v2/server |
|*provider_implementation*|String|Set to *static* for a generic SE; we recommend !BestMan sites use the *bestman* implementation and dCache sites use *dcache* or *dcache19*.  See notes below|
|*implementation*|String|Name of the SE implementation.|
|*version*|String|Version number of the SE implementation.  Some provider implementations will attempt to auto-detect this.|
|*default_path*|String|Default storage paths for the VOs; VONAME is replaced with the VO's name.|
|*use_df*|True, False|Set to True if the SE provider can use 'df' on the directory referenced in default_path to get the size of the SE.  Especially useful for !BestMan installs on top of a file system mounted on the CE (such as xrootdfs, HDFS, or GPFS).  May also work with dCache's Chimera.  |
|vo_dirs|Comma-separated string|A comma-separated list of VONAME:PATH pairs; the PATH will override the *default_path* attribute for VONAME.|
|allowed_vos|Comma-separated list of VOs|By default, all VOs are advertised as having access to your SE.  If you want only a subset of VOs to be advertised, list them here|

For !BestMan-based SEs, there are a few notes to consider for the SE attributes:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
|Option|Values Accepted|Explanation|
|*srm_endpoint*|String|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/v2/server.|
|*provider_implementation*|String|Set to 'bestman'|
|*implementation*|String|Set to 'bestman'|
The !BestMan provider will use srm-ping to query your !BestMan instance for information.  This means that the user *daemon* will need to read a valid hostcert in */etc/grid-security/http/httpcert.pem* and hostkey in */etc/grid-security/http/httpkey.pem*.  !BestMan server does not require any authorization to perform srmPing, so you do not need to change anything on your SRM server.
%ENDTWISTY%

For dCache-based SEs:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
|Option|Values Accepted|Explanation|
|*srm_endpoint*|String|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/managerv2.|
|*provider_implementation*|String|Set to 'dcache19' for dCache 1.9 sites, or 'static' for default values.   If you use the dcache provider with dCache 1.8, see [[InformationServices/DcacheGip][this page to complete installation]].  If you use the dcache19 provider, you must fill in the location of your dCache's information provider|
|*infoprovider_endpoint*|String|Url to the dcache information provider.  Only necessary for the dcache19 provider.|
|*implementation*|String|Set to 'dcache'.|
|*version*|String|dCache version; non-static providers will also attempt to auto-detect this.  This should be the version from the output of "rpm -q dcache-server".|
dCache version 1.9 added a new read-only XML information service which feeds the dcache19 provider.  The location of the info provider is most likely:
<verbatim>
http://dcache_head_node.example.com:2288/info
</verbatim>
It runs on the same node that the dcache web interface does.  Check your dCache documentation on how to enable the info service if that link does not work.
%ENDTWISTY%

---++++ Space configuration.
The information services have a concept of a "space" - logical grouping of storage or area in the namespace.  The static provider advertises the SE as one large homogeneous space, but the dCache and !BestMan providers can break the SE into several spaces (for example, one space for ATLAS, one space for DZero, and one for everyone else).

You can control how the space access is advertised through adding options to the SE's section, as documented below.
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%

For dCache, a space may be one of several dCache concepts:
   * A link group is a space, if link groups are configured.
   * A pool group is a space.
If a pool group is inside a link group, only the link group will be advertised.  The space name is the name of the link group or pool group.

For !BestMan, a space is a SRM space token.  The name of the space is the space token description.

The following space-related attributes are valid.  Add these to your SE's section.  Replace NAME with the space's name:
|Option|Values Accepted|Explanation|
|spaces|Comma-delimited list|Comma-delimited list of all the spaces that should be restricted by VO|
|space_NAME_vos|Comma-delimited list of VOs|By default, all VOs are allowed to access all spaces.  If you would like to change this for a specific space, "NAME", this should be a comma-delimited list of VOs allowed to access NAME.|
|space_NAME_default_path|String|The default storage path for VOs for the space NAME.  As in the "default_path" option, the word VONAME is evaluated to be the VO's name.|
|space_NAME_path|Comma-delimited list|A list of VONAME:PATH pairs for this space; works like the SE's vo_dirs variable.|

Both dCache and !BeStMan providers will attempt to guess the correct values, but are not always perfect.  We recommend you fill these in.
%ENDTWISTY%

---++++ No SE - Publish CE to SE Bindings for external SE
For a small number of sites, the SE may be hosted externally by another site and shared.  For this use case, the edit gip.conf in $VDT_LOCATION/gip/etc (create if not present).  Make sure the following lines are present:
<verbatim>
[cesebind]
simple=False
se_list=SE1.example.com,SE2.example.com,...
</verbatim>
The se_list is a comma delimited list of SE Unique ID's as configured at the external site.  Usually, the SE unique ID is set to the full hostname of the SRM endpoint.

---+++ GIP Section Options

The [GIP] section of the config.ini covers the advertising of extra services that do not fit naturally elsewhere.  These are relatively well-documented in the config.ini file; the majority of sites do not need to edit anything.  However, Multi-CE sites *MUST* edit both =cluster_name= and =other_ces=.

All options are given in the table below:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
| Option | Values Accepted | Explanation |
| advertise_gums | =True=, =False= | Defaults to False. If you want GIP to query and advertise your gums server set this to True. |
| advertise_gsiftp | =True=, =False= | Defaults to True.  If you don't want GIP to advertise your gridftp server set this to False. |
| gsiftp_host  | String | This should be set to the name of the gridftp server GIP will advertise if the advertise_gridftp setting is set to True. |
| cluster_name | String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the FQDN of the head node of the cluster. |
| other_ces| String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the comma-separate list of FQDNs for the other CEs at this site. |
%ENDTWISTY%

%STOPINCLUDE%
%BR%

---++ *Comments*
| See my note in the multi-CE section:  you say for a multi_ce section that site_name should  be the same for all but for OSG 1.2.4 and greater site_name is deprecated, shouldn&#39;t it be&#60;br /&#62;resource_group instead? | Main.StevenTimm | 30 Aug 2010 - 18:00 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani 

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->
