%META:TOPICINFO{author="RobertEngel" date="1285193694" format="1.1" reprev="1.21" version="1.21"}%
%DOC_STATUS_TABLE%

---+!! Configuration of the Generic Information Provider
%TOC%

---+ About this Document
<!-- conventions used in this document
   * Local UCL_CWD  = /opt/osg-%VERSION%
   * Local UCL_HOST = ce
-->

%ICON{hand}% This document is for System Administrators. It contains the procedure to configure the %LINK_GIP%.

%STARTINCLUDE%

---+ Information Services on the OSG

One important aspect of a site being part of the %LINK_OSG% is the ability of the %LINK_GLOSSARY_CE% to describe the site to external users. The %LINK_GIP% generates site information and provides it in the [[http://glueschema.forge.cnaf.infn.it/][GLUE]] schema.

The GIP is collecting information about some aspects of your site including its hardware composition, the batch system and associated storage. It relies on the resource configuration file =%UCL_CWD%/osg/etc/config.ini= to collect the information.

The following sections detail information about the options in the =[GIP]=, =[Subcluster]=, and =[SE]= sections of the configuration file.

<!--
Do we really still need the information below?
 
_Note for seasoned admins_:  This section has undergone a *significant amount of revision*; the latest update has introduced a new way to configure subclusters and SEs.  The OSG 0.8.0 style configuration should still work, but is deprecated.  *We strongly recommend updating your config to the current style.*
-->

---++ Information on Sites with Multiple Compute Elements

If you would like to properly advertise multiple %LINK_GLOSSARY_CE% per resource, make sure to:

   * set the value of =cluster_name= in the =[GIP]= section to be the same for each CE
   * set the value of =other_ces= in the [GIP] section to be the hostname of the other CEs at your site; this should be comma separated.  So, if you have two CEs, ce1.example.com and ce2.example.com, the value of other_ces on ce1.example.com should be "ce2.example.com.  This assumes that the same queues are visible on each CE.
   * set the value of =site_name= in the "Site Information" section to be the same for each CE.  (for OSG >=1.2.4 shouldn't that be resource_group instead?! ST)
   * Have the *exact* same configuration values for the GIP, SE*, and Subcluster* sections in each CE.

It is good practice to run "diff" between the config.ini of the different CEs.  The only changes should be the value of localhost in the [DEFAULT] section and the value of other_ces in the [GIP] section.

---++ Subcluster Configuration

Another aspect of the %LINK_GIP% is to advertise the physical hardware a job will encounter when submitted to your site.  This is information is provided to GIP in the =[Subcluster &lt;name&gt;]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini=.

A _sub-cluster_ is defined to be a homogeneous set of worker node hardware. At least one Subcluster section _is required_. For WLCG sites, information filled in here will be advertised as part of your !MoU commitment, so please strive to make sure it is correct.  

For each sub-cluster constituting your site, fill in the information about the worker node hardware by creating a new section choosing a unique name using the following format:  =[Subcluster &lt;name&gt;]= where &lt;name&gt; is the sub-cluster name.

%IMPORTANT% for OSG 1.2.4 and below, this sub-cluster name must be unique for the entire OSG.  Do not pick something generic like =[Subcluster Opterons]=!

Examples can be found [[ReleaseDocumentation/ConfigurationFileGIPExamples][here]]. [[InformationServices/SubclusterMapping][This page]] shows the mapping from attribute names in =config.ini= to [[http://glueschema.forge.cnaf.infn.it/][GLUE]] attribute names.

The values for this section are relatively well-documented and self-explanatory and are given below:

%TWISTY{%TWISTY_OPTS_MORE%}%
%TABLE{ cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150," }%
|*Option* | *Valid Values* | *Explanation* |
|name|_String_|This is the same name that is in the Section label.  It should be globally unique!|
|node_count|_Positive Integer_|This is the number of worker nodes in the sub-cluster|
|ram_mb|_Positive Integer_|Megabytes of RAM per node.|
|cpu_model|_String_|CPU model, as taken from =/proc/cpuinfo=.  Please, no abbreviations!|
|cpu_vendor|_String_|Vendor's name: AMD, Intel, or any other.|
|cpu_speed_mhz|_Positive Integer_|Approximate speed in MHZ of the CPU as taken from =/proc/cpuinfo=|
|cpus_per_node|_Positive Integer_|Number of CPUs (physical chips) per node.|
|cores_per_node|_Positive Integer_|Number of cores per node.|
|inbound_network|_True_ or _False_|Set to true or false depending on inbound connectivity.  That is, external hosts can contact the worker nodes in this sub-cluster based on their hostname.|
|outbound_network|_True_ or _False_|Set to true or false depending on outbound connectivity.  Set to true if the worker nodes in this sub-cluster can communicate with the internet.|
|cpu_platform|_x86_64_ or _i686_|*NEW for OSG 1.2*.  Set according to your sub-cluster's processor architecture. |
%ENDTWISTY%

---++ Batch System Information

The %LINK_GIP% queries the batch system specified and enabled in =%UCL_CWD%/osg/etc/config.ini=. Alternatively you may manually specify which batch system to query by setting the =batch= attribute in the =[GIP]= section.

---+++ Information on PBS

By default any authorized grid user may submit to every PBS queue listed in =$VDT_LOCATION/globus/share/globus_gram_job_manager/pbs.rvf=. In this case GIP automatically advertises all listed queues. 

GIP will also detect if queues restrict access to certain users (=acl_users=) or certain groups (=acl_groups=). A reverse mapping from these users and groups to their associated %LINK_GLOSSARY_VO% provides access information of VOs to certain queues which will be advertised by the GIP instead.

This process is not perfect and sometimes fails to generate the right information. In this case you may _manually_ blacklist and whitelist PBS queues for listed VOs in the =[PBS]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini=:

%CODE{"scheme"}%
[PBS]

; deny access to all VOs to the queue identified by <queuename>
<queuename>_blacklist = *

; then allow access for one or more VOs to the same queue
<queuename>_whitelist = <a VO name>, <another VO name>

%ENDCODE%

%NOTE% To generate a list of supported VOs for a queue, the blacklist is evaluated before the whitelist.

To _exclude_ queues from being advertised to the grid provide a comma-separated list of queue names to the =queue_exclude= attribute:

%CODE{"scheme"}%
[PBS]

; exclude the queue specified by their names from being advertised to the grid
queue_exclude = <a queue name>, <another queue name>
%ENDCODE%

---+++ Information on Condor

By default, owner VMs are not accounted for the total CPU numbers published by GIP. According to Condor definition an Owner state implies <i>"The machine is 
being used by the machine owner, and/or is not available to run Condor jobs"</i>.  In order to tell the GIP to count the CPUs in _"Owner"_ state for the total CPU count a site administrator should add the following line to the =[Condor]= section in the resource configuration file =%UCL_CWD%/osg/etc/config.ini=:

%CODE{"scheme"}%
[Condor]

; Do no subtract CPUs provided by owner VMs:
subtract_owner = False
%ENDCODE%

---+++ Information on SGE

The =home= attribute in the =[SGE]= section of the resource configuration file =%UCL_CWD%/osg/etc/config.ini= has been replaced by =sge_root= and =sge_cell=, which should be set to the value of  of =$SGE_ROOT= and =$SGE_CELL=, respectively.  The GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.




#StorageElementConfiguration
---+++ Storage Element (SE) Configuration

For each storage element, add a new section called [SE_CHANGEME] where CHANGEME is the name of your SE.  Each SE name must be unique for the entire grid, so make sure to not
pick anything generic like "MAIN".  Each SE section must start with the words "SE", and cannot actually be named "CHANGEME".

SE configuration can be a bit tricky.  We provide examples of working SE sections at ConfigurationFileGIPExamples.

---++++ The SE Section

We first outline the configuration for a generic SE, then have additional comments for !BestMan and dCache SEs.  *For WLCG sites to be advertised correctly, they _must_ use the bestman or dcache19 provider.*.  Because this section is relatively difficult for most people, we document it more thoroughly here:

|Option|Values Accepted|Explanation|
|*enabled*|Boolean|True/False Indicates whether or not this SE section is enabled for the GIP.|
|*name*|String|Name of the SE as registered in OIM.|
|*srm_endpoint*|String|The endpoint of the SE.  It MUST have the hostname, port, and the server location (such as /srm/v2/server).  It MUST NOT have the ?SFN= string.  Example:  srm://srm.example.com:8443/srm/v2/server |
|*provider_implementation*|String|Set to *static* for a generic SE; we recommend !BestMan sites use the *bestman* implementation and dCache sites use *dcache* or *dcache19*.  See notes below|
|*implementation*|String|Name of the SE implementation.|
|*version*|String|Version number of the SE implementation.  Some provider implementations will attempt to auto-detect this.|
|*default_path*|String|Default storage paths for the VOs; VONAME is replaced with the VO's name.|
|*use_df*|True, False|Set to True if the SE provider can use 'df' on the directory referenced in default_path to get the size of the SE.  Especially useful for !BestMan installs on top of a file system mounted on the CE (such as xrootdfs, HDFS, or GPFS).  May also work with dCache's Chimera.  |
|vo_dirs|Comma-separated string|A comma-separated list of VONAME:PATH pairs; the PATH will override the *default_path* attribute for VONAME.|
|allowed_vos|Comma-separated list of VOs|By default, all VOs are advertised as having access to your SE.  If you want only a subset of VOs to be advertised, list them here|

For !BestMan-based SEs, there are a few notes to consider for the SE attributes:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
|Option|Values Accepted|Explanation|
|*srm_endpoint*|String|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/v2/server.|
|*provider_implementation*|String|Set to 'bestman'|
|*implementation*|String|Set to 'bestman'|
The !BestMan provider will use srm-ping to query your !BestMan instance for information.  This means that the user *daemon* will need to read a valid hostcert in */etc/grid-security/http/httpcert.pem* and hostkey in */etc/grid-security/http/httpkey.pem*.  !BestMan server does not require any authorization to perform srmPing, so you do not need to change anything on your SRM server.
%ENDTWISTY%

For dCache-based SEs:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
|Option|Values Accepted|Explanation|
|*srm_endpoint*|String|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/managerv2.|
|*provider_implementation*|String|Set to 'dcache19' for dCache 1.9 sites, or 'static' for default values.   If you use the dcache provider with dCache 1.8, see [[InformationServices/DcacheGip][this page to complete installation]].  If you use the dcache19 provider, you must fill in the location of your dCache's information provider|
|*infoprovider_endpoint*|String|Url to the dcache information provider.  Only necessary for the dcache19 provider.|
|*implementation*|String|Set to 'dcache'.|
|*version*|String|dCache version; non-static providers will also attempt to auto-detect this.  This should be the version from the output of "rpm -q dcache-server".|
dCache version 1.9 added a new read-only XML information service which feeds the dcache19 provider.  The location of the info provider is most likely:
<verbatim>
http://dcache_head_node.example.com:2288/info
</verbatim>
It runs on the same node that the dcache web interface does.  Check your dCache documentation on how to enable the info service if that link does not work.
%ENDTWISTY%

---++++ Space configuration.
The information services have a concept of a "space" - logical grouping of storage or area in the namespace.  The static provider advertises the SE as one large homogeneous space, but the dCache and !BestMan providers can break the SE into several spaces (for example, one space for ATLAS, one space for DZero, and one for everyone else).

You can control how the space access is advertised through adding options to the SE's section, as documented below.
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%

For dCache, a space may be one of several dCache concepts:
   * A link group is a space, if link groups are configured.
   * A pool group is a space.
If a pool group is inside a link group, only the link group will be advertised.  The space name is the name of the link group or pool group.

For !BestMan, a space is a SRM space token.  The name of the space is the space token description.

The following space-related attributes are valid.  Add these to your SE's section.  Replace NAME with the space's name:
|Option|Values Accepted|Explanation|
|spaces|Comma-delimited list|Comma-delimited list of all the spaces that should be restricted by VO|
|space_NAME_vos|Comma-delimited list of VOs|By default, all VOs are allowed to access all spaces.  If you would like to change this for a specific space, "NAME", this should be a comma-delimited list of VOs allowed to access NAME.|
|space_NAME_default_path|String|The default storage path for VOs for the space NAME.  As in the "default_path" option, the word VONAME is evaluated to be the VO's name.|
|space_NAME_path|Comma-delimited list|A list of VONAME:PATH pairs for this space; works like the SE's vo_dirs variable.|

Both dCache and !BeStMan providers will attempt to guess the correct values, but are not always perfect.  We recommend you fill these in.
%ENDTWISTY%

---++++ No SE - Publish CE to SE Bindings for external SE
For a small number of sites, the SE may be hosted externally by another site and shared.  For this use case, the edit gip.conf in $VDT_LOCATION/gip/etc (create if not present).  Make sure the following lines are present:
<verbatim>
[cesebind]
simple=False
se_list=SE1.example.com,SE2.example.com,...
</verbatim>
The se_list is a comma delimited list of SE Unique ID's as configured at the external site.  Usually, the SE unique ID is set to the full hostname of the SRM endpoint.

---+++ GIP Section Options

The [GIP] section of the config.ini covers the advertising of extra services that do not fit naturally elsewhere.  These are relatively well-documented in the config.ini file; the majority of sites do not need to edit anything.  However, Multi-CE sites *MUST* edit both =cluster_name= and =other_ces=.

All options are given in the table below:
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
| Option | Values Accepted | Explanation |
| advertise_gums | =True=, =False= | Defaults to False. If you want GIP to query and advertise your gums server set this to True. |
| advertise_gsiftp | =True=, =False= | Defaults to True.  If you don't want GIP to advertise your gridftp server set this to False. |
| gsiftp_host  | String | This should be set to the name of the gridftp server GIP will advertise if the advertise_gridftp setting is set to True. |
| cluster_name | String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the FQDN of the head node of the cluster. |
| other_ces| String | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the comma-separate list of FQDNs for the other CEs at this site. |
%ENDTWISTY%

%STOPINCLUDE%
%BR%

---++ *Comments*
| See my note in the multi-CE section:  you say for a multi_ce section that site_name should  be the same for all but for OSG 1.2.4 and greater site_name is deprecated, shouldn&#39;t it be&#60;br /&#62;resource_group instead? | Main.StevenTimm | 30 Aug 2010 - 18:00 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani 

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->
