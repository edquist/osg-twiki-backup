%META:TOPICINFO{author="JeffDost" date="1307558936" format="1.1" version="1.10"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{%TOPIC%}%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%

*Purpose*: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.

---+ Preparation
---++ Introduction

Hadoop Distributed File System (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:

   * An SRM interface for grid access; 
   * !GridFTP-HDFS as transport layer; and  
   * A FUSE interface for localized POSIX access.
   * Version *0.20.2* of Apache Hadoop.

The VDT packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs. Two YUM repositories are available: 
   * Stable repository for wider deployments and production usage.
   * Testing repository for limited deployments and pre-release evaluation.

Please note that this distribution is independent of the Pacman packaging of the VDT: it is separately versioned, and separately packaged. We expect future releases to eventually be common with the rest of the VDT, as the "rest" of the VDT begins to be packaged as RPMs. For now, the VDT distribution of Hadoop is distinct from the rest of the VDT.

The *stable YUM repository* is enabled by default through the *osg-hadoop-20 RPM*, and contains the *golden release* supported by OSG for LHC operations. 

---+++ VDT Downloads webpage

The VDT Downloads webpage is http://vdt.cs.wisc.edu/components/hadoop.html

---+++ VDT Release notes webpage

The VDT Release notes are available at http://vdt.cs.wisc.edu/hadoop/release-notes.html

---++ Architecture

This diagram shows the suggested topology and distribution of services at a Hadoop site. Major service components and modules which need to be deployed on the various nodes are listed. Please use this as a recommendation to prepare for the Hadoop deployment procedure at your site.

<img src="%ATTACHURLPATH%/Hadoop-site-architecture.png">

---+!!Engineering Considerations

Please read the [[Storage.HadoopUnderstanding][planning document]] to understand different components of the system. 

---+!!Help!
Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email osg-storage@opensciencegrid.org and osg-hadoop@opensciencegrid.org.

---+ Checklist
An in-line numbered list of items assumed to have been already performed
E.g.:
   1. Installs that must be done before this one
   1. Security environment in place
   1. batch system in place
   1. stop existing service
   1. ports required

---+ First Time Install
A link to document containing information and install steps that only need to be performed if this a first time install.

---+ Installation Procedure

Main server components can be divided in 3 categories: 
   * HDFS core: Namenode, Datanode.
   * Grid extensions: !BeStMan SRM, Globus !GridFTP, Gratia probe, and Xrootd server plugin, etc.
   * HDFS auxiliary: Secondary Namenode, Hadoop Balancer.

Main client components are FUSE and Hadoop command line client.

---+ Initializer RPM

|Target |On all nodes:|

---++ Initializing the YUM Repository

Download and install the =osg-hadoop-20= RPM on *all* nodes. This will initialize the OSG YUM repository for Hadoop.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% rpm -Uvh http://vdt.cs.wisc.edu/hadoop/osg-hadoop-20-2.el5.noarch.rpm
</pre>

This initializes YUM repository configuration in =/etc/yum.repos.d/osg-hadoop.repo=. 

---++ Choosing Stable or ITB Repository

%NOTE% %RED% *For Integration Testbed (ITB) Sites:* %ENDCOLOR% 
   * By default, *Stable Repository* is enabled (=enabled=1=) in the YUM configuration. Production sites should use the default setting.
   * ITB sites doing testing can enable the *Testing Repository* to fetch pre-release packages. 

Simply set =enabled=0= in =[hadoop]= section and =enabled=1= in =[hadoop-testing]= section of =/etc/yum.repos.d/osg-hadoop.repo=.

*YUM Repository types in /etc/yum.repos.d/osg-hadoop.repo*

<table>
<tr colspan=2>
<td>
*Production Sites:*
<pre class="file">
[hadoop]
... ...
enabled=1
... ...

[hadoop-testing]
... ...
enabled=0
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
</pre>
</td>
<td>
*Integration Sites:*
<pre class="file">
[hadoop]
... ...
enabled=0
... ...

[hadoop-testing]
... ...
enabled=1
... ...

[hadoop-unstable]
... ...
enabled=0
... ...
</pre>
</td>
</tr>
</table>

---+ Installing Hadoop

---++ Prerequisites

%INCLUDE{"Storage/Hadoop20Installation" section="Prereqs"}%

---++ Installation

%INCLUDE{"Storage/Hadoop20Installation" section="Prep"}%

To install hadoop, run:

%INCLUDE{"Storage/Hadoop20Installation" section="Install"}%

---++ Configuration

%INCLUDE{"Storage/Hadoop20Installation" section="Config" TOC_SHIFT="+"}%

---++ Running Hadoop

%INCLUDE{"Storage/Hadoop20Installation" section="Running" TOC_SHIFT="+"}%

---++ Mounting fuse at boot time 

%INCLUDE{"Storage/Hadoop20Installation" section="Fuse"}%

---+ Installing !GridFTP

---++ Prerequisites

   1. %INCLUDE{"Storage/Hadoop20GridFTP" section="HadoopReq"}%
   1. %INCLUDE{"Storage/Hadoop20GridFTP" section="GumsReq"}%

%INCLUDE{"Storage/Hadoop20GridFTP" section="Prereqs"}%

---++ Installation

To install gridftp-hdfs server, run:

%INCLUDE{"Storage/Hadoop20GridFTP" section="Install"}%

---++ Configuration

%INCLUDE{"Storage/Hadoop20GridFTP" section="Config"}%

---++ Running !GridFTP 

%INCLUDE{"Storage/Hadoop20GridFTP" section="Running"}%

---+ Installing !BeStMan2

---++ Prerequisites
   1. You must also have already installed and mounted Hadoop using FUSE.
   1.  A !GridFTP-HDFS server must also be installed, but this does not need to be on the same server as the !BeStMan2 server.  A larger site will prefer to have their !GridFTP and !BeStMan2 servers installed on separate hosts.

%INCLUDE{"Storage/Hadoop20SRM" section="Prereqs"}%

---++ Installation

%INCLUDE{"Storage/Hadoop20SRM" section="Install"}%

---++ Configuration

%INCLUDE{"Storage/Hadoop20SRM" section="Config1"}%

!BeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations, such as mkdir, rm, and ls.  As per the Hadoop install instructions, edit =/etc/sysconfig/hadoop= and run =service hadoop-firstboot start=.  It is *not* necessary (or even recommended) to start any hadoop services with =service hadoop start=.

%INCLUDE{"Storage/Hadoop20SRM" section="Config2"}%

---++ Running !BeStMan2

%INCLUDE{"Storage/Hadoop20SRM" section="Running"}%

---+ Installing Gratia Probes

---+ Service Configuration/Startup/Shutdown

---++ Creating VO and User filesystem areas

Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. 

%NOTE% Create (and maintain) usernames and groups with UIDs and GIDs on all nodes. These are maintained in basic system files such as =/etc/passwd= and =/etc/group=.

For clean HDFS operations and filesystem management:

(a) Create top-level VO subdirectories under =/mnt/hadoop=.

Example: 

<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/cms
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/dzero
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/sbgrid
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/fermigrid
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/cmstest
%UCL_PROMPT_ROOT% mkdir /mnt/hadoop/osg
</pre>

(b) Create individual top-level user areas, under each VO area, as needed.

<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/michaelthomas
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/brianbockelman
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/douglasstrain
%UCL_PROMPT_ROOT% mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana
</pre>

(c) Adjust username:group ownership of each area. 

<pre class="rootscreen">
%UCL_PROMPT_ROOT% chown -R cms:cms /mnt/hadoop/cms
%UCL_PROMPT_ROOT% chown -R sam:sam /mnt/hadoop/dzero

%UCL_PROMPT_ROOT% chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas
</pre>

---+ Validation of Service Operation

Get familiar with Hadoop commands.

<pre class="screen">
%UCL_PROMPT% /usr/bin/hadoop -h
</pre>

An online guide is also available at [[http://hadoop.apache.org/common/docs/current/commands_manual.html][Apache Hadoop commands manual]].
You can use Hadoop commands to perform filesystem operations with more consistency.

Example, to look into the internal hadoop namespace:

<pre class="screen">
%UCL_PROMPT% /usr/bin/hadoop fs -ls /
</pre>

Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself =/mnt/hadoop= in Hadoop commands):

<pre class="rootscreen">
%UCL_PROMPT_ROOT% /usr/bin/hadoop fs -chown -R cms:cms /cms
%UCL_PROMPT_ROOT% /usr/bin/hadoop fs -chown -R sam:sam /dzero

%UCL_PROMPT_ROOT% /usr/bin/hadoop fs -chown -R michaelthomas:cms /cms/store/user/michaelthomas
</pre>

Check HDFS and FUSE:
<pre class="screen">
%UCL_PROMPT% cd /mnt/hadoop
%UCL_PROMPT% hadoop fs -ls /
</pre>

Check SRM server ping response:
<pre class="screen">
%UCL_PROMPT% srm-ping  srm://SRM.SERVER.FQDN:8443
</pre>

Check SRM copy using !GridFTP underneath:
<pre class="screen">
%UCL_PROMPT% lcg-cp -v -b -D srmv2 file:/home/user/testfile  srm://SRM.SERVER.FQDN:8443/srm/v2/server?SFN=/mnt/hadoop/user/testfile
</pre>

Check SRM based remote directory listing:
<pre class="screen">
%UCL_PROMPT% lcg-ls  -l -b -D srmv2 srm://SRM.SERVER.FQDN:8443/srm/v2/server?SFN=/mnt/hadoop
%UCL_PROMPT% lcg-ls  -l -b -D srmv2 srm://SRM.SERVER.FQDN:8443/srm/v2/server?SFN=/mnt/hadoop/user
</pre>

#DebugInfo
---+ Debugging Information
---++!!File Locations
Locations of configuration files, input/output files, log files, etc. that are important for an administrator to know about if something doesn't work.  

---++!!Debugging Procedure
What are the first things to check, step-by-step instructions to determine source of problem for this service

---++!!Caveats/Known Issues
Restrictions and/or work around for known issues 

---++!!References
---+++ Benchmarking

   * [[http://www.iop.org/EJ/article/1742-6596/180/1/012047/jpconf9_180_012047.pdf][Using Hadoop as a Grid Storage Element]], <i>Journal of Physics Conference Series, 2009</i>.
   * [[http://osg-docdb.opensciencegrid.org/0009/000911/001/Hadoop.pdf][Hadoop Distributed File System for the Grid]], <i>IEEE Nuclear Science Symposium, 2009</i>.

---++!!Screen Dump of the Complete Install Process
%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="screen">
complete screen dump of the install procedure
</pre>
%ENDTWISTY%

---+ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = JeffDost

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  TanyaLevshina
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = NehaSharma
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->

%META:FILEATTACHMENT{name="Hadoop-site-architecture.png" attachment="Hadoop-site-architecture.png" attr="h" comment="" date="1306345934" path="Hadoop-site-architecture.png" size="23565" stream="Hadoop-site-architecture.png" tmpFilename="/usr/tmp/CGItemp37821" user="JeffDost" version="1"}%
