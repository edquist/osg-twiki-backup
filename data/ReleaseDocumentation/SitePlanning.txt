%META:TOPICINFO{author="RobertEngel" date="1269246500" format="1.1" reprev="1.30" version="1.30"}%
%META:TOPICPARENT{name="WebHome"}%
<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = AlainRoy

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Planning
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = RobertEngel
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
 
-->

%SHOW_DOC_STATUS_TABLE%

---+!! *%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ About this Document

%ICON{hand}% This document is for *System Administrators*. The purpose of the document is to provide an overview about the different ways to setup an OSG site and to encourage you to plan your site before you continue to install the OSG software on your site.

%ICON{choice-yes}% After reading this document you should be able to identify the site elements needed to setup your OSG site and choose among different technology choices presented.

---+ OSG Site Elements

The OSG provides software and documentation to install and operate following services:

<div style="margin: 1em 2em 2em 2em;">
| *Element*  | *Description*  |
| Authentication Service  | enables grid users to authenticate with your site using their grid or voms proxies  |
| Compute Element  | enables grid users to run jobs on your site  |
| Worker Node Client  | enables grid jobs running on worker nodes to access grid tools  |
| Storage Element  | enables grid users to store large amounts of data at your site  |
| VO Management Service | provides functionality for VO Managers to manage the membership information of their users  |
</div>

%TWISTY{%TWISTY_OPTS_REVIEW%}%

%RED%Focus on what the task of the reader is in this section. Provide him only with as much information as needed for him to make a decision.%ENDCOLOR%

%GRAY%
There are different kinds of installations you might perform. They include:

   * *A compute element (CE)* The computing element is the front-end to your computer cluster. Users submit jobs to the CE, which submits the jobs to your local batch system. The OSG software assumes that you have installed your local batch system yourself and provides no help with that. 

   * *A worker node (WN)* A worker node is a single computer (perhaps with multiple CPUs or cores) in your computer cluster on which jobs will run and/or data will be stored. 

   * *A user mapping service* When user jobs arrive at your site, they are identified by a certificate containing the user's name. There are two ways to map a user to the local user id. One of them is a service called _GUMS_. It is a good idea to run GUMS on a separate computer from your CE. (The other method is not a service and so is never run on a separate computer. See _edg-mkgridmap_ below.) 

   * *A virtual organization management service (VOMS)* If you are running a virtual organization (VO), you need to control who is a member of your VO. You do this with VOMS. It is a very good idea to run this on a separate computer from your CE. 

   * *A storage element (SE)* A storage element manages and controls access to storage at your site. Storage management can be one of the more complicated systems to set up. OSG provides two different SEs: one based on dCache and one based on Bestman. dCache is often installed on multiple computers; it should NOT be installed on your CE. Bestman is more appropriate for smaller sites, but it is still recommended to run it on a machine other than your CE. 

   * *A web proxy* Some users submit jobs that download their executables or data files from a web site. In order to conserve bandwidth and to help applications run faster, you might wish to install a web proxy. OSG provides Squid for your use. It is a good idea to run this on a separate computer from your CE. 

   * *A shared file system* OSG recommends the use of a shared file system so that some files are accessible on all worker nodes. We do not provide instructions on how to set up a shared fileystem, but instead expect you to do that yourself. However, we will tell you what needs to be shared. 

   * *A user mapping service* A mapping service maps global user identifiers (as taken from certificate distinguished names) to local user identities. OSG uses GUMS. GUMS is effectively an authorization service as well: all users that are mapped, and only those users, are allowed access. 

If you install all of these services at your site, you could easily have several computers (and possibly many more) running OSG software.
%ENDCOLOR%

%RED%The table above makes the following section unnecessary%ENDCOLOR%

%GRAY%
+---++ What should you install?

If you want to allow users to run jobs on your computing cluster, at a minimum you need to install the computing element (CE) and worker node (WN).

Depending on how you want to control access to your site, you might want to install GUMS.

If your users are likely to access large amounts of data (many gigabytes or perhaps even terabytes), you need an SE.

If you run a virtual organization (VO), you should install VOMS (and VOMRS with it, to make it easier to manage).
%ENDCOLOR%

%ENDTWISTY%


---++ Authentication Service

Grid users will authenticate with your site using their *grid or voms proxy*. The OSG provides two different services that let you control the authentication process:

<div style="margin: 1em 2em 2em 2em;">
| *Service*  | *Description*  | *Advantages*  | *Disadvantages*  | 
| edg-mkgridmap  | a simple program that contacts VOMS servers and creates a gridmap file  | easy to install and maintain  | does not support voms proxies  |
| GUMS  | a web service providing sophisticated controls of how users authenticate  | supports voms proxies  | requires Tomcat to be run as a web service  |
</div>

%ICON{warning}% A *VOMS Server* is not an element of your site. Each *Virtual Organization* operates a VOMS Server to manage membership information of its grid users. Please contact the *VO Manager* for your virtual organization to obtain more details.

%TWISTY{%TWISTY_OPTS_REVIEW%}%
%RED%REVIEW: This should go into the authorization section above. I factored out what I believe is needed to make a decision. The other content you provide is really great, but should not be on this page, but move into the page "more details on authorization"%ENDCOLOR%

+---++ Mapping users

There are two methods for mapping users to local userids: _edg-mkgridmap_ and _GUMS_. If you can afford the time and complexity, we recommend using GUMS.

OSG has many users that are members of multiple VOs. These users can distinguish which VO under which they submit a job by creating a _VOMS proxy_ instead of the more common _grid proxy_. A VOMS proxy is a just a grid proxy with extra information (an _attribute certificate_ that identifies the VO and optionally the user's role within it).

*edg-mkgridmap* is a small program that runs four times a day. Each time it runs, it contacts all of the VOMS servers for the VOs that you support to find out all of the users in the VOs, then recreates a _grid-mapfile_. This is a file that is used by Globus to map users. It simply lists each users _distinguished name (DN)_ followed by the account that they map to.

edg-mkgridmap easy to configure and use. It is trivial to see the mappings that will be used. However, there are a several problems with edg-mkgridmap:

   * A grid-mapfile does not interpret any of the information in a VOMS proxy. The end result is that a multiple-VO user has no ability to control what VO their job appears under, and it may get mapped incorrectly at your site. 
   * If there are several computers in your setup that do authorization, and edg-mkgridmap fails at some of them (or simply runs out of sync), the list of users mapped at each of your computers may be different and thus incorrect. 

*GUMS* is a service that is performs user mapping on behalf of other software. It periodically contacts the VOMS servers that represent the VOs you support and sets up mappings. GUMS is a centralized service at your site, so each computer at your site will make the same mapping decisions. In addition, GUMS understands the information from a VOMS proxy, so it will use that information when mapping users. On the other hand, GUMS is an additional complexity for your site: it is yet one more service to maintain and monitor. Because it is a fundamental piece of your security infrastructure, we recommend that it run on a different computer from your CE. Since users can run jobs directly on your CE (via the _fork_ interface), this avoids interference with GUMS if a security flaw were found.

%NOTE% OSG requires that every OSG CE allows MIS VO users to authenticate. The MIS VO is internal to the Grid Operations Center and its proxies are used exclusively for monitoring. If MIS users are unable to authenticate and reach a CE, VORS will report a critical authentication failure on that resource.

[[AboutAuthorizationForCE][More details on Authorization]]
%ENDTWISTY%

---++ Compute Element

A *Compute Element* allows grid users to run jobs on your site. It is software that provides following services when run on your *gatekeeper*:

<div style="margin: 1em 2em 2em 2em;">
| *Service*  | *Description*  | *Comments*  |
| GRAM  | <b>G</b>rid <b>R</b>esource <b>A</b>llocation <b>M</b>anager is a service developed by the Globus Alliance to interface with your batch system.%BR%It also allows grid users to *fork* jobs on your gatekeeper.  | Required  |
| GRAM-WS  | A web service implementation of GRAM.  | Optional  |
| !GridFTP  | A version of FTP that supports grid authentication based on grid or voms proxies to transfer files.  | Required  |
| Squid  | Squid is a caching proxy for the Web that enables restricted access of worker nodes to the web.  | Optional  |
</div>

Additionally two shared file systems are required for grid users to install applications *OSG_APP* and to save data *OSG_DATA*. Both must be mounted on the gatekeeper and all worker nodes:

<div style="margin: 1em 2em 2em 2em;">
| *Shared Filesystem*  | *Description*  | *Recommended Size*  |  *Typical Size[GB]*  |  *Comments*  |
| HOME  | space for grid user home directories  | 10GB for each VO  | 100 to 1000  |  required  |
| OSG_APP  | space for grid users to install applications  | 10GB for each VO  | 100 to 1000 |  required  |
| OSG_DATA  | space for grid users to save data  | number of VO's times size of available RAM on resource  | 100 to 1000  |  required  |
| OSG_GRID  | location of the worker node client  | 10GB  |  10  |  optional  |
</div>

More details can be found in [[LocalStorageConfiguration][Local Storage Configuration]].

%TWISTY{%TWISTY_OPTS_REVIEW%}%
#SharedFS
+---++ Shared file system directories

You need to run a shared file system if you support job submission to your site. There are four common directories that are shared: not all of them are required.

<div style="margin: 1em 2em 2em 2em;">
| *Name* | *Required?* | *Purpose* |
| OSG_APP | Yes | Store applications used by multiple jobs |
| OSG_DATA | No, but highly recommended | Store data used by jobs |
| OSG_GRID | No | Location of worker node client |
| HOME | Usually | Users' home directories. Required unless you are using Condor NFS-Lite job manager |
</div>

More details can be found in [[LocalStorageConfiguration][Local Storage Configuration]]
%ENDTWISTY%

---++ Worker Node Client

The worker node client is optional software installed on each worker node to give programs running on the worker node access to grid utilities.

%TWISTY{%TWISTY_OPTS_REVIEW%}%
The decision to install the worker node client is based on the requirements of each VO. I think we need a list of VO's here that really require the worker node client and also a table illustrating what the client tools are.
%ENDTWISTY%

---++ Storage Element

A *Storage Element* provides grid users the possibility to read and write large amounts of data on your site using the <b>S</b>torage <b>R</b>esource <b>M</b>anager (*SRM*). It consists of a file system and the SRM service.

<div style="margin: 1em 2em 2em 2em;">
| *Storage Example*  | *Description*  | *OSG Solution*  | *Advantages*  | *Disadvantages*  |
| SRM access to a local disk  | provide an SRM interface to a small amount of disk space  | !BeStMan-Gateway  | easy to setup and maintain  | limited size and IO  |
| SRM access to a NFS  | provide an SRM interface to a network file system that is shared  | !BeStMan-Gateway  | easy to setup and maintain  | limited IO  |
| SRM access to a PFS  | provide an SRM interface to a parallel file system  | !BeStMan-Gateway  | separates the PFS and the SRM service  | requires a PFS like Hadoop, GFS, Lustre  |
| !BeStMan  | provides the SRM interface and the file system  | !BeStMan  |  |  |
| !dCache  | provides the SRM interface and the file system  | !dCache  |  | non-trivial setup and operation  |
</div>

---++ VO Management Service

A <b>V</b>irtual <b>O</b>ganization <b>M</b>anagement <b>S</b>ervice (VOMS) controls who is a member of your VO. Each VO needs to provide one VOMS. Please contact the VO Manager of VO to find out about the VOMS of your VO.

---+ Example Configurations

This section contains a few example that illustrate how the different elements contributing to an OSG site can be combined.

---++ OSG Resource with Compute Element

The following example illustrates an OSG resource that only consists of a Compute Element.

<a href="%ATTACHURLPATH%/BasicCE.gif"><img style="%IMG_STYLE%" width="400"  height="300" alt="BasisCE.gif" src="%ATTACHURLPATH%/BasicCE.gif"/></a>

%TWISTY{%TWISTY_OPTS_REVIEW%}%

   * The picture shows information services. I think information services should be part of the whole document. Not sure why they were left out.
   * The worker nodes are not bound by the NFS to the gatekeeper, but by a network. 
   * The rest is too detailed. This details can be left for the installation guides once the reader decided to install a CE

There are a lot of possible variations here: this is just one example of how it might be set up. Two major notes:

   1 The worker node client software must be installed on all worker nodes. Most sites install this just once using a shared file system for simplicity, but you are not required to use a shared file system. You can choose to install it on each node if you prefer. 
   1 At a minimum, the shared file system must include =$OSG_APP=, which is the location that users put applications shared between jobs.  It must also include a ./etc sub-directory with 1777 permissions.   It may also include other directories, [[#SharedFS][See below for more information]]. 
%ENDTWISTY%

---++ OSG Resource with Compute and Storage Element

The following example illustrates an OSG resource that consists of a Compute and a Storage Element.

<a href="%ATTACHURLPATH%/SmallSite.gif"><img style="%IMG_STYLE%" width="400"  height="300" alt="SmallSite.gif" src="%ATTACHURLPATH%/SmallSite.gif"/></a>

%TWISTY{%TWISTY_OPTS_REVIEW%}%

  * The picture is inconsistent with the previous. The cluster part is drawn differently for no reason.

Although the picture doesn't show it clearly, the shared filesystem is accessible by all cluster nodes as well as the CE.

This structure might be sufficient, but it is starting to push its limits. The problem is the number of services running on the CE: because this is your interface to OSG, you want to keep it from being overloaded. We recommend breaking things up so that not so many things are on the same computer. It would be more scalable to add two computers to this site:
%ENDTWISTY%

It is recommended to run the Compute Element and the Storage Element on separate machines.

<a href="%ATTACHURLPATH%/SmallSite-Modified.gif"><img style="%IMG_STYLE%" width="400"  height="300" alt="SmallSite-Modified.gif" src="%ATTACHURLPATH%/SmallSite-Modified.gif"/></a>

%TWISTY{%TWISTY_OPTS_REVIEW%}%

   * Please show the GridFTP part on the CE.

Note that there is also likely to be a !GridFTP server running on the CE, but the majority of file transfers should go through the SE. People who submit to your site but are unaware of the SE may use the !GridFTP server on the CE.

But now there is a problem: you have two different computers doing authorization/user mapping independently. We can add a GUMS service so that all of your computers are synchronized.
%ENDTWISTY%

---++ OSG Resource with Compute and Storage Element and GUMS

In the example above it would be recommended to use GUMS for authorization:

<a href="%ATTACHURLPATH%/MediumSite.gif"><img style="%IMG_STYLE%" width="400"  height="300" alt="MediumSite.gif" src="%ATTACHURLPATH%/MediumSite.gif"/></a>

%TWISTY{%TWISTY_OPTS_REVIEW%}%
  * This was explained above already.

GUMS has two significant advantages over static grid mapfiles.

   1 User mappings are maintained in a single location. 
   1 GUMS dynamically handles VOMS information, while grid mapfiles don&rsquo;t. This is explained further in [[#Mapping_users][Mapping users]]. 
%ENDTWISTY%

---++ OSG Resource with Compute Element and dCache Storage Element

%ICON{warning}%  We have used *dCache* in this diagram rather than *BeStMan*, because some of the larger OSG sites use it. Often the storage and compute nodes are not distinct from one another.

<a href="%ATTACHURLPATH%/LargeSite.gif"><img style="%IMG_STYLE%" width="400"  height="300" alt="LargeSite.gif" src="%ATTACHURLPATH%/LargeSite.gif"/></a>

%TWISTY{%TWISTY_OPTS_REVIEW%}%
+---+ What you need to do

This is part of the compute and storage installation guides and doesn't help decide the user what he wants on his side, therefor it should not be part of this page:

+--++ Certificates

In general, your CE will need two certificates: a host certificate and an http certificate. A squid proxy doesn't need any certificates.

Your installation will go most smoothly if you get your certificates first.

[[GetGridCertificates][More information about the certificates you need]]

This is also not needed on this page:
+---++ Registering with OSG
 Note that you will need to register your site with the OSG Grid Operations Center (GOC). This can be done at the [[https://oim.grid.iu.edu][OIM]] registration pages. You will need to have a valid IGTF certificate loaded into your browser to access OIM.

%RED%REVIEW: As you say this is way to advanced for this document and can be left to the compute element installation guide%ENDCOLOR%
+---++ Advanced notes

+---+++ glexec
 *glexec* is a service to help so-called _glide-in_ or _pilot_ jobs interact with the security at your site.

Some users on OSG use a technique called _glide-in_: they run a job that doesn&rsquo;t do the actual computation. Instead, it pulls in the actual computation to do the work when it is ready. Glide-in is conceptually simple but there are many complexities in practice: a full discussion is beyond the scope of this document.

glexec solves one particular problem with glide-in jobs when they pull in work from a user other than the glide-in job submitter. Your local policy might require accounting and/or security measures. Glexec will assist with this by ensuring the the work job is properly authenticated and authorized. Details on installing glexec are in the CE installation guide. Note that glexec is a setuid script so it must be installed locally, not on NFS.

   * *For more information about glexec* see GlexecInstall. 

  * The document the link refers too is of no help and out dated.
+---+ Site Best Practices

Some technical details about [[SiteFabricBestPractices][site fabric best practices]].
%ENDTWISTY%

---+ *Comments*
%COMMENT{type="tableappend"}%

%STOPINCLUDE% 

<!--
%BR% 
%COMPLETE3% %BR% 
%RESPONSIBLE% Main.AlainRoy - 18 Oct 2007 %BR% 
%REVIEW% Main.KeithJackson - 21 Jul 2009 %BR% 
%REVFLAG% %X% %BR%
-->

%META:FILEATTACHMENT{name="BasicCE.gif" attr="" autoattached="1" comment="" date="1193861491" path="BasicCE.gif" size="19744" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="MediumSite.gif" attr="" autoattached="1" comment="" date="1195511759" path="MediumSite.gif" size="24847" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SmallSite.gif" attr="" autoattached="1" comment="" date="1195511660" path="SmallSite.gif" size="24073" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="LargeSite.gif" attr="" autoattached="1" comment="" date="1195503184" path="LargeSite.gif" size="27102" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SmallSite-Modified.gif" attr="" autoattached="1" comment="" date="1195511683" path="SmallSite-Modified.gif" size="29954" user="UnknownUser" version=""}%
