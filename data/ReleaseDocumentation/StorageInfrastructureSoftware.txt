%META:TOPICINFO{author="TedHesselroth" date="1270849791" format="1.1" reprev="1.24" version="1.24"}%
%META:TOPICPARENT{name="WebHome"}%
%DOC_STATUS_TABLE%

---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
https://twiki.grid.iu.edu/twiki/pub/Storage/WebHome/images.jpg
%TOC{depth="3"}%

*The Storage Architecture*

Storage software contains multiple components that work together to provide the storage service. This list shows definitions of the major components as categories -specific implementations are described in the remainder of this topic.

   * _Distributed Storage._ A cluster deployed with software to provide a unified storage area.
   * _Namespace._ A data base connecting logical filenames to physical locations.
   * _Data transfer._ A service to move data in the form of files.
   * _Replication._ The practice of making multiple copies of data to protect against hardware failure.
   * _Resource Management._ A software layer that controls storage access.
   * _Archiving._ The practice of making permanent copies of the data.

Some or all of these six are typically bundled together and make up what is known as a "Storage Element". The selection could range from only one, for example, data transfer in the form of gridftp on an ordinary filesystem, to all six, as in a LHC Tier 1 site with tape-archive backend.

A further set of services relating to information are realized as independent components. Some of theses also pertain to the job execution stack and allow coordination of compute and storage resources.

   * _Information Services_
      * Catalogs. A database containing metadata for files, especially their locations and paths.
      * Monitoring. A system to determine and display the health and activity of storage resources.
      * Discovery. A searchable database containing site configuration information, including service endpoints, capabilities, and authorization.
      * Accounting. A database viewed with preconfigured reports that show historical grid activity from various aspects such as site, virtual organization, or access type.

In addition, experiments with very intensive data transfer requirements use dedicated software to manage the movement of files. This category is covered in the [[StorageApplicationDeveloper#FileTransferServices][File Transfer Services section]] of the [[StorageApplicationDeveloper][Storage for the Application Developer]] topic.

   * _File Transfer Services._ Automates the data movement process and provides an organized view of the status of file transfers.


---++Distributed Storage

In grid computing, just as computational power in the form of CPUs may be distributed over many computers, or "worker nodes", storage in the form of hard disks may be distributed over many computers in order to provide a large, unified storage area. Often, the same computers that serve as worker nodes for a Compute Element also hold storage for a Storage Element. There are many implementations of distributed storage, several of which may be found on the Open Science Grid. In general, the emphasis of storage on the OSG is towards high throughput based on scalability, rather than low-latency based on highly performant hardware. For a table comparing the features of some of the above software, please see the [[#StorageImplementationsTable][Storage Implementations Table]]. 

   * *Hadoop.* HDFS, or the Hadoop Distributed File System, is a distributed storage system based on the Hadoop implementation of Google's map-reduce algorithm. A key element of HDFS is robust support for [[#ReplicationLink][Replication]], allowing the use of low-cost hard drives while maintaining reliability. HDFS has a highly scalable architecture, in which the basic unit of storage is a block. Hadoop HDFS is provided by the OSG through VDT packaging, and community support through an osg-hadoop mailing list is available for operational issues. For more information, please see the [[StorageSiteAdministrator#BestmanXrootdLink][Bestman-hadoop]] section of the storage site administrator page.
   * *xrootd* was designed to provide storage for physics analysis programs based on the software package named "root" and includes an Open Load Balancing Daemon (olbd) by which distributed storage clusters may be composed. [[http://xrootd.slac.stanford.edu/][Developed]] at the Stanford Linear Accelerator Center, xrootd is written in C++ with highly-optimized algorithms to provide fast and deterministically bounded processing times, resulting in low latencies even when a large number of files is present. xrootd is provided by the OSG through VDT packaging, please see the [[StorageSiteAdministrator#LinkToxrootdInstall][Bestman-xrootd]] section of the  storage site administrator page for details.
   * *dCache.* A major part of the dCache Storage Element implementation is its distributed storage system, based on components known as "pools". Storage is file-based, and replication is supported, allowing the use of commodity hardware. Access to pools is controlled through a "Pool Manager", which allows logical storage areas to be created and access to be granted based on user identity or role, client IP address, operation (read or write), or transfer protocol. OSG provides packaging and support for dCache, please see the [[StorageSiteAdministrator#LinkTodCacheInstall][dCache]] section of the storage site administrator page. https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions  
   * *DPM* Of interest to OSG users because of its deployment on the European grid EGEE, the [[https://twiki.cern.ch/twiki//bin/view/LCG/DpmGeneralDescription][Disk Pool Manager]] is a lightweight solution for managing disk storage. It can be accessed via SRM 1 & 2, and also provides data access through the  !GridFTP, rfio transfer protocols.
   * *Other* distributed file systems include Lustre, ZFS, !ReDDNet, L-Store, and NFS 4.1. Of these, only Luster and ZFS may be found on the Open Science Grid, though their use may increase in the future. There are plans to support NFS 4.1 in dCache and DPM.

Note that it is not _required_ that a Storage Element use a distributed file system. Storage appliances can provide tens of terabytes of storage in a single unit. The globus implementation of the gridftp data transfer mechanism can serve files from any mounted file system, and may be used in combination with  the Bestman Storage Element for SRM access. For further information, see [[StorageSiteAdministrator][Storage for Site Administrators]].

---++Namespace

All distributed file systems rely on the use of a namespace component which allows the logical name and path of a file to be separated from its physical location. Typically, a database is maintained with the needed "metadata" for each file. Since there is typically just one instance of this component in a distributed file system, it can represent a single point of failure. In dCache, frequent backups of the database In addition, for large systems, a performance bottleneck may occur at the namespace node. 


#DataTransfer
---++Data transfer

While most distributed storage systems have their own access protocols, they do allow for other file server mechanisms. These are of particular use for interoperability, when a client at a remote site may not be using the native protocol of the storage service. The most commonly-used data transfer software for this purpose is gridftp.

   * *gridftp* is used for serving files over the wide area network. Security options include the Grid Security Infrastructure, the authentication framework adopted by the OSG. A major feature of gridftp is the ability to transfer files over multiple data channels, which can increase throughput compared to one channel by a factor of ten. There are two implementations of gridftp: by [[http://www.globus.org][Globus]], and by [[http://dcache.org][dCache]]. The dCache implementation is bundled with SRM-dCache and is not installable as a separate component. For an introduction to gridftp, please see [[Documentation/StorageGridFTP][Overview of GridFTP in the Open Science Grid]].
   * *Other* file servers may be categorized by the protocols they support. Data transfer protocols include dcap, gsidcap, xroot, http, https, bbftp and ftp. The protocol for gridftp is gsiftp.


#ReplicationLink
---++Replication

Replication of files is used to mitigate data loss in the case of hard disk failure. In implementations that support replication (see the [[#StorageImplementationsTable][Storage implementations Table]]) replication occurs automatically, with the number of copies being detected by the replication service. When a disk is lost the system automatically creates additional replicas for the affected files and in the interim uses existing replicas for uninterrupted service. In dCache the Replica Manager creates replicas of whole files, among a specified subset of pools. The Hadoop HDFS storage service does replication at the block level, and allows specifying that block replication not occur within a set of storage nodes, such as all those situated in one rack.

An alternative to file or block replication is the use of RAID arrays, typically RAID-5, by which data redundancy is executed at the hardware level. Upon loss of a disk, the vendor-supplied rebuilding process restores the redundancy.


---++Resource Management

[[https://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.pdf][SRM]] is a software specification for access to mass storage systems. The specification allows for interoperability among clients and servers of various storage implementations. Any client which satisfies the specification can operate with any server which also does so. The specification supports commonly-used storage operations such as get, put, copy (for moving files from one SRM storage element to another), bring-online (to cause a file to be moved from a tape archive to the disk cache for later transfer), and space reservation. SRM also supports protocol negotiation, so the client may request a data transfer protocol or state which protocols it supports, allowing the SRM service to connect it to a suitable file server endpoint.

This diagram shows how a gridftp client would access storage, and how a srm client would access storage. In the case of gridftp the client contacts the file server directly. In the case of SRM, the client contacts the SRM server, which communicates to the client the file server to be used, based on availability and requested protocol. In each case, the file server uses the namespace component of the storage system to determine the pool or pools to be involved in the transfer.<br />
     <img src="%ATTACHURLPATH%/storage-architecture.GIF" alt="storage-architecture.GIF" width='528' height='396' />    

---+++ Bestman

%INCLUDE{"BestmanStorageElement"}% For more information, please see [[BestmanStorageElement]].

---+++ dCache

%INCLUDE{"DCacheStorageElement"}% For more information, please see [[DCacheStorageElement]].


---+++ Other SRM Implementations

There are other implementations of the Storage Resource Manager specification. While these implementations are not supplied or directly supported by the OSG, there are interactions with these storage systems when data is moved from one grid to another.

   * *CASTOR.* The [[http://castor.web.cern.ch/castor/][CERN Advanced STORage manager]] is a tape-backed hierarchical storage management (HSM) system developed at CERN used to store physics production files and user files. 
   * *DPM* The [[https://twiki.cern.ch/twiki/bin/view/LCG/DpmGeneralDescription][Disk Pool Manager]] is a lightweight storage system that supports GSI and SRM.
   * *[[http://www.egrid.it/sw/storm][StoRM]]* is an SRM implementation from [[http://www.egrid.it][EGRID]], [[http://www.infn.it][INFN]], and  [[http://www.grid.it][GRID.IT]] that can run on top of any posix filesystem.

---++Archiving

Some storage systems have magnetic tape drive components which allow files to be stored for long periods. Storage on tape at a large site is on the order of 10 petabytes. Files are staged to and from the tape drives via a hard-disk caching system. In the SRM specification, files that are on tape but on in the cache are said to have an "access latency" of OFFLINE, and files that are in the cache have an access latency of NEARLINE. SRM clients have an option of specifying the final access latency of a file. For more information on storage clients, please see [[StorageEndUser][Storage for the End User]].

Among the storage software provided and supported by the OSG, only dCache includes the option of having a tape backend. Sites on the OSG that have tape archival capability are Brookhaven ATLAS Tier 1, Fermilab CMS Tier 1, Fermilab CDF, and Fermilab public dCache.

#StorageImplementationsTable
---++Table of Storage implementations  used in the Open Science Grid

The following table summarizes the capabilities of various storage software implementations.

%TABLE{ headerbg="#eeeeee" headercolor="#000000" databg="#ffffff" tableborder="1" columnwidths="120," cellpadding="2" cellspacing="1" dataalign="left" valign="top" sort="off"}%

| *Software*  | *Distributed Storage* | *Resource Management* | *Data Transfer Protocols* | *Replication* | *Archiving* | *Namespace* |
| gridftp | any mounted | | gsiftp | | | | 
| xrootd | !XrdOss | olbd | xroot,posix+ | | !XrdOss | !XrdSfs | 
| Bestman | any mounted | Bestman SRM |gsiftp,posix| | | | 
| Bestman-xrootd | xrootd | Bestman SRM Gateway  | gsiftp,xroot,posix+ | | | | 
| Hadoop SE | HDFS| Bestman SRM Gateway | gsiftp| Block Replication | |!NameNode/fuse |
| SRM-dCache | dCache | Fermi SRM |gsiftp,dcap,posix+,gsidap,xroot| Replica Manager | HMS |pnfs or chimera | 

+with preloaded libraries 


---++Information Services

In order to fully utilize and track the storage capabilities of the Open Science Grid, various ancillary services come into play. Some of the services apply to both compute and storage elements. General information on this topic can be found on the [[MonitoringInformation/WebHome][OSG Monitoring and Information Systems]] web.

---+++Catalogs

Catalogs are used in association with storage elements to hold metadata for files, which are identified by their "Logical File Name".  Metadata of interest to experiments includes such items as

   * Physical file name (including path) or URL.
   * Preference ranking of the above for downloads.
   * File size.
   * File checksum.
   * Other experiment-specific info.

Catalog implementations and schema are often customized for a particular experiment. One generic catalog, the Replica Location Service, is included in the OSG software distribution as a part of the Globus component.

---++++ Replica Location Service

The Replica Location Service is primarily used to find the physical file name or URL for a given logical file name, though other attributes are supported. RLS consists of two components: the Local Replica Catalog, and the Replica Location Index. This is due to the fact that the path portion of the physical file name may be site-dependent. The lookup is a two-stage process: a set of Local Replica Catalogs is determined from the logical file name by querying the Replica Location Index, and each of these is consulted to determine the physical file name at a site. For more information on the Replica Location service, please see the [[http://www.globus.org/toolkit/data/rls][globus web site]].

---+++Monitoring

There are two monitoring services in use in the OSG that apply to storage. 

---++++ RSV

The main monitoring service of the Open Science Grid is the Resource and Service Validation service. RSV is a component allows administrators to run probes locally on resources to test their functionality. The results are sent to a central collector at the [[http://osggoc.blogspot.com/p/services-run-by-goc.html][Grid Operations Center]], where the information is collected, presented, and redistributed. Storage Elements are among the resources that have RSV probes. The two tests covered by the storage element RSV probe are srmcp and srmping. RSV results may be accessed through the GOC's [[http://myosg.grid.iu.edu/][MyOSG]] page, where storage element results may be found in web page sections labeled by the storage element ID.

---++++ srmTester

As on offering outside the OSG, the SRM consortium runs a monitoring service for SRM Storage Elements. All of the srm client commands are tested for each site. To use the service

   * Visit the site http://datagrid.lbl.gov/sitereg/ and follow instructions for registration.
   * Then check back here: http://datagrid.lbl.gov/ to view test results.

---+++Discovery

The Discovery mechanism provides specific information on grid services that allow user operations to be planned and executed. For example, users of storage need to know which sites authorize their VO to access storage, and what specific storage services are offered; their endpoints, paths, etc. In the Open Science Grid, the means of providing this information is through the BDII, a centralized service which contains configuration information for each site and can be queried by users. The [[http://is.grid.iu.edu/documentation.html][OSG BDII]] is populated via processes based on the Generic Information Provider (GIP). See the [[InformationServices/WebHome][Information Services Activity]] page for more information on the GIP.

A tool for querying the BDII, the [[Storage.OSGStorageDiscoveryTool][OSG Discovery Tool]], is provided through the VDT client package. Pre-written queries wrapped an a command-line interface are included, for which the user need only specify their VO. Queries related to storage include

   * Example gridftp URL to use for a site.
   * Example srm URL to use for a site.
   * Example srm copy command.
   * Example space reservation command.
   * Mount points of storage systems on compute worker nodes.

The examples returned by the tool can be used to construct user-specific commands that should succeed on the storage resource. The mount points can be used when a process needs to access a storage element from a worker node using posix IO. The 
mount point gives the necessary site-specific path information the defines the user-accessible storage area.

#UsingInformationServicesStorage
---++++ An example of the use of information services with Storage

Putting the Information Services components together with the Storage Element components is shown in the following diagram. Suppose a user wants to place a file on a Storage Element and at some later time run a job on an associated Compute Element, which accesses the file. In this scenario for the first part the user's script first executes a  [[Storage.OSGStorageDiscoveryTool][OSG Discovery Tool]] command which tells it which sites authorize the user's VO and what path the VO is to use. A site is chosen and the script then consults monitoring information to verify that that the site is currently online. The file is then written through the srm server and its location is recorded in a catalog. For more demanding data placement requirements, a  [#FileTransferServices][file transfer service]] may be needed.

For the job execution part, the script reads from the catalog to determine the sites which hold the particular file. A site is chosen and the monitoring info is checked to ensure that the site is online. During the job the file is either accessed directly from the storage element or temporarily copied from the storage element to the worker node. In the first case, files on the storage element must be available through mount points on the worker nodes; if so the mount point is determined by the discovery tool and inserted into the job script. Otherwise, the file is copied via a URL obtained from the catalog or from the composition of the filename with an endpoint and path found through the discovery service.

<br /><img src="%ATTACHURLPATH%/InfoStorageXfer.jpg" alt="InfoStorageXfer.jpg" width='800' height='550' />

---++++<nop>ReSS

Automated matchmaking at job execution time with additional constraints on job execution environment and other parameters is available through the OSG Resource Selection Service. Attributes for storage elements are included in !ReSS classads. For more information, please see the topic  [[ResourceSelection/WebHome][OSG Resource Selection Activity]]. 

---++++OSGMM

The [[http://osgmm.sourceforge.net][OSG Matchmaker]] is based on !ReSS and adds additional classad information. Monitoring probes allow resource availability to be included in the matchmaking process, and VO-specific attributes are added to direct processing to resources owned by the experiment.

---+++Accounting

It is useful for site and grid administrators to have an overview of activity on their site. An accounting system typically shows plots or other kinds of reports of historical usage data.

---++++Gratia

The [[Accounting/WebHome][Gratia accounting service]] keeps a record of resource usage at a site or in the OSG as a whole. The usage is recorded after the fact and does not interact with the actual running of jobs or storage transactions. Usage data is collected by "gratia probes" running on grid resources, which write either to a site's local collector or to a central OSG collector. There are two types of probes that report storage related information. 

    * The transfer probe reports to Gratia the details of each file transfer into or out of a file server. Because the the potentially large volume of information, the transfer probe writes to a site-local gratia collector. There are flavors of transfer probe, depending on the storage implementation being deployed.
      * For Bestman using gridftp file servers or for stand-alone gridftp servers, the  [[ReleaseDocumentation.GratiaTransferProbe][Gratia Transfer Probe]] is used. This probe works by scraping the gridftp log files.
      * For dCache, the [[ReleaseDocumentation.GratiaDcacheProbes][Gratia Dcache Probe]] is used. The probe gets this information from the dCache "billing" database and should run on the dcache node on which the dCache http domain is running. The probes are installed on their correct nodes by the VDT install scripts during an initial dCache installation or upgrade.
   * The other type of storage probe is responsible for reporting storage capacity and storage usage. The reporting is to the central Gratia repository. The information reported is:
      * The storage capacity and amount used for each dCache pool.
      * The storage capacity and amount used for each SRM Space reservation.

Reports from the grid-central gratia service may be viewed from the [[http://myosg.grid.iu.edu/rgaccount/index?datasource=account&summary_attrs_showservice=on&summary_attrs_showrsvstatus=on&summary_attrs_showfqdn=on&gip_status_attrs_showtestresults=on&downtime_attrs_showpast=&account_type=daily_hours_byvo&ce_account_type=gip_vo&se_account_type=vo_transfer_volume&start_type=7daysago&end_type=now&facility=on&facility_10009=on&gridtype=on&gridtype_1=on&service_central_value=0&service_hidden_value=0&active=on&active_value=1&disable_value=1][MyOSG]] page, though at present there are no pre-written reports for storage.

---++++dCache Chronicle

The dCache Chronicle sends daily summaries of capacity an usage of dCache storage elements via email. It is part of the [[http://datagrid.ucsd.edu/toolkit/index.html][OSG Operations Toolkit]] and may be installed [[http://datagrid.ucsd.edu/toolkit/docs.html][via rpm]].

---++++Hadoop Chronicle

A daily email report on capacity, usage, and health for Hadoop HDFS-based storage elements is available through a yum installation. Details may be found at the [Storage/HadoopStorageReports][Hadoop Storage Probe]] page.

#FileTransferServices
---++ File Transfer Services

Please see the section [[StorageApplicationDeveloper#FileTransferServices][File Transfer Services]] in the [[StorageApplicationDeveloper][Storage for the Application Developer]] topic.
     

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = TedHesselroth

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = Storage
   * Local DOC_AREA       = 

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = All

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = Knowledge
   * Local DOC_TYPE       = 
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       =  TanyaLevshina
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->


-- Main.TedHesselroth - 13 Jan 2010
 

%META:FILEATTACHMENT{name="bestman-gateway-howitworks.jpg" attachment="bestman-gateway-howitworks.jpg" attr="" comment="Bestman - How it works." date="1266347460" path="bestman-gateway-howitworks.jpg" size="50852" stream="bestman-gateway-howitworks.jpg" tmpFilename="/usr/tmp/CGItemp12893" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="bestman_gateway_arch.jpeg" attachment="bestman_gateway_arch.jpeg" attr="" comment="BeStMan-gateway architecture" date="1266347511" path="bestman_gateway_arch.jpeg" size="9978" stream="bestman_gateway_arch.jpeg" tmpFilename="/usr/tmp/CGItemp13012" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="bestman-gateway-xrootd-howitworks.jpg" attachment="bestman-gateway-xrootd-howitworks.jpg" attr="" comment="!BeStMan Gateway with Xrootd - How it works" date="1266349542" path="bestman-gateway-xrootd-howitworks.jpg" size="52148" stream="bestman-gateway-xrootd-howitworks.jpg" tmpFilename="/usr/tmp/CGItemp25197" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="betsman_gateway_xrootd.jpeg" attachment="betsman_gateway_xrootd.jpeg" attr="" comment="BeStMan-gateway/Xrootd architecture" date="1266349681" path="betsman_gateway_xrootd.jpeg" size="35418" stream="betsman_gateway_xrootd.jpeg" tmpFilename="/usr/tmp/CGItemp25142" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="storage-architecture.GIF" attachment="storage-architecture.GIF" attr="" comment="Diagram of Components of a Storage Element" date="1269451385" path="storage-architecture.GIF" size="110772" stream="storage-architecture.GIF" tmpFilename="/usr/tmp/CGItemp13604" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="InfoStorage.gif" attachment="InfoStorage.gif" attr="" comment="Using information services in a file transfer" date="1270577862" path="InfoStorage.tif" size="270594" stream="InfoStorage.tif" tmpFilename="/usr/tmp/CGItemp18599" user="TedHesselroth" version="2"}%
%META:FILEATTACHMENT{name="InfoStorage.tif" attachment="InfoStorage.tif" attr="" comment="Using information services in a file transfer" date="1270577924" path="InfoStorage.tif" size="270594" stream="InfoStorage.tif" tmpFilename="/usr/tmp/CGItemp15430" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="InfoStorage.jpg" attachment="InfoStorage.jpg" attr="" comment="Using information services in a file transfer" date="1270577992" path="InfoStorage.jpg" size="96938" stream="InfoStorage.jpg" tmpFilename="/usr/tmp/CGItemp15389" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="InfoStorage.bmp" attachment="InfoStorage.bmp" attr="" comment="Using information services in a file transfer" date="1270578429" path="InfoStorage.bmp" size="3461014" stream="InfoStorage.bmp" tmpFilename="/usr/tmp/CGItemp15411" user="TedHesselroth" version="1"}%
%META:FILEATTACHMENT{name="InfoStorageXfer.jpg" attachment="InfoStorageXfer.jpg" attr="" comment="Using information services in a file transfer" date="1270578660" path="InfoStorageXfer.jpg" size="98478" stream="InfoStorageXfer.jpg" tmpFilename="/usr/tmp/CGItemp15312" user="TedHesselroth" version="1"}%
