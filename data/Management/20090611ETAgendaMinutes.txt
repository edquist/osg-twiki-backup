%META:TOPICINFO{author="ElizabethChism" date="1466010511" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="AreaCoordinator"}%
---+ OSG Area Coordinators Meeting
---+++ Meeting Coordinates

| <b>Date</b> | Thursday, June 11, 2009 |
| <b>Time</b> | Noon Central |
| <b>Telephone Number</b> | 510-665-5437 |
| <b>Teleconference ID<b> | 1111  |


---+++ Attendees
Rich, Abhishek, Alina, Tanya, Britta, Ruth, David, Alain, Rob G, Chander


---++ Agenda

   * Canvass for News Items - David Ritchie

   * Production Coordinator Report - Dan Fraser (did not attend - no report submitted)

   * 1.1 Software – Alain Roy (Storage – Tanya Levshina)

   * 1.3 [[Trash/IntegrationJune2009][Integration & Sites]] – Rob Gardner

   * 1.4 VOs – Abhishek Singh Rana

---++ Software Report

I last reported on the state of OSG software at the [[200900416ETAgendaMinutes][April 16th, 2009]] area coordinator meeting.

---+++ Major Changes
Since the last meeting, we have:

   * Released VDT 1.11.0, a pre-release for VDT 2.0.0 with support for Debian 5, requested by LIGO
   * Released VDT 2.0.0, which will be the basis for OSG 1.2. [[http://vdt.cs.wisc.edu/releases/2.0.0/release.html][VDT 2.0.0 release notes]]
   * Released two minor updates to VDT 2.0.0. [[http://vdt.cs.wisc.edu/releases/2.0.0/release-p1.html][VDT 2.0.0p1 release notes]] [[http://vdt.cs.wisc.edu/releases/2.0.0/release-p2.html][VDT 2.0.0p2 release notes]]
   * Worked with the VTB to validate VDT 2.0.0. This work culiminated in VDT 2.0.0p2. 
   * Prepped for, and began ITB testing. This just started on June 9, 2009 so there hasn't yet been significant feedback. 
   * Installed a new test stand at Fermilab to test Bestman-gateway with Xrootd and Hadoop, as well as Bestman by itself. 
   * Created a [[https://gw014k0.fnal.gov:8443/validation_tests/][web site that contains all certification test results]] for BeStMan and dCache as well as monitoring tools output (ganglia, srmwatch, etc)
   * Released new vdt-dCache version 2.3.1 (dcache -1.9.2-5, new pnfs, gratia probes 1.02.1-5)
   * Tested new SRM Fermi client (included in VDT 1.10.1 and 2.0.0 now).
   * Provided feedback on Hadoop installation documentation

---+++ Interesting changes in VDT 2.0.0
A few particularly interesting things happened in VDT 2.0.0:

   * We added clients for four network diagnostic tools requested by ATLAS. They are NPAD, NDT, OWAMP, and BWCTL. They will be widely deployed in OSG 1.2 so that site administrators have access to them when diagnosing network problems. This fulfills the work for [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/SoftwareTools/AllRequirements][Requirement 5 on the Software Tools Group requirements list]].
   * We've improved MyProxy support, requested by ATLAS. MyProxy is now linked against VOMS, has been upgraded to version 4.7, and has proper configuration within the VDT (previously we just laid down the bits with no configuration), and has been separated into client and server packages for better control over what is installed. This fulfills the work for [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/SoftwareTools/AllRequirements][Requirement 36 on the Software Tools Group requirements list]].
   * We have released an update to glexec to address a serious security problem. [[http://vdt.cs.wisc.edu/advisories/glexec-0.5.36.html][Text of the VDT glexec security advisory]].
   * We have begun a major project to provide source Debian packages and RPMs. Initially we are focused on LIGO's short-term needs, but this will lead to better support for native packages for the VDT. 

---+++ Upcoming work
During the summer of 2009, I expect we'll focus primarily on:

   * The release of OSG 1.2. [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/SoftwareTools/OSG12Plan][The OSG 1.2 Plan]] In particular, we plan to freeze changes to the VDT and OSG caches in the middle of ITB testing, on July 1. We plan to release OSG 1.2 at the end of July so that it's ready to go for the site administrator meeting at the beginning of August and can be installed before the LHC restart.
   * Source packages for Debian and RPM.
   * Organizing the OSG Storage Forum at Fermilab (June 30 & July 1)
   * Participating in US LCG Tier-3 Liason Group. Goal of the group to advise Tier-3 on storage selection/configuration
   * Understanding/testing Hadoop.
   * Improving d-cache installation script. The dCache-vdt release will include community toolkit rpms.
   * Discussing xrootd monitoring package installation with developers

---++ Virtual Organizations Group

*Weekly VO Forum Meetings*
   * Usual attendance from CDF, D0, DES, Engage-VO, Fermilab-VO, ILC, nanoHUB, NYSGrid, OSG-VO, SBGrid.
   * Increased focus in recent months has been on VOs with low activity: !CompBioGrid, GPN, GUGrid, GROW, !IceCube, !NEBioGrid.
      * !CompBioGrid: Work ongoing to bring up a local site. To be followed by application integration.
      * GPN: Work ongoing with Engagement.
      * GUGrid: Has conveyed a desire to decommission the VO.
      * GROW: Work ongoing to bring up a local site. GROW's target community is local CMS users.
      * !IceCube: Has conveyed a need to expand production beyond GLOW, i.e., opportunistic mode across OSG. Has non-trivial storage needs, and a good candidate for using SRM-based storage or Squid caches. Requirements requested from !IceCube, jointly with OSG Storage.
      * !NEBioGrid: Work ongoing with Engagement. Has deployed OSG-Matchmaker.

*At-Large VOs Consortium Stakeholder Input to OSG Council and to OSG Executive Board*
   * Document with input from 17 at-large VOs, submitted Mar'09.
   * Looking to collect updates from these VOs, and to submit an updated version for the Aug'09 Council meeting.
   
*Quantitative wall-hour consumption and Efficiency of Usage*
   * CDF, D0, DOSAR, Fermilab-VO, GLOW are sustained users with high effectiveness.  E.g., Using OSG, D0 monte carlo production reached a peak of 13 million events/week. Highest recorded rate in D0 collaboration so far.
   * Usage by the remaining segment is nominal. Causes: (a) custom low-scale needs, or otherwise (b) real low activity.
   * Geant4, ILC, nanoHUB, NYSGrid, SBGrid, STAR are moderately active in usage. Cyclic or in spikes.
   * !CompBioGrid, GPN, NWICG, GROW are making efforts to start up.
   
*Joint nanoHUB-OSG !TaskForce*
   * Mission - Investigated ways to make workflow improvements, to enable nanoHUB science production to high job volume at high efficiency. Closure - end of May'09.
   * Milestones reached - Jan'09: nanoHUB site monitoring system enhanced, with influx of Condor-G based !GridProbe jobs. Feb'09: A high level of OSG site stability established. March'09: Influx of nanoHUB Grid-application-test jobs started. April'09: A new nanoHUB-OSG site validation + web monitoring display designed and deployed. May'09: Important new agreements reached during !F2F meeting. Usage ramped up substantially. Monitoring display enhanced. Future impact: Influx of end-user jobs. Possible sharing of source code of site validation system.   
   * General - Constant debugging with !TaskForce sites. New error-codes that were agreed with nanoHUB being used. Error-codes and frequency of occurrence at every site now tabulated on the web.
   * From !F2F:
      * Newly agreed nanoHUB-side items were completed. These were: addition of probe-to-app feedback loop, site ranking decisions, adoption of the new agreement on failures, evaluation of OIM.
      * nanoHUB's adoption of the notion of 'links' success/failures is complete. That of the 'chain' success/failure is half-complete, and can be accomplished in long-term. When notion of 'chain' is fully adopted, failed 'links' that ultimately resulted in success may be given credit.
      * Most other new OSG-side items from !F2F are wider scoped, and to be pursued in long-term with help from Production team.
   * With current 6 application jobtypes, nanoHUB's production across OSG has ramped up (from near-zero), during May'09, and has been sustained at 600-800 wall hours/day.
   * Failure rate specification.
      * OSG Resource failure rate is less than 1%. That is, it is not 13 %.
      * Link failure rate (lack of availability) is nearly 12%. 
   * Influx of nanoHUB end-user jobs is expected in near future.

*Joint ALICE-OSG !TaskForce*
   * Mission - To enable ALICE's specialized !AliEn framework and ALICE production on OSG Facility. Closure - end of May'09.
   * Milestones reached - Dec'08: Jobs successfully submitted using !AliEN-OSG common integrated interface at 1 site. Paused Jan-Mar'09 to facilitate momentum and consensus within ALICE. Mar'09: ALICE registered as an official VO in OSG. Security understanding of certificate renewal mechanism completed. Apr'09: Scalability tests started at NERSC site; validating sustained load of 100 jobs/day; 1200 wallhours/day. Results available in global ALICE web display. May'09: Producion dropped due to resource contraints.
   * The production-readiness exercise was limited by resource availability. ALICE's outline was 200 jobs/day to sign off on production readiness of US ALICE. This goal was not achieved with current ownership/allocation of ALICE resources in the US.
   * Further expansion at PDSF itself, and to LLNL and other US and Brazil sites is dependent on ALICE's procurement of hardware. Subsequent to it, in future, we can consider possibility of a phase-II taskforce as needed.
   * Opportunistic-usage not considered a desirable goal, based on initial discussions.

*Joint Geant4-OSG !TaskForce*
   * Work ongoing with Geant4 and Engagement.
   * Goal is to enable Geant4's regression testing cycles on OSG.
   * Now able to run on OSG.
      * 60-100 jobs per day. Peak usage 1300 wall hours per day. Average usage 600 wall hours per day.
      * Using the same workflow model as LCG. Ganga/Diana software stack.
   * Production validation run in progress.
   * Investigation ongoing on drop-off of completion rate, after initial good rate.
   * More details available from Chris/Engagement.

*Science Validation of Trash/Integration Testbed*
   * ITB 1.1 validation by VOs coming up in Jun/Jul'09, with Sites/Trash/Integration and Storage groups.
   * Expecting nearly 12 VOs, including ATLAS, CMS, LIGO.
   
*General Concerns*
   * Opportunistic Storage:
      * D0 is a sustained user since Jul'08. 
      * CDF got started at 1-2 sites in '08, but slowed down in '09. Expected to provide requirements for OSG Storage.
      * !IceCube has conveyed a need, and is expected to soon provide requirements for OSG Storage.
      * Workflows of most other at-large VOs are not in immediate need of SRM-based usage; simple !GridFTP and $OSG_* directories being used.
   * MPI usage: Not clear if VO Group should provide assistance.
   * VOs with low activity: In need of more effort from OSG, to help engage and sustain VOs with low activity.
   
-- Main.ChanderSehgal - 27 May 2009
   * [[%ATTACHURL%/OSG_docs_published_summary.txt][OSG_docs_published_summary.txt]]: Summary of collection of OSG papers

%META:FILEATTACHMENT{name="OSG_docs_published_summary.txt" attachment="OSG_docs_published_summary.txt" attr="" comment="Summary of collection of OSG papers" date="1244741358" path="C:\Documents and Settings\ritchie\Desktop\OSG docs published summary.txt" size="795" stream="C:\Documents and Settings\ritchie\Desktop\OSG docs published summary.txt" tmpFilename="/usr/tmp/CGItemp19987" user="DavidRitchie" version="1"}%
