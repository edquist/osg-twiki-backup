%META:TOPICINFO{author="TimCartwright" date="1503522083" format="1.1" reprev="1.3" version="1.3"}%
%META:TOPICPARENT{name="AreaCoordinator"}%
---+ OSG Area Coordinators Meeting &mdash; 23 August 2017


---++ Meeting Coordinates

%TABLE{}%
| When: | Wednesday, 2:00 p.m. Central Time |
| Online: | https://IU.zoom.us/j/320940886 |
| Phone (toll): | +1 (408) 638-0968 &nbsp;<em>or</em>&nbsp; +1 (646) 558-8656 &nbsp; &rarr; &nbsp; PIN 320-940-886 |


---++ Top of Meeting Business
   1. Review the [[https://indico.fnal.gov/categoryDisplay.py?categId=86][OSG calendar]]
   1. Talk about [[NewsletterList][upcoming research highlights]]

*Note for presenters:* Please report on updates to the area goals we had established at the staff retreat; these goals are available at https://osg-docdb.opensciencegrid.org:440/cgi-bin/ShowDocument?docid=1232. Remember that the next OSG annual report for each area will explicitly state the goals and the results.


---++ Production Support, by Ken Herner
[[%ATTACHURL%/082317-osg-ac.pdf][Presentation as PDF]]


---++ Minutes

*Attendees:* Brian Lin, Ken Herner, Kyle Gross, Rob Quick, Scott Teige, Shawn !McKee, Susan Sons, Tim Cartwright

---+++ Calendar
   * Rob&nbsp;Q. will add a UNESP workshop for December.
   * Rob&nbsp;Q. also reminded Tim&nbsp;C. to work on AHM plans sooner rather than later.

---+++ Research Highlights (Kyle Gross)
   * Rob and Kyle will send emails to education/outreach folks by end week for summaries of summer activities.

---+++ Production Support (Ken Herner)
   * Pilot hours in past 90 days are quite steady, with OSG+HCC+GLOW+SBGRid totalling about 18M pilot hours per month, and the combined Fermilab experiments are nearly equal to those four in this period. CMS, which has trailed ATLAS since early- to mid-summer, is regaining parity especially in the past 3 weeks. In the plots, note that each FIFE experiment pilot may be counted in the Fermilab VO or in its own project-specific VO, depending on how it is submitted.

   * Opportunistic payload hours are down over the period. Likewise, opportunistic pilot hours at CMS sites are down, because CMS is using those resources. And then ATLAS and CMS opportunstic resources are down in the same manner. All of these changes are presumed to be a result of increased direct CMS usage.

   * !GlueX is starting to run again, peaking at over 14K hours one day last week. They are expected to continue running for a while.

   * Varied news from working with sites: Imperial and CERN Tier 0 are now in production for DUNE. !NOvA is having issues with OSC, because OSC worker nodes are now EL7 whereas !NOvA requires EL6; talking to !NOvA about how to proceed. Looking at Ole Miss (a primarily CMS site) for opportunistic use by Fermilab, but on hold at the moment due to higher priority tasks. Targeting Colorado next, because they have some DUNE exposure. Planning to get back to looking at UCSD&rsquo;s GPU resources.

   * In HPC news, CMS and MINOS+ were successful on Stampede with startup XSEDE allocations, and CMS was successful recently at NERSC. ATLAS got 90M CPU hours on Titan. Mu2e is now testing at NERSC (via HEPCloud) and other FIFE experiments have expressed interest in same. Ken repeated his question from the planning retreat (Are we doing as much as can to get access to big HPC resources?), but there was no follow-on discussion.

   * GPU progress is proceeding. !IceCube used GPU resources at Nebraska, and is trying now at Syracuse, but having issues matching slots. Fermilab is also working at Nebraska and trying for Syracuse, but Singularity support at SU is not done (main issue seems to be Singularity). And the BNL cluster is unlikely to be publicly available soon. In general, Ken made the case for trying to have a common approach to GPUs. To that end, there was a meeting 2 weeks ago with FIFE experiment and CMS LPC representatives.

   * !StashCache use is increasing steadily: !NOvA, DES, and Microboone are using it via CVMFS; mu2e is coming soon, and DUNE is coming someday. Plus, there are many other users via OSG Connect + stashcp.

   * Lately, opportunistic availability has been OK, but spring and summer of 2018 will be very busy; there are many mouths to feed over the next 12 months. OSG must keep growing the pool of available resources and be sure to train users to use the limited resources efficiently.

   * For CPU resources in particular, it is important to increase opportunistic availability and make sure that VOs can use as many sites as possible. For GPU resources, the key is settling on standard approaches to accessing resources.

   * Keep pushing VOs to use as much of the standard software stack as possible, and focus on getting containers as many places as possible. And continue to watch new developments in other places (e.g., !S2I2).

   * HEPCloud is coming&hellip; starting with Fermilab and CMS, and then who?

---+++ Concerns
   * The GPU ramp-up is going slower than Ken expected, but the main concern is the lack of standard approaches.
   * There are questions (and not many answers) about the best way to deliver containers to worker nodes.
   * As more international sites come onboard, it is important to figure out how best to coordinate with them.
   * It would be helpful to have glidein logs in Elasticsearch for debugging and monitoring, especially new sites.

---+++ Action items
None.

%META:FILEATTACHMENT{name="082317-osg-ac.pdf" attachment="082317-osg-ac.pdf" attr="" comment="Prod Support Update 23 Aug 2017" date="1503518664" path="082317-osg-ac.pdf" size="4832026" stream="082317-osg-ac.pdf" tmpFilename="/usr/tmp/CGItemp34369" user="KennethHerner" version="1"}%
