%META:TOPICINFO{author="FkW" date="1262218521" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WebHome"}%
---+ Advanced Network and Distributed Storage Laboratory (ANDSL)

%TOC%

---++ Brief Overview of Goals

The primary goal of ANDSL is to shrink the time between 100Gbps network links becoming available, and the OSG
Science Stakeholders being able to benefit from their capacity. As we have very modest means within this project,
we are focusing on the following areas:

   * Creation of an easy to operate "load generator" that can generate traffic between Storage Systems on the Open Science Grid.
   * Instrumenting the existing, and emerging production software stack in OSG to allow benchmarking.
   * Benchmark the production software stack, identify existing bottlenecks, and work with the external software developers on improving
      the performance of this stack.

---++ Architecture Description

OSG is a Grid of "sites". Each site has compute and storage resources, and is connected to one or several of the major US Research Networks.
As of 2009, roughly twenty sites have shared 10Gbps WAN connectivity, and a few have multiple 10Gbps links available to them.

The Storage Elements (SE) 
at the sites are generally distributed in nature. WAN data movement between two sites thus generally involves O(100) or more simultaneous flows
between these distributed SEs. This data movement is orchestrated via a layered software stack. The implementation details vary considerably
from site to site, and we will provide here only a conceptual description.

   * hardware layer: commodity disk systems with or without software or hardware RAID configuration. Disks are generally read from and written
     to simultaneously, from both LAN and WAN. The individual disk IO at the hardware layer that can be relied upon per flow is thus typically modest.
   * distributed filesystem layer: a variety of different technical implementations are deployed. The common characteristics among them is that 
      they virtualize the hundreds or thousands of hard disks per site into one coherent
     system. Some of the implementations furthermore provide replication and/or traffic shaping capabilities in order to increase reliability
     and/or availability of the stored files.
   * transfer protocol layer: gridftp is the de facto standard for WAN transfers, and there are at least two implementation of gridftp deployed. 
     LAN transfer protocols are much more heterogeneous, and it is conceivable that WAN transfers may become more heterogeneous in
     the future. Possible alternatives to gridftp include FDT and Xrootd.
   * Storage Resource Management (SRM): All large SEs on OSG presently support SRM v2. However, not all of them support a 
     complete implementation thereof.
     The purpose of SRM is generally to provide a single entrypoint to the O(100) gridftp servers per site. SRM is thus largely used
     as an aggregation point for transfer request handling etc. The space reservation features are sometimes omitted in the
     deployments because they slow down the performance.
   * File Transfer Service (FTS): The SRM specification does not include any form of bandwidth shaping or throtteling. SEs at sites are thus easily
      overloaded, especially if they are used by many VOs simultaneously. Some sites thus require WAN accesses to be managed by FTS on top
      of SRM.
   * VO specific data management system: This layer of Middleware is outside the OSG software stack, but nevertheless essential
      for understanding end-to-end performance characteristics. Each Virtual Organization operates their own data management and
      transfer systems. These systems include databases for managing namespaces ("file catalogues"), as well as middleware
      to queue up transfer requests, recover from transfer errors, etc.
      Depending on the scope of the data to be managed and transfered, this layer
      of Middleware can be quite complex.

Over time, we expect 100Gbps network connectivity between OSG sites to become an essential capability for our Science Stakeholders.
At first, this capability will be needed for aggregation of traffic between a matrix of !NxM sites on the transnational backbone. Sites themselves
will at that point be required to support no more than a couple to a few 10Gbps. At some future point, 100Gbps connectivity
will become available to more and more OSG sites.

We expect that network bandwidth reservations will become an important new capability in the Middleware stack. However, at this point, it is
not clear where in the stack this functionality belongs. In fact, we expect a period of experimentation with the emerging APIs at different
layers of this stack.

---++ State of the Art

Data movement in excess of 10Gbps into, or out of OSG sites has been observed under standard operating conditions. 
Under testing conditions like Supercomputing 2009, and with heavily modified software stacks, aggregate bidirectional IO rates in excess of 100Gbps have been accomplished. At the same time, transferring a complete dataset of O(10) TBs in a predictably short period of time
continues to be a challenge. A typical example is depicted in the figure below. Here, the CMS experiment reprocessed it's entire 2009
dataset from LHC collision data, and the corresponding Monte Carlo simulation data over Xmass, 
and physicists at UCSD wanted access to this data
asap between Xmass and New Years 2009. A total of roughly 15TB was transferred, mostly from the two archival centers in France and the US.

What is remarkable in this plot is that roughly half the data arrived at UCSD in less than 3 hours at a rate of about 6Gbps via a mix
of several national and international research networks, 
while the remainder trickled in over another 21 hours. It's these transfer tails that limit the overall end-to-end throughput. 

To benefit from high capacity networks of the future, we need to gain a much better understanding of the existing limitations in the
Middleware stack at the endpoints. Especially the interplay between problems in the various layers of the stack is at present poorly
monitored, and even more poorly understood.

   * Example IO pattern into an OSG site: <br />
     <img src="%ATTACHURLPATH%/quantity_rates.png" alt="quantity_rates.png" width='400' height='250' />   

This example is typical of large volume data movement on OSG for multiple reasons:
   * The Science goal is to move a significant but fixed amount of data in a short period of time. I.e. traffic expectations are very spiked.
      There is not much data moving before and after the specific transfer request. The Scientists would like to have full use
       of a significant fraction of a 10Gbps connection for a period of a few hours to a day.
   * The are multiple sources for the data, both national as well as international in nature. 
   * The OSG site is more often a sink than a source.
      Typical use pattern is to move datasets to a site, analyze them, then eventually delete them, and move different datasets in. 

---++ Initial Testbed

As explained in the previous section, the present limitation on the end-to-end performance
is generally not in the WAN layer but rather in the complex layers of Middleware on top of the hardware at the endpoints.

To make progress in better understanding the limitations, and prepare endpoints for the emerging 100Gbps transnational
connectivity, we are thus focusing on a testbed capable of up to 30Gbps across the UCSD campus. To do so, we are presently
exploring possible collaboration with SDSC, the Rocks cluster management team, and Future Grid.

One endpoint will be located at the CMS T2 Center, an OSG production site. 
This endpoint is equipped with a Cisco 6509 with one 4port 10Gbps module, and sufficient fiber to hook up all 4 ports to
the campus backbone. At present, only two of the 4 ports are used, one to provide 10Gbps connectivity to ESnet, a second
for 10Gbps to CENIC. Two more ports are presently not used.

A second endpoint will be dynamically deployed
for the duration of performance tests on hardware located at SDSC. The details still need to be worked out. A basic diagram of the
network capabilities on the OSG side is shown below.

   * testbed diagram: <br />
     <img src="%ATTACHURLPATH%/diagram_network.gif" alt="diagram_network.gif" width='300' height='400' />    

---++ Future 100Gbps testbed

We'll fill this in once the details of what will be available are more clear.
Fundamentally, we want to dynamically deploy SE endpoints on hardware that connects to the 100Gbps testbed.
We are presently somewhat unclear as to the future availability of hardware at the endpoints that would allow realistic tests.
To set the scale, we estimate that roughly 1000 spinning disks are needed to realistically sink 100Gbps at the endpoint.

---++ Activities, Papers and Publications

Part of the objective of this project is to assemble documentation on the performance of the Middleware stack.
In general, the work referred here is done in collaboration with a variety of partners. ANDSL does not own any hardware itself,
nor does it have the effort to do any serious development of Middleware. We thus depend on these collaborations in order to
achieve our goals.

   * Supercomputing 2009 (link write-up here when in docdb)
   * hadoop filesystem paper  (link here to docdb document)


-- Main.FkW - 29 Dec 2009
 

%META:FILEATTACHMENT{name="diagram_network.pdf" attachment="diagram_network.pdf" attr="" comment="testbed diagram" date="1262143704" path="diagram_network.pdf" size="21320" stream="diagram_network.pdf" tmpFilename="/usr/tmp/CGItemp16792" user="FkW" version="1"}%
%META:FILEATTACHMENT{name="diagram_network.gif" attachment="diagram_network.gif" attr="" comment="testbed diagram" date="1262144212" path="diagram_network.gif" size="45192" stream="diagram_network.gif" tmpFilename="/usr/tmp/CGItemp16769" user="FkW" version="1"}%
%META:FILEATTACHMENT{name="quantity_rates.png" attachment="quantity_rates.png" attr="" comment="Example IO pattern into an OSG site" date="1262157449" path="quantity_rates.png" size="36570" stream="quantity_rates.png" tmpFilename="/usr/tmp/CGItemp16842" user="FkW" version="1"}%
