%META:TOPICINFO{author="FkW" date="1262144462" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+ Advanced Network and Distributed Storage Laboratory (ANDSL)

%TOC%

---++ Brief Overview of Goals

The primary goal of ANDSL is to shrink the time between 100Gbps network links becoming available, and the OSG
Science Stakeholders being able to benefit from their capacity. As we have very modest means within this project,
we are focusing on the following areas:

   * Creation of an easy to operate "load generator" that can generate traffic between Storage Systems on the Open Science Grid.
   * Instrumenting the existing, and emerging production software stack in OSG to allow benchmarking.
   * Benchmark the production software stack, identify existing bottlenecks, and work with the external software developers on improving
      the performance of this stack.

---++ Architecture Description

OSG is a Grid of "sites". Each site has compute and storage resources, and is connected to one or several of the major US Research Networks.
As of 2009, roughly a dozen sites have shared 10Gbps WAN connectivity, and a few have multiple 10Gbps links available to them.

The Storage Elements (SE) 
at the sites are generally distributed in nature. WAN data movement between two sites thus generally involves O(100) or more simultaneous flows
between these distributed SEs. This data movement is orchestrated via a layered software stack. The implementation details vary considerably
from site to site, and we will provide here only a conceptual description.

   * hardware layer: commodity disk systems with or without software or hardware RAID configuration. Disks are generally read from and written
     to simultaneously, from both LAN and WAN. The individual disk IO at the hardware layer that can be relied upon per flow is thus typically modest.
   * distributed filesystem layer: a variety of different technical implementations are deployed. The common characteristics among them is that 
      they virtualize the hundreds or thousands of hard disks per site into one coherent
     system. Some of the implementations furthermore provide replication and/or traffic shaping capabilities in order to increase reliability
     and/or availability of the stored files.
   * transfer protocol layer: gridftp is the de facto standard for WAN transfers, and there are at least two implementation of gridftp deployed. 
     LAN transfer protocols are much more heterogeneous, and it is conceivable that WAN transfers may become more heterogeneous in
     the future.
   * Storage Resource Management (SRM): All large SEs on OSG presently support SRM v2. However, not all of them support a 
     complete implementation thereof.
     The purpose of SRM is generally to provide a single entrypoint to the O(100) gridftp servers per site. SRM is thus largely used
     as an aggregation point for transfer request handling etc. The space reservation features are sometimes omitted in the
     deployments because they slow down the performance.
   * VO specific data management system: This layer of Middleware is outside the OSG software stack, but nevertheless essential
      for understanding end-to-end performance characteristics. Each Virtual Organization operates their own data management and
      transfer systems. These systems include databases for managing namespaces ("file catalogues"), as well as middleware
      to queue up transfer requests, recover from transfer errors, etc.
      Depending on the scope of the data to be managed and transfered, this layer
      of Middleware can be quite complex.

---++ State of the Art

Add more info here.

---++ Testbed

Add more info here.

   * testbed diagram: <br />
     <img src="%ATTACHURLPATH%/diagram_network.gif" alt="diagram_network.gif" width='500' height='600' />    

---++ Activities, Papers and Publications

Part of the objective of this project is to assemble documentation on the performance of the Middleware stack.
In general, the work referred here is done in collaboration with a variety of partners. ANDSL does not own any hardware itself,
nor does it have the effort to do any serious development of Middleware. We thus depend on these collaborations in order to
achieve our goals.

   * Supercomputing 2009 (link write-up here when in docdb)
   * hadoop filesystem paper  (link here to docdb document)


-- Main.FkW - 29 Dec 2009



%META:FILEATTACHMENT{name="diagram_network.pdf" attachment="diagram_network.pdf" attr="" comment="testbed diagram" date="1262143704" path="diagram_network.pdf" size="21320" stream="diagram_network.pdf" tmpFilename="/usr/tmp/CGItemp16792" user="FkW" version="1"}%
%META:FILEATTACHMENT{name="diagram_network.gif" attachment="diagram_network.gif" attr="" comment="testbed diagram" date="1262144212" path="diagram_network.gif" size="45192" stream="diagram_network.gif" tmpFilename="/usr/tmp/CGItemp16769" user="FkW" version="1"}%
