%META:TOPICINFO{author="RobQ" date="1272391233" format="1.1" reprev="1.7" version="1.7"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 13 Apr 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) John, Chris, Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan
 
---++ CMS (Burt)


---++ Atlas (Armen & Xin)

   * General production status
      * During the last week ATLAS production was at the average level of 7-8k/day running jobs, mixture of reprocessing and simulation. Reprocessing started at the end of the week, and is progressing without problems. LHC had improvements with high intensity collisions and delivered record luminosity over the weekend. Now technical stop until Thursday. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.1M jobs, with CPU/Walltime ratio of 82%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 1M managed MC production, validation and reprocessing jobs 
         * average 150K jobs per day
         * failed 92K jobs
         * average efficiency:  jobs  - 92%,  walltime - 92%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate is 200~300TB/day last week. 
   * Issues
      * BDII uploading to WLCG: info size limit exceeded, causing no info from BNL on cern BDII, critical for production
         * cern BDII limit increased to 10MB (confirmation?)
         * some mis-configured info from BNL CE GIP, solved, reduced the size of the info to 2MB now. 
      * Opportunistic SE usage for D0 : mapping issues should be fixed. Further tests are interrupted by a major problem on the local xrootd SE at UTA T2, which is now understood. Will continue and do more testing this week. 

---++ LIGO (Britta, Rob E.)

 ---+++ Gratia Reports
   * Current week's total usage: 6 users utilized 36 sites
      * 91368 jobs total (26386 / 64982 = 28.9% success)
      * 598492.8 wall clock hours total (478053.4 / 120439.4 = 79.9% success)
   *  Previous week's total usage: 4 users utilized 37 sites
      * 84855 jobs total (26574 / 58281 = 31.3% success)
      * 580605.8 wall clock hours total (478393.4 / 102212.4 = 82.4% success)

---+++ LIGO / E@H
   * Recent Average Credit (RAC): 1,794,375.88166, Last week: 1,809,868.32657
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0) 

---+++ LIGO / INSPIRAL
   * Testing work-flows (one day data sets) on Firefly/LIGO ITB cluster for run-time comparison, Firefly:  gatekeeper crash
      * LIGO_CIT: 3 hours, 21 mins
      * Firefly: submitted 4/16 1pm, still running, all jobs idling
   * Testing dagman ( file transfers and md5sum check) on Firefly/LIGO ITB (1000 gwf files):
      * LIGO_CIT: 56 mins
      * Firefly: 3 days

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Availability metrics for the last week 
      * [[http://tinyurl.com/23nvj99][GOC Services: BDII, MyOSG, RSV Collector, OSG Display]]
      * [[http://tinyurl.com/26z7t2w][GOC hosted Security services managed by OSG security team]]
   * [[http://osggoc.blogspot.com/2010/04/osg-129-release-announcement.html][ *OSG 1.2.9 Release Announcement* ]] - complete - Need to discuss release timing. 
   * There was an ReSS problem due to the immense amount of data coming out of BNL, Steve Timm worked with the Condor team to publish results faster.

---+++ Operations This Week
   * [[http://osggoc.blogspot.com/2010/04/goc-service-update-tuesday-april-27th.html][ *GOC Production Service Update* ]] - Tuesday, April 27th at 14:00 UTC
      * Short intermittent outage of (~5 minutes) is expected for all the services listed below while the GOC adjusts the virtual machine settings one VM host at a time; For example, myosg2 might be down for 5 minutes while myosg1 will still available be for anyone trying to access myosg.grid.iu.edu; Additionally, the GOC reserves four hours (14:00 - 18:00 UTC) in the unlikely event that unexpected problems are encountered. 
   * *Ongoing - Ticket Exchange (TX)*: 
      * *GGUS*
         * ATLAS is working with users to ensure that only alarm tickets go to critical/top priority. We have seen a few users who have used this for limited-impact problems.
      * *BNL RT*
         * No known issues
         * Discussing handling of merged ticket (new feature request) with BNL developer
      * *FNAL Remedy* 
         * No Change, waiting on FNAL action; FNAL developer going to be on vacation
      * *VDT*, *PROD_SLAC*, *UC_CI*, *SBGrid* - No Change, waiting on GOC action discussed below
         * <em>No change since last week</em>: GOC has promised simple instructions on how to setup RT to use GOC-TX; depending on ease and priority, these SCs may adopt use of GOC-TX
   * *Ongoing - Top Level WLCG BDII Monitoring* 
      * MyOSG GIP-validation view modified to include these results; expect release on May 11th.
      * <em>No change since last week</em> ; Action items upon finding a problem still not clear - being worked out with CMS and OSG management


---++ Engage (Mats, John, Chris)


---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)


---++ Security (Mine)
