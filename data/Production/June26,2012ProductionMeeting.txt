%META:TOPICINFO{author="RobQ" date="1340740432" format="1.1" version="1.8"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 12 Jun 2012
---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Xin, Armen, Brian, Suchandra, Tony, Marco, Rob Q., Scott T., Mine, Chander, Dan
 
---++ CMS (Tony)
   * Job statistics for last week
      * 16,902 jobs/day

   * Transfer statisics for last week
      * ~529 TB/day

---++ Atlas (Armen & Xin)

   * General production status
      * Since June 18 LHC was in scheduled stop, doing machine developments, and this week is in technical stop, will go back to normal operations at the end of the week. Current collected luminosity for ATLAS ~6.3 fb-1. Data from this year is being analyzed, with results to be presented next week at ICHEP conference. Very much anticipated CERN seminar on July 4, where first official Higgs results of this year will be presented.
      * US ATLAS production during the past week continue to be very stable, at the average level around 20K running jobs, mostly simulation type, with high priority to finish MC production for Higgs analysis for ICHEP.
   * Job statistics for last week.      
      * Gratia report: 1.4M pilot jobs run on USATLAS sites, with CPU/walltime ratio of 89%  
      * Real Jobs processed by US sites for last week, reported from PanDA monitor 
         * 1.2M
   * Data Transfer statistics for last week
      * Data transfer rate was 400~500TB/day at BNL T1 in last week.  
   * Issues

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week
   * [[http://tinyurl.com/6nh3glo][GOC Services Availability/Reliability]]
   * [[http://myosg.grid.iu.edu/miscstatus/index?datasource=status&count_sg_1=on&count_active=on&count_enabled=on][Current Status]]
   * Internal LDAP server became unstable early AM Thu. 21/Jul, root cause too many open file descriptors caused by a large (x12) increase in processes on osg-xsede.grid.iu.edu. Simple configuration changes and an LDAP restart solved the problem. System has survived a larger process spike without problem. Some rsynch and backup delays were encountered, staff access was intermittent during the problem, no outward facing outages. Resolved ~10 AM 21/Jul.
   * BNL encountered problems contacting BDII 7:30 PM 21/Jul. GOC notified of critical issue 00:10 22/Jul. Problem traced to networking issue between !GigaPoP and internet2. Resolved ~10 AM 22/Jul. We have received details of the problem from Indiana University network engineers and assurance the problem will not recur.
   * ITB release, [[http://osggoc.blogspot.com/2012/06/goc-service-update-tuesday-june-26th-at.html][release note]] is available.
   * Glide In Factory
      * Still testing glideinWMS v2_6_rc1.  Currently trying to work out some possible rrdSupport bugs.
      * Condor 7.8.1 CE testing completed successfully
      * Backup script deployed to UCSD and GOC Factories: GOC disk usage reduced to 77% (more optimization in work)

---+++ Operations This Week
   * Production release, [[http://osggoc.blogspot.com/2012/06/goc-service-update-tuesday-june-26th-at.html][release note]] is available.
   * OS updates, reboots will be required.
      * glide-in, osg-xsede reboots at or shortly after Noon, eastern time, have notified, will warn.
   * BDII update dependent on timely completion of OS updates.
      * This was done at ~2:00PM EDT. Stand alone SEs are now properly reported in BDII. 
   * [[http://osggoc.blogspot.com/2012/06/new-vo-pakcage-coupp-addition-and-lsst.html][New VO-Package Released]]
   * Glide In Factory
      * Configuring Storage Element advertisement into the UCSD Production Factory

---++ Campus Infrastructures / HTPC (Dan, Brooklin)
   * Had a good, long technical meeting on Bosco at UW-Madison yesterday with Jaime, Todd, Derek in person and Dan and Marco online. Have a path forward for traceroute debugging feature and udpated progress on all deliverables.
   * UW continues to delay deploying more partitionable slots for HTPC testing. In lieu, will leverage work done a Nebraska and start working on docs for other campuses to use.
   * Saw a nice Campus Grids success story from UAB of late, leveraging Condor for opportunistic cycles for protein docking app autodock. They will be deploying condor more agressively for this app. We will check in on them periodically and see if we can get them included in our 'Deployed Campus Grids' profiles.

---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ User Support (Chander, Mats)

   * UMD-IGS: Have host and service certificates. Are in the process of installing software for site.

   * PNNL/Belle: In the process of installing a Compute Element.

   * iPlant: TACC started the modification of their portal software to support submission to OSG. Tests in Jul / Aug (depending on availability of new hires).

   * Public storage (using iRODs) ready to move to limited production deployment (with 1-2 VOs)


---++ Security (Mine)

---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings