%META:TOPICINFO{author="DanFraser" date="1251742944" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="USLHCTier-3Group"}%
-- Main.DanFraser - 28 Apr 2009
---++ Background and General Analysis 
Sites are now (and for the next few months will be) receiving funds for setting up T3s.
   * Typical funds are on the order of $30-50K,  This is less than what some had anticipated. Hence these new T3s are expected to be "small".
   * Operating resources are expected to be small, some fraction of a graduate student who may or may not be a "trained" Admin.
   * Atlas is anticipating as many as ~20 new sites coming online over the next few months.
   * (Need data from CMS)

Sites are encouraged to set up T3s for the following reasons:
   * User storage is limited on T2s
   * It is anticipated that T2 queues may fill up when LHC data starts arriving 
   * Configuring and debugging grid jobs still poses challenges


For Atlas, there are seven OSG T3 sites (need the OSG names). There are approximately 15?? other sites although these primarily use DQ2 to get data from T2's remotely and do not currently run an OSG stack. Those running an OSG stack are:
   * ANL (SE)
   * IllinoisHEP (CE, SE, DDM) Tier3gs
   * U-Chicago (SE + CE + PanDA + DCache)
   * U- Wisconsin (SE + CE + PanDA + DCache)
   * University of Texas/Dallas (SE)
   * Louisiana Tech (SE)
   * Hampton (SE)
   * Tufts(SE)
   * Duke (SE)

For CMS, there are ten OSG T3 sites (need the OSG names). ...(need more data from CMS)
   * OSU-CMS (SE, CE)
   * TTU Texas Tech - Alan Sill
   * Riverside - Bill Strossman
   * Maryland - Malina Kirn
   * Colorado - Doug Johnson
   * Cornell - Dan Riley
   * FNALLPC - Paul Rossman
   * Kansas - John Clune
   * Minnesota - Jeremiah Mans
   * Princeton - Peter Elmer, Abhishek Gupta, Vinod Gupta
   * Princeton ICSE - Christopher Tully
   * Rice - Pablo Yepes, Luca Sabbatini
   * Rutgers - Pieter Jacques, Eva Halkiadakis
   * Tennessee - Gerald Ragghianti
   * Vanderbilt - Paul Sheldon, Kevin L. Buterbaugh
   * UCLA - David Saltzberg, Prakashan Korambath
   * Florida International U - Jorge Rodriguez
   * FSU ?  - Harrison Prosper, Jeff Kent
   * Florida Inst of Tech - Marcus Hohlmann, Patrick Ford
   * Nebraska (+ Omaha?) - Carl Lundstedt
   * Virginia - Bob Hirosky
   * Ohio State - Grayson Williams
   * John Hopkins - Petar Maksimovic, Morris Swartz
   * UC Davis - Michael Case 
   * Mississippi - David Sanders
   * Iowa ? - Shaowen Wang
   * Caltech ? - 

---++ CE Considerations

There are two main reasons why sites deploy CEs:
   1 To enable their data resources to be shared with other grid users
   1 To provide a common interface between grid jobs submitted locally and off-site jobs

In the case of Atlas:
   * All (or nearly all) jobs are submitted via PanDA and users do not directly interface to CEs.
   * Enabling the same job submission interface on local resources implies that sites must install PanDA software as well as DDM software. Only three??? of the existing T3s currently do this. This type of infrastructure layer implies a significant amount of overhead that can be handled by more mature T3 sites with ~1FTE to support them. It is not recommended for new or beginning sites.
   * Most of the new T3s coming online are not expected to share resources.
   * Hence the conclusion is that most Atlas T3s will NOT need to install a CE.

For CMS: ...

---++ SE and File System Considerations

There are ?? main reasons why sites deploy SEs:
   1 Access to data on LHC sites can be automated by means of an SE (e.g. data subscriptions).
   1 T2's prefer accesses via SEs since data transfers can be managed better and reduce the risk of overloading T2 sites
   1 SEs provide a uniform access layer for moving large amounts of data both locally and off-site. (not sure about this one)

An important consideration in setting up an SE is the choice of file system. SE's provide a grid accessible interface (BeStMan/SRM) to a variety of file systems (NFS, XROOTD, dCache, HDFS). An important capability of BeStMan/SRM is that it can interface to multiple file systems at the same time. 
   * dCache is not recommended for new T3s due to the level of support required for this product
   * HDFS is being tested and is not yet being recommended for T3s except under experimental circumstances
   * For T3's that already have commercial grade central file systems (e.g. Network Appliances or BluArc), it is beneficial for SE's to leverage that capability in setting up their SEs. Conversely, sites without this capability may prefer to consider installing their own file system such as XROOTD. 
   * Sites should consider the following items concerning XROOTD:
      * An XROOTD file system is strongly recommended for sites that anticipate running PROOF.
      * Central file systems require that users manage their own datasets. While this can be quite manageable for small numbers of datasets, for larger numbers of datasets XROOTD provides significant help in managing the datasets and can be very beneficial.
      * An important capability of XROOTD is the ability to run jobs where the data resides as opposed to moving large datasets througout the network that can significantly impact processing time. Users can of course acquire this benefit via scripting on central file systems, but XROOTD handles this automatically.
      * In situations where a distributed file system is needed, XROOTD provides a commonly used solution.

For Atlas:
   * Sites are encouraged to deploy an SE for data interfacing to the T1/T2s.
   * Sites may not have a complete choice over existing file systems and infrastructure since T3s may be (or become) part of an existing system.
   * Since the SE can interface with multiple file systems at the same time, sites have the option to add a separate XROOTD filesystem as a stand alone file system in parallel to the existing file system when XROOTD is needed.

For CMS:

---++ Cluster Configuration Considerations:
   * Since not all sites are dedicated T3s, there is expected to be a wide variation in the infrastructure that sits below the usual OSG stack (e.g. batch scheduler, cluster management tools)
   * Some interactivity for small jobs is desirable especially for setting up and debugging in preparation for running large jobs
   * Using a batch system such as Condor is a basic component for configuring a T3. Some sites w

---++ Security Considerations
   * There are three potential sources of security risks for sites that install OSG software:
      * A potentially new security model is introduced into the organization that needs to be analyzed
         * In general the site is not all that new
      * A seemingly unfamiliar security model provides well defined access to resources by users that may not have been vetted in the usual way.
      * Additional ports are opened in the firewall that could be subject to attack
         * Standard security principles apply
      * Users that have not necessarily been vetted by the site "organization" can access specified data repositories and/or submit jobs
      * Additional software packages are introduced where potential security vulnerabilities may be exploited
         * The OSG team has an active securi
   * Security is an important consideration for T3s for several reasons:
      * 
      * Often the organizational level system administrators need to be educated about running Grid software. In at least one case so far (Tufts), a call to discuss the OSG security model was required before they would allow grid software to be utilized.

---++ Community Building Considerations

---++ Requirements Summary
