%META:TOPICINFO{author="DanFraser" date="1251770646" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="USLHCTier-3Group"}%
-- Main.DanFraser - 28 Apr 2009
---++ Background and General Analysis 
Sites are now (and for the next few months will be) receiving funds for setting up T3s.
   * Typical funds are on the order of $30-50K,  This is less than what some had anticipated (~$100K).
   * Operating resources are expected to be small, some fraction of a graduate student who may or may not be a "trained" Admin.
   * Atlas is anticipating as many as ~20 new sites coming online over the next few months.

Sites are encouraged to set up T3s for the following reasons:
   * User storage is limited on T2s
   * It is anticipated that T2 queues may fill up when LHC data starts arriving 
   * Configuring and debugging grid jobs still poses challenges

For Atlas, there are seven? T3 sites running an OSG stack, five of these are reporting to OIM. There are approximately 15?? other sites although these primarily use DQ2 to get data from T2's remotely and do not currently run an OSG stack.
Atlas T3s running an OSG stack include:
   * GLOW-Atlas (-SRM) (CE, SE, DDM) Tier3gs
   * IllinoisHEP (CE, SE, DDM) Tier3gs
   * UTD-HEP (CE)
   * LTU_CCU (??)
   * LTU_OSG (??)
   * Duke_University_Tier_3 (??)
   * U-Chicago (SE, DDM) Tier3gs -- Not reporting to OSG (University_of_Chicago_Teraport_Cluster??)
   * ANL (??) - Not reporting to OSG
Atlas T3s that are not running an OSG stack include:
   * Hampton??
   * Tufts (SE)
   * ...

For CMS, there are 27 T3 sites: 
   * osu-cms (CE)
   * TTU Texas Tech (CE)
   * umd-cms (CE)
   * UColorado_HEP (CE, SE)
   * UCR-HEP (CE)
   * Cornell - NYSGRID_CORNELL_NYS1 (CE) ??
   * USCMS-FNAL-XEN (CE)
   * FNALLPC - Not Reporting on OSG ??
   * Kansas - GPN-HUSKER (CE) ??
   * Minnesota - 
   * Princeton - 
   * Princeton ICSE - 
   * Rice (CE)
   * rutgers-cms (CE)
   * Tennessee -
   * Vanderbilt (CE)
   * UCLA_Saxon_T3 (CE)
   * FIU-PG (CE)
   * FSU - decommissioned
   * FLTECH (CE) 
   * Nebraska or GPN_HUSKER (CE) ??
   * Nebraska CMS T3 (CE)   (Omaha)
   * Virginia - UVA-sunfire (CE) ??
   * osu-cms (CE)
   * John Hopkins - 
   * UCD (CE) 
   * UMissHEP (CE)
   * Iowa ? - Shaowen Wang
   * Caltech ? - 

---++ CE Considerations

There are three reasons why sites deploy CEs:
   1 To enable their data resources to be shared with other grid users
   1 To provide a common interface between grid jobs submitted locally and off-site jobs
   1 Sites can be remotely accessed via Certificate (no password required)

In the case of Atlas:
   * All (or nearly all) jobs are submitted via PanDA and users do not directly interface to CEs.
   * Enabling the same job submission interface on local resources implies that sites must install PanDA software as well as DDM software. Only three??? of the existing T3s currently do this. This type of infrastructure layer implies a significant amount of overhead that can be handled by more mature T3 sites with ~1FTE to support them. It is not recommended for new or beginning sites.
   * Most of the new T3s coming online are not expected to share resources.
   * Hence the conclusion is that most Atlas T3s will NOT need to install a CE.

For CMS: ...

---++ SE and File System Considerations

There are four reasons why sites deploy SEs:
   1 Data can be transferred using 3rd party transfers. This also provides automation (e.g. data subscription).
   1 T2's prefer accesses via SEs since data transfers can be managed better and reduce the risk of overloading T2 sites
   1 SEs allow space management operations (quota enforcement, space reservations)
   1 SSO access to storage provided by a grid certificate
   1 In some cases the SE is packaged with the File system (e.g. BeStMan-XROOTD)

An important consideration in setting up an SE is the choice of file system. SE's provide a grid accessible interface (GridFTP or BeStMan/SRM) to a variety of file systems (NFS, XROOTD, dCache, HDFS). An important capability of GridFTP or BeStMan/SRM is that it can interface to multiple file systems at the same time. 
   * dCache is not recommended for new T3s due to the level of support required for this product
   * HDFS is being tested and is not yet being recommended for T3s except under experimental circumstances
   * For T3's that already have commercial grade central file systems (e.g. Network Appliances or BluArc), it is beneficial for SE's to leverage that capability in setting up their SEs. Conversely, sites without this capability may prefer to consider installing their own file system such as XROOTD. 
   * Sites should consider the following items concerning XROOTD:
      * An XROOTD file system is strongly recommended for sites that anticipate running PROOF.
      * Central file systems require that users manage their own datasets. While this can be quite manageable for small numbers of datasets, for larger numbers of datasets XROOTD provides significant help in managing the datasets and can be very beneficial.
      * An important capability of XROOTD is the ability to run jobs where the data resides as opposed to moving large datasets througout the network that can significantly impact processing time. Users can of course acquire this benefit via scripting on central file systems, but XROOTD handles this automatically for ROOT based files.
      * In situations where a distributed file system is needed, XROOTD provides a commonly used solution.

For Atlas:
   * Sites are encouraged to deploy an SE for data interfacing to the T1/T2s.
   * Sites may not have a complete choice over existing file systems and infrastructure since T3s may be (or become) part of an existing system.
   * Since the SE can interface with multiple file systems at the same time, sites have the option to add a separate XROOTD filesystem as a stand alone file system in parallel to the existing file system when XROOTD is needed.
   * Atlas is recommending a separation between Grid storage and local only storage.
      * Prevents a user from interfering with managed storage

For CMS: ...

---++ Cluster Configuration Considerations:
   * Since not all sites are dedicated T3s, there is expected to be a wide variation in the infrastructure that sits below the usual OSG stack (e.g. batch scheduler, cluster management tools)
   * Some interactivity for small jobs is desirable especially for setting up and debugging in preparation for running large jobs
   * Using a batch system such as Condor is a basic component for configuring a T3. 

---++ Security Considerations
   * The main concerns from the VOs are:
      * That someone could break into the sites and steal (or compromise) security credentials and run jobs at other sites
      * That there could be an incident that would cause the institutional infrastructure to shut down the site
   * There are three potential sources of security risks for sites that install OSG software:
      * A potentially new security model is introduced into the organization that needs to be analyzed
         * In general the site is not all that new
      * A seemingly unfamiliar security model provides well defined access to resources by users that may not have been vetted in the usual way.
      * Additional ports are opened in the firewall that could be subject to attack
         * Standard security principles apply
      * Users that have not necessarily been vetted by the site "organization" can access specified data repositories and/or submit jobs
      * Additional software packages are introduced where potential security vulnerabilities may be exploited
         * The OSG team has an active securi
   * Security is an important consideration for T3s for several reasons:
      * 
      * Often the organizational level system administrators need to be educated about running Grid software. In at least one case so far (Tufts), a call to discuss the OSG security model was required before they would allow grid software to be utilized.

---++ Community Building Considerations
   * It is generally agreed from both CMS and Atlas that the best (and perhaps only) way to support T3s is to enable the T3s to help each other.
   * Both CMS and Atlas have communication channels and wiki pages but they are open to help from OSG on this front

---++ Requirements Summary
   * Sites - Help in enabling social networking for T3 admins
   * STG - A T3 guide for configuring Condor as a cluster manager
   * Storage - A T3 configuration guide for BeStMan-XROOTD (for Atlas)
   * Security - A clear online training program/guide for T3 admins (Material from the Admin workshop in Aug '09 is a good start)
   * Security - A Document (or an aggregated collection of docs) that explain why using the Grid security model is at least as safe (if not safer) than SSH models most commonly used within organizations (targeted toward institutional site admins)
