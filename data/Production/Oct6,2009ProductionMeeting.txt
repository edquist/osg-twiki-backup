%META:TOPICINFO{author="BurtHolzman" date="1254860149" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 06 Oct 2009
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:

---++ Attendees:
   * (to be updated after the meeting) Xin, Armen, Britta, Mats, Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan 
---++ CMS (Burt)
   * Computing: 100 khour/day, 88% success. CPU/wallclock at 81%.
   * Storage: Tier 1 transferred 1.2 PB last week (peak of 320 TB/day). Tier 2s transferred 315 TB (peak: 85 TB/day). Probes not functional at UERJ, MIT, Florida, Purdue.
   * OSG: now have 5 T2 CEs at OSG 1.2 (Nebraska, UCSD, .5 CIT). Tier 3s at 1.2: UCDavis, FIT, UMD, Vanderbilt, UCR, Notre Dame.
   * Physics challenge: started October 6.   Spike in number of jobs visible: http://home.fnal.gov/~burt/physchal.png
   * RSV: RSV Gratia Probe Delenda Est! (Status of gratia probe?)

---++ Atlas (Armen & Xin)

   * RLast week USATLAS sites on average keep 6~7k running jobs. Big part of them were reprocessing jobs in T1 and T2s. The reprocessing now is over. No problem in US sites. The post-mortem is Oct.14 .

   * Throughput test for all T1s going on now (Oct. 5-8): Monday, Tuesday: every 60 seconds 4 datasets will be produced for a total of 12 files and 3.6 GB. All datasets will be shipped to all T1s. This will produce total of 17300 files/day per T1 corresponding to approx 5 TB/day of data volume per T1. Wednesday, Thursday: same of Monday/Tuesday but file sizes will be a factor 10 bigger. This means every site will get 17300 files/day, corresponding to approx 50TB of space.

   * Another big performance test will be User Analysis Test (UAT), with initial dates Oct.21-23. Several big datasets (100M events each) are in preparation. The exercise will include analysis by many expert users, and then transfer of the results to T3s and other T2s. The exercise must not disrupt the regular user analysis. Twiki with details, including how to run the exercise, is in preparation.

   * job statistics for last week. 
      * Gratia report: USATLAS ran 651K jobs, with CPU/Walltime ratio of 90.3%. 
      * PanDA world-wide production report (real jobs): 
         * completed successfully 582K managed MC production, validation and reprocessing jobs ( 271K of them in US cloud)
         * average 83K jobs per day
         * failed 60K jobs
         * average efficiency is very good:  jobs  - 90.7%,  walltime - 94.3%

   * Site issues
      * BNL availability numbers is still available in the daily RSV VO report, thanks Brian. 



---++ LIGO (Britta)
 * Gratia reports:
      * 4 users utilized 36 sites
      * 68864 jobs total (56689 / 12175 = 82.3% success)
      * 528797.6 wall clock hours total (497693.4 / 31104.2 = 94.1% success)

      * Last week: 4 users utilized 26 sites
      * 25970 jobs total (22090 / 3880 = 85.1% success)
      * 180739.3 wall clock hours total (173315.5 / 7423.8 = 95.9% success)

  * E@H reportsRecent Average Credit (RAC): 
      * Recent Average Credit (RAC): 908,209.63508, Last Week: 410,627.43632
      * E@H rank based on RAC: 2 (+2)
      * E@H rank based on accumulated Credits: 13 (+4)

  * Details:
      * Robert is ramping up
      * Running into space issues in $DATA at some sites (RCAC, Cornell)
      * Planning to extend to Fermilab sites in the future
      * Account name change?
         * "LIGO on OSG"
         * "LIGO@OSG"
         * "LIGO and the Open Science Grid"
         * "LIGO on the Open Science Grid"
         * "The Open Science Grid for LIGO"

---++ Integration (Suchandra)
   * Ongoing efforts underway to realign documentation and to improve it
   * Ongoing vtb testing of rsv gratia probes and other updates
      * Still working a few rsv gratia probe bugs out


---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.3
      * 27 OSG 1.2.X resources (7 are 1.2.3)
      * 1 OSG 1.1.X (OSG 1.1.28 UColorado_ITB reporting to OSG, not ITB)
      * 33 OSG 1.0.X resources (19 are 1.0.4)                                                                                                                                                                  
      * 27 OSG 1.0.0
      * 2 OSG 0.8.0 (OU_OCHEP_SWT2, UIC_PHYSICS)
   * Site administrators meeting Thursday 10/1
      * minutes: https://twiki.grid.iu.edu/bin/view/SiteCoordination/SitesCoord091001
      * next one 10/22

---++ Engagement (Mats)

15 users utilized 34 sites

8558 jobs total (6828 / 1730 = 79.8% success)

30366.6 wall clock hours total (27704.1 / 2662.5 = 91.2% success)

No production issues. I still have to write the email about gram tmp clean ups (it is on my todo list..). I will not be on today's call as I'm traveling.


---++ Metrics (Brian)
   * Bestman/Xrootd -> SRMv2 renaming appears to have gone well; next on our list is the Gratia naming consistency
   * Tried to "fix" the various Gratia complaints Mats et al had coming from Omaha's Gratia setup.  This did not go well; will have to roll back and try again next week.
      * Did not go well because of incorrect Gratia configuration overlooked by me.

---++ Virtual Organizations Group (Abhishek)


---++ Grid Operations Center (Rob Q.)


---+++ Operations Last Week 
   * !BestmanXrootd will be removed as a service on OIM on Wednesday. GOC has added [[http://tinyurl.com/nde4sd][SRMv2 service mapping for all resources that are currently mapped to the BestmadXrootd]] service. See [[https://ticket.grid.iu.edu/goc/viewer?id=7457][related ticket]]
   * [[MyOSG18][MyOSG 1.8 and OIM 2.8]] were released on Friday. No users were effected, except for those expecting bugfixes, which they received with the upgrade.
   * GOC Services - Status Last Week: [[http://tinyurl.com/y8wtdq3][BDII]]

---+++ Operations This Week
   * GOC will add DNS round-robin for the following services: MyOSG, Ticket, Software. [[http://osggoc.blogspot.com/2009/10/upcoming-goc-maintenance.html][RSS feed entry about maintenances]]
      * GOC will move DNS entry to point the following services to Indianapolis based instances: OIM, Twiki -- expect short outages. 
      * Expect notification from GOC with full details
   * Ticket 1.7 will be released on Tuesday - main change [[https://ticket.grid.iu.edu/goc/viewer?id=7498][related to security notifications]]  [[http://osggoc.blogspot.com/2009/10/upcoming-goc-maintenance.html][RSS feed entry about maintenances]]


---+++ Future Events
   * Machine Room Move in Bloomington October 17 2009 - Some non-critical GOC Services -- twiki, www -- in Bloomington are expected to be down. 
      * GOC has put (or will put) in place instances of critical services BDII, MyOSG, GOCTicket, Software cache -- and non-critical services OIM  -- on IUPUI based servers. GOC will attempt to move OSG twiki to IUPUI. 

---++ Security (Mine)