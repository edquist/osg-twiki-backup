%META:TOPICINFO{author="XinZhao" date="1290538904" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 15 Nov 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Mats, Xin, Armen, Britta, Robert E., Brian, Suchandra, Burt, Marco, Marcia, Rob Q., Scott T., Mine, Chander, Dan
 
---++ CMS (Burt)


---++ Atlas (Armen & Xin)

   * General production status
      * LHC continue to run stable heavy ion collisions. Number of bunches per beam is 121, 113 colliding in ATLAS. Collected luminosity 3150 mb-1. Already interesting physics results coming out of it. ATLAS data reprocessing campaign moving quite nicely, without major problems, will be done by the end of the week. Production at US is stable at the level of ~10k running jobs. MC Geant4 simulation is 80% done. After that MC reprocessing (digi+reco) stage. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 1.4M jobs, with CPU/Walltime ratio of 81%. 
      * Panda world-wide production report (real jobs): 
         * Failure rate raised on Nov 18~19, due to panda server connection problem with Oracle DB at cern. 
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 400~500TB/day.
   * Issues
      * CERN/SAM BDIIs: back to full set of information publishing for BNL, so far so good. 

---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports
   * Last week's total usage: 5 users utilized 33 sites
      * 86048 jobs total (29590 / 56458 = 34.4% success)
      * 478554.7 wall clock hours total (292500.8 / 186053.9 = 61.1% success)
   * This  week's total usage: 5 users utilized 32 sites
      * 93112 jobs total (33040 / 60072 = 35.5% success)
      * 521261.9 wall clock hours total (330553.0 / 190708.9 = 63.4% success)

---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,238,130.94941, Last week: 1,307,391.95052
   * E@H rank based on RAC: 1 (+1)
   * E@H rank based on accumulated credits: 3

---+++ LIGO / INSPIRAL

   * FILE TRANSFERS
      * Successfully transferred data into 6 OSG SEs
         * Nebraska, FF, CIT_CMS_T2, OUHEP_OSG, GridUNESP_CENTRAL, NWICG_NotreDame, 
      * Testing SBGrid-Harvard-East 
 
   * TESTS WITH SRM SETUP
      * CIT_CMS_T2: currently running on 3 day data set
      * Nebraska: 3 day data set run finished successfully
      * OUHEP_OSG: 32 bit cluster

   * GLIDEINS
      * more testing at FF in progress: 10 simultaneous work-flows are submitted
      * Mats made some configuration changes that should improve performance

---++ Grid Operations Center (Rob Q.)


---++ Engage (Mats, John)


---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ Virtual Organizations Group (Marcia)

---+++ CDF 

   * still some pbs crashes at KISTI
   * still working on issue of slow performance when copying files back. Gabriele has been working with Rick St. Denis to get gridftp running; worked well. KISTI suggested using iperf, which was much slower. 

---+++ D0
   *  [[https://ticket.grid.iu.edu/goc/viewer?id=9566][Ticket #9566]] open for Cornell: DZero authentication is failing at nys1.cac.cornell.edu

---+++ GEANT 4 
   *  Setting up to do runs on GEANT4; Hoping for release in mid-December. Used opportunistic cyles during testing.

---+++ LSSD
   * Recently finished run on OSG (Phase 2). Image simulation group John Peterson asked to do validation on images between what they do and what we do on the OSG. Essentially 6 images, each made of 3000+ chip images. Evaluation of OSG to do validation was successful.
Gabriele now in discussions re. next phrase. 

---+++ !SBGrid
   * !SBGrid software distribution portal in beta testing for members for main workflow; have run about 12 so far using  20-40K+ hours each (varies).  Good success rates at this point and infrastructure at this level is stable.
   * Continuing issue: user job starts, then instantly fails. Front end resubmits--> thousands of jobs resubmitting then failing. Igor was on call to provide guidance. Peter Doherty says this doesn't happen often, but is a significant problem when it does. Igor is still waiting for actual data from Ian Stokes-Rees.
   * You can monitor the glidein factory here (for internal ops; not production and not supported.): http://glidein-1.t2.ucsd.edu:8319/glidefactory/monitor/glidein_Production_v3_1/factoryStatus.html 

---++ Security (Mine)
