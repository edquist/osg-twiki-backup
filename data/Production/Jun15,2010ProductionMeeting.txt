%META:TOPICINFO{author="MatsRynge" date="1276631981" format="1.1" version="1.9"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 07 Jun 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan
 
---++ CMS (Burt)

   * Job statistics for last week
      * ~76 khours/day
      * 29504 jobs/day
      * 97% success
   * Transfer statisics for last week
      * ~504TB/day

---++ Atlas (Armen & Xin)

   * General production status
      * LHC is continuing the commissioning to have stable high intensity (1011 proton/bunch) bunches. ATLAS production was relatively stable over the week at the level of 7K running jobs. At the moment production level is low. Waiting for new samples. Ongoing validation of the new simulation. 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2M jobs, with CPU/Walltime ratio of 84%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 506k managed MC production, validation and reprocessing jobs 
         * average 72K jobs per day
         * failed 35K jobs
         * average efficiency:  jobs  - 94%,  walltime - 94%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was 300TB/day.  
   * Issues
      * Opportunistic SE usage for D0 : site should be ready, waiting for test jobs from D0. 


---++ LIGO (Britta, Robert E.)

---+++ Gratia Reports
   * This week's total usage: 3 users utilized 38 sites
      * 78292 jobs total (33283 / 45009 = 42.5% success)
      * 631193.1 wall clock hours total (533408.5 / 97784.6 = 84.5% success)
   * Last week's total usage: 4 users utilized 36 sites
      *  58214 jobs total (28627 / 29587 = 49.2% success)
      * 324529.4 wall clock hours total (275016.6 / 49512.8 = 84.7% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,323,692.63918,  Last week: 949,356.17174
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)
---+++LIGO/INSPIRAL
   * Three bug reports submitted, waiting for approval 

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
   * Last Week's..
      * [[http://tinyurl.com/2fxgbg8][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/2chulrn][Reliability/Availability of Security Services]]
   * *Production Update Tuesday, 8th -- Complete* 
      * Routine update -- [[http://osggoc.blogspot.com/2010/06/revised-goc-service-update-tuesday-june.html][detailed notes]]
         * Per management request, CEMon Collector upgrade was postponed (indefinitely)
      * Resource Provider-only VOs like NERSC can now register
      * Implemented [[https://ticket.grid.iu.edu/goc/navigator2][/navigator2]] - this will replace /navigator and /mytickets in the coming weeks. 
      * Reorganized ticket viewer for non-editors and added description update field along with current CC editor
   * [[http://osggoc.blogspot.com/2010/06/osg-1210-release-announcement.html][ *OSG 1.2.10 released* ]]
   * [[http://osggoc.blogspot.com/2010/06/power-outage-affecting-vdt-services-on.html][ *VDT Services impacted by planned power outage on June 12th* ]]
       *vdt-version hangs indefinitely during the power outage, S. Timm opened ticket 8735.



---+++ Operations This Week
   * [[http://myosg.grid.iu.edu/map?all_sites=on&active=on&active_value=1&disable_value=1&gridtype=on&gridtype_1=on][<strong>Operations RSV Status Map</strong>]]
   * *ITB Update Tuesday, 15th* 
      * Watch for notification tomorrow
   * *Ticket Exchange*
      * GOC-TX server outage on Saturday - GOC investigating -- *no GGUS tickets were impacted*
      * GGUS - no updates, working properly
      * BNL - no updates, working properly - some more discussion between GOC Infrastructure and Jason
      * FNAL - Further tests using FNAL provided script, still no success to report yet
   * Slowdown of half of !ReSS service over the weekend between 6/11 and 6/14,  !ReSS service remained available due
       to high availability features.  We have updated auto-correction script which should prevent slowdowns like this in future.
   * !ReSS machines will have kernel update on 6/17/10, HA features will keep !ReSS up.
   * Gratia machines will delay kernel update pending new Gratia release forthcoming in a couple of weeks, which will partially
       address issues of the reporting database being behind the collector database during housekeeping.
      * Fermilab anticipates an 5-10 minute outage, but everything should recover itself, we will send an announcement


---++ Engage (Mats, John)

10 users utilized 30 sites

4118 jobs total (3601 / 517 = 87.4% success)

14863.2 wall clock hours total (10957.4 / 3905.8 = 73.7% success)

Slow week. Nothing to report.



---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)

   * D0
      * Monte carlo production ongoing at good rate; multiple D0-internal infrastructure problems last week; now resolved. 10 M Evts/week. 105,000 hours/day at 80% efficiency.
      * Site related issues:
         * UTA/ATLAS SE now added. 
         * Low efficiencies at Purdue RCAC and Florida.

   * CDF
      * Restarting work on the production expansion plan to use more OSG sites opportunistically. CDF needs SL5, along with additional packages. Already deployed at Fermilab resources. Looking for an arrangement with large SL5 sites (ATLAS, CMS) on OSG to possibly install these packages.
      * Initial package list: 
<verbatim>
            tcl.i386 tcl-devel.x86_64 tcl-devel.i386 
            compat-libf2c.x86_64 compat-libf2c.i386 
            perl-Crypt-SSLeay lapack.i386 lapack.x86_64 
            libXp-devel.i386 libXpm-devel.i386 libXi-devel.i386 
            libXp-devel.x86_64 libXpm-devel.x86_64 libXi-devel.x86_64 perl-DBI 
            valgrind libstdc++-devel 
</verbatim>

   * OSG-VO/CHARMM Group
      * Work ongoing is to expand CHARMM production from 3 sites to 23 sites; using !PanDA with active help from BNL team. 
      * In beginning of May, production was halted due to !PanDA server problems. Certificate issues brought down the central autopilot system. Maxim pointed out that BNL is tracking the problem at CERN and resolution is expected soon. <u>Note</u>: BNL !PanDA server is physically located at CERN.

   * GLUE-X
      * D0 trying to start using CE and SE at the GLUE-X site UConn-OSG. A few problems under investigation: UConn-OSG sites has NAT, and uses NFS-lite. Site configuration has led to blocking of !GlideinWMS jobs after a threshold rate. 
         * Investigating effects of local network security on other VOs job threshold: appears that vendor-supplied firewall/NAT blocking jobs when large groups of jobs are submitted together by a VO. Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8579
         * D0 jobs blocked; SSL negotiation issue during socket setup. Problem can be rooted in the proxy being double-limited due to NFS-lite configuration. Investigation ongoing. Ticket: https://ticket.grid.iu.edu/goc/viewer?id=8710

   * !IceCube
      * Deployment of VO glideinWMS front-end is complete. Dan Bradley and Igor providing assistance. Steve and Abhishek will further discuss the ramp-up plan of !IceCube in coming months.

   * SBGrid
      * Continuing work with !GlideinWMS. Running 2000-2500 jobs sustained over hours. Looking to go a little higher, but some large sites really busy right now; will continue to run and see how scalability continues.

---++ Security (Mine)