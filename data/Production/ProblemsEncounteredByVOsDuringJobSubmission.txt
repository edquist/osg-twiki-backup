%META:TOPICINFO{author="DanFraser" date="1275421675" format="1.1" reprev="1.10" version="1.10"}%
%META:TOPICPARENT{name="WebHome"}%
%TOC%

---++Purpose
This document is being used to create a centralized list of problems encountered by different VOs when they submit jobs to the OSG. The purpose of this document is to use this information in developing a strategic approach for tackling/testing for as many of these problems as is reasonable. A more broadly scoped list of problems directly reported by VOs is available  [[VirtualOrganizations.DirectFeedbackOnProductionProblemsOrBottlenecks][here]].

---++Text Color Interpretation
   * %BLUE% Blue text represents problems that are solved (or rendered irrelevant) with either OSGMM or a Pilot based system. Some tests will need to be added to the Pilot systems to bring equivalency. %ENDCOLOR%
   * %AQUA% Aqua text represents problems that are solved by OSGMM, but not by Pilots %ENDCOLOR%
   * %ORANGE% Orange text represents problems that are not solved by either OSGMM or Pilots %ENDCOLOR%
   * Regular black text represents "other" issues

---++ Problems encountered when selecting/targeting an optimal subset of sites
   1 %BLUE% Which information system and which mechanism to use for retrieving a list of all OSG sites? (nanoHUB, SBGrid) %ENDCOLOR%
   1 %BLUE% Which workflow management system (and which logical tests) to use for probing all sites? (nanoHUB, SBGrid) %ENDCOLOR%
   1 %BLUE% Which logic/system/mechanism to use for selecting a 'suitable targeted subset' of sites from all sites across OSG? (!CompBioGrid, nanoHUB, SBGrid, Glue-X) %ENDCOLOR%
   1 %BLUE% Which mechanisms to use for dynamically ranking all sites within this targeted subset of sites? (!CompBioGrid, nanoHUB, SBGrid, Glue-X) %ENDCOLOR%
   1 %ORANGE% Fermilab VO relies on !ReSS. Some of the information is not readily available via the GLUE Schema 1.3 and thus not via !ReSS. For instance, the GLUE schema only specifies the total memory which is available on a given worker node, but it gives no information if, for instance, the batch system will kill a job if it uses >900MB of memory. (Fermilab VO) %ENDCOLOR%
   1 %ORANGE% How to understand Preemption Policies of sites? Currently there is only a binary yes or no, with no way to tell how long a VO's job hasto finish, or which VOs get pre-empted, etc. There are some other fields not present in the GLUE schema, i.e. scratch disk space available per node, which Fermilab VO had to add as customized attributes. (SBGrid, Fermilab VO, nanoHUB) %ENDCOLOR%

---++ Problems encountered when accessing remote sites
   1 %BLUE% Do sites that advertise access to specific VOs actually allow that VO to run?
   1  Is GRAM working?
   1 Is the Grid-proxy expired?
   1 For users registered as members of more than one VO, is the access properly mapped at sites? (SBGrid, !NEBioGrid) %ENDCOLOR%

---++ Problems encountered when trying to deploy software (either on a WN or on a Gatekeeper)
   1 %AQUA% Is !GridFTP up and available to be used to stage-in scripts for building software stacks? (LIGO) %ENDCOLOR%
   1 %BLUE% Are outgoing connections available?
   1 Is $OSG_APP defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_APP mounted read-only on the worker nodes?
   1 Is $OSG_APP over quota? (Is the size reasonable, sometimes it is underestimated?) %ENDCOLOR%
   1 %AQUA% Are development tools available (configure, make, C & C++ compilers)? (LIGO)
   1 Are development tools available (configure, make, C & C++ compilers) only on the head node? %ENDCOLOR%

---++ Problems encountered when submitting a job
   1 %BLUE% Does the output location for the job exist and have the correct permissions? %ENDCOLOR%
   1 %AQUA% Is the output location over quota? %ENDCOLOR%
   1 %BLUE% Is !GridFTP available to stage the job?
   1 Is Condor-G working correctly on the submit host? (Is the configuration correct?)
   1 Is the jobmanager working?
   1 Is the Condor grid_monitor working? %ENDCOLOR%

---++ Problems encountered when running a job
   1 %BLUE% Is $OSG_DATA defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_DATA over quota? (again reasonable use is often underestimated)
   1 Is $OSG_WN_TMP defined, mounted, and provisioned, and does it have the correct permissions set?
   1 Is $OSG_WN_TMP over quota? %ENDCOLOR%
   1 %ORANGE% Is the worker node architecture compatible with the software that was built on the head node? %ENDCOLOR%
   1 %BLUE% Are outgoing connections being blocked by a firewall?
   1 Are hostnames being resolved? %ENDCOLOR%

---++ Problems encountered when managing persistent data (using SRM)
   1 %ORANGE% Which sites on OSG provide persistent opportunistic storage? (D0, SBGrid, !IceCube, Glue-X) %ENDCOLOR%

---++ Problems encountered when accounting usage and counting errors
   1 How to resolve anomalies in reporting of errors, and thus error in wasted wall hours, when using Pilot based job management? (GEANT4, SBGrid, and other VOs using Pilots)
   1 Is OSG Accounting error-proof? E.g., how to understand wall hours consumption of partially executed workflows or evicted jobs? (SBGrid)

---++ How are Job error messages handled?
   1 %BLUE% GridJobStatus: Done; Globus error 8 %ENDCOLOR%
   1 ...

---++ Strategic thoughts
   1 %BLUE% "The various discussions I had during the AHM and your presentation made me think that the pilot job should do more than just verify the OSG environment. I think the user should be able to configure the pilot in such a way that it verifies whatever requirements the user's application has. The pilots that are started on behalf of the user should also keep a record of errors which in turn could be used to create statistics. Both features are currently present in our E@OSG software." (Robert E., LIGO) %ENDCOLOR%

---++ OSGMM Site Verification Checks

Fatal / fork (head node)

   * $HOME disk usage over 30 GB
   * $OSG_APP not defined
   * $OSG_DATA not defined
   * $OSG_WN_TMP not defined

Advertised (non-fatal) / fork (head node)

   * Is $OSG_APP writable?
   * Is $OSG_DATA writable?
   * $GLOBUS_LOCATION path

Fatal / jobmanager (worker node)

   * $OSG_APP not defined
   * $OSG_DATA not defined
   * $OSG_WN_TMP not defined
   * $OSG_GRID not defined

Advertised (non-fatal) / jobmanager (worker node)

   * Is $OSG_APP/$VO_NAME/jobenv.sh sourceable?
   * Is $OSG_GRID/setup.sh sourceable?
   * GB disk free for $OSG_DATA
   * GB disk free for $OSG_APP
   * GB disk free for $OSG_WN_TMP
   * Is globus-url-copy installed?
   * Is wget installed?
   * Does the site allow outbound network traffic?
   * Is $OSG_APP writable?
   * Is $OSG_DATA writable?
   * Is $OSG_WN_TMP writable?
   * $GLOBUS_LOCATION path
   * CPU bitness
   * Number of cores on the node
   * Total memory
   * Memory per core
   * Is the site using pool accounts?
  
The above tests are the standard ones shipped with OSGMM. VOs can add their own. Using Engagement as an example, we use the non-fatal tests for advertising available software (installed by the OSGMM maintenance jobs), and run more functional tests such as verifying that globus-url-copy can copy a file, and by that testing if for example the CRLs are updated correctly on the worker nodes.

---++ Recommendations & Issues (for discussion)

   1 New VOs should if at all possible adopt one of the OSG models, either OSGMM, GlideinWMS, or PANDA for the job submission infrastructure.
   1 GlideinWMS and PANDA should incorporate all the checks currently being used by the site verification portion of OSGMM.
   1 The site verification script that is part of OSGMM should be generalized for use in glide-ins (and other OSG site verification systems) and kept as a separate package within VDT for VOs to use. (Is this a possibility?)
   1 Opportunistic storage remains a problem and will be tackled separately, although this may eventually require some new checks to be added to the site verification script. 
   1 One seeming problem with pilot jobs is that there is no mechanism for building software on the head node (where the development tools are usually found), since pilot jobs land on a worker node. (Is there a work around?) 
