%META:TOPICINFO{author="AbhishekSinghRana" date="1262731290" format="1.1" version="1.8"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.BrittaDaudert - 04 Jan 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * Xin has been monitoring the BDII problem with the new CE probe provided by Burt but did not see any anomalies. This seems to be a WLCG problem. This could be related to a CMS BDII problem where CERN BDII data was not getting updated. A ticket was filed with GGUS and worked on; the CERN BDIIs appear to be working now. Need to keep an eye on this and discuss strategies for monitoring. (Xin, Burt, GOC, Dan)
   * Engage needs the RESS RSV probe as soon as it is available. (Mats)
   * Burt to ping the CMS T2s for an SE request to help D0. 
   * Xin identified several T2s to help D0. Need to make sure everyone gets connected. (Xin)
   * 
---++ Attendees:
   * (to be updated after the meeting) Mats, Xin, Armen, Britta, Brian, Suchandra, Burt, Marco, Abhishek, Mine, Chander, Miron, Dan
 
---++ CMS (Burt)
   * CRL bug affected UCSD and Florida
      - Somehow some CRLs became zero-length on two different CMS sites.  CRL update then does not update those CRLs anymore.
   * LIGO jobs filled $OSG_DATA at Florida; LIGO user was informed.
   * Computing: 84 khour/day, 95% success. CPU/wallclock at 68%.
   * Storage: 276 TB xfer (T1), 42 TB (others)
   * OSG: Nearly all at 1.2 (only Rutgers and OSU @ 1.0)
   * Non-CMS/CDF preemption issue @ MIT_CMS: second suggested fix not yet implemented at MIT.
   * D0 opportunistic storage -- no recruits yet (could only make the tail end of the T2 mtg today).

---++ Atlas (Armen & Xin)

   * General production status
      * During the last two weeks USATLAS production was quite stable at the level of 8K running jobs. Reprocessing was finished by the end of 2009, as planned. BNL T1 processed ~50% of all jobs.  The reprocessing jobs are short in CPU but long in stage-in and stage-out, making cpu/walltime ratio lower, reflected in the following job stats. 
   * Job statistics for last two weeks. 
      * Gratia report: USATLAS ran 2.9M jobs, with CPU/Walltime ratio of 31 for the last week of 2009, and 81% for the first week of 2010. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 2M managed MC production, validation and reprocessing jobs 
         * average 291K jobs per day
         * failed 226K jobs
         * average efficiency:  jobs  - 89%,  walltime - 90%       
   * Data Transfer statistics for last week
      * Transfer rate stays the same as previous weeks. BNL T1 transferred ~75 TB/day data last week, with peak at 150 TB/day.  
   * Issues and GOC Tickets
      * GOC ticket 7772: Issues with WLCG BDII periodically loses information about some USATLAS Tier2 sites. Got the new ldap query from Burt, running it now on ganglia.
      * Opening more USATLAS T2 sites to D0 VO as opportunistic storage:  We will try to contact Joel from D0, figuring out details about configuration etc.

---++ LIGO (Britta)

   * Gratia reports:
   * Current week's total usage: 4 users utilized 30 sites;
      * 2424 jobs total (2372 / 52 = 97.9% success);
      * 30.4 wall clock hours total (25.7 / 4.7 = 84.5% success);
   * Previous week's total usage: 4 users utilized 33 sites;
      * 2641 jobs total (2571 / 70 = 97.3% success);
      * 87.8 wall clock hours total (64.7 / 23.1 = 73.7% success);
 
   * E@H reports
      * Recent Average Credit (RAC): 170,585.60004
      * E@H rank based on RAC: 10 (-7)
      * E@H rank based on accumulated Credits: 11 (+0)
   
   * Robert is working on code changes required to expand to Fermilab sites and Sprace

 
---+++ Binary Inspiral
    * 3 day test work-flow on Firefly: Gap in data error

---++ Engage (John, Chris, Mats)


---++ Integration (Suchandra)
   * OSG 1.2.5 release delayed due to last minute issues
   * Considering making an interim release of xrootd/bestman components for ATLAS
  
 
---++ Site Coordination (Marco)
Note that this report lists the currently active resources in OSG.
If a site is down or not reporting it will not be counted.
Therefore there may be fluctuations.
   * Site update status (from !MyOSG as of today):
      * Most recent production version is OSG 1.2.4
      *       58 OSG 1.2.X resources (      23 are 1.2.4)
      *       10 OSG 1.0.X resources (       0 are 1.0.5)
      *       19 OSG 1.0.0 resources
      *        2 OSG 0.8.0 resources
         * OU_OCHEP_SWT2, tier2-01.ochep.ou.edu , Contact: Horst Severini
         * UIC_PHYSICS mstr1.cluster.phy.uic.edu , Contact: John Wolosuk

---++ Metrics (Brian)

---++ Grid Operations Center (Rob Q.)


---++ Virtual Organizations Group (Abhishek)

---+++ VOs with High Activity

   * D0 MC reported good production in mid-Dec, followed by a drop. 
      * Workload very low in past 2 weeks; 3 M, 0.5 M Evts/week.
      * Nearly 11.4 M Evts in mid of Dec'09; was new 6-month peak; OSG view was 9 M Evts; discrepancy was resolved.
      * MIT situation looks better, as reported by D0 in late Dec'09.
      * [Carried over items:
         * ATLAS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at ATLAS T2 sites. 
            * Currently 2 SEs: MSU, MWT2-IU.
         * CMS: CE's rate of preemption. 
            * Possibly, MIT T2 applied the fix to CE. (Burt may have more accurate status).
            * Dan Bradley's solution: use <u> !LastHeardFrom </u> instead of the more popular <u> !CurrentTime </u>.
            * https://ticket.grid.iu.edu/goc/viewer?id=7814
         * CMS: Need for more SEs.
            * D0 can benefit from more opportunistic SEs at CMS T2 sites. 
            * Currently 3 SEs: Purdue, UCSD, UNL]
           
   * SBGrid/NEBioGrid 
      * Meeting and plan discussed on Dec 16 '09.
      * Immediate goal: 
         * To increase job efficiency at moderate job volume. Then, increase job volume. 
         * Target sustained peak 3000 jobs running simultaneously. Increase to 6000 later. Target 1-2 times jobs queued as running. 
      * Full details: https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/SBGrid_NEBioGrid_OSG  
      * Immediate issue: Possibly, SBGrid submit infrastructure is co-located on same hardware as CE/Gatekeeper headnode. 
      * Current status:
         * ~200 simultaneous jobs. 
         * Working with Mats to resolve OSG-MM issues.
         * New version of MM v0.8 is now being installed, rank calculation to be tweaked.
      
   * GEANT4
      * Biannual EGEE-based exercise ran on OSG in Dec'09. 
      * Scale was low; more analysis and report later this month. 
      * Problem: Discrepancy in wasted wall hours due to Pilots.
         * 50% in OSG resource-view, 0% (full success) in Geant4-view.
         * Reason likely to be OSG-side issue: Exit-code discrepancy due to Pilots.

   * CDF 
      * Successfully upgraded new portal to !GlideinWMS; working well.

   * Fermi-VO
      * Communication channel in Dec'09 site upgrade; not discussed beforehand.
   
---+++ VOs with Limited Activity
      
   * !GridUNESP / DOSAR
      * Brought up !GridUNESP as community grid VO.
      * Infrastructure configured with OSG software stack (site side, then VO side).
      * Charter for !GridUNESP made public.  
      * Experiences: https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/DOSAR_GridUNESP_OSG
      * First !GridUNESP researchers submitted MPI jobs through full infrastructure, running on OSG.

         
   * !IceCube
      * Proof of principle completed in Oct'09. Limited data-access model.
      * Total usage across 6 sites; 4,000 wall hours; 600 jobs at 50% efficiency.
      * Progress in Oct-Dec'09 slow; !IceCube team was on travel to South Pole.
      * Work restarted in Dec'09 to integrate HTTP cache / Squid for data staging.
 
---++ Security (Mine)
1. Root cause of the problem: how ucsd ended up with empty crl?
when fetch-crl script fetches a new CRL, it first saves the file in tmp,
verifies it (check validity/hash/etc), and then replaces the old CRL on
disk. We think the move 
÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷÷somehow was corrupted due to local OS or
hardware or automount... Terrence needs to tell us more on this.

How to remedy: once the move is completed, fetch-crl can do one more
verification of the new crl. if the mv did not succeed, it could repeat
it or roll back to the old crl

2. Once a corrupt crl happens on disk, why fetch-crl quits
This was a design choice controlled by a variable in fetch-crl. The
default value of the variable was not to overwrite a corrupt CRL file on
disk.

How to remedy:  We will change the variable's default behavior to
overwrite corrupt file

3. Report the error:
fetch-crl only sends error emails to the root.

how to remedy: We will change this such that email can go to sys admin
and also get logged. There is no log file currently.

4. Discovering the error situation.
RSV probes especially security test probes do not run on WN.  How come a
site admin would discover something is wrong on the CE?

how to remedy: This needs broader discussion. Frank will initiate a new
twiki page to document what to monitor on a WN. The monitoring can
include items broader than security.

5. Direct monitoring of CAs
GOC runs a set of security probes that directly query the CAs. Security
team checks the probe results and contact CAs if they see a failure.

Nothing to improve. This case was a local issue. CA service was
functioning properly.

6. Should we distribute CRL as we distribute CA certs from GOC

This would improve the situation some,  but it would not solve this
particular problem. Instead of going to each CA's web server, sites
would get their CRLs directly from a single point. If a CRL is expired
or CA is unreachable, GOC cannot correct the situation either.

How to remedy: I do not see this as urgent because we are monitoring CA
services at GOC. We can do this later.