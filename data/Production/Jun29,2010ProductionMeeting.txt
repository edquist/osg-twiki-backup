%META:TOPICINFO{author="DanFraser" date="1277838773" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 07 Jun 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan
 
---++ CMS (Burt)


---++ Atlas (Armen & Xin)

   * General production status
      * 
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.6M jobs, with CPU/Walltime ratio of 67%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 308k managed MC production, validation and reprocessing jobs 
         * average 44K jobs per day
         * failed 24K jobs
         * average efficiency:  jobs  - 93%,  walltime - 94%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was ~400TB/day, low at ~100TB/day over the weekend.   
   * Issues
      * Opportunistic CE usage for CDF : will set a testbed internally, with all extra rpms installed, to verify they don't break other VO (ATLAS) jobs. 
      * SAM test issue --- sam bdii under investigation
      * Publish SE only info to BDII --- quick release of GIP from GIP group, CEMon still needs to be done. 

---++ LIGO (Britta, Robert E.)
---+++ Gratia Reports
   * This week's total usage: 3 users utilized 38 sites
      * 59452 jobs total (28596 / 30856 = 48.1% success)
      * 627850.3 wall clock hours total (436354.5 / 191495.8 = 69.5% success)
   * Last week's total usage: 5 users utilized 39 sites
      * 61465 jobs total (22162 / 39303 = 36.1% success)
      * 579260.7 wall clock hours total (318382.1 / 260878.6 = 55.0% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,224,179.24924, Last week: 924,1,157,951.14
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)

---+++LIGO/INSPIRAL
   * Bug fix tested, waiting for approval to push
   * Trouble shooting USCMS-FNAL-WC1-CE3 fail

---++ Grid Operations Center (Rob Q.)


---++ Engage (Mats, John)


---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)


---++ Security (Mine)
   * Pakiti server is ready and installed from the source code. Anand is working with Rob to ensure it meets our expectations.
   * GUMS 1.3 is built and tested from teh source code and works correctly. Now VDT is going through the same experiment of building from the source with our notes
   * CDF has asked to use pakiti server in order to discover existing RPMs in a cluster. There is no security needs here. They merely want to know which sources they can use to submit jobs.
   * Question: Pakiti cannot ensure the queries it sends lands on each one of the worker nodes. Is there a way to configure condor to send jobs in a round-robin fashion so each worker node gets queried at least once? If no technical solution is possible, can we at least define what is expected to be in a standard SL5 worker node? is there is a consensus on this, it will be easier to identify suitable sites.

   * there was a problem with CA distribution last week. WE will have a meeting dedicated to this today.

   * There is a security problem with ROCKS distribution. Ticket is assigned to Barlow from security team.

   * Security team met with Atlas tier 3 site (Susquehanna university) and their site security teams. The layout has been approved and there is no objections from the university.
