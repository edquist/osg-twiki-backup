%META:TOPICINFO{author="XinZhao" date="1277839964" format="1.1" version="1.6"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 07 Jun 2010
---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings

---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Mats, Xin, Armen, Britta, Rob E., Brian, Suchandra, Burt, Marco, Abhishek, Rob Q., Mine, Chander, Miron, Dan
 
---++ CMS (Burt)


---++ Atlas (Armen & Xin)

   * General production status
      * LHC is back online with improved proton intensity per bunches. Since weekend managed to double the existing integrated luminosity to about 40nb-1. Overall ATLAS production was up and down during the week and was generally low. Still waiting for new simulation samples.
   * Job statistics for last week. 
      * Gratia report: USATLAS ran 2.6M jobs, with CPU/Walltime ratio of 67%. 
      * Panda world-wide production report (real jobs): 
         * completed successfully 308k managed MC production, validation and reprocessing jobs 
         * average 44K jobs per day
         * failed 24K jobs
         * average efficiency:  jobs  - 93%,  walltime - 94%
   * Data Transfer statistics for last week
      * BNL T1 data transfer rate last week was ~400TB/day, low at ~100TB/day over the weekend.   
   * Issues
      * Opportunistic CE usage for CDF : will set a testbed internally, with all extra rpms installed, to verify they don't break other VO (ATLAS) jobs. 
      * SAM test issue --- sam bdii under investigation
      * Publish SE only info to BDII --- quick release of GIP from GIP group, CEMon still needs to be done. 

---++ LIGO (Britta, Robert E.)
---+++ Gratia Reports
   * This week's total usage: 3 users utilized 38 sites
      * 59452 jobs total (28596 / 30856 = 48.1% success)
      * 627850.3 wall clock hours total (436354.5 / 191495.8 = 69.5% success)
   * Last week's total usage: 5 users utilized 39 sites
      * 61465 jobs total (22162 / 39303 = 36.1% success)
      * 579260.7 wall clock hours total (318382.1 / 260878.6 = 55.0% success)
---+++ LIGO / E@OSG
   * Recent Average Credit (RAC): 1,224,179.24924, Last week: 924,1,157,951.14
   * E@H rank based on RAC: 2 (+-0)
   * E@H rank based on accumulated Credits: 4 (+-0)

---+++LIGO/INSPIRAL
   * Bug fix tested, waiting for approval to push
   * Trouble shooting USCMS-FNAL-WC1-CE3 fail

---++ Grid Operations Center (Rob Q.)

---+++ Operations Last Week 
      * [[http://tinyurl.com/2cpq82y][Reliability/Availability of GOC Services]]
      * [[http://tinyurl.com/28gecm2][Reliability/Availability of Security Services]]
   * *Production Update Tuesday, June 23rd*
      * [[http://osggoc.blogspot.com/2010/06/goc-service-update-tuesday-june-22nd-at.html][Announcement]]
   * *GGUS Production Update*
      * The [[https://gus.fzk.de/][GGUS ticketing system]] will be down Wednesday, June 23rd 2010 from 06:00 - 09:00 UTC for a new GGUS portal release.
         * All ALARM Testing was successful. 
   * CA Distribution Outage Friday for ~30 minutes
      * Due to errors in the CA Package (1.15) released to the GOC
      * When no immediate solution was presented we rolled back to the previous good version (1.13) 

---+++ Operations This Week

   * No Scheduled Release this week as it is a 5th Tuesday 
   * [[http://myosg.grid.iu.edu/map?all_sites=on&active=on&active_value=1&disable_value=1&gridtype=on&gridtype_1=on][<strong>Operations RSV Status Map</strong>]]
   * *Ticket Exchange*
      * FNAL - No Action This Week as Soichi is on Vacation
   * Investigating ATLAS resources disappearing from BDII
      * [[https://gus.fzk.de/ws/ticket_info.php?ticket=59188][GGUS Ticket]]
      * Initial Testing of the SAM-BDII vs the Top Level WLCG BDII
         * # of Tests - 56315
         * # of Failures Top Level BDII - 74 (0.13%)
         * # of Failures SAM BDII - 219 (0.38%)
   * SE Only Publishing is in the hands of the Software Tools Group and ATLAS for testing
      * GOC Change in BDII that would allow SE publishing is currently set for July 6th

---++++ Gratia and !ReSS (Represented by Fermigrid Ops)

   * A Gratia software upgrade is in the works, an announcement will precede it. This will be more than a week out from today. 


---++ Engage (Mats, John)


---++ Integration (Suchandra)


---++ Site Coordination (Marco)


---++ Metrics (Brian)


---++ Virtual Organizations Group (Abhishek)


---++ Security (Mine)
   * Pakiti server is ready and installed from the source code. Anand is working with Rob to ensure it meets our expectations.
   * GUMS 1.3 is built and tested from teh source code and works correctly. Now VDT is going through the same experiment of building from the source with our notes
   * CDF has asked to use pakiti server in order to discover existing RPMs in a cluster. There is no security needs here. They merely want to know which sources they can use to submit jobs.
   * Question: Pakiti cannot ensure the queries it sends lands on each one of the worker nodes. Is there a way to configure condor to send jobs in a round-robin fashion so each worker node gets queried at least once? If no technical solution is possible, can we at least define what is expected to be in a standard SL5 worker node? is there is a consensus on this, it will be easier to identify suitable sites.

   * there was a problem with CA distribution last week. WE will have a meeting dedicated to this today.

   * There is a security problem with ROCKS distribution. Ticket is assigned to Barlow from security team.

   * Security team met with Atlas tier 3 site (Susquehanna university) and their site security teams. The layout has been approved and there is no objections from the university.