%META:TOPICINFO{author="RobQ" date="1242158145" format="1.1" reprev="1.11" version="1.11"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 05 May 2009
---++ Action Items
   * What should OSG be preparing for the STEP09 drill coming later this month?
---++ Attendees (to be updated after meeting):
   * Xin, Britta, Mats, Brian, Suchandra, Burt, Abhishek, Rob Q., Mine, Chander, Miron, Dan
---++ CMS (Burt)
   * We ran only 95k hours/day.  This is a big decrease from last week.
      * Gap in workflow requests
      * We are actively solving with bottlenecks in the CMS data registration system
   * Job efficiency: 94%.  CPU/wallclock efficiency was 71%.
   * LIGO @ UNL followup: we provided data back to LIGO on the circumstances regarding the death spiral at Nebraska's CE.  Rob Engel identified one issue with the interaction between LIGO's submitter and GRAM status updates that could lead to overload.  Solution is not yet understood and I have not yet had time to correlate his findings with the UNL gatekeeper logs.
   * STEP09 is coming: this is a serious of functional tests to get WLCG as close to data-taking as possible
      * We will be ramping up analysis using glide-in technology on the OSG Tier 2s.  This depends on stability of CEs, SEs, and BDII.

---++ Atlas (Armen & Xin)

   * job statistics for last week. 
      * Last week there were ~4000 running jobs all the time on USATLAS sites, lower than the normal number of ~6000. Because of the site issues mentioned below. 
      * Gratia report: USATLAS ran 880K jobs, with CPU/Walltime ratio of 80.3%. 
      * PanDA world-wide production report (real jobs):
         * completed successfully 478,682 managed MC production, validation and reprocessing jobs
         * average  ~68,383 jobs per day
         * failed   123,608  jobs
         * average efficiency: 79.5% for jobs and 94.9% for walltime
   * Site issues
         * Several T2 sites remained down last week, for upgrading to newer versions of DDM software (DQ2), SRM server and dCache. 
   * STEP09 
         * Purpose: try to create an as real a data taking condition as possible, with all parts of job processing and data movements system involved, including production, reconstruction, user analysis. All steps will involve tape staging. T0/T1/T2 sites will also participate. 
         * All LHC VOs will do the exercise together, to test resource sharing. 
         * Schedule : Last week of May is setup step, first two weeks of June will be running time.    
   * Condor-G 
         * After adding more sites to condor-g submission, slow sites with slow WAN slows down the whole condor-g grid manager, making BNL T1 sites only 50% utilized. 
         * Condor team plans to improve this by starting a new grid manager for each site.  
 
---++ LIGO (Britta)

   * Current week's total usage: 2 users utilized 19 sites
      * 18362 jobs total (17066 / 1296 = 92.9% success);
      * 52669.9 wall clock hours total (47203.2 / 5466.6 = 89.6% success);

   * Previous week's total usage: 3 users utilized 17 sites
      * 10050 jobs total (5293 / 4757 = 52.7% success);
      * 42749.0 wall clock hours total (37539.6 / 5209.4 = 87.8% success);

   * E@H stats

      * Recent Average Credit (RAC): 111,384.16742 (+30,000)
      * E@H rank based on RAC: 9 (-2)
      * E@H rank based on accumulated Credits: 38 (+1)
   
   * Job success rate up after removal of CIT_CMS_T2, SPRACE, MIT_CMS from submit list (CMS eviction)

   * Reduced job submissions at all sites (high load on GK with GT2 submissions) 

   * 05/07, 05/08 not running at

      * ANTAEUS (high load)
      * CIT_CMS_T2, SPRACE, MIT_CMS (CMS eviction)
      * AGLT2, OUHEP_OSG (can't authenticate - GOC open/closed-- reduce load limit to 2)
      * UmissHEP,  NYSGRID_CORNELL_NYS1 (down)

   * 05/11 (-- ?)
      * LIGO submit host down 

---++ Site Coordination, Integration (Suchandra)
---+++ Integration
   * Planning still in progress for next cycle
   * Some dCache testing is currently being run by Iwona and Suchandra, will give results/feedback to storage group
   * The VTB and ITB ticketing will probably move to using the footprints system at the goc.
---+++ Sites
   * Marco will be surveying sites to get information on OSG 1.0.1 adoption and their plans to upgrade
 
---++ Engagement (Mats)


------------------------------------------------------------------------
     | VO             | # of Jobs | Wall Dur. | Cpu / Wall |      Delta
------------------------------------------------------------------------
  8  | engage         |     6,923 |    38,832 |       74.3 |         15


Production problem: Strong thunderstorms brought down RENCI's machine
room on Saturday, and broke the main UPS. Many of Engagement's
infrastructure machines went offline. This included VOMRS/VOMS,
engage-central (Engagement's top-level OSGMM/ReSS host, which does
the site maintenance and verification), and our main submit host
(engage-submit.renci.org). Systems are now back, but we think a scratch
file system is not recoverable, so some outputs were lost.


---++ Metrics (Brian)
   * We now have a weekly "installed capacity" report set up, done by Karthik.  Waiting on OIMv2 for some of the inputs for the report.
      * We don't have the transfer of installed capacity OSG->WLCG finalized; perhaps need to work with GOC on this?
   * New "Live Display" of incoming Gratia records: http://gratia.fnal.gov/Files/osg_gratia_display/today/live_display.png
   * Up to 14 sites are now reporting Gratia Transfer records; an alpha version of a Hadoop probe is running at Nebraska (developed by a local grad student).
      * I'd really like to see this working again at BNL; it stopped a few weeks ago.

---++ Virtual Organizations Group (Abhishek)

Issues affecting multiple VOs --

D0 --

Alice --

!CompBioGrid --

!IceCube --

!NanoHub --

GLOW --

!CalTech --

UCSD -- 

---++ Grid Operations Center (Rob Q.)

   * BDII Issues from Friday (Intermittent Connectivity 15:30 EDT to ~19:00 EDT) [[http://osggoc.blogspot.com/2009/05/bdii-instability.html][Notification]]
   * BDII Maintenance Tomorrow and Thursday [[http://osggoc.blogspot.com/2009/05/osg-bdii-maintenance-wednesday-may-13.html][Wed Maintenance Notification]], [[http://osggoc.blogspot.com/2009/05/osg-bdii-maintenance-thursday-may-14.html][Thursday Maintenance]]
   * Trouble Ticket Exchange Issues with FNAL [[https://twiki.grid.iu.edu/bin/view/Operations/TTExchange2009May12][Issues Report from Elizabeth]]
   * OIMv2 Feedback is rolling in...
      * Installed Capacity - Ken Bloom has played in OIM but not made it here yet, no word from Rob Gardner
   * SLA for BDII [[https://twiki.grid.iu.edu/bin/view/Operations/BDIIServiceLevelAgreement][(Available Here)]] has been approved by Frank and Burt at CMS, is being sent on the Ian Fisk for final CMS sign off

---++ Security (Mine)
