%META:TOPICINFO{author="SarahCushing" date="1272299975" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.SarahCushing - 26 Apr 2010

%TABLE{ tablewidth="700" columnwidths="5%, 15%, 80%, 10%, 10%" cellpadding="2" dataalign="left" tablerules="all" tableborder="2" databg="#FFFFFF, #FFFFFF"}%
| # | Owner | Action/Significant Item | Open Date | Close Date |
|1| Xin | The wide variance in data transfers at Atlas T1 (Hundreds of TB/day to near zero on some days) appears to be due to actual load variances and not a problem in Gratia. | 4/6/2010 | |
|2| Abhishek | DZero now running at near peak capacity (12M Events) and not hitting any production limits. Their thank you to the CMS T1 folks for increasing the number of batch slots available. | 4/6/2010 | |
|3| Abhishek | SBGRID hit a new peak of 70K hours/day. | 4/6/2010 | |
|4| Abhishek | NanoHub?<https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting> reported a bug in DagMan?<https://twiki.grid.iu.edu/bin/edit/Production/DagMan?topicparent=Production.Apr6,2010ProductionMeeting> after upgrade to Condor 7.4.1. A patch has been issued that NanoHub?<https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting> is using. | 4/6/2010 | |
|5| Mine, Brian | A known bug (feature?) in Apache was re-discovered this week where Apache must be restarted in order to pick up new certs. Currently there is no work around available. | 4/6/2010 | |
|6| All | Burt is out with Jury Duty on Tuesdays for the next few months but agreed to find a replacement person to represent CMS Grid services until he is back. | 4/13/2010 | |
|7| Xin | Atlas data rates were consistent between Ganglia and Gratia and verifyied the increase to over 400 TB/day a few days ago. There is still some concern about the rates dropping to near zero about 12 days ago. Will continue to watch data transfer rates closely at BNL and cross-compare any anomalies between Ganglia and Gratia. | 4/13/2010 | |
|8| Rob | The GOC will transition to the Web Services mechanism for ticket transfers with RT (BNL) tomorrow at 10am  | 4/13/2010 | |
|9| Rob | CMS Tier-3 tickets are now being routed through the newly setup CMS T3 support center.  | 4/13/2010 | |
|10| All |  Three sites made the transition from OSG 1.0 to 1.2 this week. | 4/13/2010 | |
|11| Xin, Brian | The CERN BDIIs exceeded an internal 5MB limit and info became unavailable when BNL-Atlas added two additional CEs to its resource group. The problem was restored when the CEs were pulled back. Brian is working with Atlas to upgrade to the multi-CE configuration but this requires a GIP upgrade as well. Tests are underway to temporarily upgrade the Gratia binaries since a full upgrade will need to wait until Atlas downtime. This should reduce the data to less than 3MB, even with the new CEs. | 4/20/2010 | |
|12| Rob | CERN BDII operations are not currently considered as a critical service although both CMS and Atlas consider their dependency on BDII to be critical, with CERN requiring 30 minute response time. This is being addressed by WLCG management. | 4/20/2010 | |
|13| All | There was a Gratia outage for over 20 hours. Brian discovered this by watching the DOE display. There were no reports from the Gratia team. outage was restored when the backup Gratia DB was switched to the active one. Need to have a post-mortem of this outage and importantly the communications channel. The DOE display handled this outage correctly and gracefully as designed. | 4/20/2010 | |
