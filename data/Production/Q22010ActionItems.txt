%META:TOPICINFO{author="SarahCushing" date="1274111293" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.SarahCushing - 26 Apr 2010

%TABLE{ tablewidth="700" columnwidths="5%, 15%, 80%, 10%, 10%" cellpadding="2" dataalign="left" tablerules="all" tableborder="2" databg="#FFFFFF, #FFFFFF"}%
| # | Owner | Action/Significant Item | Open Date | Close Date |
|1| Xin | The wide variance in data transfers at Atlas T1 (Hundreds of TB/day to near zero on some days) appears to be due to actual load variances and not a problem in Gratia. | 4/6/2010 | |
|2| Abhishek | DZero now running at near peak capacity (12M Events) and not hitting any production limits. Their thank you to the CMS T1 folks for increasing the number of batch slots available. | 4/6/2010 | |
|3| Abhishek | SBGRID hit a new peak of 70K hours/day. | 4/6/2010 | |
|4| Abhishek | NanoHub?<https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting> reported a bug in DagMan?<https://twiki.grid.iu.edu/bin/edit/Production/DagMan?topicparent=Production.Apr6,2010ProductionMeeting> after upgrade to Condor 7.4.1. A patch has been issued that NanoHub?<https://twiki.grid.iu.edu/bin/edit/Production/NanoHub?topicparent=Production.Apr6,2010ProductionMeeting> is using. | 4/6/2010 | |
|5| Mine, Brian | A known bug (feature?) in Apache was re-discovered this week where Apache must be restarted in order to pick up new certs. Currently there is no work around available. | 4/6/2010 | |
|6| All | Burt is out with Jury Duty on Tuesdays for the next few months but agreed to find a replacement person to represent CMS Grid services until he is back. | 4/13/2010 | |
|7| Xin | Atlas data rates were consistent between Ganglia and Gratia and verifyied the increase to over 400 TB/day a few days ago. There is still some concern about the rates dropping to near zero about 12 days ago. Will continue to watch data transfer rates closely at BNL and cross-compare any anomalies between Ganglia and Gratia. | 4/13/2010 | |
|8| Rob | The GOC will transition to the Web Services mechanism for ticket transfers with RT (BNL) tomorrow at 10am  | 4/13/2010 | |
|9| Rob | CMS Tier-3 tickets are now being routed through the newly setup CMS T3 support center.  | 4/13/2010 | |
|10| All |  Three sites made the transition from OSG 1.0 to 1.2 this week. | 4/13/2010 | |
|11| Xin, Brian | The CERN BDIIs exceeded an internal 5MB limit and info became unavailable when BNL-Atlas added two additional CEs to its resource group. The problem was restored when the CEs were pulled back. Brian is working with Atlas to upgrade to the multi-CE configuration but this requires a GIP upgrade as well. Tests are underway to temporarily upgrade the Gratia binaries since a full upgrade will need to wait until Atlas downtime. This should reduce the data to less than 3MB, even with the new CEs. | 4/20/2010 | |
|12| Rob | CERN BDII operations are not currently considered as a critical service although both CMS and Atlas consider their dependency on BDII to be critical, with CERN requiring 30 minute response time. This is being addressed by WLCG management. | 4/20/2010 | |
|13| All | There was a Gratia outage for over 20 hours. Brian discovered this by watching the DOE display. There were no reports from the Gratia team. outage was restored when the backup Gratia DB was switched to the active one. Need to have a post-mortem of this outage and importantly the communications channel. The DOE display handled this outage correctly and gracefully as designed. | 4/20/2010 | |
|14| Dan | One of the ReSS?<https://twiki.grid.iu.edu/bin/edit/Production/ReSS?topicparent=Production.Apr27,2010ProductionMeeting> servers failed this week, but properly failed over, so there was no downtime. Causes are being investigated. | 4/27/2010 | |
|15| Brian, Rob | LIGO was asked to reduce pressure on NFS servers at Nebraska. Rob E mentioned that he will run a different code version on Nebraska as well as the Fermi T1. | 4/27/2010 | |
|16| Brian, Tony | SBGRID jobs were causing trouble on the Fermi T1 and at Nebraska and were temporarily banned on both sites. | 4/27/2010 | |
|17| Rob, Tony | LIGO also making sure that boinc no longer pings Google when running E@H.<mailto:E@H.> This was causing problems on the Fermi T1  | 4/27/2010 | |
|18| Dan, Tony | Data transfer data from Fermi T1 is not showing on Gratia chart for past 3-4 days. Dan asked Tony to take a look. | 4/27/2010 | |
|19| Marco | Three more sites upgraded to OSG 1.2.x. | 4/27/2010 | |
|20| Abhishek | New peak in DZero jobs this week, 13.3 Million Events, 120K hours last week | 5/4/2010 | |
|21| Dan | Root Cause Analysis of Gratia problem on April 20 affecting the OSG Display is ongoing. Main problem is with a known problem in mySQL | 5/4/2010 | |
|22| Abhishek, Dan | SBGRID overloading issues at FNAL and UNL have been resolved, jobs running normally again | 5/4/2010 | |
|23| Rob, Burt | There was a BDII outage (caused by a CEMON collector failure) that affected the visibility of the CMS T1 and other sites last Tuesday evening. After hours response from the GOC team was excellent. Primary causes are still being investigated although it looks to have been caused by a bug in NSCD. Also exploring monitoring requirements to detect future failures in the BDII end-to-end architecture. The failed BDII node has temporarily been taken out of the system for diagnostics. More details below in Burt and Rob's reports. | 5/11/2010 | | 
|24| Dan, Rob, Brian |  A draft report from the Root Cause Analysis of the Gratia problem detected by the OSG Display on Apr 20 is being circulated and iterated on. Plan to have a final report available soon. | 5/11/2010 | |
|25| Brian | Dan noted that error rates in the number of CMS wall clock hours seem abnormally high (sometimes ~50%). Brian noted that it is difficult to determine if this represents a real problem, since pilot jobs increase the difficulty in tracking errors. Brian investigating. | 5/11/2010 | |