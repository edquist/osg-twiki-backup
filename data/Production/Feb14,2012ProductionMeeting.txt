%META:TOPICINFO{author="ArmenVartapetian" date="1329251448" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
---++ Action/Significant Items:
   * 

---++ Attendees:
   * (to be updated after the meeting) Alain, Mats, Xin, Armen, Britta, Brian, Suchandra, Tony, Marco, Rob Q., Scott T., Mine, Chander, Dan
 
---++ CMS (Burt / Tony)


---++ Atlas (Armen & Xin)

   * General production status
      * LHC decision to go to 4TeV/beam this year, bunch spacing stays same. Planned luminosity grow 3 times, expected 15fb-1 per experiment for this year (last year was 5fb-1 collected). Also during the year carry out testing and preparation for the much higher energy, higher luminosity, shorter bunch spacing for the after long shutdown run.
      * US ATLAS production during the week was not very stable, at the average level of ~8K running jobs, mainly simulation type. The reason is that the bulk production which was keeping the production at high and stable level during the last months is almost done. Still some new requests are arriving. Final validation of MC12 production with 4TeV/beam is almost done. That production will re-simulate all the MC data, will start in 1-2 weeks.
   * Job statistics for last week.      
      * Gratia report: 2.6M pilot jobs run on USATLAS sites, with CPU/walltime ratio of 82%  
      * Real Jobs processed by US sites for last week, reported from PanDA monitor 
         * 920K
   * Data Transfer statistics for last week
      * Data transfer rate was 200~350TB/day at BNL T1 in last week.  
   * Issues

---++ LIGO (Britta)
  * Current week's total usage: 2 users utilized 14 sites
      * 21299 jobs total (7474 / 13825 = 35.1% success)
      * 29612.1 wall clock hours total (21498.3 / 8113.8 = 72.6% success)
   * Previous week's total usage: 2 users utilized 14 sites
      * 93261 jobs total (27665 / 65596 = 29.7% success
      * 128256.9 wall clock hours total (95141.0 / 33115.9 = 74.2% success)
---+++ LIGO/PULSAR
   * Monitoring 50,000 job dag submitted to local glidein factory, running over 13 OSG sites
   * Shadow exceptions/disconnects: 
        * Removed all Nebraska sites
        * All remaining jobs sitting in queue in state I 

---++ Grid Operations Center (Rob Q.)

---+++ Announcements
   * [[https://twiki.grid.iu.edu/bin/view/Security/CAReleaseStreamlinePP][software.grid CADist deprication plan]]
   * [[http://osggoc.blogspot.com/2012/02/ticket-exchange-attachment-interface.html][Ticket attachment exchange issues continue]]. Affects GGUS<-->FP attachment exchanges
   * Software team, remaining FNAL VOs change over to ticket exchange this week.

---+++ Operations Last Week
   * [[http://tinyurl.com/42j3mgl][GOC Services Availability/Reliability]]
   * [[http://myosg.grid.iu.edu/miscstatus/index?datasource=status&count_sg_1=on&count_active=on&count_enabled=on][Current Status]]
   * ITB release, OS updates.
   * !FermiGrid Ops
      * Rebooted services for kernel upgrades last week as scheduled
   * WMS Glide In Factory
      * registered new XSEDE frontend (Mats Rynge, OSG) (2/1/12)
      * registered new ATLAS production frontend (Rodney Walker, LMU Munich) (2/1/12)
      * registered glow frontend to use GOC factory

---+++ Operations This Week
   * Production release [[http://osggoc.blogspot.com/2012/02/goc-service-update-tuesday-january-24th.html][Release note]]
      * 8 hour window
      * OS updates, note non-standard week. Reboots will be required including glidein.
      * New CAdist release, 1.26
      * Changes to the !DoE cert-renewal page: [[https://software.grid.iu.edu/cert/certrenew.php][old]], [[https://software-itb.grid.iu.edu/cert/certrenew.php][new]]
   * Administrative changes to opensciencegrid.org have been made, DNS change from FNAL to IU will be done on Feb 28.

---++ Campus Grids / HTPC (Dan)


---++ Engage (Mats)


---++ Integration (Suchandra, Alain)


---++ Site Coordination (Marco)


---++ User Support (Chander)

Last week, we (with help from Production) assisted GLOW in adding some sites to reach higher production levels; in addition, the glidein factory identified and resolved a couple of issue affecting this production.  As of late last week, GLOW was running about 60K hours per day and servicing their work queues effectively.  See http://gratiaweb.grid.iu.edu/gratia/vo?vo=glow  .  GLOW identified some items affecting the movement of data with their jobs; halted production briefly;  and resolved those issues and is back to production.    

As part of this we identified the need to have pro-active monitoring of production in OSG enabled by all the glideinWMS front-end.  We will start to identify the needed reports and graphs  (with the glideinWMS project) and put processes in-place to pro-actively detect production issues and offer help to VOs.


---++ Security (Mine)

---++ The full report with links is available at https://twiki.grid.iu.edu/bin/view/Production/WeeklyProductionMeetings



-- Main.DanFraser - 08 Feb 2012
