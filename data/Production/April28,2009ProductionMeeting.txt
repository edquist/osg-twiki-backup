%META:TOPICINFO{author="DanFraser" date="1241021832" format="1.1" reprev="1.3" version="1.3"}%
%META:TOPICPARENT{name="WeeklyProductionMeetings"}%
-- Main.DanFraser - 28 Apr 2009
---++ Attendees:
   * Xin, Britta, Mats, Brian, Suchandra, Burt, Abhishek, Rob Q., Mine, Chander, Miron, Dan
---++ CMS

---++ Atlas


---++ LIGO


---++ Engagement


---++ Metrics


---++ Virtual Organizations Group

Issues affecting multiple VOs --

D0 --

Alice --

CompBioGrid --

IceCube --

NanoHub --

GLOW --

CalTech --

UCSD -- 

---+ Site Coordination, Integration

---+ Security



D0 -- D0 MC production led to an average consumption of 100,000 wall hours per day, at 84% job efficiency and 93% wall efficiency. Due to increased wall efficiency this week, corresponding D0 Event production was higher than average: almost 9.7 million events for the week. Network hardware problem at SPRACE site caused file transfer blockage at OU cache, affecting all OSG sites using that cache. SPRACE's removal from production restored the system transfer efficiency. Related to OSG, there are two lingering problems: (a) Long-lived D0 jobs are expunged at the CMS sites, likely due to the recently enforced policy on maximum wall clock time. (b) A bug which was uncovered in Globus LSF Manager; affecting OU and TTU sites. Work is ongoing between GOC, VDT and Globus. Alain is coordinating with Globus to get a resolution and bugfix; Dan is in contact with Alain. URLs - https://oim.grid.iu.edu/gocticket/viewer?id=6489 and http://bugzilla.globus.org/bugzilla/show_bug.cgi?id=6688

ALICE -- Taskforce is in effect. Have started scalability and stress-testing exercise of VO-Box on OSG site at NERSC/LBL. Using EU team's recommendation, target is 200 jobs with local data access sustained over a week. During last week, a peak rate of nearly 100 jobs per day (ALICE view) with an equivalent of 820 wall hours per day (OSG view) was achieved. URL with ALICE view - http://pcalimonitor.cern.ch/display?page=jobs_per_site&SiteBase=LBL

CompBioGrid -- Site deployment has succeeded; recent problems are related to submit side packages. Most problems are due to the mixed topology of the site: Windows filesystems exported and in use on Linux systems, with OSG software stack on top. We have recommended CompBioGrid to start a blog, to get a more coherent expression of issues.

IceCube -- A few members have started to attend weekly VO forum. IceCube is planning to expand production to other sites besides GLOW; and have asked for more guidance on data management solutions available on OSG sites at-large. Storage need is 14 GB total persistent, and 1-2 GB per job transient, at a site. Britta/VO-Group and Mats/Engagement are helping IceCube understand GFTP use on OSG with $osg_app, $osg_data, $osg_wn_tmp. If needs exceed these legacy mechanisms, we will try to get IceCube started with SRM based persistent storage.

VORS deprecation: Fermilab-VO/FermiGrid has come to heavily rely on VORS API. Fermilab-VO has conveyed hesitation toward VORS deprecation, unless all features and programmatic API of VORS are fully replaceable by the new combination of MyOSG/OIM/RSV. Other known dependents on VORS are NYSGrid, SBGrid, STAR. VO Group and GOC have agreed that a short document listing the new functionality/API, will be a good starting point to collect more feedback from the affected VOs.

Matters related to Site Preemption Policy: Production of at-large VOs is affected by job suspensions related to site preemption at LHC sites. At least 3 VOs - D0, Engage, nanoHUB, (also LIGO) - have pointed out that each's running jobs are sometimes suspended or evicted, at CMS sites. E.g., at Caltech, GLOW, UCSD. This can be related to newly enforced policies of CMS. Unofficial estimates indicate a 36-hours wall clock limit and a 2-weeks queue time limit. Dan suggests that resolution can be a combination of more precise ways for sites to declare these policies, and/or for VOs to checkpoint the jobs. *Action Item*: Dan, Miron, Abhishek - To discuss further.

=== Security ===

Work is being done to investigate more on phalanx attacks. FNAL Security concluded that it was not a grid incident. OSG Security team plans to hold office hours to cater to concerns of sites, and to outline a list of 'best practices' on twiki. The listing will include specifics on vulnerable platforms, and clear steps to check for the rootkit. Goal is to ensure that there is no compromised account on OSG Facility.

WLCG Security Challenge has been postponed to the first two weeks of May. Results are likely to be presented to GDB on May 12. Drill code has a few remaining issues, as noted at BNL; work is ongoing.

DOEGrids CA 'Risk assessment' work is in progress. Doug is leading the effort and is currently assigning a grade to each risk. ESNet colleagues are also helping with the work. This is also a key topic for the upcoming Blueprint meeting. URL - https://twiki.grid.iu.edu/bin/view/Security/IdMContingencyPlanning

Survey work on Audit of Core OSG Services is going well. Mine has started evaluating the survey responses, with a possible follow up over phone with Area Coordinators. 13/37 surveys seem to be missing.

=== Operations ===

Rob indicates that GOC is committed to resolve all requirements prior to the actual deprecation of VORS. GOC is looking at the webserver logs of VORS to find out possible undeclared users. Notably, almost 80% of HTTP requests seem to have originated from CERN. It is unclear which services at CERN may be behind these; investigation is in progress. Other originating IPs are from FNAL, BNL, etc.

New incremental OSG release is in final preparation phase. Official announcement of OSG 1.0.1 is expected to go out around April 22-23. Wide adoption by sites across OSG, and upgrades to this new release, are not very likely. This is because the next major release OSG 1.2 is also nearby in timeline. Chander points out that lack of wide adoption can possibly pose a problem in paving a way for the next release.

MyOSG's release as a production service has been made last week. URL - http://myosg.grid.iu.edu

=== Site Coordination, Integration ===

No attendance. Report is in email.

=== Metrics ===

No attendance. No report.
