%META:TOPICINFO{author="KyleGross" date="1193838959" format="1.1" version="1.34"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction
An introduction to installing the worker node client package.

---++What is the Worker Node Client and why you need it

The worker node client is a minimal set of executables that need to be made available to the worker nodes in a Compute Element. It must be installed in a location visible to jobs executing on the worker node.   No host cert/key files are necessary for the worker node client installation.  

These client programs need to be available from the worker node so that jobs which need to access grid-enabled mass storage during the job (e.g. via globus-url-copy, Pegasus, or srmcp) can do so.  The worker node also has to have either via NFS or locally a good set of certificate authority files with up to date certificate revocation lists (CRL's) for those clients to work properly.

---++ Installation Options
There are three installation options:
   1. _Suggested Installation:_ Install the wn-client package on the file server separate from the CE that you use for $APP. You will be installing the fetch-crl program onto this node, which requires root access, and adds a script into cron. The updates require outgoing network access from the node it is installed on.  This directory should be accessible from the CE as well.    
   2. _CE installation:_ Install the wn-client package on your CE. While we do not suggest you do this, we provide instructions, and support this installation as an option.
   3. _Local installation:_  Install the worker node client software on each node.  %RED%This installation method is required if you plan to use gLexec software.%ENDCOLOR%  There are required modifications to the default fetch-crl configuration so that either Squid or rsync is used to distribute the fetch-crl files.  Instructions to set up both of these methods are given below.

In all cases you need to choose a path that is common to all worker nodes and the CE. You will be prompted for this path when running the =configure-osg.sh= script on the gatekeeper in order to set the =$OSG_GRID= variable.

---++ How to install
Install using: 
<pre class="screen">
pacman -get %CACHE%:wn-client
</pre>
---+++ !!Full log of Installation
A full log of an installation is given [[WorkerNodeClientInstallLog]], followed by comments on why each choice is made.

---+++ !!Comments on Installation Questions
---++++ !!Caches and licenses
The installation will first ask if you trust the pacman caches, and then if you agree to the VDT licenses.  Answer =y= to both.

---++++ !!CA's and CRL's
It then asks where you would like to install the CA files and whether to run the fetch-crl cron script. There are a couple of options:

---+++++!!Option 1
For the suggested installation option, i.e. option (1) where you install this worker node client package on the file server that serves $APP, and you have no previous installation of the fetch-crl cron script, you should answer =l= (lowercase letter L) for the location and =y= to run the fetch-crl cron.

---+++++!!Option 2
If you are installing this worker node client on the OSG CE, i.e. option (2) answer =l= (lowercase letter L) for local location of CRL and =n= on running the cron script.  You will change the symlink to point to the main VDT location, =$VDT_LOCATION/globus/share/certificates= once the OSG CE is installed.

---+++++!!Option 3 
---++++++!!3a: Local installation plus squid for CA/CRL's
Answer =l= for local location of CRL and "y" on running the cron. But this should not 
be attempted without adding a squid proxy to the configuration.  This is done 
by creating the file /etc/sysconfig/fetch-crl with contents
<pre>
export http_proxy=fermigrid4.fnal.gov:3128
</pre>
where fermigrid4.fnal.gov would be replaced with whatever the name of your
site squid server is.  

---++++++!!3b: Rsync for CA/CRL's
Answer =l= for local location of CRL and "n" on running the cron.  Then download
the tarball at http://fermigrid.fnal.gov/files/csync/csync-0.9.0.tgz and follow
the instructions on how to set up the rsync server and clients to 
get your CA certs. This package has been contributed to the VDT and will 
be part of the VDT in future versions.

---++++  !!Log rotation
Answer =n= to whether or not to rotate the VDT logs

---++++ !!Automation
The Worker Node Client installation process can be automated for any 
of the scenarios above, whether installing on the head node or on a bunch of worker nodes., so that you don't have to type in the letters.

<pre class="programlisting">
VDTSETUP_AGREE_TO_LICENSES=y
export VDTSETUP_AGREE_TO_LICENSES
VDTSETUP_INSTALL_CERTS=l
export VDTSETUP_INSTALL_CERTS
VDTSETUP_EDG_CRL_UPDATE=y
export VDTSETUP_EDG_CRL_UPDATE
VDTSETUP_ENABLE_ROTATE=n
export VDTSETUP_ENABLE_ROTATE
pacman -trust-all-caches -get %CACHE%:wn-client
</pre> 
---++ Enabling VDT services
If you answered "y" to rotate the logs, or run the fetch-crl daemon, then you should
enable it with the following command
=vdt-control --on=

---++ gLexec Installation.

gLexec is an optional package new to VDT 1.8.0 and greater which, if installed,
should be installed with the software on the worker node client.  gLexec is designed
for authentication of pilot jobs.  Several VO's submit pilot jobs under the DN of one user 
which then call back to the VO submit host, e.g. Panda or !GlideWMS, to get a job
from a different user.  gLexec is designed to be installed on every worker node
and authenticate the user who is submitting the real job. It does this by calling 
a configurable callout (either LCG's lcas/lcmaps or GUMS/PRIMA).

%IMPORTANT% Instructions on how to install and configure gLexec can be found at [[GlexecInstall]]. 

---++ Post-install checks
If the OSG WN client is installed along with some other VDT package on any given node, the CE for instance, as in option 2, you should do the following steps afterwards.

Look at =crontab -l= for root, make sure that the fetch-crl that is there is running from the correct area. Make sure that it's sourcing the correct location.

For example, if the OSG CE is installed in =/usr/local/vdt= , and the OSG WN Client is installed in =/usr/local/grid=

Look at the =/usr/local/grid/globus/TRUSTED_CA= link. It should point to the real area where your certificates will live, in this example =/usr/local/vdt/globus/share/certificates= . If it doesn't, change it to that location.


<pre class="screen">
[root@fgtest3 globus]# <b>pwd </b>
/usr/local/grid/globus
[root@fgtest3 globus]# <b>ls -l TRUSTED_CA </b>
lrwxrwxrwx  1 root root 41 Mar 16 15:44 TRUSTED_CA -> /usr/local/grid/globus/share/certificates
[root@fgtest3 globus]# <b>rm TRUSTED_CA </b>
[root@fgtest3 globus]# <b>ln -s /usr/local/vdt/globus/share/certificates TRUSTED_CA </b>
</pre>

%NOTE% If Condor is used as the job manager, you should also make a symlink from =/etc/grid-security/certificates= to =$VDT_LOCATION/globus/share/certificates= .

---++ Notes
%IMPORTANT% If you are installing this package on a head node / compute element where an OSG CE install will also be done afterwards, it is recommended that you log out and log back in before continuing with the main OSG CE Install, otherwise all the wn-client stuff will be in your path and confuse everything.
<!--
long-standing GLOBUS_LOCATION problem now fixed in vdt 1.8.1/osg 0.7.0 and
greater.  S. Timm 10/4/07
%IMPORTANT% A side effect of the worker node client as currently configured, is that $GLOBUS_LOCATION can and does wind up in a different location on the head node than it does on the worker node.  The environment at the beginning of the job will give you $GLOBUS_LOCATION on the head node.  You have to source =$OSG_GRID/setup.sh= to get where $GLOBUS_LOCATION is available to you on the worker node. %BR% %BR%   To avoid this, it is required to add an appropriate softlink on every worker node such that $GLOBUS_LOCATION as defined on the head node points to the place where it is located on the worker node after the wn client installation. E.g., CMS analysis jobs submitted via Resource Broker will presently fail without this softlink. %RED% It has been claimed that in OSG 0.8.0/VDT 1.8.x, the vdt-job-environment routines which are called by the Globus jobmanager.pm to 
insert all the OSG variables into the job environment, will fix the $GLOBUS_LOCATION too, but in fact the problem still persists.%ENDCOLOR%.
-->

---++ Current WN Contents

   * http://software.grid.iu.edu/itb/wn-client-0.7.1.pacman
<pre class="screen">
[root@fgitb113 grid]# vdt-version
You have installed a subset of VDT version 1.8.1:
    CA Certificates v30 (includes IGTF 1.16 CAs)
    cURL 7.16.2
    Fetch CRL 2.6.2
    Globus Toolkit, pre web-services, client 4.0.5
    Globus Toolkit, web-services, client 4.0.5
    GPT 3.2
    Java 5 SDK 1.5.0_12
    Logrotate 3.7
    Pegaus Worker Package 2.0.1
    RLS, client 3.0.041021
    SRM V1 Client 1.25
    SRM V2 Client 2.2.0.3
    UberFTP 1.24
    Wget 1.10.2
</pre>

<!--


Installation of the worker node client package does imply maintenance of the certificates that it installs.  
It is possible to rsync either the full worker node client package to the worker nodes, or just the
CA certificate directory found in $VDT_LOCATION/globus/TRUSTED_CA.  This eliminates the 
need to do multiple pacman updates every time a new CA-Certificates package comes out, which 
is on the average of every couple of months.  

It is not enough to rsync the OSG CE installation, or NFS-mount it to the worker nodes.  The Worker 
Node client package now contains software that is not in the base OSG CE installation and VO's are
expecting it to be there, so it must be made available somewhere.

Doing anything automatically on the worker nodes requires care.  An example script that looks to see what worker nodes on a cluster using pbsnodes is attached to this page.  It can be edited or replaced with a more general version to match your specific cluster conditions.  Note that the example script does not use the batch system itself, but uses a feature of the local batch scheduler to avoid trying to push certificates to worker nodes that are down and to build a list of operating nodes to receive the update.  To use this script with the greatest amount of integration into existing certificate update procedures, it can be called just after the edg-fetch-crl call in the edg/sbin/edg-crl-upgrade script that should be in the edg/sbin area of your OSG VDT installation.  Example code to do so would look like:
%RED% These instructions are obsolete from osg 0.6.0 and greater because they refer to adding code to the now-obsolete edg-crl-upgrade script.  A new wn-cert-sync package is in 
preparation currently and should be released soon %ENDCOLOR%
<pre class="programlisting">
# Add section to upgrade the worker nodes:
    if [ -x $VDT_LOCATION/edg/sbin/wn-cert-sync ]; then
        $VDT_LOCATION/edg/sbin/wn-cert-sync -l $CRL_UPGRADE_PATH -q
    else
        echo "In edg-crl-upgrade: Worker node certificates not synchronized! "
    fi
</pre>

where =wn-cert-sync= is a soft link to your particular cluster's worker node certificate synchronization script, as in the example attached. (Customize one to your cluster configuration.)
   * This issue deserves further discussion and repair in the current OSG releases.
-->

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.SuchandraThapa - 18 Oct 2007 %BR%
%REVIEW%

%META:TOPICMOVED{by="ForrestChristian" date="1166058606" from="Integration.WorkerNodeClient050" to="Integration/ITB_0_5.WorkerNodeClient"}%
