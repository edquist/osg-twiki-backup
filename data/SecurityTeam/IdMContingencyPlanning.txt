%META:TOPICINFO{author="KyleGross" date="1476284217" format="1.1" reprev="1.22" version="1.22"}%
%META:TOPICPARENT{name="SecurityTeamWorkingArea"}%
%RED% *OSG Restricted Information*

Information on this web page is restricted to authorized individuals only.  It should not be copied or distributed without
the express consent of the OSG Security Team and only to individuals, and while retaining this information protection notice.
%ENDCOLOR%

<!--
   * Set TABLEATTRIBUTES = sort="off"
-->

---++!! Contents

%TOC%

---+ Contingency Planning for the dependencies of OSG on the DOEGrids CA service.
The Computer Security Risk Trash/Trash/Trash/Assessment of OSG ([[https://osg-docdb.opensciencegrid.org:440/cgi-bin/ShowDocument?docid=488][doc488]]) identified the
DOEGrids CA and RA services as one of the few elements capable of causing significant disruption to overall OSG operations.
That assessment considered only human sources of threats for the core OSG assets because  physical damage caused by fire or other
natural disasters is dealt with in the security plans of the organizations hosting the services that OSG relies on.
For this contingency planning we also consider the effects of any type of disaster or disruption to the CA service operated by ESnet
on OSG operations.
To evaluate this risk more clearly and to mitigate it we want to understand the status of the ESnet PKI contingency plan and disaster recovery plan
as well as develop the OSG contingency plan in case of disruption of the CA and certificate services.

This contingency planning is modeled on the [[http://csrc.nist.gov/publications/nistpubs/800-34/sp800-34.pdf][NIST publication SP800-34]]
"Contingency Planning Guide for Information Technology Systems".
This defines a seven step process consisting of:
   1. Develop the contingency planning policy statement.
   2. Conduct the business impact analysis (BIA).
   3. Identify preventive controls.
   4. Develop recovery strategies.
   5. Develop an IT contingency plan.
   6. Plan testing, training, and exercises.
   7. Plan maintenance.
The recommended three phases of actions to take following a system disruption are:
   * The *Notification/Activation* Phase describes the process of notifying recovery personnel and performing a damage assessment. 
   * The *Recovery* Phase discusses a suggested course of action for recovery teams and personnel to restore IT operations at an alternate site or using contingency capabilities. 
   * The final phase, *Reconstitution*, outlines actions that can be taken to return the system to normal operating conditions.

The purpose of a contingency plan is illustrated nicely in this figure.

<img src="%ATTACHURLPATH%/sp800-34-fig2.1.gif" alt="sp800-34-fig2.1.gif" width='612' height='219' />

The contigency plan should "serve as a user’s manual for executing the strategy in the event of a disruption".

It is useful to read Section 2 "Background" of [[http://csrc.nist.gov/publications/nistpubs/800-34/sp800-34.pdf][NIST SP800-34]]
for a description of how a contingency plan fits into the broader picture of a Business Continuity Plan.

---++ Status

|  *Step*  |  *Status*  | *Comments*   |
|  1  |  Done  |   |
|  2  |  Done  |   |
|  3  |  Done  |   |
|  4  |  Done  |   |
|  5  |  Done  |  |
|  6  |  Not Started  | Follows 5. |
|  7  |  Not Started  | Follows 6. |

---++ Steps in Contingency Planning Process
The section below hold the materials and descriptions of activities that go into developing the contingency plan but are not
the contingency plan itself.

---+++ 1. Develop the contingency planning policy statement.

In principle, it should relate to the guidelines used in performing the risk assessment in defining quantitatively what is
acceptable risk so the conditions that can cause a contingency plan to be put into action can be identified.
From the Risk Trash/Trash/Trash/Assessment document (doc488) we have the following definitions of impact.

<BLOCKQUOTE>
A security event has LOW impact if it occurs less than 10 times per year and does not disrupt the perception of the OSG as a computational facility that can be relied on AND no single occurrence of the event disables the substantially all OSG’s operational Compute Element service for more than two days.

A security event has MODERATE impact if it occurs less than 20 times/ year disables the compute element service for up to a week.

A security event has SEVERE impact if it occurs 20 or more times/year or disables the compute element service for more than a week.
</BLOCKQUOTE>

Until/unless additional guidance is received, we will use the following policy statement:

%BLUE%
The OSG is a relying party in the use of the DOEGrids Certification Authority (CA) and has identified the services of the CA as
critical to the operations of OSG.   The OSG operates a Registration Authority (RA) function of the CA.
In accordance with the OSG Risk Trash/Trash/Trash/Assessment evaluations and impact ratings any disruption to the CA or RA services
should be considered in the Contingency Plan (CP) appropriate to the impact rating of the potential service disruption.
For CA service disruptions, the CP should describe the actions that OSG can perform to mitigate the impact on OSG operations
while the ESnet staff restore the CA services.
For RA service disruptions, the CP should describe the actions that OSG can perform to restore the RA services as well
as describe any necessary mitigation actions to be used temporarily while RA services are restored.
The cost of the mitigation should be commensurate with the impact and risk.
%ENDCOLOR%

---+++ 2. Conduct the business impact analysis (BIA).
_The BIA enables the Contingency Planning Coordinator to fully characterize the system requirements, processes, and interdependencies and use this information to determine contingency requirements and priorities. The BIA purpose is to correlate specific system components with the critical services that they provide, and based on that information, to characterize the consequences of a disruption to the system components._

According to the NIST SP800-34 we should identify the critical business processes and use that to identify the critical IT resources.
We will invert that process a bit and consider what the effects are of various IT resources being disrupted.
To perform the analysis we will use the following template to characterize each resource.
---++++ 2.1 Resource characterization template
   * Brief description of the resource, i.e., name, purpose
   * Points of contact for the resource
   * People directly affected by the resource being disrupted
   * Other resources being directly affected by the resource being disrupted
   * List of specific functions of the resource including TTL, periodicity, and other time-related characteristics. 

---++++ 2.2 Risk Analysis
In the table below the Severity column is derived from the impact categorization above so that
   * Low severity means all compute service and/or all users are affected for less than two days.
   * Medium severity means that all compute service and/or all users are affected for less than one week (but greater than 2 days)
   * High severity means that all compute service and/or all users affected for more than one week.

Excerpts from OSG Doc488, Table 4, Risk Management.  Grades D and E are considered acceptable risk. Grade C requires monitoring. Grades A and B require mitigation.

| *No* | *Description of Risk*  | *Frequency* | *Severity* | *Grade* |
| 14 | OSG staff publicizing false information about User Process  | L | L | E |
| 15 | Careless 3rd party mistakenly granting agent privilege to incorrect person, possible impact on OSG since all agents of DOE Grids have privilege for all certificates  | L | L | E |
| 16 | Malicious authorization of agent privilege for illegitimate person, by impersonating a valid RA, malicious agent can revoke all certificates causing DNS  | L | H | C |
| 17 | Hack into CA website and perform malicious actions, install fake agents, revoke certificates, cause DOS  | L | H | C |
| 18 | Careless or incomplete training for agents, agents do not follow correct procedures, potential exploit for malicious user if combined with exploit of VOMS server | L | M | D |
| 19 | Malicious mis-information provided for agent training, like 10 but implies intent to exploit  | L | M | D |
| 20 | Careless or incomplete user training for PKI, user may not handle private key correctly  | M | L | D |
| 21 | Accidental corruption of RA process audit log, leads to incomplete of lack of audit ability | L | L | E |
| 22 | Intentional corruption of RA process audit log to disguise audit trail, potential privacy compromise  | L | L | E |
| 23 | Malicious disruption of PMA process, potential damage to credibility of PKI, potential delays in handling authentication and revocation requests | L | L | E |
| 24 | Careless or incomplete identity vetting in certificate process, potential exploit for malicious user can permit abuse of resources if identity theft occurs  | L | M | D |
| 25 | Malicious abuse of identity vetting, insider participation in identity theft by agents | L | M | D |
| 26 | Careless management of CA services, potential DOS caused by CRL lack of availability | L | H | C |
| 27 | Loss of CA integrity due to physical access to CA, potential compromise of CA key and need for re-issuing thousands of certificates | L | H | C |
| 38 | Third party careless agent authorization (CA managers mistake) Risk of mistaken agent authorization, potential damage to trust of grid PKI if it happens too often  | L | L | E |
| 39 | Third party agent authorization (impersonate OSG RA to install malicious agent)Risk that malicious agent is authorized via impersonation of legit RA, immediate impact is low, potential threat to PKI trust, potential DOS by revocation of certificates  | L | H | C |
| 40 | OSG RA careless authZ of agent (Accidental authorization of agent that does not want agent access, low impact unless agent also exploits VOMS)  | L | L | E |
| 41 | Software exploit agent authZ (CA managers issue)  | L | H | C |
| 42 | Agent authZ physical access (CA managers issue) Malicious physical access to CA machines  | L | H | C |
| 43 | OSG staff alarmist – removal of agent authZ (Agent authorization is removed due to incorrect OSG staff complaints , may delay some certificate requests)  | L | L | E |
| 44 | 3rd party careless agent training (Agents in VOs give incorrect info to other agents, requests handled incorrectly)  | L | L | E |
| 45 | 3rd party malicious agent training (Intent to give agents incorrect instructions, low impact unless malicious person also has access to VOMS)  | L | L | E |
| 46 | OSG staff careless agent training (OSG RA provides incorrect training info to agents, agents do not follow correct procedures)  | L | L | E |
| 47 | Malicious remote access agent training (change web instructions)Hacker modifies agent instructions and post bad info  | L | L | E |
| 48 | Physical access & agent training (OSG staff trashing training materials)  | L | L | E |
| 49 | Careless 3rd party user PKI training (Users don’t get proper instructions on how to handle private keys)  | M | L | D |
| 50 | 3rd party careless user support for PKI (Users get mad, try to circumvent controls that are not working)  | L | L | E |
| 62 | ID vetting 3rd party carelessness(Accidental incomplete ID verification, or accidental duplicate DN issuance, impact on PKI trust if happens too often)  | L | L | E |
| 63 | ID vetting 3rd party vandal(Malicious sponsor or agent violates authentication process, issues incorrect DN, low impact unless malicious party has access to VOMS admin)  | L | M | D |
| 64 | ID vetting malicious remote access (Hacker access to Registration Manager server, DNS potential from installing vandal agent doing revocation or certificates)  | L | H | C |
 | 65 | ID vetting physical access  | L | H | C |
| 66 | ID vetting OSG staff alarmist (OSG staff making incorrect statements about quality of ID vetting, potential damage to trust in PKI)  | L | L | E |
| 67 | CA integrity 3rd party carelessness (Potential disruption of services, such as CRL’s, and delays in request processing)  | L | M | D |
| 68 | CA integrity 3rd party vandal (Potential disruption of services, like CRL distribution, or violation of CA key)  | L | H | C |
| 69 | CA integrity malicious remote access(Potential disruption of services like CRL distribution, potential DNS from certificate revocation)  | L | H | C |
| 70 | CA integrity physical access(Potential loss of PKI) | L | H | C |
| 71 | CA integrity OSG staff alarmist(Mis-statements about PKI may damage trust in PKI)  | L | L | E |
| | | | | |
||  *Additional sources of risk (not from OSG doc488) include:*  |||
| A1 | fire (suppression systems already in place) | L | L | E |
| A2 | earthquake (Hayward fault is close by)  | L | H | C |
| A3 | flood (LBL sits on hillside with no water above)  | L | L | E |
| A4 | airplanes falling from sky (LBL is near flight path of east-bound flights from SFO)  | L | ? | ? |
| A5 | power failure (UPS of limited duration in place)   | M | L | D |
| A6 | SHA-1 encryption algorithm broken (collision) | Note-A6 | H  | Note-A6 |

*Note-A6 on SHA-1* The likelihood of occurrence is time dependent. NIST recommends that SHA-1 use be discontinued and replaced by SHA-2
by the end of 2010. See Security.HashAlgorithms.

From the risk analysis it is apparent that there are many scenarios where something may go wrong somehow 
without stopping the service and the primary question is "how can we tell it went wrong?".
These issues are not part of the Contingency Plan.  In the Contingency Plan we want to consider only those risks which can  cause a service outage for
an extended period of time.  Based on the impact definitions from OSG Doc488 we should consider time periods of two days, one week, one month, and greater than one month.  Note that one month is appropriate to consider since there are CA processes on that time scale (CRL lifetime, cert expiration notices, etc.).

---++++ 2.3 Resources
*2.3.1 DOEGrids CA (pki1.doegrids.org)*
   * DOEGrids CA - issues X509 certificates for people and services of OSG
   * Mike Helm for CA, Doug Olson for RA
   * Affected people are CA operators, Agents, Gridadmin, Subscribers, relying parties
   * Other resources affected include all hosts & services trusting the DOEGrids CA and using DOEGrids CRLs, i.e., all of OSG.
   * Functions
      * CA signing function - internal with no direct external exposure, occurs on demand from Agents, Gridadmins and subscribers' replacement interfaces; averages about 30 time/day
      * Agent interface - accessed 10-20/day to issue, revoke, query certificates
      * Gridadmin interface - accessed 20-40/day to issue new host/service certificates
      * Public enrollment interface - accessed 10/day to submit certificate request
      * Email notifications - (see below)
      * CRL generation - new CRL generated on revocation action and periodically and copied to CRL publishing URL (see below)
      * Publish to LDAP - certificates and RA/sponsor information published to LDAP directory when certificates are issued(?)

*2.3.2 CRL Publishing*
   * CRL published by CA at crl.doegrids.org, accessed about 70K/day from 13K unique hosts
   * Mike Helm, Dhiva
   * Affected people - like CA
   * Affected services - like CA
   * Functions
      * CRL  published at http://crl.doegrids.org/1c3f2ca8/1c3f2ca8.r0  (DOEGrids CA 1)
      * CRL published at http://es.net/CA/d1b603c3/CRL/d1b603c3.r0 (ESnet Root CA)

*2.3.3 Public ldap certificate repository*
   * Directory entries for all issued certificates, includes some RA and sponsor info as submitted by the subscriber, unclear how much it is used
   * Mike Helm, Dhiva
   * Affected people - unknown
   * Affected services - unknown
   * Functions
      * serve ldap queries about certificates

*2.3.4. Email notifications*
   * Email notices sent to various roles and individuals on certain events of the CA, several/day
   * Mike Helm, Dhiva
   * Affected people - CA operators, RA agents, subscribers
   * Affected services - ?
   * Functions
      * Notice to Agents on CSR submission
      * Notice to Subscriber on certificate issuance
      * Weekly notice to Agents on pending requests
      * Weekly notice to subscribers about certificates nearing expiration
      * (others?)


---++++ 2.4 Impacts of outages
This section quantifies the impact of service outages for the relevant time periods.
The classification of impact is taken from the OSG Risk Trash/Trash/Trash/Assessment document (doc 488).
<BLOCKQUOTE>
A security event has LOW impact if it occurs less than 10 times per year and does not disrupt the perception of the OSG as a computational facility that can be relied on AND no single occurrence of the event disables the substantially all OSG’s operational Compute Element service for more than two days.

A security event has MODERATE impact if it occurs less than 20 times/ year disables the compute element service for up to a week.

A security event has SEVERE impact if it occurs 20 or more times/year or disables the compute element service for more than a week.
</BLOCKQUOTE>

---+++++ 2.4.1 1 Day outage
   * All CA functions unavailable
   * IMPACT - LOW

*CA consequences*
   * No certificates signed - 1 day delay in subscribers getting new certificates
   * Agent interface inaccessible - as above, plus lost capability to query about certificates
   * Gridadmin interface inaccessible - as above
   * Public enrollment inaccessible - as above
   * Email notifications stopped - no actions to cause an email notice
   * CRL generation stopped - 1 day delay in revoking certificates
   * Publishing to LDAP - no new certificates to publish

*CRL consequences*
   * No new CRLs available - 1 day delay in distributing a new CRL

*LDAP consequences*
   * No new certificate entries published to ldap - will ldap re-sync with CA on recovery?

---+++++ 2.4.2 1 week outage
   * All CA functions unavailable
   * IMPACT - LOW

*CA consequences*
   * No certificates signed
   * 1 week delay in new subscribers/certificates, can cause several days delay in work for few people
   * 1 week unavailability to query CA database by Agents, potentiall delay in some security processes
   * CRL generation stopped - potential risk from inability to revoke certificates
   * Email notifications - no actions to send notices about
 
*CRL consequences*
   * No new CRLs available - potential risk from inability to revoke certificates

---+++++ 2.4.3 1 month outage
   * All CA functions unavailable
   * IMPACT - MODERATE
The impact of an outage changes rapidly from LOW to SEVERE around 1 month of outage due to the expiration
time of the CRLs, which is 30 days. After the CRL expires on a relying party service transactions using certificates from that CA will fail
without administrator intervention. An administrator can restore service by removing the expired CRL but with the consequence that revoked certificates
will validate successfully.

*CA consequences*
   * No certificates signed for 1 month 
      * some services will not be able to acquire replacement certificates before the old ones expire causing a service outage.
      * some people will not be able to renew or replace their certificates before expiration causing a delay in work

*CRL Consequences*
      * No new CRLs available
      * risk from inability to revoke certificates
      * old CRL expires after 30 days so transactions with services will start to fail unless administrator removes CRL checking, which will reduce the security profile of the service

---+++++ 2.4.4 1 month + 1 week outage
   * All CA functions unavailable
   * IMPACT - SEVERE

*CA consequences*
   * No new certificates available
      * a fraction of all certificates will have expired and not have been replaced, this will be about (length of outage)/(1 year) since certificates have a 1 year lifetime
*CRL consequences*
   * No new CRLs available - risk from inability to revoke certificates
   * No valid CRL exists - transactions and services will fail unless administrators turn off CRL checking

---++++ 2.5 Service Corruption

Another type of service failure is for some aspect of the CA service to be degraded or corrupted rather than the entire service being out.
This can be caused by software or hardware subcomponent failure, by human error or malicious action.
The normal access controls prevent malicious caused by unauthorized persons.
Monitoring controls are used to detect an occurrence of service disruption.
---+++++ 2.5.1 Excessive certificate revocation

   * IMPACT - MODERATE

A person with Agent privilege can revoke any or all certificates issued by DOEGrids quite easily.
This would probably be reported by the community before being noticed by CA monitoring.
WIthin days the most active certificates can be replaced so it probably affects the bulk of the computer service
for less than a week.

---+++++2.5.2 CA key compromise
   * IMPACT - HIGH

This is very unlikely in the DOEGrids or ESnet ROOT CA cases but experience shows that unforseen things can and do sometimes happen
so ESnet should have a CA key roll-over plan that  explains how service disruptions can be minimized.
Note that ESnet has gone through a couple CA transitioins (one rekey & rename and one resigning) with no
impact on operations. This likely results in needing to re-issue all certificates. Depending upon the circumstances
this may be done in an orderly fashion without prior need to revoke certificates.

---+++++2.5.3 Key hash algorithm broken
   * IMPACT - HIGH

NIST recommends that the SHA-1 algorithm be considered unsatisfactory after 2010. If an exploit occurs that breaks SHA-1
before the OSG has verified that SHA-2 functions across all sites and VOs then the delay in transitioning to SHA-2
can be significant. Potentially requiring many sites to do an OS upgrade and middleware upgrade  as well as VOs validating application software
for the new OS and middleware versions.  An investigation is underway to identify all software in use
that does not support SHA-2.

---++++ 2.6 End of CA service by organization (ESnet)
   * IMPACT - HIGH

This case is for ESnet ends the CA service for some reason and OSG must transition all certificate service
needs to other CAs (assuming that OSG is not also EOL at the same time).

---++++ 2.7 Recovery Priorities
_This section will show within what time period services must be restored or work arounds adopted following a service disruption._

   * First priority is to minimize impact on services that actively validate certificates against DOEGrids CRLs and CA certificate.
   * Second priority is to identify means to get new certificates for those that expire while DOEGrids is down.
   * Third priority is to deal with the consequences of not being able to revoke compromised certificates while DOEGrids is down.

---+++ 3. Identify preventive controls.
Most preventive controls belong to ESnet.

---++++ 3.1Certificate replacement overlap time
A control that OSG can institute is to advise on how far in advance of certificate expiration a new certificate should be
acquired in order to minimize the impact of a service outage.

---++++ 3.2 Encryption algorithm upgrade
OSG should maintain an awareness of the strength of the encryption algorithms used and plan software
upgrades and certificate replacement in a timely manner in order to avoid use of an algorithm once it is broken.

---++++ 3.3 Replicated certificate information
OSG should maintain up-to-date information on the DNs, revoked certificate serial numbers and email addresses of
all certificate owners to support mitigation actions while the CA service is note operational.

---++++ 3.4 Alternative CA service providers
OSG should identify alternative CA service providers for those cases where service is needed before it
can be restored following distruption, or is never restored.  Alternative CA providers should be included in CA distribution so that
the time to propagate a new CA certificate is not required in order to switch suppliers.

---+++ 4. Develop recovery strategies. (mitigation)
This should provide the meat of the contingency plan, and include describing what actions OSG should take until the service is restored.

---++++ 4.1 Total service outage
*CRL generation*

The operational impact of CRL expiration is mitigated by service administrators turning off CRL checking for the DOEGrids CA.  This takes manual intervention by each administrator.  OSG has the means to communicate these measures to administrators.  This will reduce the security profile since certificates previously revoked can then be used to conduct transactions if they are still authorized in VOMS servers.  This secondary effect can be minimized by removing authorization from any certificate DN's that were previously revoked and are still valid.  This list of revoked certificates will exist in the last CRL issued by DOEGrids.

*Certificate revocation*

In place of certificate revocation OSG has the ability to inform VO managers of certificate DN's that can be removed from VOMS servers and therefore from authorization to OSG resources if it is determined that a previously revoked certificate is being exploited.  It should be noted that certificate 
compromise and exploitation is a very rare event and that most certificate revocations happen due to cessation of operation or key destruction.
This means that the mitigation action should not be to automatically try to remove every revoked DN from VOMS servers but to look carefully and determine which if any are necessary to remove from VOMS authorization.

*New certificate issuance*

The only alternative for getting new certificates while the DOEGrids CA is unavailable is to get them from
another CA.  People could get registered with FNAL or NERSC and get short lived certificates from the FNAL KCA or the NERSC SLCS.
Note that as of 21 April 2009 the FNAL KCA is not yet IGTF accredited but NERSC SLCS is accredited.  The process of a person getting registered at either FNAL or NERSC probably takes a couple days.  If a large number of requests happen at once it may take longer.
What is an alternative for service certificates?

Jens Jensen has recovery scenarios in which previously submitted CSRs can be used to issue new certificates that EEs can match to
existing private keys.

Possibly generate CSR from existing certificate and submit to new CA. Some code for this exists in the cert-scripts package.

---++++ 4.2 Excessive certificate revocation
OSG can ask/work with DOEGrids to publish that last  good CRL until new certificates are issued to replaced the ones
improperly revoked.

---++++ 4.3 CA key compromise
The CA certificate must be removed from OSG distribution. This means no user has any certificates and we should find a new CA to issue certificates. This is the same problem as New certificate issuance problem in Total Outage except that we have to issue certificates for ALL users not just the users with expired certificates.  People could get registered with FNAL or NERSC and get short lived certificates from the FNAL KCA or the NERSC SLCS. Note that as of 21 April 2009 the FNAL KCA is not yet IGTF accredited but NERSC SLCS is accredited. The process of a person getting registered at either FNAL or NERSC probably takes a couple days. If a large number of requests happen at once it may take longer. NCSA CA or TACCcan be contacted for service certificates. 

---++++ 4.4 CA & EE certificates invalid due to encryption algorithm failure
SHA-1 is expected to be obsolete at some point and SHA-2 is the recommended replacement before the end of 2010.
This has potentially very serious impact so OSG should continue to maintain an awareness of the strength of the
encryption algorithms used and plan effort to ensure timely transitions to stronger algorithms as necessary.


---+++ 5. Develop an IT contingency plan.
This is "write the document". The CP document is being developed at IdmContingencyPlan09.

---+++ 6. Plan testing, training, and exercises.
This means the contingency plan developed should be included in the OSG security ST&E process.

---++++ 6.1 Gratia DB

*Weekly Report from the job level Gratia db for _date_* has information for which sites have
the most jobs. This will allow us to prioritize replacement certificates for CE services.

*Weekly Report by user for _date_* has information for users (by CN) which
will allow us to prioritize replacement certificates for users.

---+++ Plan maintenance.
This means the contingency plan developed should be included in the OSG security ST&E process and continuously be updated.

---++ Acknowledgements

Help and assistance provided by
   * Dan Peterson - ESnet Security Officer
   * Mike Helm - ESnet ATF Lead
   * Dhiva Muruganantham - DOEGrids CA operations

Also [[http://csrc.nist.gov/][NIST Computer Security Resource Center]].

-- Main.DougOlson - 07 Apr 2009


   * Set ALLOWTOPICVIEW = Main.SecurityTeamGroup, Main.MichaelHelm, Main.DanPeterson, Main.DhivakaranMuruganantham, Main.RuthPordes, Main.BrianBockelman, Main.FkW, Main.JohnHover, Main.DanFraser, Main.RobQ, Main.ChanderSehgal, Main.VonWelch
   * Set ALLOWTOPICCHANGE = Main.SecurityTeamGroup    
   * Set DENYTOPICVIEW = Main.TwikiGuest

%META:FILEATTACHMENT{name="sp800-34-fig2.1.gif" attachment="sp800-34-fig2.1.gif" attr="" comment="Contingency Plan illustration" date="1239145106" path="sp800-34-fig2.1.gif" size="18194" stream="sp800-34-fig2.1.gif" tmpFilename="/usr/tmp/CGItemp4912" user="DougOlson" version="1"}%
