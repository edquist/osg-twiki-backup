%META:TOPICINFO{author="AnandPadmanabhan" date="1446569214" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="SecurityTeamWorkingArea"}%
---+ OSG Connect - Security Drill

---++ OSG Connect Architecture
OSG Connect offers scientists simple and efficient access to distributed high throughput computing (DHTC) for accessing campus grid, science cloud or the opportunistic cycles available through the Open Science Grid. OSG Connect is built on the Globus and CI Connect platforms which offer resource providers the ability to create connected cyberinfrastructure environments. The OSG Connect uses Stash - a fast, high-volume file storage service - to make it easy to share and retrieve distributed data.
 
A user could register themselves through the OSG connect registration site. A user could sign in using their institution login (via CILogon) or through a Globus Online account. Once this is done the users can complete the signup by requesting to login to the osg group with affiliation and contact information being verified. An OSG support staff will then contact the user by phone to verify and confirm details. Once the OSG staff or project delegate authorizes the access the user is able to access the system.  See details at: http://osgconnect.net/signup. The users also agree to an AUP at: https://osgconnect.net/aup-full.
 
OSG connect uses GlideinWMS pliot infrastructure under the hood. There is two mechanisms to submit jobs through OSG connect: (a) ssh login in to the system through login.osgconnect.net; and (b) through the osg connect client. The login.osgconnect.net functions as any other glideinwms front end and uses the OSG factory to submit jobs to OSG. The connect client or campus connect client on the other hand bridges the campus and OSG's national grid infrastructure. The tool enables users of a campus facility to submit jobs into the Open Science Grid without leaving their own institutional working environment or even their workstations. The connect client provides an interface for job and data transfer and synchronization with the OSG Connect server, and an interface for remote job management. This is especially suited for people who are not experienced with HTCondor and are comfortable on their local computational resource, and no need to interact directly with HTCondor. Connect client targets these users, making it easy to extend local resources with the full national grid. Using the same login password set up sing osg connect, the connect client can access connect-client.osgconnect.net. The setup step first sets up a ssh key pair for supporting future submission.

 %ATTACHURL%/OSG_Connect_Arch.png

Figure: OSG Connect Architecture (Source: https://osgconnect.net/)

---++ Drill
Let us pretend there is a security incident detected by couple of sites and they saw the jobs were submitted using the OSG connect's front end certificate. Here are the list of jobs we want to trace back to users

   * Approximate Time when job was active: Wed Sep 23 18:15 - 18:30 EDT 2015
   * Remote node hostname: node0296.palmetto.clemson.edu 
   * Remote user: uid=285610(osgconnect) gid=94175(osg) groups=94175(osg)
   * Remote scratch directory: /local_scratch/rcc.sSbCniGhlS/execute.10.125.2.37-24774/dir_29950

   * Approximate Time when job was active: Wed Sep 23 18:15 - 18:30 EDT 2015
   * Remote node hostname: node0340.palmetto.clemson.edu
   * Remote user: uid=285610(osgconnect) gid=94175(osg) groups=94175(osg)
   * Remote scratch directory: /local_scratch/rcc.z4HRcXAYWU/execute.10.125.2.81-16435/dir_14418


   * Approximate Time when job was active: Mon Oct 5 15:30 - 16:00 PDT 2015
   * Remote node hostname: cabinet-4-4-28.t2.ucsd.edu
   * Remote user: uid=36058(cuser12) gid=36058(cuser12) groups=36058(cuser12)
   * Remote scratch directory: /data1/condor_local/execute/dir_243958/glide_NuMbld/execute/dir_4749

 
   * Approximate Time when job was active: Mon Oct  5 17:30 - 18:00 CDT 2015
   * Remote node hostname: golub082.campuscluster.illinois.edu
   * Remote user: uid=21039(osg) gid=21039(osg) groups=21039(osg),21000(osgvo)
   * Remote scratch directory: /scratch.local/condor/execute/dir_58856/glide_QUB7yk/execute/dir_71454

For each of these jobs 

   1. Given the information above can you as the frontend admin trace this bad behavior to one frontend user. (If not could this atleast narrow down to a small set of users.)?
   2. Can we then figure out the users contact information and how he authenticated into the frontend?
   3. What mechanism was used to submit jobs login to osg-connect or via connect-client or some other mechanism?
   4. Can we figure out what IP(s) the user logged in from, the executable s/he might have run.
   5. Given a time period, can we then identify which other sites the user's job ('bad behaving' or otherwise) might have run at.

---++ Summary of Response Received 

Note that since we are a scheduler site and not a collector/negotiator, we don't have any knowledge of the scratch directory on the worker node.  Therefore in the incidents below, we
cannot necessarily isolate a single job.  A query at the flocking node, osg-flock.grid.iu.edu, would be required to determine exactly which GlobalJobID matches a given host and scratch directory, and we don't have access to that system.  Therefore the information provided only allows us to find jobs that we sent to a given node during a given timeframe.  That can potentially be dozens of results per incidents: each node has multiple slots, and some jobs only last a few minutes within the provided time frame.

A full set of class ad for each job were provided as a json file

The OSG Connect team was able to provide details and found a list of users that included the drill user for jobs submitted through the OSG connect submit node, however for jobs submitted through the osg connect client the OSG connect team has yet to provide the necessary information about the user jobs.

---+ Conclusion




-- Main.AnandPadmanabhan - 03 Nov 2015

%META:FILEATTACHMENT{name="OSG_Connect_Arch.png" attachment="OSG_Connect_Arch.png" attr="" comment="" date="1446569213" path="OSG_Connect_Arch.png" size="194155" stream="OSG_Connect_Arch.png" tmpFilename="/usr/tmp/CGItemp11216" user="AnandPadmanabhan" version="2"}%
