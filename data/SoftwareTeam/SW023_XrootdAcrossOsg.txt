%META:TOPICINFO{author="BrianBockelman" date="1423773956" format="1.1" version="1.7"}%
---+ Project 23: !StashCache – Distributing User Data With !XRootD

The goal of this project is to support all OSG users who have common input data files, with an initial goal of handling up to 1 TB datasets. Users will upload copies of their data to predefined entry points and will set up jobs to fetch data without having to understand implementation details. Internally, !XRootD will copy files from the entry points to OSG caches that are located at or near OSG sites where user jobs run.

---++ The Problem

The two large LHC VOs, ATLAS and CMS, own storage at many OSG sites and use them as _storage elements_, or remotely accessible file systems. The storage element concept does not work for opportunistic VOs – without a mechanism for automated eviction, files stay in the storage element forever. Without access to site storage elements, opportunistic VOs cannot efficiently deliver files to individual jobs. The CMS and ATLAS systems for moving data between storage elements are robust and efficient, but have proven impossible for any other VO to operate.

Users not affiliated with ATLAS and CMS have few remaining choices for distributing data files, especially as dataset size increases. Currently, most of these users rely on HTCondor file transfer from submit to execute host, using on-site HTTP caches, or OASIS. All three mechanisms effectively limit the size of data files to less than 1&nbsp;GB. In 2014, over 30% of OSG cycles went to non-HEP users, and anecdotally at least OSG knows that some of those users have common input datasets that exceed the best-practice limits for available methods.

The general problem is quite complex; we choose to focus on “common input data”. We assume the user is running at least 1,000 jobs (lasting 1 hour each) across a common input dataset of at least 1&nbsp;GB.

---++ Deliverables

   1. Deploy a data federation service such that any VO can export their central filesystem and have it be accessible to clients.
      * OSG VO will provide one such filesystem centrally for VOs who do not want to set up their own.
   1. Develop and deploy a series of caches such that a running job can access the data federation through a “nearby” cache; caches should be centrally monitored and managed
   1. Develop user-friendly tools so jobs can fetch data without understanding the implementation details; three access methods are needed:
      * A "cp"-like utility
      * HTCondor file transfer plugins
      * POSIX-based access
   1. Deploy tools so central operations can understand usage and failures from the service

---++ Objectives

This project encompasses several broad types of work:
   * Modifying existing software and creating some new software to support the project technical design
   * Deploying new OSG-wide services to support the system
   * Deploying new data entry points and caches at key OSG sites
   * Monitoring the new system for operational status and usage metrics
   * Documenting the system for end users, site administrators, and GOC staff

---++ Scope

The goal of the project is limited by the following considerations:
   * Support will be for *input* data files, not output files;
   * Support is intended for *common* data files that are used by many jobs;
   * Files are expected to be up to 1 TB in total size and relatively few in number per job (&le; 10);
   * Files will be stored temporarily in OSG and removed as needed to make room for new demand
   * Once in the system, files are immutable – they will never grow or change
   * VOs are responsible for keeping their section of the distributed filesystem organized and consistent

---++ Risks

An initial list of possible risks to project success:

   * The technical design is not complete and may yet result in tasks that are more challenging and take longer than currently expected
   * Some technical work must be done by other groups (e.g., HTCondor, AAA) who have their own priorities and schedules
   * The plan calls for a new OSG service to run at the GOC (!XRootD redirector); the GOC has capacity but not infinitely so
   * The GOC must also update an existing production service (an HTCondor collector) to accept ads from !XRootD caches

---++ Links

   * [[https://indico.cern.ch/event/330212/session/6/contribution/23/material/slides/0.pdf][Frank Würthwein’s proposal slides]] from [[https://indico.cern.ch/event/330212/other-view?view=standard][the !XRootD Workshop at UCSD]] (29 January 2015)
   * [[https://indico.cern.ch/event/330212/session/6/contribution/31/material/slides/0.pdf][Anna Olson’s talk on UChicago’s StashCache]] from [[https://indico.cern.ch/event/330212/other-view?view=standard][the !XRootD Workshop at UCSD]] (29 January 2015)
   * https://confluence.grid.iu.edu/display/STAS/Installing+an+XRootD+server+for+Stash+Cache

---++ Milestones and Schedule

<!--
   * Set PENDING =  <div style="color: white; background-color: #AAA; padding-left: 1em; padding-right: 1em;">Waiting</div>
   * Set ACTIVE = <div style="color: white; background-color: #0A0; padding-left: 1em; padding-right: 1em;">In&nbsp;Progress</div>
   * Set BEHIND =   <div style="color: white; background-color: #F60; padding-left: 1em; padding-right: 1em;">Behind</div>
   * Set AT_RISK =  <div style="color: white; background-color: #C00; padding-left: 1em; padding-right: 1em;">At&nbsp;Risk</div>
   * Set SKIPPED =  <div style="color: white; background-color: #C00; padding-left: 1em; padding-right: 1em;">Skipped</div>
   * Set COMPLETE = <div style="color: white; background-color: #000; padding-left: 1em; padding-right: 1em;">Done</div>
-->

---+++ Beta implementation (aka working prototype)

The objective is to have an end-to-end implementation of all components, even if those components are still in testing or beta release form. Once in place, at least one brave user (FIFE?) will test the system and provide feedback on usefulness, functionality, etc.

%TABLE{ sort="off" valign="top" }%
| *Milestone* | *State* | *Who* | *Est. Start* | *Est. Finish* | *Act. Finish* | *Notes* |
| 1. Complete the technical design and initial task list | %ACTIVE% | !TimC, !BrianB | — | 2015-02-27 | | |
| |||||||
| 2. Software updates and additions | | !TimC (manage) |||||
| 2.1. Create !XRootD cache service to push information to central collector | | OSG Software | 2015-02-24 | | | Needs design |
| 2.2. Create HTCondor shim(s) so that Master can control !XRootD server | | OSG Software | | | | Needs design; beta in HTCondor 8.3 |
| 2.3. Modify (?) stashcp to use chirp to insert file transfer metadata into job !ClassAd | | OSG Connect (Anna) | | | | Needs design. |
| 2.4. Create HTCondor file-transfer plugin for =stash= URI type | | OSG Connect (Anna) | | | Doesn't need new HTCondor release - can put in gWMS scripts  |
| 2.5. Streamline the setup scripts that establish OSG runtime environment | | OSG Connect (Lincoln) | | | | Must coordinate with OSG-XD / Mats |
| 2.6. Review code of preload library | | AAA | | | | |
| 2.7. Prevent cache corruption issues | | AAA | | | | |
| |||||||
| 3. Deployment | | !RobQ (manage)? |||||
| 3.1. Deploy !XRootD redirector at GOC | | OSG Ops | | | |
| 3.2. Deploy !XRootD caches at UC, UCSD, IU, BU | | | | | |
| 3.3. Recruit more sites (BNL, UNL, UW, SLAC) | | Frank? | | | UNL done. |

---++ Notes

---+++ Meeting notes – 29 January 2015, UCSD

Attendees: Brian Bockelman, Tim Cartwright, Rob Gardner, Bo Jayatilaka, Rob Quick, Frank Würthwein

The notes below are mostly transcribed (with a few clarifications) from whiteboard notes of a project brainstorming session. The <a href="%ATTACHURLPATH%/xrootd-stash-whiteboard-20150129.png"/>original whiteboard photo</a> is attached for reference.

90 TB Stash with !XRootD at University of Chicago &rarr; Custodian A

Proxy caches:
   * University of Chicago, UCSD, Indiana University, Boston University
   * (BNL, UNL, UW, SLAC)

Top-level namespace [as agreed upon by all present]:
   * =/stash= – for OSG Connect users
   * =/osgxd= – for OSG-XD users
   * =/<em>VO-NAME</em>= – for VO members (where VO names are according to OIM)

Changes:
   * Add OSG redirector
   * Each cache has some HTCondor information to push info to collector at Indiana University:<br>(OSG Software team will write cron/Python to do this)
      * Total size of cache
      * Amount of data in cache
      * “Heartbeat” [= Timestamp of last report?]
      * Amount of data written today
      * &hellip;
   * HTCondor master runs the !XRootD server
   * Explicit HTCondor group within OSG VO pool
      * Business rule for !ClassAd attribute that declares “I want to use data federation”
   * Use chirp in combination with =stashcp= to put success/failure/performance [information] into !ClassAd
   * Plugin for HTCondor such that files from cache can be used as part of HTCondor file transfer
   * Streamline the setup scripts that establish OSG runtime environment
   * AAA to do a code review of preload library

Brian B. and Tim C. will take care of all of the above changes except for the first one (“Add OSG redirector”)

%META:FILEATTACHMENT{name="xrootd-stash-whiteboard-20150129.png" attachment="xrootd-stash-whiteboard-20150129.png" attr="h" comment="whiteboard notes" date="1422645280" path="xrootd-stash-whiteboard-20150129.png" size="6428604" stream="xrootd-stash-whiteboard-20150129.png" tmpFilename="/usr/tmp/CGItemp45916" user="TimCartwright" version="1"}%
