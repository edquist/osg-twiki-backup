%META:TOPICINFO{author="BrianBockelman" date="1423766879" format="1.1" reprev="1.4" version="1.4"}%
---+ Project 23: !StashCache – Distributing User Data With !XRootD

The goal of this project is to support all OSG users who have common input data files, with an initial goal of handling up to 1 TB datasets. Users will upload copies of their data to predefined entry points and will set up jobs to fetch data without having to understand implementation details. Internally, !XRootD will copy files from the entry points to OSG caches that are located at or near OSG sites where user jobs run.

---++ Description

---+++ Problem

%RED% Background from Brian %ENDCOLOR%. The two large LHC VOs, ATLAS and CMS, own storage at many sites on the OSG and utilize them as storage elements (remotely accessible file systems).  The SE concept does not work for opportunistic VOs - without a mechanism for automated eviction, files stay in the SE forever.  Without access to site SEs, opportunistic VOs cannot efficiently deliver files to their individual jobs.  The CMS and ATLAS systems for moving data between storage elements are robust and efficient, but have proven impossible for any other VO to run.

Users who are not affiliated with ATLAS and CMS have fewer choices for distributing data files, especially as dataset size increases.  Most current relying on HTCondor file transfer from submit to execute host, using on-site HTTP caches, or OASIS.  All three mechanisms effectively limit the size of data files to less than 1GB.  In 2014, over 30% of OSG cycles went to non-HEP users, and anecdotally at least OSG knows that some of those users have common input datasets that exceed the best-practice limits for available methods.

The general problem is quite complex; we choose to focus on the "common input data".  We assume the user is running at least 1,000 jobs (lasting 1 hour each) across a common input dataset of at least 1GB.

---+++ Deliverables

%RED% From Brian %ENDCOLOR%.
   1 Deploy a data federation service such that any VO can export their central filesystem and have it be accessible to clients.
   1 Develop and deploy a series of caches such that a running job can access the data federation through a "nearby" cache.  These caches should be centrally monitored and managed.
   1 Develop user-friendly tools so jobs can fetch data without understanding the implementation details.  Three access methods are needed:
      * A "cp"-like utility.
      * HTCondor file transfer plugins.
      * POSIX-based access.
   1 Deploy tools so central operations can understand usage and failures from the service.

---+++ Scope

The goal of the project is limited by the following considerations:
   * Support will be for *input* data files, not output files;
   * Support is intended for *common* data files that are used by many jobs;
   * Files are expected to be up to 1 TB in total size and relatively few in number per job (&le; 10);
   * Files will be stored temporarily in OSG and removed as needed to make room for new demand

---+++ Major Objectives

This project encompasses several broad types of work:
   * Modifying existing software and creating some new software to support the project technical design
   * Deploying new OSG-wide services to support the system
   * Deploying new data entry points and caches at key OSG sites
   * Monitoring the new system for operational status and usage metrics
   * Documenting the system for end users, site administrators, and GOC staff

---+++ Risks

An initial list of possible risks to project success:

   * The technical design is not complete and may yet result in tasks that are more challenging and take longer than currently expected
   * Some technical work must be done by other groups (e.g., HTCondor, AAA) who have their own priorities and schedules
   * The plan calls for a new OSG service to run at the GOC (!XRootD redirector); the GOC has capacity but not infinitely so. (%RED% Note: central collector is already production; we would just change this service to accept ads from Xrootd caches.%ENDCOLOR%)

---+++ Links

   * [[https://indico.cern.ch/event/330212/session/6/contribution/23/material/slides/0.pdf][Frank Würthwein’s proposal slides]] from [[https://indico.cern.ch/event/330212/other-view?view=standard][the !XRootD Workshop at UCSD]] (29 January 2015)
   * [[https://indico.cern.ch/event/330212/session/6/contribution/31/material/slides/0.pdf][Anna Olson’s talk on UChicago’s StashCache]] from [[https://indico.cern.ch/event/330212/other-view?view=standard][the !XRootD Workshop at UCSD]] (29 January 2015)

---++ Milestones and Schedule

<!--
   * Set PENDING =  <div style="color: white; background-color: #AAA; padding-left: 1em; padding-right: 1em;">Waiting</div>
   * Set ACTIVE = <div style="color: white; background-color: #0A0; padding-left: 1em; padding-right: 1em;">In&nbsp;Progress</div>
   * Set BEHIND =   <div style="color: white; background-color: #F60; padding-left: 1em; padding-right: 1em;">Behind</div>
   * Set AT_RISK =  <div style="color: white; background-color: #C00; padding-left: 1em; padding-right: 1em;">At&nbsp;Risk</div>
   * Set SKIPPED =  <div style="color: white; background-color: #C00; padding-left: 1em; padding-right: 1em;">Skipped</div>
   * Set COMPLETE = <div style="color: white; background-color: #000; padding-left: 1em; padding-right: 1em;">Done</div>
-->

---+++ Beta implementation (aka working prototype)

The objective is to have an end-to-end implementation of all components, even if those components are still in testing or beta release form. Once in place, at least one brave user (FIFE?) will test the system and provide feedback on usefulness, functionality, etc.

%TABLE{ sort="off" valign="top" }%
| *Milestone* | *State* | *Who* | *Est. Start* | *Est. Finish* | *Act. Finish* | *Notes* |
| 1. Complete the technical design and initial task list | %ACTIVE% | !TimC, !BrianB | — | 2015-02-27 | | |
| |||||||
| 2. Software updates and additions | | !TimC (manage) |||||
| 2.1. Create !XRootD cache service to push information to central collector | | OSG Software | 2015-02-24 | | | Needs design |
| 2.2. Create HTCondor shim(s) so that Master can control !XRootD server | | HTCondor, OSG Software | | | | Needs design; beta in HTCondor 8.3 |
| 2.3. Modify (?) stashcp to use chirp to insert file transfer metadata into job !ClassAd | | | | | | Needs design |
| 2.4. Create HTCondor file-transfer plugin for =stash= URI type | | HTCondor? | | | Can/should OSG Software do this?; beta in HTCondor 8.3 |
| 2.5. Streamline the setup scripts that establish OSG runtime environment | | | | | | |
| 2.6. Review code of preload library | | AAA | | | | |
| 2.7. Prevent cache corruption issues | | AAA | | | | |
| |||||||
| 3. Deployment | | !RobQ (manage)? |||||
| 3.1. Deploy !XRootD redirector at GOC | | | | | |
| 3.2. Deploy !XRootD caches at UC, UCSD, IU, BU | | | | | |
| 3.3. Recruit more sites (BNL, UNL, UW, SLAC) | | Frank? | | | |

---++ Notes

---+++ Meeting notes – 29 January 2015, UCSD

Attendees: Brian Bockelman, Tim Cartwright, Rob Gardner, Bo Jayatilaka, Rob Quick, Frank Würthwein

The notes below are mostly transcribed (with a few clarifications) from whiteboard notes of a project brainstorming session. The <a href="%ATTACHURLPATH%/xrootd-stash-whiteboard-20150129.png"/>original whiteboard photo</a> is attached for reference.

90 TB Stash with !XRootD at University of Chicago &rarr; Custodian A

Proxy caches:
   * University of Chicago, UCSD, Indiana University, Boston University
   * (BNL, UNL, UW, SLAC)

Top-level namespace [as agreed upon by all present]:
   * =/stash= – for OSG Connect users
   * =/osgxd= – for OSG-XD users
   * =/<em>VO-NAME</em>= – for VO members (where VO names are according to OIM)

Changes:
   * Add OSG redirector
   * Each cache has some HTCondor information to push info to collector at Indiana University:<br>(OSG Software team will write cron/Python to do this)
      * Total size of cache
      * Amount of data in cache
      * “Heartbeat” [= Timestamp of last report?]
      * Amount of data written today
      * &hellip;
   * HTCondor master runs the !XRootD server
   * Explicit HTCondor group within OSG VO pool
      * Business rule for !ClassAd attribute that declares “I want to use data federation”
   * Use chirp in combination with =stashcp= to put success/failure/performance [information] into !ClassAd
   * Plugin for HTCondor such that files from cache can be used as part of HTCondor file transfer
   * Streamline the setup scripts that establish OSG runtime environment
   * AAA to do a code review of preload library

Brian B. and Tim C. will take care of all of the above changes except for the first one (“Add OSG redirector”)

%META:FILEATTACHMENT{name="xrootd-stash-whiteboard-20150129.png" attachment="xrootd-stash-whiteboard-20150129.png" attr="h" comment="whiteboard notes" date="1422645280" path="xrootd-stash-whiteboard-20150129.png" size="6428604" stream="xrootd-stash-whiteboard-20150129.png" tmpFilename="/usr/tmp/CGItemp45916" user="TimCartwright" version="1"}%
