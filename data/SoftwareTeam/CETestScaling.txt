%META:TOPICINFO{author="EdgarFajardo" date="1415386147" format="1.1" reprev="1.2" version="1.2"}%
%TOC%
---+ Large Scale Testing of a Computing Element

---++ Introduction
==TO BE REVIEWED==
This document is intended as a general overview of the process involving the large scale testing of a Computing Element Software (CE). The examples will be given for the current scaling of the HTCondor CE, however they should be applicable for other CE Software.

---++ Sleeper Pool
==TO BE REVIEWED==
---+++ Objective of a Sleeper Pool
Be able to run "almost " resourceless (CPU, IO) jobs parallel to real production jobs.
---+++ Why a sleeper pool?
When large scale testing a CE one of the objectives is to see if new software can fully utilize all resources (cores) behind it or the bottlenecks preventing it. However to this would normally require to use those production slots. However running resourceless jobs in parallel to production jobs allows for the testing without interfering with real work.
---+++ How to setup a sleeper pool?
A sleeper pool is set up "tricking" a worker node to actually think it has more cores than it physically has. Then a configuration is done so  that jobs marked for a "sleep pool" are routed to those "sleep slots". In HTCondor it is done via tricking the Startd expression, and example of such an expression in a 32 core machien is:

<verbatim>
START = ( \
          (SlotID >= 1) && \
          (SlotID < 33) && \
          (RequiresWholeMachine =!= TRUE ) && \
          (SleepSlot =!= TRUE) && \
          (distro =?= "RHEL6" ) && \
          (CPU_Only == TRUE ) \
          ) || \
          ( (SlotID >= 33) && (distro =?= "RHEL6" ) && (SleepSlot == TRUE) )
</verbatim>

---++ Usual Topology of the tests
==TO BE REVIEWED==
A brief introduction to the topology involved in the tests
---+++ Batch System and Sleeper Pool
This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site Admin.
---+++ CE
This is the physical hardware where the CE Software will sit, hopefully mimicking real production hardware specifications. 
---+++ Submitter
An HTCondor Schedd. It can be a Virtual machine for most of the condor-g grid test submissions.
---++ Monitoring tools
==TO BE REVIEWED==
In order to do the monitoring two pieces are needed (which can be isntalled in the same node): ganglia-gmond and ganglia-gmetad. Once this is set up then some ad-hoc metrics can be created to monitor the CE, for example:

<verbatim>
condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const 'JobStatus=?=2'| wc -l
gmetric --name RunningJobsCE 
</verbatim>
---++ Load creator tools
==TO BE REVIEWED==
---+++ Location
The load_generators are found in Sourceforge [[http://sourceforge.net/projects/osgscal/][SourceForge OSgscal]] as part of the OSG Scalability repo. The one of interest here is loadtest_condor
---+++ Use
Just untar it or check it out from trunk in the HTCondor Submitter:
<verbatim>
svn checkout svn://svn.code.sf.net/p/osgscal/code/ osgscal-code
</verbatim>
Bear in mind you also need a valid proxy for grid submissions.

If for example to keep 1k jobs in the queue and send 6 hours jobs (in average), you can run this command:

<verbatim>
./loadtest_condor/bin/loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50
</verbatim>



-- Main.EdgarFajardo - 06 Nov 2014
