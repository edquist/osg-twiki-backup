%META:TOPICINFO{author="MatyasSelmeci" date="1456509910" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="InternalDocs"}%
%TOC%

---+ Switching Software Area (/software) to !GitHub


This document details the plan for migrating the projects in the VDT SVN
software area to Git repositories located primarily on !GitHub.  This document
supersedes [[SoftwareTeam.GitMigrationProposal]].



---++ Repo characteristics


This plan involves creating two repositories per software project: a "GitHub
repository" and an "AFS repository". Their characteristics are as follows:

   * An AFS repository is the "one true copy" of the project repository.\
      It has the following properties:
      * If there is a conflict between an AFS repository and *any* other repository (including the !GitHub repository), the AFS repository "wins".
      * If the AFS repository goes offline or becomes inaccessible, it is considered a fire and should be fixed within two business days.

   * A !GitHub repository is the project repository that developers will use for their daily work.\
      It has the following properties:
      * It is easy to access and update (for authorized users).
      * Developers publish their changes to the !GitHub repository.
      * If the !GitHub repository goes offline or becomes inaccessible, it is considered a major hassle, but not a fire.
      * The AFS repository pulls data from the !GitHub repository.


---++ Objectives


   * The OSG (or CHTC) must own and fully manage the operations of the AFS Git repositories.
   * We must have dependable backups of the repositories.
   * Anyone should be able to submit a contribution.
   * The OSG Technology team must have sole write privileges on the AFS repositories.
   * Team members must be able to easily review and approve submitted contributions.
   * The workflows for both contributors and team members must be clearly documented.\
     Common use cases should be efficient and easy to learn.
   * The migration process must preserve existing data, should interrupt work little, and should be reversible.
   * The AFS repositories should be kept up-to-date with the !GitHub repositories.
   * Anyone should be able to read the !GitHub repositories, including histories.
   * We can do releases when the !GitHub repositories are down.


---++ High-Level Architecture


Developers use the !GitHub repos for day to day work, using the typical !GitHub
workflow: fork, commit, make pull requests. Repos are frequently synced to AFS
to create a backup in case something goes wrong.

---+++ AFS repos

The authoritative repos will be on the UW's AFS.  We'll use the UW CSL's
Kerberos for authentication and access control.  In the normal case, team
members do not touch or clone from the AFS repositories.  If a
!GitHub repo becomes unusable for some reason (outage, corruption, etc.),
trusted developers (i.e. those with the right AFS ACLs) will push directly to
the AFS repo until the problem is resolved.

---+++ !GitHub repos

The repos that developers use for day-to-day work will be on !GitHub.  They
will be owned by the "opensciencegrid" organization.  Team members will need to
have !GitHub accounts, and have write access granted via !GitHub's access
control mechanisms in order to approve contributions.  The !GitHub repos will
be configured to reject "unsafe" commits (commits that alter history, cause
conflicts, or otherwise cause breakage or potential loss of information).

---+++ Synchronization

Each AFS repo will periodically pull from the corresponding !GitHub repo.  The
properties of the AFS repo will be set so that it only accepts changes that are
"safe".

If an "unsafe" update is attempted, the developers will be notified via email.
They will have to resolve the conflict and push the changes back to !GitHub.

---+++ Backups

The AFS repos will live on an AFS volume that is backed up nightly by the CSL.
We will make a "snapshot" of the repo before each backup runs, to ensure
consistency.  This process is modeled after what we do for the VDT SVN repos.

---+++ Contributions

External collaborators or team members will fork one of the !GitHub repos and
submit their changes as pull requests; a team member will review and merge the
pull requests to incorporate the changes.


---++ Detailed Architecture


---+++ AFS repos

The AFS repo for a software project will live under
<code>/p/condor/workspaces/vdt/git/software/<em>PROJECT</em>.git</code>
Once the !GitHub repo for a project has been created, making the corresponding
AFS repo is as follows:
<pre>
PROJECT=&lt;your project&gt;

cd /p/condor/workspaces/vdt/git/software/
git clone --mirror https://github.com/opensciencegrid/$PROJECT.git
cd $PROJECT.git/

# Check integrity of each object fetched:
git config transfer.fsckObjects true

# Enable the reflog (helps debugging):
git config core.logAllRefUpdates true

# Only allow non-fast-forward updates for pull requests:
git config remote.origin.fetch '+refs/pull/*:refs/pull/*'
git config --add remote.origin.fetch 'refs/*:refs/*'

# Allow pushing only one branch/tag at a time back to GitHub.
# Pushing all branches/tags can be done by explicitly specifying
# --mirror to "git push"
git config --unset remote.origin.mirror
</pre>

The AFS ACLs for each repo will be copied from
<code>/p/condor/workspaces/vdt/git</code>.

---+++ !GitHub repos

The !GitHub repo for a project will be owned by the Open Science Grid !GitHub
organization (username "opensciencegrid"). The URI for the project will be:
<code>https://github.com/opensciencegrid/<em>PROJECT</em></code>

We will enable
[[https://help.github.com/articles/about-protected-branches/][protected branches]]
for all branches of the project, to prevent non-fast-forward pushes.

---++++ Access control

!GitHub repos have three kinds of permissions:

   $ Read: the ability to see the contents of the repository, fork the repo,
     and submit a pull request.
   $ Write: the ability to accept pull requests or push to the repository; implies read access.
   $ Admin: the ability to grant or revoke privileges, and the ability to make changes that affect the entire repo, such as transferring ownership, or deleting the repo; implies read and write access.

Read access is given to everyone, because !GitHub repos are public unless we
pay !GitHub money.

Write access will be given to the
[[https://github.com/orgs/opensciencegrid/teams/software][Software team]] and
the primary contributors of the software if they are not in the Software team.

Admin access will be given to the
[[https://github.com/orgs/opensciencegrid/teams/core][Core team]].

The
[[https://github.com/orgs/opensciencegrid/people?utf8=%E2%9C%93&query=role%3Aowner+][owners of the Open Science Grid organization]]
control team membership.

---+++ Synchronization

---++++ Fetcher cron job

   * There will be one "fetcher" cron job, owned by =cndrutil=, to download
     !GitHub repo changes to the AFS repos by running =git fetch= for each repo.
   * The cron job will run every 10 minutes.  This is often enough to keep the
     AFS repos in sync most of the time, but should avoid placing too much load
     on the machine doing the syncing.
   * Any nonzero exit codes will result in an email to Madison people.
   * The list of repos will be taken from a file called =/p/condor/workspaces/vdt/git/remotes.list=.
   * Results will be logged to =/p/condor/workspaces/vdt/git/backups.log=. The log will be rotated.
   * After a successful fetch, update a file called =fetchstats= containing:
      * the timestamp of the last successful fetch, in human-readable format
      * a count of the number of successful fetches
   * A team member can create a file named =NO_FETCH= in an AFS repo to disable
     fetches for that repo, in case there is a persistent error that needs manual
     intervention to solve.

---++++ Watchdog cron job

   * There will be one "watchdog" cron job that will run nightly to examine the AFS
     repos and send email to the Madison people about:
      * any =NO_FETCH= files
      * last update time for each repo (mismatches should be investigated)
      * number of successful fetches since last report (unexpectedly low numbers
        should be investigated)
   * After successfully sending an email, the watchdog should log the count of
     successful fetches from each =fetchstats= file, and reset that number to
     zero.

---+++ Nightly AFS Backups

A single cron job will go through each AFS repo dir <em>PROJECT</em>.git, and do this:

<pre>
PROJECT=&lt;your project&gt;

rm -rf "/p/condor/workspaces/git-backup/${PROJECT}.git.tmp"
git clone --mirror "/p/condor/workspaces/git/${PROJECT}.git" \
                   "/p/condor/workspaces/git-backup/${PROJECT}.git.tmp"
rm -rf "/p/condor/workspaces/git-backup/${PROJECT}.git"
mv "/p/condor/workspaces/git-backup/${PROJECT}.git.tmp" \
   "/p/condor/workspaces/git-backup/${PROJECT}.git"
</pre>

In the unlikely event of an AFS repo becoming unrepairable <em>and</em> we are
unable to make a new clone from !GitHub, this is how we restore:

<pre>
PROJECT=&lt;your project&gt;

rm -rf "/p/condor/workspaces/git/${PROJECT}.git"
git clone --mirror "/p/condor/workspaces/git-backup/${PROJECT}.git" \
                   "/p/condor/workspaces/git/${PROJECT}.git"
cd "/p/condor/workspaces/git/${PROJECT}.git"
# restore "config" from an AFS backup
</pre>

---++ Migration

---+++ One-time setup

   1. Create AFS volume for =/p/condor/workspaces/vdt/git= (done)
   1. Create authors file to map SVN user names to email addresses for Git
      (there is one at "/p/condor/workspaces/vdt/svn-access/authors.txt" --
      make sure it's up-to-date)
   1. Create "backup" cron job, run by =cndrutil= (see "Backups" section)
   1. Create "fetch" and "watchdog" cron jobs, run by =cndrutil= (see
      "Synchronization" section)

---+++ Per-project migration

One of the !GitHub migration owners (read: Main.MatyasSelmeci and
Main.CarlEdquist) will convert the project from SVN to Git.

Overall steps are:

   1. Schedule a switchover date with the owner.
   1. Create the !GitHub repo under the "opensciencegrid" organization; do not
      enable protected branches yet.
   1. Do the conversion: create a new Git repo based on the data from SVN using the script https://vdt.cs.wisc.edu/svn/software/tools/git-svn-bare-setup.
      1. <code>./git-svn-bare-setup <em>PROJECT</em></code>
      1. <code>cd <em>PROJECT</em>.git</code>
      1. Inspect the repo (eg, with <code>gitk --all</code>) and perform any manual tidying, if appropriate.
      1. =git push github=
   1. Enable protected branches for the repo in !GitHub.
<!--
   1. Use =subgit import= (it's nicer than =git-svn=) to create a new Git repo
      based on the data from SVN.
-->
   1. Add the repo to =/p/condor/workspaces/vdt/git/remotes.list=.
   1. Wait for the fetcher script to run.
   1. Make a file in the trunk of the SVN dir called =MOVED-TO-GIT= that
      contains the following text:<pre>
The source code for this project is now contained in GitHub at opensciencegrid/<b>PROJECT</b>
Changes committed to SVN will be ignored.</pre>
   1. =svn lock= the project in SVN.

%RED%If anything fails, either debug, _carefully recording all steps_ or just try
again.%ENDCOLOR%


---++ Safety


From Main.CarlEdquist:
<verbatim>
What kind of vulnerabilities are we concerned with?

1. "corruption" -- anything that destroys history; specifically, the ability
to do fast-forward fetches.

2. "garbage" -- an unauthorized or rogue user (or a rogue github) pushes
unwanted commits that do not destroy history.  ("Sufficiently-fat-fingered"
can also be considered rogue.)
</verbatim>

3. "destruction" -- someone hacks into github and pushes the "delete" button on
   a repo.

---+++ Preventing "corruption"

From Main.CarlEdquist:

<verbatim>
- If "corruption" happens on github, the automated fetch to the AFS repos
  will fail, and we'll be notified immediately.  Since non-fast-forward
pushes will (should) be disabled on github, this would be an indication that
somebody's not playing nice, and we may want to investigate that.

In any case, the AFS repos won't be changed, so we can force-push everything
back to github to recover.  (Temporarily re-enabling force-pushes for this
recovery.)

On the developer end, corrupted updates from the github upstream get
rejected by pulls.  New checkouts of the corrupted github repos (before our
recovery) will of course contain the corruption, so they can be discarded as
we'll direct everyone in an email.
</verbatim>

---+++ Preventing "garbage"

From Main.CarlEdquist:

<verbatim>
- If "garbage" commits make their way into the github repos; first someone
will have to notice.  If someone is trying to be sneaky and sneak something
in, it may not be obvious.  (The case seems about the same as for the UW SVN
repo.)  There are ways to protect against it (namely having everyone sign
all their own commits, and enforcing checking for this), but I suspect
that's more than we really want.

But if a bad commit is discovered (aha! gotcha!), and it's offensive enough
that we want to remove it from our repo entirely (instead of just adding a
further commit to revert it), we can 'git reset' the AFS repos to just
before the offending commit, optionally run 'git gc' to remove any offending
blobs from the object store, force-push back to github, and email everybody.

As a special case, if a 4TB garbage commit makes it to the github repos
(assuming github itself doesn't reject it!), then the automated fetches to
AFS would fail, exhausting the vdt disk quota on AFS.  In this case, the AFS
repos will not be in a corrupted state, and the garbage commit will not be
included in the repo, but a (large) incomplete temporary file will be left
behind (with an obvious name like objects/02/tmp_obj_YCIXgK).

We will have to manually delete such a temp file, and then force-push the
(unchanged) AFS repo back to github.
</verbatim>

---+++ Responding to !GitHub outages

From Main.CarlEdquist:

<verbatim>
There are two cases to consider.

1. Temporary outages.  (Kind of like when JIRA goes down  :)

2. Severe or permanent outages.  (When it's too long to tolerate...)


For temporary outages, developers can continue to commit to their local
repos, and do whatever development they want.  This includes making new tags
and cutting release tarballs to use as new upstream sources for builds.
Anyone can also clone or update from the official ones in AFS in the mean
time -- though as long as the outage is considered temporary, no one should
push back to AFS, but wait until it becomes possible to push back to github
again.

For severe or permanent outages (which someone with Say can determine), then
everyone can just switch their upstream (remote) back to AFS.

Something like:

    upstream=/p/condor/workspaces/vdt/git/software/<project>.git
    git remote set-url origin $upstream

for checkouts with direct AFS access; or for access via ssh:

    upstream=/p/condor/workspaces/vdt/git/software/<project>.git
    cshost=library.cs.wisc.edu
    git remote set-url origin $cshost:$upstream

Everyone then gets to play nice (no pull requests) and push directly to the
AFS location (like they do in FW's CONDOR_SRC git).  If someday github comes
back online, we should be able to just fast-forward push the updates to
github, and tell everybody to switch back.  (But, it's important that they
listen!)
</verbatim>

---+++ Recovering from repo destruction

Destruction of a !GitHub repo can be handled the same way as a permanent
outage, since once the repo is destroyed, fetches from it will fail. While we
wait for !GitHub to be usable again, we switch the location of the upstream Git
repo to the AFS repo, and work as above.

---+++ Preventing tag corruption

From Main.CarlEdquist:

<verbatim>
[2] Removing the '+' in the fetch refspec prevents non-fast-forward updates,
but tag updates are actually a separate issue.  Normally for a checkout, if
an upstream tag changes, a pull will not update the local tag.  Also, a
(non-force) push will fail for an updated tag (that is, it will refuse to
update a tag reference on a remote.)  But in our case, we have a bare repo
(not a working copy/checkout) and we do fetches, not pulls or pushes.  To my
horror, I have found no way to prevent tag updates via fetch to a bare repo.
If tags are included in your fetch, tag updates will come also.

(Again, tag updates will not get pulled into working copy/checkouts that
have the original tag, but the main issue is how to protect the tags in the
AFS repos from upstream corruption.)

I would very much welcome any suggestions on how to handle this potential
issue.  (@bb?)  So far, I have come up with two ways to protect against
this, both of which I consider a little ugly.

1. fetch to an intermediate bare repo, and then push to the main bare repo.
The fetch will update the tag, but the push will fail.  This has the
advantage of being able to detect easily when tag updates happen (which
should be forbidden, so we do want to know).  The downside is this approach
uses an extra staging repo.  (Then again, we could use that as an extra
backup...)

2. use a non-standard fetch refspec for tags; for example:

        fetch = refs/heads/*:refs/heads/*
        fetch = refs/tags/*:refs/tag-stage/*

Instead of fetching everything ('refs/*'), single out heads and tags, but
fetch tags to a non-standard location.  As a side effect, new tags will get
created in the standard location (under refs/tags), along with
refs/tag-stage, but tag updates will only make it to tag-stage, and will
(silently) not update the tags under refs/tags.  This appears to work, and
has the advantage of not needing a separate staging repo.  One disadvantage
is it does not automatically bring in 'other' types of refs (eg, github
'pull' refs), although if we really want them, we can list them explicitly
in the fetch list.  Also, this is a little less obvious to detect; there are
no failures, though if you parse the command output you can detect trouble
any time there is a tag-stage update, eg:

   8c1bcea..cec9488  v1.2.3         -> refs/tag-stage/v1.2.3

Also worrisome is that the semantics here don't seem to be clearly
documented... It would make me feel a little better if we could point to
something that says yes it's supposed to work that way.  But in any case,
I've confirmed it works in both git 1.7.12 and 2.5.0.  (So, the behavior
seems stable...)

---

Alternatively, if we are OK with just being informed of tag updates (instead
of preventing them), we can add reflogs for all tags, which under normal
circumstances will remain empty.  Making reflogs for tags automatically is
not really convenient in git; for some reason core.logAllRefUpdates works
for refs/heads but not refs/tags (which at least is documented), and
'git tag --create-reflog' only seems to create a reflog for a new tag.

We can add something like the following to the fetch script, after the fetch
command runs, to create reflogs for all new tags, and detect tag updates
from existing ones.

    # from $GIT_DIR
    for tag in $(git tag); do
        taglog=logs/refs/tags/$tag
        tagdir=${taglog%/*}
        [[ -d $tagdir ]] || mkdir -p $tagdir
        [[ -e $taglog ]] || touch $taglog
        if [[ -s $taglog ]]; then
            echo "Warning: tag $tag has a history..."
            git reflog refs/tags/$tag
            echo
        fi >&2
    done

</verbatim>

A third option is to have developers sign tags with their GPG keys.  That might
not prevent tag corruption, but it would let us know if it had taken place.


-- Main.MatyasSelmeci - 12 Feb 2016