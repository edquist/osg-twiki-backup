%META:TOPICINFO{author="AlainRoy" date="1331314626" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="Projects"}%
<!--
   * Set NOT_STARTED = Not Started
   * Set NOT_RELEASED = Not Released
   * Set ON_TRACK = <div style="background-color: #3366FF; color:white">&nbsp;On Track&nbsp;</div>
   * Set BEHIND = <div style="background-color: #CCFF00;">&nbsp;Behind&nbsp;</div>
   * Set AT_RISK = <div style="background-color: #CC0000; color:white">&nbsp;At risk&nbsp;</div>
   * Set ACHIEVED = <div style="background-color: #00FF66;">&nbsp;Achieved&nbsp;</div>
   * Set RELEASED = <div style="background-color: #00FF66;">&nbsp;Released&nbsp;</div>
   * Set TWISTY_OPTS_DETAILS = mode="div" showlink="Details" hidelink="Hide" showimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" hideimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" remember="on" start="hide" 
-->

---+!! Plan for Usability Testing of BOSCO

---# What is usability?

From [1]:

<blockquote>
Usability is not a quality that exists in any real of absolute sense. Perhaps it can be best summed up as being a general quality of the <i>appropriateness to a purpose</i> of any particular artefact.
</blockquote>

---# Limits to current phase of usability testing

Thorough usability testing is an art and a science. Given the short time-frame and lack of in-house experience, we are going to limit the scope of usability testing. Good usability testing should include a fair number of candidate subjects, multiple rounds of testing, etc. Our goal is to gain insight into the usability we have in a cost-effective way.

---# Inputs for the usability testing

Our usability testing requires a few things to be in place before it can begin:

   1. The software must be ready for usability testing.
   1. An initial round of beta testing should be complete. The goal of usability testing is not to find bugs (though we welcome bug reports) but to understand the usability. Therefore basic bug fixing should be complete before usability testing begins. 
   1. There must be sufficient documentation for the user to be able to do all of the tasks required for the usability testing. 
   1. Dan will work with the team to identify a set a couple of testers for usability testing. These should not be the same as the beta testers. 

---# Plan

---## Target audience

Our target audience is composed of scientists and researchers who:
   * Have access to a cluster of computers using PBSPro or Torque. 
   * Have need for high-throughput computing
   * Are comfortable with the basics of using Linux and the command-line. 

---## Task

We will ask our testers to spend about one hour doing the following two tasks:
   1. Install Bosco and connect it to their PBS/Torque cluster.
   1. Run one job via Bosco.

The tester will be given:
   * [[https://twiki.grid.iu.edu/bin/view/CampusGrids/BoSCO][Instructions on how to install Bosco]]
   * A brief introduction to what Bosco is and a chance to ask questions.

The test is expected to have:
   * An appropriate computer on which to install Bosco
   * Access to a remote cluster (currently PBS) to use with Bosco.

---## Usability testing
Our usability testing will consist of three parts. All three are desirable, but due to time constraints we may only be able to do the second and third parts.

---### Observation (for v1 and beyond)

Have an OSG staff member (preferably a Bosco team member) watch the tester complete the task. The staff member will not interfere with the task. Specifically, we will not provide any guidance or help. We will take extensive notes on what happened. In particular, we will note what parts of the task were hard or confusing and whether the users used the recommended workflow. <br>This task will require someone to be present. In future usability testing, we may find appropriate remote collaboration testing to do this remotely. 

Questions that the OSG staff member will try to answer are:

   1. Did the tester follow the documentation and process linearly? If they jumped around, what did they do?
   1. What parts of the process made the tester get stuck?
   1. What unexpected errors did the users find? How did they deal with them?
   1. Did the tester show signs of frustration? 
   1. Did the tester run any jobs? If not, why not?

---### Survey: System Usability Scale (for v1 and beyond)
We will ask the tester to fill out the System Usability Scale survey (below) and mail it to Alain. This survey has been widely used ([1],  [2]) and acccording to [2], it's been well studied and people have found it to consistently be a reliable measure of usability. It gives a single number (0 - 100) that summarizes the usability of the product. It's particularly useful when we iterate because we can see if we are improving. 

Each question is answered on a scale from 1 to 5 where 1 means "Strongly disagree" and 5 means "Strongly agree". If they are unsure, they should select the middle (3). 

   1. I think that I would like to use this system frequently	
   1. I found the system unnecessarily complex
   1. I thought the system was easy to use                      	
   1. I think that I would need the support of a technical person to be able to use this system	
   1. I found the various functions in this system were well integrated
   1. I thought there was too much inconsistency in this system
   1. I would imagine that most people would learn to use this system very quickly			
   1. I found the system very cumbersome to use
   1. I felt very confident using the system
   1. I needed to learn a lot of things before I could get going with this system 	

Scoring, from [1]:

<blockquote>
SUS yields a single number representing a composite measure of the overall usability of the system being studied. Note that scores for individual items are not meaningful on their own.

To calculate the SUS score, first sum the score contributions from each item. Each item's score contribution will range from 0 to 4. For items 1,3,5,7,and 9 the score contribution is the scale position minus 1. For items 2,4,6,8 and 10, the contribution is 5 minus the scale position. Multiply the sum of the scores by 2.5 to obtain the overall value of SU. 

SUS scores have a range of 0 to 100.
</blockquote>

---### Survey: Detailed comments
In addition to the numerical System Usability Scale, we'll ask the tester for specific feedback. 

   1. Did you find the documentation clear? (Do you understand what Bosco does?)
   1. Did you have any difficulty with the installation and configuration? If so, what?
   1. Did you successfully run any jobs? If not, what happened?
   1. Is this software you would like to use? Will it help you? If not, why not?

---## Timeline: External dependencies
%TABLE{ sort="off" valign="top" }%
| *Task* | *State* | *Owner* | *Target Start* | *Target Finish* | *Actual Finish* | *Notes* |
| BOSCO v0 complete | %ON_TRACK% | Fraser/Gore | - | 27-Feb-2012 | |  |
| Documentation/web site complete | %ON_TRACK% | Mambelli/Fraser | - | 27-Feb-2012 | | |
| Beta testing complete | %NOT_STARTED% | Fraser/Gore | - | 5-Mar-2012 | | |
| Find usability testers | %NOT_STARTED% | Fraser | 20-Feb-2012 | 5-Mar-2012 | | |

----## Timeline: Usability testing
%TABLE{ sort="off" valign="top" }%
| *Task* | *State* | *Owner* | *Target Start* | *Target Finish* | *Actual Finish* | *Notes* |
| Determine impact of Human Subjects Research | %ACHIEVED% | Alain | 16-Feb-2012 | 24-Feb-2012 | | |
| Update plan to reflect Human Subjects Research | %ACHIEVED% | Alain | 24-Feb-2012 | 28-Feb-2012 | | | 
| Work with tester #1 and #2 | %NOT_STARTED% | Alain | 5-Mar-2012 | 9-Mar-2012 | | |
| Work with testers #3 and #4 | %NOT_STARTED% | Alain | 12-Mar-2012 | 16-Mar-2012 | | |
| Write up usability testing results | %NOT_STARTED% | Alain | 16-Mar-2012 | 19-Mar-2012 | | | 

---# Appendices

---## Human Subjects Research

Miron advised Alain to check out rules governing Human Subjects Research, and ensure that we follow appropriate guidelines. 

After some research, he communicated with the directory of the relevant Institutional Research Board at the UW-Madison, who said:

<blockquote>
If these people are only commenting on the software, it is evaluation, not human subjects research.  If you are getting details that are personal or identifiable-- it becomes research.

To avoid moving into the research realm-- your questions should focus on  a "reporting" nature (tell me what you see)  rather than, for example, "Compare this to another software and give me your opinion."

Its a gray area, but so far, what you have described is not human subjects research.  It involves human subjects-- but what you are doing so far is not human subjects research.
</blockquote>

So if we are careful not to collect personal information, we will be fine. 

Some other background:

   * [[http://www.grad.wisc.edu/research/policyrp/rcr/humansubjects.html][UW information on Human Subjects Research]]
   * [[http://my.gradsch.wisc.edu/hrpp/10018.htm][Defining _Human Subjects Research:_]]
   * [[http://www.grad.wisc.edu/research/hrpp/submissioninstructions.html][Step by Step Instructions for Obtaining IRB Approval for Human Subjects Research]]

---## References

[1] Brooke, J. (1996). "SUS: a "quick and dirty" usability scale". In P. W. Jordan, B. Thomas, B. A. Weerdmeester, & A. L. McClelland. Usability Evaluation in Industry. London: Taylor and Francis. [[http://www.usabilitynet.org/trump/documents/Suschapt.doc][Word]]

[2] Lewis, J.R. & Sauro, J. (2009). The factor structure of the system usability scale. international conference (HCII 2009), San Diego CA, USA. [[http://www.measuringusability.com/papers/Lewis_Sauro_HCII2009.pdf][PDF]]
