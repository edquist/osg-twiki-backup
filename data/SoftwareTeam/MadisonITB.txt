%META:TOPICINFO{author="TimCartwright" date="1479240284" format="1.1" reprev="1.25" version="1.25"}%
%META:TOPICPARENT{name="InternalDocs"}%
---+ Software/Release Team ITB Site Design


---++ Madison ITB Machines

All physical hosts are located in 3370A in the VDT rack.

| *Host* | *Purpose* | *OS* | *Arch* | *CPU Model* | *CPUs* | *RAM* | *Storage* | *Notes* |
| itb-data1 | worker node | SL 6.3 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as HDFS data node |
| itb-data2 | worker node | SL 6.3 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as HDFS data node |
| itb-data3 | worker node | SL 5.8 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 0) | planned as HDFS data node |
| itb-data4 | worker node | SL 6.3 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as !XRootD data node |
| itb-data5 | worker node | SL 6.3 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as !XRootD data node |
| itb-data6 | worker node | SL 5.8 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 384 GB in /var/lib/condor/execute | planned as !XRootD data node |
| itb-host-1 | KVM host | SL 6.3 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 500 GB &times; 2 (?) | |
| &nbsp;&middot;&nbsp; itb-ce1 | HTCondor-CE | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-glidein | !GlideinWMS VO frontend? | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-gums-rsv | GUMS, RSV | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-hdfs-name1 | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-hdfs-name2 | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-se-hdfs | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-se-xrootd | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-submit | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-xrootd | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| itb-host-2 | worker node | SL 6.3 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 401 GB in /var/lib/condor/execute | |
| itb-host-3 | worker node | SL 7.1 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 50 GB in /; 384 GB in /home | |

(Data last updated 2016-11-15 by Tim C.)


---++ ITB Goals, Revisited 2016-11-03

Roughly in order of priority, at least for the top few items.

   * Test pre-release builds of HTCondor and HTCondor-CE in an OSG context
      * Install software and run jobs as soon as possible after a pre-release
      * An implementation idea: Maybe have more than one CE and/or HTCondor instance? One for &ldquo;production&rdquo; and one for rapid testing
   * Add the ability to create and control job pressure locally
      * Run all the time? Some of the time? Most of the time, but be able to drain or spike on demand?
      * Maybe add a VO front-end?
   * Review and significantly tighten firewall rules on ITB hosts
   * Add HTCondor-CE-Bosco to the overall site plan, so that we can test it, too
   * Test the OSG stack before release (i.e., updates out of osg-prerelease)
   * Have a mix of EL6 and EL7 execute hosts and user jobs to test EL&nbsp;6&rarr;7 migration
   * Test other batch systems
      * Slurm
      * PBS, in some flavor
      * LSF and SGE usage is waning in the field, so they are lowest priority
   * Add other types of hosts (e.g., VO frontend, storage, RSV, squid, etc.)
      * Current HDFS/XRootD nodes could be repurposed or we could spin up new VMs on =itb-host-1=
      * We could potentially run our own factory in the future
   * Investigate whether there is a way to switch easily between ITB and production pilots and payloads
   * Test large-customer specific workflows (e.g., LIGO)
   * Investigate monitoring/testing setup for site verification
      * Ask sites (e.g., UNL) for their solutions
   * Flexible site installation
      * Configuration management for production and/or base hosts (logins, ntpd, repo RPMs, etc.)
      * Determine upgrade procedure: do we upgrade the hosts in place with the option of reverting to production images or do we hotswap production machines with freshly installed testing machines


---++ Configuration

Configuration is managed by Puppet using the UW Center for High-Throughput Computing's Puppet server on =wid-service-1.chtc.wisc.edu=.

See [[MadisonITBInstanceConfiguration]] for details.


---++ Archive

Archived Madison ITB notes are on [[MadisonItbArchive][another page]].

%META:TOPICMOVED{by="TimCartwright" date="1479223144" from="SoftwareTeam.MadisonITBInstance" to="SoftwareTeam.MadisonITB"}%
