%META:TOPICINFO{author="MatyasSelmeci" date="1389391694" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="InternalDocs"}%
---+ Software/Release Team ITB Site Design

---++ Proposed Structure

| *Old Name*        | *New Name*              | *OS* | *Purpose*                                          | *Processor* | *Disk* | *RAM* |
| vdt-centos5-amd64 | itb-se                  | SL6  | Bestman Storage Element                            | Pentium D 2x3.4GHz | 250 GB | 2 GB |
| vdt-itb           | itb-ce                  | SL6  | !GRAM + !HTCondor-CE Compute Element + Squid       | Pentium D 2x3.4GHz | 250 GB | 4 GB |
| vdt-rhap5-amd64   | itb-gums-rsv            | SL6  | !GUMS, !RSV host                                   | Pentium D 2x3.4GHz | 250 GB | 2 GB |
| vdt-rhap5-ia32    | itb-submit              | SL6  | Submit node, NFS server                            | Xeon 4x2.8GHz      | 250 GB | 2 GB |
| vdt-sl5-amd64     | itb-glidein             | SL6  | !GlideinWMS frontend                               | Xeon 4x2.8GHz      | 120 GB | 2 GB |
| vdt-centos5-ia32  | itb-worker1             | SL5  | Worker node                                        | Xeon 4x2.8GHz      | 250 GB | 2 GB |
| vdt-debian5-amd64 | itb-worker2             | SL6  | Worker node                                        | Xeon 4x2.8GHz      | 250 GB | 2 GB |
| vdt-rhas4-amd64   | itb-data1               | SL5  | Data node                                          | Xeon 4x3.0GHz      | 160 GB | 2 GB |
| vdt-rhas4-ia32    | itb-worker3             | SL6  | Worker node                                        | Xeon 4x2.8GHz      | 120 GB | 2 GB |
| vdt-slf4-amd64    | itb-data2               | SL6  | Data node                                          | Xeon 4x2.8GHz      | 160 GB | 2 GB |
| vdt-slf42-ia32    | itb-data3               | SL6  | Data node                                          | Xeon 4x2.8GHz      | 160 GB | 2 GB |
   * Xeons are mostly Nocona E0; are dual-core + hyperthreading; Pentium D's do not have hyperthreading
   * Most machines should run SL6, but should have 2 SL5 hosts for testing
   * All machines are 64bit

---+++ Other notes

   * [[Documentation.Release3.SitePlanning]] recommends _not_ using NFS as our network filesystem, but does not say what to use instead
   * Suchandra reports that he does not have any load issues with NFS
   * All worker nodes will run two batch systems: Condor and one other (probably !SLURM)

---++ Site Elements

   * GUMS (run on its own host)
   * Run both HTCondor-CE and GRAM (can do both on the same CE)
   * Run HTCondor as the back-end (for now, may consider adding PBS in the future)
   * Run osg-info-services instead of CEMon
   * Run RSV on a separate box (maybe one of the worker nodes)
   * (done) Fix [[Documentation.Release3.SitePlanning]] page to remove reference to WS-GRAM
   * Squid -- which machine to set it up on
   * Pass on syslog-ng?
   * NFS / other shared file system (for OSG_APP and OSG_DATA)

---++ Questions
   * What VO are we going to belong to? (Ask other ITB sites)
   * How to do OIM registration

---++ University of Chicago ITB Instance

---+++ Compute Elements
   * itbv-ce-pbs - gatekeeper talking to PBS
   * itbv-ce-condor - gatekeeper talking to Condor
   * vtbv-ce-condor - SL6 gatekeeper talking to Condor
   * itbv-ce-htcondor - soon-to-be functional HTCondor CE

---+++ Batch Systems
   * Condor running on all worker nodes
   * PBS running on all worker nodes

---+++ Worker Nodes
   * 4 worker nodes (1 on SL6, 3 on SL5) with glexec installed

---+++ Storage
   * xrootd/bestman SE with 3 data nodes
   * hdfs/bestman SE with 3 data nodes
   * dcache SE with 3 data nodes

---+++ Other
   * 4 submit nodes / interactive machines / build nodes (2 on SL6 / 2 on SL5)
   * GUMS host running on SL5
   * Central RSV monitor system
   * !glideinWMS front-end talking to integration glide in factory


---++ 2013 Nov 8 meeting
   * Should put HTCondor-CE CE and GRAM CE on separate machines, but can have multiple job managers on each one
   * We should run something other than PBS when we want to try a separate batch system (because Suchandra is already running a PBS gatekeeper)
   * SLURM may be a good choice because we have local expertise on campus -- but may not be useful for managing "small" nodes
   * SGE is a possibility but we have few sites running it and those are not expected to increase
   * Will probably have more than 4 worker nodes -- mostly 2-slot machines
   * (Mat) Need to learn more on storage. We _should_ run Bestman since we have the most experience with this, but do we also want to run hdfs and/or xrootd ?
   * 3 storage nodes / 4 worker nodes ?
   * Can you put !GlideinWMS frontend on a CE? (Ask Suchandra)

   * Put everything in the "OSG" VO so we won't have to run a VOMS server
   * Shared filesystem (NFS?). Do not run fileserver on SE or CE
   * Ask Suchandra: are his data nodes both xrootd and HDFS nodes?
   * Docs recommend putting Squid on the CE. (If we have two CEs, do we need two squids?)
   * !GUMS host should not be on a worker node or data node
