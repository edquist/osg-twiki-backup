%META:TOPICINFO{author="TimCartwright" date="1479162063" format="1.1" version="1.23"}%
%META:TOPICPARENT{name="InternalDocs"}%
---+ Software/Release Team ITB Site Design

---++ ITB Goals, Revisited 2016-11-03

Roughly in order of priority, at least for the top few items.

   * Test pre-release builds of HTCondor and HTCondor-CE in an OSG context
      * Install software and run jobs as soon as possible after a pre-release
      * An implementation idea: Maybe have more than one CE and/or HTCondor instance? One for &ldquo;production&rdquo; and one for rapid testing
   * Add the ability to create and control job pressure locally
      * Run all the time? Some of the time? Most of the time, but be able to drain or spike on demand?
      * Maybe add a VO front-end?
   * Add HTCondor-CE-Bosco to the overall site plan, so that we can test it, too
   * Test the OSG stack before release (i.e., updates out of osg-prerelease)
   * Have a mix of EL6 and EL7 execute hosts and user jobs to test EL&nbsp;6&rarr;7 migration
   * Test other batch systems
      * Slurm
      * PBS, in some flavor
      * LSF and SGE usage is waning in the field, so they are lowest priority
   * Add other types of hosts (e.g., VO frontend, storage, RSV, squid, etc.)
      * Current HDFS/XRootD nodes could be repurposed or we could spin up new VMs on =itb-host-1=
      * We could potentially run our own factory in the future
   * Investigate whether there is a way to switch easily between ITB and production pilots and payloads
   * Test large-customer specific workflows (e.g., LIGO)
   * Investigate monitoring/testing setup for site verification
      * Ask sites (e.g., UNL) for their solutions
   * Flexible site installation
      * Configuration management for production and/or base hosts (logins, ntpd, repo RPMs, etc.)
      * Determine upgrade procedure: do we upgrade the hosts in place with the option of reverting to production images or do we hotswap production machines with freshly installed testing machines

---++ Previous Documentation

---+++ Ownership

| *Component*       | *Owner* |
| CE                | Mat     |
| SE / Data Nodes   | Tim C   |
| GUMS / RSV        | Edgar   |
| Submit Node / NFS | Carl    |
| Glidein           | Brian   |
| Worker Nodes      | Tim T   |

---+++ Current setup

---++++ Physical
Up-to-date structure on a Google Doc spreadsheet; ask Tim C or Mat about access.

---++++ Configuration


Configuration is managed by Puppet using the UW Center for High-Throughput Computing's Puppet server on _wid-service-1.chtc.wisc.edu_.

See [[MadisonITBInstanceConfiguration]] for details.


---+++ Design

---++++ Site Elements

See [[Documentation.Release3.SitePlanning]], though it has some outdated parts.

   * GUMS (run on its own host)
   * Run both !HTCondor-CE and GRAM (can do both on the same CE)
   * Run !HTCondor as the back-end (for now, may consider adding PBS in the future)
   * Run osg-info-services instead of !CEMon
   * Run RSV on a separate box
   * Squid -- which machine to set it up on
   * Pass on syslog-ng?
   * NFS for OSG_APP and OSG_DATA

---++++ Questions
   * What VO are we going to belong to? (Ask other ITB sites)
   * How to do OIM registration

---+++++ Case Study: University of Chicago ITB Instance

---+++++ Compute Elements
   * itbv-ce-pbs - gatekeeper talking to PBS
   * itbv-ce-condor - gatekeeper talking to Condor
   * vtbv-ce-condor - SL6 gatekeeper talking to Condor
   * itbv-ce-htcondor - soon-to-be functional HTCondor-CE

---+++++ Batch Systems
   * Condor running on all worker nodes
   * PBS running on all worker nodes

---+++++ Worker Nodes
   * 4 worker nodes (1 on SL6, 3 on SL5) with glexec installed

---+++++ Storage
   * xrootd/bestman SE with 3 data nodes
   * hdfs/bestman SE with 3 data nodes
   * dcache SE with 3 data nodes

---+++++ Other
   * 4 submit nodes / interactive machines / build nodes (2 on SL6 / 2 on SL5)
   * GUMS host running on SL5
   * Central RSV monitor system
   * !glideinWMS front-end talking to integration glide in factory


---++++ 2013 Nov 8 meeting
   * Should put HTCondor-CE CE and GRAM CE on separate machines, but can have multiple job managers on each one
   * We should run something other than PBS when we want to try a separate batch system (because Suchandra is already running a PBS gatekeeper)
   * SLURM may be a good choice because we have local expertise on campus -- but may not be useful for managing "small" nodes
   * SGE is a possibility but we have few sites running it and those are not expected to increase
   * Will probably have more than 4 worker nodes -- mostly 2-slot machines
   * (Mat) Need to learn more on storage. We _should_ run Bestman since we have the most experience with this, but do we also want to run hdfs and/or xrootd ?
   * 3 storage nodes / 4 worker nodes ?
   * Can you put !GlideinWMS frontend on a CE? (Ask Suchandra)

   * Put everything in the "OSG" VO so we won't have to run a VOMS server
   * Shared filesystem (NFS?). Do not run fileserver on SE or CE
   * Ask Suchandra: are his data nodes both xrootd and HDFS nodes?
   * Docs recommend putting Squid on the CE. (If we have two CEs, do we need two squids?)
   * !GUMS host should not be on a worker node or data node