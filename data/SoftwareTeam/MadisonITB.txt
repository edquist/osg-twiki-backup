%META:TOPICINFO{author="TimCartwright" date="1483544356" format="1.1" reprev="1.27" version="1.27"}%
%META:TOPICPARENT{name="InternalDocs"}%
<style type="text/css">pre em { color: red; font-weight: normal; font-style: normal; }</style>

---+ Software/Release Team ITB Site Design


---++ Madison ITB Machines

All physical hosts are located in 3370A in the VDT rack.

| *Host* | *Purpose* | *OS* | *Arch* | *CPU Model* | *CPUs* | *RAM* | *Storage* | *Notes* |
| itb-data1 | worker node | SL 6.3 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as HDFS data node |
| itb-data2 | worker node | SL 6.3 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as HDFS data node |
| itb-data3 | worker node | SL 5.8 | x86 64-bit | Celeron G530 2.4Ghz | 2 / 2 | 8 GB | 750 GB &times; 2 (SW RAID 0) | planned as HDFS data node |
| itb-data4 | worker node | SL 6.3 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as !XRootD data node |
| itb-data5 | worker node | SL 6.3 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 750 GB &times; 2 (SW RAID 1) | planned as !XRootD data node |
| itb-data6 | worker node | SL 5.8 | x86 64-bit | Xeon E3-1220 3.10GHz | 2 / 4 | 8 GB | 384 GB in /var/lib/condor/execute | planned as !XRootD data node |
| itb-host-1 | KVM host | SL 7.2 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 1 TB &times; 4 (HW RAID 5) | |
| &nbsp;&middot;&nbsp; itb-ce1 | HTCondor-CE | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-glidein | !GlideinWMS VO frontend? | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-gums-rsv | GUMS, RSV | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-hdfs-name1 | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-hdfs-name2 | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-se-hdfs | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-se-xrootd | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-submit | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| &nbsp;&middot;&nbsp; itb-xrootd | &mdash; (so far) | SL 6.3 | x86 64-bit | VM | 3 | 6 GB | 50 GB | |
| itb-host-2 | worker node | SL 6.3 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 401 GB in /var/lib/condor/execute | |
| itb-host-3 | worker node | SL 7.1 | x86 64-bit | Xeon E5-2450 2.10GHz | 16 / 32 | 64 GB | 50 GB in /; 384 GB in /home | |

(Data last updated 2017-01-04 by Tim C.)


---++ ITB Goals, Revisited 2016-11-03

Roughly in order of priority, at least for the top few items.

   * Test pre-release builds of HTCondor and HTCondor-CE in an OSG context
      * Install software and run jobs as soon as possible after a pre-release
      * An implementation idea: Maybe have more than one CE and/or HTCondor instance? One for &ldquo;production&rdquo; and one for rapid testing
   * Add the ability to create and control job pressure locally
      * Run all the time? Some of the time? Most of the time, but be able to drain or spike on demand?
      * Maybe add a VO front-end?
   * Review and significantly tighten firewall rules on ITB hosts
   * Add HTCondor-CE-Bosco to the overall site plan, so that we can test it, too
   * Test the OSG stack before release (i.e., updates out of osg-prerelease)
   * Have a mix of EL6 and EL7 execute hosts and user jobs to test EL&nbsp;6&rarr;7 migration
   * Test other batch systems
      * Slurm
      * PBS, in some flavor
      * LSF and SGE usage is waning in the field, so they are lowest priority
   * Add other types of hosts (e.g., VO frontend, storage, RSV, squid, etc.)
      * Current HDFS/XRootD nodes could be repurposed or we could spin up new VMs on =itb-host-1=
      * We could potentially run our own factory in the future
   * Investigate whether there is a way to switch easily between ITB and production pilots and payloads
   * Test large-customer specific workflows (e.g., LIGO)
   * Investigate monitoring/testing setup for site verification
      * Ask sites (e.g., UNL) for their solutions
   * Flexible site installation
      * Configuration management for production and/or base hosts (logins, ntpd, repo RPMs, etc.)
      * Determine upgrade procedure: do we upgrade the hosts in place with the option of reverting to production images or do we hotswap production machines with freshly installed testing machines


---++ Configuration

Basic host configuration is handled by Ansible and a local Git repository of playbooks.

---+++ Git Repository

The authoritative Git repository for Madison ITB configuration is =gitolite@git.chtc.wisc.edu:osgitb=. Clone (?) the repository and push validated changes back to it.

---+++ Ansible

The =osghost= machine has Ansible 2.2.0.0 installed via RPM. Use other hosts and versions at your own risk.

---++++ Common Ansible commands

*Note:*
   * For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person.
   * All commands below are meant to be run from the =osgitb= directory from Git.

To run Ansible for the first time on a new machine:

<pre class="screen">ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l <em>HOST-PATTERN</em></pre>

The <em>HOST-PATTERN</em> can be a glob-like pattern or a regular expression that matches host names in the inventory file; see Ansible documentation for details.

After an initial successful run of the =secure.yml=, subsequent runs should replace the =-u root -k= part with =-bK= to use your own login and =sudo=.

To perform the remaining configuration tasks for a machine:

<pre class="screen">ansible-playbook site.yml -i inventory -bK -f 20 <em>[ -l HOST-PATTERN ]</em></pre>

Omit the =-l= option to apply configuration to all hosts.

If you have your own playbook to manage personal configuration, run it as follows:

<pre class="screen">ansible-playbook <em>PLAYBOOK-PATH</em> -i inventory -f 20 <em>[ -l HOST-PATTERN ]</em></pre>

---+++ Old Puppet information &mdash; now obsolete and awaiting decommissioning

Configuration is managed by Puppet using the UW Center for High-Throughput Computing's Puppet server on =wid-service-1.chtc.wisc.edu=. See [[MadisonITBInstanceConfiguration]] for details.

---+++ New Puppet ideas &mdash; now obsolete and not implemented

   * We maintain and build our own current (4.x) Puppet RPMs (including dependencies) for EL&nbsp;6 and EL&nbsp;7 based on Fedora SRPMs&nbsp;&mdash; this looks very doable
   * We run a Puppet Master on =osghost= or, if absolutely necessary, on a VM on =osghost=
   * Firewall rules will limit Puppet client access to the Puppet Master to a minimal range of IP addresses
   * We use public Puppet classes wherever possible (e.g., NTP from Puppet Labs)
   * When necessary, we write and maintain simple-as-possible Puppet classes for basic machine setup
   * Goals for automated configuration management:
      * Manage the system clock (i.e., a properly configured NTP daemon)
      * Manage a set of local user accounts, groups, and public SSH keys
      * Manage a =sudoers= file for local user accounts
      * Manage firewall rules
      * Install a basic set of common tools (nominations: vi, emacs)
   * In the meantime, we will write, maintain, and apply documentation to make these changes manually

---++ Archive

Archived Madison ITB notes are on [[MadisonItbArchive][another page]].

%META:TOPICMOVED{by="TimCartwright" date="1479223144" from="SoftwareTeam.MadisonITBInstance" to="SoftwareTeam.MadisonITB"}%
