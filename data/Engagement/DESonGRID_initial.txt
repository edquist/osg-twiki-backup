%META:TOPICINFO{author="MarkoSlyz" date="1355175472" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="DES_SingleEpochOverview"}%
---+Running Standalone DES Processing on the Grid

Here are older instructions for running [[DES_SingleEpochOverview][first or final cut DES processing]]
on fermigrid using the local nfs (bluearc) for storage. The software and procedures
are still being worked out.
<pre>
  A. Make sure the input directory is where we expect, and get list of
     valid exposures using is_science_exposure.py.

  B. Make new output directory:

     uberftp fnpcosg1.fnal.gov
     cd /des/orchestration/DTS/dst
        # May need to 'cd /des/' then 'cd orchestration' etc to give
        # bluearc a chance to mount this.
     mkdir finalcut_20121102_outputA

  C. Near the top of des.csf, change
       'exposures' to have the file name produced in Step A,
       'num_exposures' to have the number of lines in this file,
       'output_dir' to be the directory created in Step B, and
       'pipeline' to be the processing pipeline needed.

  D. Make a directory to run from, and copy the needed
     files there. These are:

         run_desB.sh
         des.csf
         exposureZZZZ.txt    # filename from Step A
         submit_finalcut_multiexpo.wcl
         submit_firstcut_multiexpo.wcl
         get_filter.py

  E. Get set up to use condor:

          voms-proxy-init -valid 48:00  -voms des:/des/production

       To see information about a proxy, the command is

         voms-proxy-info -all

       To delete the local copy of the proxy, the command is

          voms-proxy-delete

  F. Start the jobs running with 'condor_submit des.csf'
     To check on the status, the command is

       condor_q

     or

       condor_q CLUSTER_NUMBER

     If you need to stop the jobs, the command is

         condor_rm CLUSTER_NUMBER

     Can also watch

       /blue-orch/mslyz_des_test/e939/LOCK/LOCKS

     to see how the data transfers are going.
</pre>

-- OSG/FNAL User Support
