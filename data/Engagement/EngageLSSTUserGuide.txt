%META:TOPICINFO{author="GabrieleGarzoglio" date="1280520066" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="EngageLSST"}%
---+!! LSST OSG User's Guide (Phase I)

This document describes how to submit an LSST job set to the OSG.

The required files are all stored in =https://osg-svn.renci.org/svn/Engage/trunk/LSST/submit/=. In order to submit an LSST job to the LSST, check out or export the above directory on engage-submit or some other place where the condor submission system is configured to use the =engage-central.renci.org= OSGMM instance. The files in that directory are as follows:

<dl>
<dt> =submitLSSTsim= </dt>
<dd>The command to submit a job set to the OSG. Invoke without arguments or with =-h= for usage information.</dd>
<dt> =runLSSTsim= </dt>
<dd>The execution steering script submitted to the remote site with =submitLSSTsim=. Not for standalone execution.</dd>
<dt> =LSSTtemplate.cmd= </dt>
<dd>The Condor JDL (Job Description Language) template which is used to create the specific JDL for the given job set. Not for standalone use.</dd>
<dt> =local-preamble= </dt>
<dd>This script is executed *on the submit node* prior to execution of a single grid job (ie chip).</dd>
<dt> =local-postamble= </dt>
<dd>This script is executed *on the submit node* after execution of a single grid job (ie chip). It ascertains the status of the just-completed job and makes sure output and log files are accessible.</dd>
</dl>A directory, =runs= is created in the current working directory when =submitLSSTsim= is invoked. Log files and those containing =stdout= and =stderr= for each job and the output tarfiles will be placed under here as they are generated or are returned from the execution nodes.

Note: during phase I, the simulation is, "pointed" at only one place in the sky <em>ie</em> there is only one set of per-chip catalog files, and these have been installed on the remote sites.

The job set is submitted as a single DAG (Directed Acyclic Graph). This is the mechanism by which we enable jobs to be retried on different sites after failing on one, up to a maximum of 5 times. It also ensures that we detect correctly the success or failure of a particular job and ensure that the log files for that job are kept even if the job is successfully run elsewhere, this enabling troubleshooting of failed jobs.

The execution script described above is submitted with the job; however the execution environment, catalog and galaxy files are expected to be found on the remote system.

In order to obtain information about the running job set(s), execute:
<verbatim>
condor_q [-long] <user|job#>
</verbatim>
or
<verbatim>
condor_grid_overview
</verbatim>


-- Main.ChrisGreen - 29 Jul 2010