%META:TOPICINFO{author="MarkoSlyz" date="1313591459" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="EngageOpenSeesB"}%
---+ Production-demo Phase of Integration of the Network for Earthquake Engineering Simulation (NEES) running !OpenSees on the Open Science Grid

<b>
This page describes the production-demo phase of integration of NEES on OSG.<br>
The [[EngageOpenSeesB][proof-of-principle phase of integration of NEES with OSG is described at this link]].<br>
</b>

---++ Introduction

%TOC%

During the  [[EngageOpenSeesB][proof-of-principle phase of integration of NEES with OSG]], the OSG User Support team has worked with NEES to enable a proof-of-principle of direct job submission as well as of portal-based job submission.

For the case of direct job subssion, NEES ran 300 !OpenSEES jobs for an estimated 2400 hours to analyze the response of a structure called the Self-Centering Steel Plate Shear Wall (SC-SPSW) system.

The OSG User Support team is now helping NEES scale up its workload on OSG by about 2 orders of magnitude. This activity is considered a demo of production computations for NEES. This work is done in support of Andre' Barbosa, a graduate student from UCSD.

---++1. Requirements for Direct Job Submission

   * Computational needs:
      1. Resource needs: RAM < 1 GB; Scratch disk: 20 GB
      1. Number of jobs per simulation: 180 (corresponds to 180 different earthquake records)
      1. Number of parameter sets to simulate: 21 (10 strength/stiffness parameters * 2 different percentile values + 1 mean value)
      1. Total number of jobs = 3780 (180 * 21)
      1. Job duration: 6-12 hours
      1. Measured CPU hours per simulation (180 jobs): ~40 cores * ~4 days = ~3840 CPU hours
      1. Total estimated CPU hours = 80640 CPU hours (~4 days on OSG @ 1000 CPU / day)

   * Data needs:
      1. Input per job: 60 MB
      1. Output per job: 2 GB ASCII uncompressed = ~1 GB compressed.
      1. Total output: ~ 4 TB

   * Network needs:
      1. Assume: 1 GB output, 8 hours / job, 1000 jobs continuously = 1000 (MB/s per job) / (8 h * 3600 s/h) * 1000 jobs = 35 MB/s average bandwidth. If we transfer data to the storage repository while running jobs, we'll need twice as much bandwidth. Note: this is a calculation of average, but this bandwidth will be required in bursts. The submit node has 2 * 1 Gbit eth interfaces, which should satisfy this requirement.

   * Calendar time:
      1. Need results in a few weeks (by June?)

---++2. Division of Responsibilities

The OSG User Support team will be responsible for:
   * Build a version of !OpenSEES that does not depend on MPI. Our goal is to eventually transfer this responsibility to the maintainers of the code.
   * Help Andre' adapt his application to work on the WAN. The main potential issues are data handling and operation monitoring. In particular, to transport the job output (1 GB per job) we'll initially use condor and evaluate if a different solution is needed.
   * Help Andre' set up a data transferring infrastructure to move his output from the Engage submit node to his storage repository. We expect that this activity will produce ~4 TB of data, more than the current available space on the submit node.

Andre' Barbosa will be responsible for:
   * Andre' has been running his simulation on a 5 nodes condor cluster. On the WAN we recommended to organize his input and output as single *compressed* archives. Andre' has changed his jobs and input preparation process to support this.
   * Andre' will need to validate the results obtained on OSG. In particular, he'll need to validate the results generated by the MPI-less build of !OpenSEES.
   * Andre' will run his operations on OSG via the Engage Glidein WMS front end. Andre' is responsible for deciding what jobs to run, for monitoring their progress, for maintaining enough disk space to accept the output of the jobs, and for cataloging the output.

Our goal is to enable Andre' to run his operations by the 1st week of May.


---++3. Instructions to Transfer Data Using [[http://www.globusonline.org][globusonline.org]]

In our workload management model, IO is transferred to and from the OSG worker nodes by Condor. Once jobs are finished, output data is left on the file system of the submission node. Since the user quota on this file system is limited, we need a process to move data to a final repository. We plan to use  [[http://www.globusonline.org][globusonline.org]] to accomplish this, accessing the data at the submission system via a !GridFTP server.


   1. Get an account on [[http://www.globusonline.org][globusonline.org]]
   1. Add your x509 certificate to the list of your globusonline identities
      * Go to [[https://www.globusonline.org/account/manageidentities][My Account / Manage Identities]]
      * Login to engage-submit3.renci.org and get the encoding of your certificate with
        <pre>$ cat ~/.globus/usercert.pem</pre>
      * On your browser, click on "Add X.509 Certificate", enter an alias name (e.g. "DOEGrids id"), and cut and paste the output from above, between and including the lines "-----BEGIN CERTIFICATE-----" and "-----END CERTIFICATE-----"
      * Wait a few minutes for the information to propagate
   1. Install and start globusconnect on the computer that will be the repository of the data. This will start a gridftp server i.e. a globusonline endpoint. From your browser you will need to name the endpoint (say "data_repository") and generate a setup key to "activate it" (i.e. delegate your globusonline credentials to it). Follow instructions at [[https://www.globusonline.org/xfer/initiateXfer?gcdialog=true][the "Globus Connect" link from this page]]
   1. login to engage-submit3.renci.org and activate the engage-submit3 endpoint:
      * <pre>$ voms-proxy-init -valid 72:00 -voms Engage:/Engage/NEES</pre>
        (Note: grid-proxy-init is also sufficient)
      * <pre>$ gsissh -t USERNAME@cli.globusonline.org endpoint-activate tlevshina#engage_test_1 -g</pre>
        Where USERNAME is your globusonline user name.
   1. Initiate a transfer.
      * Go to  [[https://www.globusonline.org/xfer/initiateXfer][https://www.globusonline.org/xfer/initiateXfer]]
      * On the left window (source) select tlevshina#engage_test_1 endpoint from the pull down menu *or* type tlevshina#engage_test_1 in the endpoint text box.
      * On the right window (destination) select the data repository endpoint e.g. USERNAME#data_repository.
      * Go to the path containing the directories that you want to transfer.
      * You can select multiple directories e.g. clicking on them AND pressing ctrl
      * Start the transfer clicking on the central arrow from source to destination.
   1. You can monitor the progress of your transfer with "View Transfers"

Other potentially useful globusonline commands (most of the same information is also available via the web interface):

<menu>
<li>Endpoint list
<pre>
$  gsissh -t garzoglio@cli.globusonline.org endpoint-list
</pre>
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
<pre class="screen">
cd-115154_dhcp_fnal_gov -
fndapq_2_fnal_gov       -
fndapq_fnal_gov         22:34:54
tlevshina#engage_test_1 68:11:15
</pre>
%ENDTWISTY%
</li>

<li>Detailed information for an endpoint
<pre>
$  gsissh -t garzoglio@cli.globusonline.org endpoint-list -v tlevshina#engage_test_1
</pre>
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
<pre class="screen">
Name              : tlevshina#engage_test_1
Host(s)           : gsiftp://engage-submit3.renci.org:2811
Subject(s)        :
MyProxy Server    : myproxy.fnal.gov
Credential Status : ACTIVE
Credential Expires: 2011-05-08 17:13:22Z
Credential Subject: /DC=org/DC=doegrids/OU=People/CN=Gabriele Garzoglio 762243/CN=232722590/CN=534888350
Connection to cli.globusonline.org closed.
</pre>
%ENDTWISTY%
</li>

<li>Summary status of all transfer tasks
<pre>
$  gsissh -t garzoglio@cli.globusonline.org status -a
</pre>
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
<pre class="screen">
Task ID     : f39fe3b2-7754-11e0-a4bb-123139152864
Request Time: 2011-05-05 20:19:56Z
Command     : API
Status      : SUCCEEDED

Task ID     : 8797d1da-774e-11e0-a3e0-123139152864
Request Time: 2011-05-05 19:36:27Z
Command     : API
Status      : ACTIVE
Connection to cli.globusonline.org closed.
</pre>
%ENDTWISTY%
</li>

<li>Detailed status of a given transfer task
<pre>
$ gsissh -t garzoglio@cli.globusonline.org details 8797d1da-774e-11e0-a3e0-123139152864
</pre>
%TWISTY{%TWISTY_OPTS_OUTPUT%}%
<pre class="screen">
Task ID          : 8797d1da-774e-11e0-a3e0-123139152864
Task Type        : TRANSFER
Parent Task ID   : n/a
Status           : ACTIVE
Request Time     : 2011-05-05 19:36:27Z
Deadline         : 2011-05-06 19:36:23Z
Completion Time  : n/a
Total Tasks      : 3312
Tasks Successful : 3244
Tasks Expired    : 0
Tasks Canceled   : 0
Tasks Failed     : 0
Tasks Pending    : 68
Tasks Retrying   : 68
Command          : API
Files            : 3311
Files Skipped    : 0
Directories      : 1
Bytes Transferred: 17102637628
Bytes Checksummed: 0
MBits/sec        : 11.830
</pre>
%ENDTWISTY%
</li>

<li>Long detailed list of all events for a transfer task
<verbatim>
$ gsissh -t garzoglio@cli.globusonline.org events 8797d1da-774e-11e0-a3e0-123139152864 > events.csv
</verbatim>
</li>


<li>Cancelling a transfer task
<verbatim>
$ gsissh -t garzoglio@cli.globusonline.org cancel 8797d1da-774e-11e0-a3e0-123139152864
Canceling task '8797d1da-774e-11e0-a3e0-123139152864'.... OK
</verbatim>
Checking what files have NOT been transferred after the cancel
<verbatim>
$ gsissh -t garzoglio@cli.globusonline.org details -t -f status,src_file -O csv 8797d1da-774e-11e0-a3e0-123139152864 | grep FAILED
FAILED,/home/abarbosa/NEES/run179_long_v3_wrapper/run123/run_out123.tar.gz
FAILED,/home/abarbosa/NEES/run179_long_v3_wrapper/run118/run_out118.tar.gz
...
</verbatim>
</li>



</menu>

---+ 4. Current Status

As of 6/28/11:
   So far we have run about 11 clusters. A few long
   jobs in these are still running, and we've given up
   on some others.%BR%
7/14/11: Getting credentials for assistant. Haven't yet
   started new runs.%BR%
8/16/11:  Have started 11 of the remaining 15 clusters. About
  200 jobs from these are still running. Data transfer
  is more difficult than expected.


Expect that we will run 15 more clusters which were in
the initial plan. May also run some extra clusters depending
on the user's needs.

Here is a sample graph (prepared by Derek Weitzel) that
shows the time that jobs in a particular cluster took
to finish. Most of the jobs get done after two days
although there are longer ones.

%ATTACHURL%/sites_usedA.png

The mean run time for these is reported as 11 hours,
but this depends on the cluster and can be
significantly higher. For another cluster the maximum
job length is at least 42 hours.

Globusonline was recently allowing us to download at
around 80 or 90 Mbps so a whole cluster took about 5
or 6 hours. This rate had improved from the original
5Mbps mainly because a network admin replaced a faulty
fiber optic cable, and we changed the globusonline
parameters to allow more concurrent streams.

The biggest problem now is evictions and preemptions
causing jobs to start from scratch after already having
run for a while. Some possible fixes:
(1) More carefully selecting the initial sites at which
   to run, and also using condor_qedit to adjust that list
   later.
(2) Having each job avoid running at sites that recently
   evicted it.
(3) Trying to estimate the time each job needs, and
   submitting the long ones to long queues.
Options 2 and 3 would require changes to the front end
configuration.



-- OSG User Support

%META:FILEATTACHMENT{name="sites_usedA.png" attachment="sites_usedA.png" attr="" comment="" date="1309305451" path="sites_usedA.png" size="99198" stream="sites_usedA.png" tmpFilename="/usr/tmp/CGItemp34643" user="MarkoSlyz" version="1"}%
