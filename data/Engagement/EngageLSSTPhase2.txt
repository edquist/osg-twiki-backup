%META:TOPICINFO{author="GabrieleGarzoglio" date="1276010338" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="EngageLSST"}%
%TOC%

---+!! LSST Image Simulation on OSG Phase 2

---++ Introduction.

The Phase 2 is starting in Jun 2010 with the goal of simulating one entire night of data taking by LSST. This document includes several sections

   * requirements for the LSST simulation
   * architectural overview

This is the [[EngageLSST][documentation for the phase 1 of the project]]

---++ Requirements

<b>DRAFT</b><br>
These requirements need to be vetted with LSST. <br>
Items marked with question marks (??) are the most uncertain.<br>

---+++ Application requirements

The LSST detector consists of 189 adjacent chips. The LSST Simulation application simulates as independent jobs the 189 chips of the LSST detector to create one simulated observation of the sky i.e. an image. The output of the 189 jobs can be optionally merged to generate the complete simulated image (does LSST want this step done on OSG for phase II ??).
   * Workflow: 189 simulation jobs --> 1 (optional) merge job

There are two main modes in which to run the simulation:
   * deep drill: simulate small region of the sky continuously imaged
   * wide field survey: simulate as much sky as possible for 1 night of observation

---+++ Platform requirements
Direct experience in job durations, IO requirements, etc. is reported below. This experience comes from running the LSST application on common OSG platforms for Phase I of the project.

The common OSG platforms considered are all combinations of {Scientific Linux sl4,sl5} and {i386,x86_64}. The LSST simluation binaries have been compiled for all these platforms and that were made available at the sites in $OSG_APP (in phase I using the OSG MM).

   * Simulation job duration on common OSG platforms = 2 - 4 hours / job (average over 189 jobs = 140 minutes)
   * Simulation job application = unix binaries
   * Simulation job Memory requirement = 1 GB ?? (some jobs failed w/ out-of-memory)
   * Merging job duration = Estimate 15 minutes (uncompress 10 MB of input * 189 + concatenate the images) ??
   * Merging job application = MatLab (is there a unix binary for this?)
   * Memory requirement = 1 GB ??

---+++ I/O Data requirements

   * Input to the simulation job:
      * Galaxy files: a catalog of the objects in the sky.
         * They are fairly static: it may change once per year.
         * They consists of 4000 files, 46 MB overall compressed, 362 MB overall uncompressed.
         * Each job needs access to all galaxy files
         * In phase I, these were pre-installed in OSG_DATA
      * CAT files: generated from the galaxy files to describe what each chip will see, including effects such as atmospheric aberrations, the direction and speed of the wind, the position of the moon, etc.
         * Each job needs a different and specific CAT file as input. This is true for both deep drill and wide field survey simulation modes.
         * Each file consist of 7MB compressed.
         * In phase I, these were pre-installed in OSG_DATA
      * One random seed per job

   * Output of the simulation job:
      * 1 FITS file per simulation job: an image in the NASA FITS format, which keeps together the image with its metadata
         * These are 10 MB compressed, 1 per chip


   * Input to the merging job
      * 189 FITS files from the simulation jobs
         * 10 MB compressed, 1 per chip

   * Output of the merging job:
      * 1 final image: this is the final output of the workflow. It is an image in FITS format.
         * About 2 GB compressed ??


---+++ Phase 2 specific requirements
The goal of Phase 2 is to simulate 1 night of data collection for LSST. This consists of 500 pairs of focal plane images = 1000 images

This consists of about 200,000 simulation jobs (1 chip at a time) and 1000 merging jobs.

Assuming an upper limits of 4 hours per job, this consists of  800,000 CPU hours

Assuming we can sustain 2000 simultaneous simulation jobs across the grid i.e. 50,000 CPU hours / day, this takes
   * 17 days to complete the production, w/o counting failures
   * 12,000 jobs / day , w/o counting merging jobs
   * 84 GB / day of input CAT files (different for every job); each job also needs access to 46 MB of Galaxy files (same for every job)
   * 120 GB / day of output for the simulation
   * If files are kept in a Storage Element, this workflow requires about 1000 connections per hour (500 for input, 500 for output)
   * Total number of simulation files = 400,000 (200,000 input + 200,000 output)
   * Total output compressed from simulation jobs = 2.0 TB (10 MB per job)

Requirements for the merging jobs (possibly not necessary)
   * 1000 merging jobs
   * 189 FITS files of input, 2 GB of total input per job
   * 1 FITS file of output, 2 GB in size ??
   * Amount of computation for 1 job = 15 minutes (estimate) ??
   * Total amount of computation = 250 CPU h (estimate) ??
   * Total output of final images: 1000 images, with total size of 2TB

Lifetime and access requirements
   * For how long should the storage elements keep the files? = ??
   * Are the output of the simulation jobs valuable after merging? = ??
   * Where should the output be kept? = ??
   * Who should have access to the files ? How often per day ?
   * How much data will be moved per day?


---++ Architecture

<b>IN PROGRESS</b>

---+++ Workload Management
Node requirements for GlideIn frontend

   * Frontend:
      * Fast Multicore CPU
      * Webspace for monitoring
      * 1GB memory

   * User Pool + Schedd:
      * Medium fast CPU
      * Large Memory (16GB recommended). The memory requirement for the Schedd are 4 MB / jobs (in detail, the shadow process requires 8M Virt; 4M Res). For LSST it would be at least 4M * 2000+ = 8+GB.

Examples of current production setups -

   * Minos: Frontend+User Pool and WMS Collector+ User and WMS Schedd + Factory  ( 8 core server, 16 GB Memory)
   * Samgrid: minor difference w.r.t. the Minos setup
   * CMS Setup1: Frontend+User Pool Collector (32GB, 8CPU) ; User Schedd (32GB, 8CPU)
   * CMS Setup2: Frontend (8GB, 8CPU); User Pool Collector + User Schedd (32GB, 8CPU)

For LSST, we recommend 1 machine with Frontend + User Pool Collector + User Schedd (16GB, 8CPU)


-- Main.GabrieleGarzoglio - 08 Jun 2010