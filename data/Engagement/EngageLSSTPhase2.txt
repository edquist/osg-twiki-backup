%META:TOPICINFO{author="GabrieleGarzoglio" date="1279918746" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="EngageLSST"}%
%TOC%

---+!! LSST Image Simulation on OSG Phase 2

---++ Introduction.

The Phase 2 is starting in Jun 2010 with the goal of simulating one entire night of data taking by LSST.

This is the [[EngageLSST][documentation for the phase 1 of the project]]

---++ Requirements

<b>DRAFT</b><br>
These requirements need to be vetted with LSST. <br>
Items marked with question marks (??) are the most uncertain.<br>

---+++ Application requirements

The LSST detector consists of 189 adjacent chips. The LSST Simulation application simulates as independent jobs the 189 chips of the LSST detector to create one simulated observation of the sky i.e. an image. The output of the 189 jobs can be optionally merged to generate the complete simulated image (does LSST want this step done on OSG for phase II ??).
   * Workflow: 189 simulation jobs --> 1 (optional) merge job

There are two main modes in which to run the simulation:
   * deep drill: simulate small region of the sky continuously imaged
   * wide field survey: simulate as much sky as possible for 1 night of observation

---+++ Platform requirements
Direct experience in job durations, IO requirements, etc. is reported below. This experience comes from running the LSST application on common OSG platforms for Phase I of the project.

The common OSG platforms considered are all combinations of {Scientific Linux sl4,sl5} and {i386,x86_64}. The LSST simluation binaries have been compiled for all these platforms and that were made available at the sites in $OSG_APP (in phase I using the OSG MM).

   * Simulation job duration on common OSG platforms = 2 - 4 hours / job (average over 189 jobs = 140 minutes)
   * Simulation job application = unix binaries
   * Simulation job Memory requirement = 1 GB (some jobs failed w/ out-of-memory)
   * Merging job duration = Estimate 15 minutes (uncompress 10 MB of input * 189 + concatenate the images) ??
   * Merging job application = MatLab (is there a unix binary for this?)
   * Memory requirement = 1 GB ??

---+++ I/O Data requirements

   * Input to the simulation job:
      * Galaxy files: a catalog of the objects in the sky.
         * They are fairly static: it may change once per year.
         * They consists of 4000 files, 46 MB overall compressed, 362 MB overall uncompressed.
         * Each job needs access to all galaxy files
         * In phase I, these were pre-installed in OSG_DATA
      * CAT files: generated from the galaxy files to describe what each chip will see, including effects such as atmospheric aberrations, the direction and speed of the wind, the position of the moon, etc.
         * Each job needs a different and specific CAT file as input. This is true for both deep drill and wide field survey simulation modes.
         * Each file consist of 7MB compressed.
         * In phase I, these were pre-installed in OSG_DATA
      * One random seed per job

   * Output of the simulation job:
      * 1 FITS file per simulation job: an image in the NASA FITS format, which keeps together the image with its metadata
         * These are 10 MB compressed, 1 per chip


   * Input to the merging job
      * 189 FITS files from the simulation jobs
         * 10 MB compressed, 1 per chip

   * Output of the merging job:
      * 1 final image: this is the final output of the workflow. It is an image in FITS format.
         * About 2 GB compressed ??


---+++ Phase 2 specific requirements
The goal of Phase 2 is to simulate 1 night of data collection for LSST. This consists of 500 pairs of focal plane images = 1000 images

This consists of about 200,000 simulation jobs (1 chip at a time) and 1000 merging jobs.

Assuming an upper limits of 4 hours per job, this consists of  800,000 CPU hours

Assuming we can sustain 2000 simultaneous simulation jobs across the grid i.e. 50,000 CPU hours / day, this takes
   * 17 days to complete the production, w/o counting failures
   * 12,000 jobs / day , w/o counting merging jobs
   * 84 GB / day of input CAT files (different for every job); each job also needs access to 46 MB of Galaxy files (same for every job)
   * 120 GB / day of output for the simulation
   * If files are kept in a Storage Element, this workflow requires about 1000 connections per hour (500 for input, 500 for output)
   * Total number of simulation files = 400,000 (200,000 input + 200,000 output)
   * Total output compressed from simulation jobs = 2.0 TB (10 MB per job)

Requirements for the merging jobs (possibly not necessary)
   * 1000 merging jobs
   * 189 FITS files of input, 2 GB of total input per job
   * 1 FITS file of output, 2 GB in size ??
   * Amount of computation for 1 job = 15 minutes (estimate) ??
   * Total amount of computation = 250 CPU h (estimate) ??
   * Total output of final images: 1000 images, with total size of 2TB

Lifetime and access requirements
   * For how long should the storage elements keep the files? = ??
   * Are the output of the simulation jobs valuable after merging? = ??
   * Where should the output be kept? = ??
   * Who should have access to the files ? How often per day ?
   * How much data will be moved per day?


---++ Project Execution Plan

This link shows an [[http://home.fnal.gov/~garzogli/LSST/LSST-on-OSG-Phase-2-plan/LSST-on-OSG-Phase-2-v1.2.html][outline of the project execution plan]], with timeline and associated resources. 

Older versions of the plan are maintained in [[http://home.fnal.gov/~garzogli/LSST/LSST-on-OSG-Phase-2-plan/][the same directory]]


---++ Architecture

---+++ Simple Architecture
     <img src="%ATTACHURLPATH%/LSST-Architecture-Jul22-10.jpg" alt="LSST-Architecture-Jul22-10.jpg" width='960' height='720' />    

In this architectural configuration, computing resources are allocated via the GlideIn WMS system (OSG Glidein Factory) and are accessible to the user through a submission node running a condor scheduler. Input data is organized "manually" by the user at the submission node (Hadoop disk in the fig) and it is transferred to the worker node with the job by condor. Output is also returned to the submission node by condor. Binaries and common input file (Galaxy files) are pre-installed at all sites using the OSG Match Maker. The bookkeeping of the computational tasks (what jobs have failed, what need to be resubmitted, what percentage of the jobs have produced output, etc.) is done manually through condor.

More in detail, in reference to the figure

   1. The OSG Factory submit Glidein jobs to OSG clusters. When running, Glideins report the availability of computing resources (worker nodes) to the condor collector. As jobs are submitted to the user scheduler (step 3 below), the Glidein VO frontend adjusts the number of glidein submitted to OSG to respond dynamically to the demand for computing cycles. 
   2. An LSST user logs into the Glidein submission machine (HCC at Nebraska). Using a command line user interface, she submits jobs with the granularity of a group of 189 jobs at the time (1 simulated LSST image). The user interface requires a seed, unique for the group of jobs (each job will then have its own unique seed), and a path to a directory containing the 189 input catalog files. In this model, these files can be organized in individual directories in the Hadoop file system 
   3. In a unique local directory, the user interface generates a DAGMan description, a job description file for the 189 jobs, and a post-completion evaluation job.  The user interface then submits the DAG and the jobs to the scheduler. 
      The DAG consist of 
      * an initial job that creates the context for the monitoring process
      * the 189 simulation jobs
      * a post-completion job that gather statistics, such as the distribution of job resubmission, total job duration, job duration when successful, condor "goodput", etc.</br>
      * <img src="%ATTACHURLPATH%/LSST-DAG-Jul22-10.jpg" alt="LSST-DAG-Jul22-10.jpg" width='385' height='252' />    
   4. The scheduler interacts with the condor collector to find an appropriate resource match for each job. The input file and the job are then dispatched directly from the hadoop storage to the selected worker node. As opposed to other grid submission systems (e.g. OSG MM), user jobs submitted through the glidein WMS system start immediately i.e. without waiting in any remote batch system queue. The output is then transferred back using condor mechanisms.
   5. A periodic monitoring process publish on the web operationally-oriented characteristics of each job group, such as the number of completed output files, the link to post-completion statistics (see point 3), the related input and run directory, submission and modified time, etc. 

The system is designed to be fault tolerant. If a job fails because of problems with the resource (e.g. network connection problems, job pre-emption, etc.) the job is automatically resubmitted by the system indefinitely. If the job fails because of application errors (missing binaries at the remote location, bug in the code, etc.), DAGMan resubmits the job up to 5 times. After this process, if the job did NOT succeed (exit with status 0), DAGMan creates a "rescue" DAG description file, which can be manually resubmitted to rerun only failed jobs. 

---+++ Data Handling-enabled Architecture

In parallel to the simple architecture describe above, we have considered the use of IRODS as a candidate OSG data handling solution. A data handling system would enable file cataloging, access of input and storage of output from / to multiple (redundant) storage elements, output file-oriented bookkeeping of the computation. At this time (Jul 1, 2010), it is unclear that IRODS will be deployed to OSG in time for the integration with the LSST Phase 2 project. 


---+++ VO Workload Management (possibly useful for Phase 3 and beyond)
Node requirements for GlideIn frontend

   * Frontend:
      * Fast Multicore CPU
      * Webspace for monitoring
      * 1GB memory

   * User Pool + Schedd:
      * Medium fast CPU
      * Large Memory (16GB recommended). The memory requirement for the Schedd are 4 MB / jobs (in detail, the shadow process requires 8M Virt; 4M Res). For LSST it would be at least 4M * 2000+ = 8+GB.

Examples of current production setups -

   * Minos: Frontend+User Pool and WMS Collector+ User and WMS Schedd + Factory  ( 8 core server, 16 GB Memory)
   * Samgrid: minor difference w.r.t. the Minos setup
   * CMS Setup1: Frontend+User Pool Collector (32GB, 8CPU) ; User Schedd (32GB, 8CPU)
   * CMS Setup2: Frontend (8GB, 8CPU); User Pool Collector + User Schedd (32GB, 8CPU)

For LSST, we recommend 1 machine with Frontend + User Pool Collector + User Schedd (16GB, 8CPU)

---++ Commissioning

---+++ Results from 61 job groups

The infrastructure has been initially tested submitting 61 job group, corresponding to 61 simulated images. Each job group consists of 189 individual jobs, each simulating a CCD chip. Considering 4 CPU hours as the CPU usage of an individual job (an over-estimate), 60 job groups (made of 189 individual jobs) can be processed in 24 hours with 2000 nodes used 100% efficiently i.e. ~ 50,000 CPU hours. For these and the following tests, the input files from the simulation of the image in Phase 1 was replicated 61 times in as many directories. 

The LSST jobs were run on Jul 13 using the Glidein WMS frontend at the Holland Computing Center (HCC) in Nebrasca. 

The following plot shows various quantities related to the HCC Glidein frontend vs. time. The number of idle jobs (violet line) spikes up as the 61 jobs are submitted one after the other, then slowly decreases as the jobs move to running state. The number of running jobs (green area) increases with time as more pilot jobs are dynamically submitted by the OSG factory to the OSG resources to fulfill the computational needs of the frontend; the baseline of 1000 jobs is made of Einstein@home / boinc jobs running at Nebraska and pre-empted when the LSST jobs are submitted. The bulk of the jobs were completed in 10 hours.

    <img src="%ATTACHURLPATH%/61jobs_on_2010-07-13.png" alt="61jobs_on_2010-07-13.png" height='400' />    

The total amount of computation used in those two days was about 30,000 CPU hours, as shown in the plot belo. Note that a small fraction of this is due to Einstein@home:

     <img src="%ATTACHURLPATH%/61jobs_CPUhours_on_2010-07-13.png" alt="61jobs_CPUhours_on_2010-07-13.png" height='500' />    

For the job groups that successfully completed at the first submission, the following were typical statistics from the condor system for a representative job group. The distribution considers the values associated with the individual jobs (chips simulations) in the group (overall image simulation). 

The following histogram shows the distribution of the number of automatic resubmissions per job. Automatic resubmissions are typically due to failures of the infrastructure at the worker nodes (e.g. condor deamon started by the pilot has crashed, failures in the authorization mapping due to problems with gLExec, etc.). Most jobs completed with only 1 submission, some with 2, a few more with 3. A job was resubmitted automatyically 19 times (the maximum number of resubmission that I have seen is 45 times). A resubmission value of 0 indicates that condor did not save the number of resubmissions.

     <img src="%ATTACHURLPATH%/NumJobMatches.png" alt="NumJobMatches.png" width='480' height='480' />    

The following histogram shows the distribution of the duration of the jobs, counting time from the first time they started running (i.e. resubmitted jobs have longer times) . Most jobs finish within a couple of hours, some take up to 6 hours.

     <img src="%ATTACHURLPATH%/JobDuration.png" alt="JobDuration.png" width='480' height='480' />    

The histogram below, instead, shows the distribution of the duration of the successful jobs, counting time from the last time they started running (i.e. of the resubmitted jobs, only the last successful job time is considered). The two histograms are very similar due to the small percentage of resubmitted jobs:

     <img src="%ATTACHURLPATH%/SuccessfulJobDuration.png" alt="SuccessfulJobDuration.png" width='480' height='480' />    

The following histogram shows the distribution of the amount of computation for the job group. It is centered around 2 CPU hours, with some input files requiring more processing:

     <img src="%ATTACHURLPATH%/CPUUsageHoursHist.png" alt="CPUUsageHoursHist.png" width='480' height='480' />    

The histogram below shows a distribution of the condor "goodput". This measurement is the percentage of allocated time used towards successful computation. In general, an application is prevented from using the allocated time when it is waiting for IO or when it must restart due to a failure. LSST jobs tend to have very good goodput (~100%) when there is no failure, because IO takes a negligible part of the job. In real-life conditions with a typical amount of failures, goodputs above 95% seem very common.

     <img src="%ATTACHURLPATH%/Goodput.png" alt="Goodput.png" width='480' height='480' />    

At the end of the run, we found and addressed the following problems:

   1. 5344 jobs had failed and were automatically resubmitted, because of a configuration problem at the CMS MIT site (lsst application could not be found on the file system). Of these, 335 had failed more than 5 times and had to be manually resubmitted, using the automatically generated rescue dag. These 355 jobs were distributed across 47 job groups, thus alsmost 80% of the images could not be completed without this resubmission. The failures were tracked to a change in the mounting point of the OSG_APP area from the time that the LSST software was installed (the application hardcodes paths). A reconfiguration of the installation addressed the problem. The lesson learned is that configuration problems at one site may cause 80% of the jobs to be incomplete. As a result, we discussed mechanisms to penalize automatically sites that fail with user-level errors. A ticket was then opened with the Condor team.
   2. Another problem was due to 29 input files missing from one input directory. 29 /189 jobs would not be submitted by condor. Error logs were very clear and pointed immediately to the cause of the problem. 

---+++ Results from 183 job groups

On Jul 22, 2010, the infrastructure was tested submitting 3 times 61 job groups in 24 hours. Submissions were done at 10 am, 5 pm, and 11 pm. After 6-7 hours of a submission, about 80% of the jobs were completed and 61 new job groups were submitted. All 34587 jobs finished without the need of human intervention. The bulk of the jobs were completed within 21 hours; the last job (belonging to the batch submitted at 11 pm) completed after 29 hours of the initial submission. This test produced the equivalent of 18% of the 1000 images targeted for phase 2 in one day. 

The plot below shows the status of the HCC / Nebraska Glidein WMS frontend during these 24 hours. The violet line, representing idle jobs, spikes up for the three submission times and decreses each time as jobs start running on the grid. The green area, representing running jobs, shows that the system can run continuously an average of about 2,500 jobs, with peaks of more than 6,000 jobs at the same time. As for the test above, the baseline of 1000 jobs is made of Einstein@home / boinc jobs running at Nebraska and pre-empted when the LSST jobs are submitted. 

     <img src="%ATTACHURLPATH%/183_Job_run-idle_2010-07-22.png" alt="183_Job_run-idle_2010-07-22.png" width='762' height='459' />    

The plot below shows the location (sites) of the running jobs: USCMS at FNAL is the larger contributor, followed by Omaha, Nebraska, MIT, and UNESP.

     <img src="%ATTACHURLPATH%/183_Job_CPUours_2010-07-22.png" alt="183_Job_CPUours_2010-07-22.png" height='500' />    

As shown by the Gratia plot below, in these 24 hours, LSST could use about 80,000 CPU hours on the Open Science Grid (a small percentage of this was used by Einstein@home).

     <img src="%ATTACHURLPATH%/183_Job_Sites_2010-07-22.png" alt="183_Job_Sites_2010-07-22.png" width='759' height='456' />    

We recommend to try operations with other submission strategies. For example, we could explore the submission of fewer job groups (e.g. 30) more often (e.g. every 3 hours) as a way to provide a more constant "pressure" of idle jobs and a more constant number of available pilots in OSG. 


%META:FILEATTACHMENT{name="LSST-Architecture.jpg" attachment="LSST-Architecture.jpg" attr="h" comment="Simple Architecture" date="1277921754" path="LSST-Architecture.jpg" size="110770" stream="LSST-Architecture.jpg" tmpFilename="/usr/tmp/CGItemp18593" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="LSST-Architecture-Jul22-10.jpg" attachment="LSST-Architecture-Jul22-10.jpg" attr="" comment="" date="1279831240" path="LSST-Architecture-Jul22-10.jpg" size="77671" stream="LSST-Architecture-Jul22-10.jpg" tmpFilename="/usr/tmp/CGItemp18740" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="LSST-DAG-Jul22-10.jpg" attachment="LSST-DAG-Jul22-10.jpg" attr="" comment="" date="1279832593" path="LSST-DAG-Jul22-10.jpg" size="28669" stream="LSST-DAG-Jul22-10.jpg" tmpFilename="/usr/tmp/CGItemp18744" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="61jobs_on_2010-07-13.png" attachment="61jobs_on_2010-07-13.png" attr="" comment="" date="1279835838" path="61jobs on 2010-07-13.png" size="41321" stream="61jobs on 2010-07-13.png" tmpFilename="/usr/tmp/CGItemp18664" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="61jobs_CPUhours_on_2010-07-13.png" attachment="61jobs_CPUhours_on_2010-07-13.png" attr="" comment="" date="1279835864" path="61jobs CPUhours on 2010-07-13.png" size="97518" stream="61jobs CPUhours on 2010-07-13.png" tmpFilename="/usr/tmp/CGItemp18830" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="CPUUsageHoursHist.png" attachment="CPUUsageHoursHist.png" attr="" comment="" date="1279913035" path="CPUUsageHoursHist.png" size="5480" stream="CPUUsageHoursHist.png" tmpFilename="/usr/tmp/CGItemp18707" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="Goodput.png" attachment="Goodput.png" attr="" comment="" date="1279913048" path="Goodput.png" size="17463" stream="Goodput.png" tmpFilename="/usr/tmp/CGItemp18696" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="JobDuration.png" attachment="JobDuration.png" attr="" comment="" date="1279913068" path="JobDuration.png" size="5884" stream="JobDuration.png" tmpFilename="/usr/tmp/CGItemp18788" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="SuccessfulJobDuration.png" attachment="SuccessfulJobDuration.png" attr="" comment="" date="1279913080" path="SuccessfulJobDuration.png" size="5867" stream="SuccessfulJobDuration.png" tmpFilename="/usr/tmp/CGItemp18659" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="NumJobMatches.png" attachment="NumJobMatches.png" attr="" comment="" date="1279913106" path="NumJobMatches.png" size="5692" stream="NumJobMatches.png" tmpFilename="/usr/tmp/CGItemp18605" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="183_Job_CPUours_2010-07-22.png" attachment="183_Job_CPUours_2010-07-22.png" attr="" comment="" date="1279915334" path="183 Job CPUours 2010-07-22.png" size="137518" stream="183 Job CPUours 2010-07-22.png" tmpFilename="/usr/tmp/CGItemp18738" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="183_Job_run-idle_2010-07-22.png" attachment="183_Job_run-idle_2010-07-22.png" attr="" comment="" date="1279915350" path="183 Job run-idle 2010-07-22.png" size="155201" stream="183 Job run-idle 2010-07-22.png" tmpFilename="/usr/tmp/CGItemp18776" user="GabrieleGarzoglio" version="1"}%
%META:FILEATTACHMENT{name="183_Job_Sites_2010-07-22.png" attachment="183_Job_Sites_2010-07-22.png" attr="" comment="" date="1279915366" path="183 Job Sites 2010-07-22.png" size="128100" stream="183 Job Sites 2010-07-22.png" tmpFilename="/usr/tmp/CGItemp18786" user="GabrieleGarzoglio" version="1"}%
