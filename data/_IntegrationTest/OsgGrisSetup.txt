%META:TOPICINFO{author="KyleGross" date="1225985977" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="OSGCEInstallGuide"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


---++Introduction

The setup of Grid3 Information Provider (also known as MDS) is partly accomplished with the assistance of the configuration script <b>$VDT_LOCATION/monitoring/configure-grid3.sh</b> and partly
by modifying a few files with detailed information. Enter the 
<a href="#unique_name"><b>unique name</b></a> when asked for the <B>GRID3 SITE NAME</B>.

(How much of this needs to change with the move to the Generic Information Providers ?)


The Grid3 schema strives to make a minimum number of assumptions and requirements on a site. The filesystem sharing and filesystem mount points available for a cluster is an area which requires specific coordination for applications to be installed and to be executed correctly. To this purpose four special directory hierarchies (mount points) will be required to be defined and allocated to the  environment. These directories will be required to be available on the head node or gatekeeper node and also available, using the exact path, available on each of the worker nodes or simply shared filespace.


* %RED%Grid Directory%ENDCOLOR%

    This directory is where the Grid environment will be installed. This directory will contain the Globus middleware and other middleware applications. It should be writable for the user root. This directory contains both server and client utilities for the middleware and therefore should be shared between gatekeeper and worker nodes.

* %RED%Temporary Directories%ENDCOLOR%

    There will be two temporary directories one will be local to the worker node and the other shared. The shared temporary directory will be the current working directory of a running application and persist only as long as the job is executing. The files in this directory should be managed by the running application Automated clean up may be required to be handled by the site administrator. All users should 
be able to use this directory for writing and reading files. At least 10G byte of space should be allocated per worker node. The local temporary directory may be used by applications to reduce latency to data. At least 10G should be available in this directory. 

* %RED%Data Directory%ENDCOLOR%

    The data directory will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10G byte of space should be allocated per worker node. 

* %RED%Application Directory%ENDCOLOR%

    If required or desired this will be the location of Grid3 the application software. Only users in the application VirtualOrganizations/VOInfo will have write privileges to these directories. At least 10G byte of space should be allocated per application. 


One of the questions to be answered is regarding the <b>VirtualOrganizations/VOInfo sponsor</b> of this site.
This is attempting to determine the VirtualOrganizations/VOInfo ownership of the current cluster. The notation
incorperates a VirtualOrganizations/VOInfoname followed by a percentage so that clusters are able to split ownership between VirtualOrganizations/VOInfo's. 


Change into the VDT root directory.

=&gt; <b>cd $VDT_LOCATION</b>=
Source the environment; for sh and bash:
=$ <b>. ./setup.sh</b>=
For csh and tcsh:
=% <b>source ./setup.csh</b>=
The *configure-grid3.sh* script is located in the <b>monitoring</b>
directory.  (It must be run as root.)

<pre>
# <b>cd monitoring</b>
# <b>./configure-grid3.sh</b>


Please specify your GRID3 SITE NAME [_hostname.domain.tld_]: <b><i>UNIQUE_NAME</i></b>

Please specify your GRID3 BASE_DIR [/usr/local/grid]:

Please specify your GRID3 APP_DIR [/app]:

Please specify your GRID3 DATA_DIR [/data]:

Please specify your GRID3 TMP_DIR [/scratch]:

Please specify your GRID3 TMP_WN_DIR [/tmp]:

Possible Sponsors are usatlas, ivdgl ligo uscms sdss.
You can express the percentage of sponsorship using
the following notation. 'usatlas:50 ivdgl:10 uscms:20 local:20'
Please specify the VO sponsor of this site [iVDGL]:

Please specify the Batch Queuing to be used [condor]:


Please review the information:
Grid Site Name:  <i>UNIQUE_NAME</i>
Grid3 Location:  <i>/usr/local/grid</i>
Application:     <i>/app</i>
Data:            <i>/data</i>
Shared Temp:     <i>/scratch</i>
WorkerNode Temp: <i>/tmp</i>
JOB Manager:     <i>condor</i>

Is this information correct (y/n)? [n]: <b>y</b>
#</pre>
This configure script creates a $VDT_LOCATION/monitoring/grid3-info.conf file. <font color="red">We don't know where this file is used,or why gris comes next.  And how do we tell if gris is working?</font>

Start the information service daemon (optional):
<pre># <b>/etc/init.d/gris start</b>
</pre> 


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.DaneSkow - 10 May 2005

%STOPINCLUDE%