%META:TOPICINFO{author="KyleGross" date="1225985980" format="1.1" version="1.17"}%
%META:TOPICPARENT{name="LocalStorageRequirements"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>Local Storage Configuration
%TOC%
%STARTINCLUDE%


---++Introduction

This document describes how CE administrator can configure the OSG attributes, including ones referencing "CE storage" (LocalStorageRequirements) during the installation and after (if he needs to change the layout of his CE). 

LocalStorageRequirements describes what the different CE storage are and LocalStorageRequirements#Minimum_requirements specifies the minimum requirements for being OSG-compliant. It is not mandatory to provide all the CE storages.  Also see LocalStorageUse for a very draft idea on how
these should be used.

---++Known Problems

---++Configuring the OSG Attributes

This section covers the configuration of information about your CE that you must make known to OSG. These will be published as pat of the GLUE schema using the GIP (or the Grid3 schema), and used directly or indirectly by other OSG applications (MonALISA, ACDC, !GridCat, ...) and users submitting jobs.
<!-- The current version also provides backward compatibility with applications using Grid3 conventions. -->
At the core is a standard configuration file (<tt>$VDT_LOCATION/monitoring/osg-attributes.conf</tt>), that can be edited or reviewed directly. There is a configuration script (<tt>$VDT_LOCATION/monitoring/configure-osg.sh</tt>) which automates much of the configuration.

The meaning and purpose of the various elements of the attributes in that file are documented further in LocalStorageRequirements and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. New resource administrators may want to read that information carefully and determine how to map those elements onto their Resource before proceeding. Guidance on the basic elements and common defaults is provided below. 

---+++!!Gather configuration information

OSG strives to make resources available with minimal requirements; however, in order to provide a basic execution environment, certain information about files and filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. For this purpose, some special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node/gatekeeper node, and (using the exact path) on each of the worker nodes. They generally do NOT have to be made available in form of a shared filesystem across the whole cluster. Read-only spaces can generally be provisioned with or without a shared filesystem as long as care is taken in providing consistent paths.

* %RED%OSG Location ($OSG_LOCATION)%ENDCOLOR%

This directory is where OSG software will be installed and must be writable by root.
It contains the OSG-specific software as well as the Globus middleware and other middleware applications, including server and client utilities used by the system itself.  Users will use a different client installation.  This
directory should not be exported to the worker nodes.

* %RED%OSG Grid ($OSG_GRID) %ENDCOLOR%

This directory is where OSG Client Software will be installed -- see WorkerNodeClient for description.  It includes
client utilities for Grid middleware, such as VDS and srmcp.  It should be writable by root and readable by all users. It
must be accessible by both gatekeeper and worker nodes (via a shared filesystem, or different installations on local disks using a consistent pathname).

* %RED%Application Directory ($OSG_APP) %ENDCOLOR%

This is the base location for VirtualOrganizations/VOInfo-specific application software. Only users with software
installation privileges in their VirtualOrganizations/VOInfo should have write privileges to this directories. At least 10 GB of space should be allocated per VO.
It is read-only mounted on all worker nodes in the cluster.

* %RED%Data Directories ($OSG_DATA or $OSG_SITE_READ/$OSG_SITE_WRITE)%ENDCOLOR%

    These directories must be accessible from the head node as well as each of the worker nodes. This is intended
as the space for applications to write input and output data files for running jobs with persistency that must exceed the lifetime of the job which created it. This directory should be writable by all users.  Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10 GB of space should be allocated per worker node; some VOs require much larger allocations.

The following different options are possible: 
   * $OSG_DATA: shared directory with read-write access for all users
   * $OSG_SITE_READ: shared directory with read-only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
   * $OSG_SITE_WRITE: shared directory with write-only access for all users (data may be staged out by the administrator or using a SE pointing to the same space)
A CE can provide $OSG_DATA, both $OSG_SITE_READ and $OSG_SITE_WRITE, or none of them if it has a local SE specified in $OSG_DEFAULT_SE. The keyword to say that one hierarchy is not provided is UNAVAILABLE.
For additional details please refer to LocalStorageRequirements .

* %RED%Temporary Directory ($OSG_WN_TMP)%ENDCOLOR%

    This is a temporary directory local to the worker node used as a working directory.  At least 10 GB per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs that can run up to 4 jobs, should have 40GB). 
Files placed in this area by a job may be deleted upon completion of the job.

* %RED%Default Storage Element ($OSG_DEFAULT_SE)%ENDCOLOR%

    The default Storage Element, $OSG_DEFAULT_SE, is a storage element that is close and visible from all the nodes of the CE (worker nodes and head node). <!-- Usually, it is local to the CE and accessible from outside with the same or a different URL. --> The value to be specfied in $OSG_DEFAULT_SE is the full URL, including method, host/port and path of the base dir. This full URL must be reachable from inside as well as outside the cluster. The $OSG_DEFAULT_SE generally supports only put and get, rather than open/read/write/close as discussed in LocalStorageRequirements . At present, the supported methods for the $OSG_DEFAULT_SE are SRM and gftp.
If the CE has no default SE it can use the value UNAVAILABLE for $OSG_DEFAULT_SE. 

The final question asks about the <b>VirtualOrganizations/VOInfo sponsor</b> of your site.
This attempts to determine the VOs paying for the resources of the cluster. The notation
incorporates a VirtualOrganizations/VOInfo name followed by a percentage, so that CEs are able to denote multiple VirtualOrganizations/VOInfo partners. 

---+++!!Execute the configuration script

Run the following script as root to execute the configuration script.

<pre>
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
</pre>

For a typical installation, the script will look something like this:
<pre>


***********************************************************************
################# Configuration for the OSG CE Node ###################
***********************************************************************

This script collects the necessary information required by the various
monitoring and discovery systems for operating for the OSG.

A definition of the attributes that you will have to enter below is in:
http://osg.ivdgl.org/twiki/bin/view/Integration/LocalStorageRequirements
Intructions on how to use this script are in:
  http://osg.ivdgl.org/twiki/bin/view/Integration/LocalStorageConfiguration

Your CE may not provide some of the CE-Storages (DATA, SITE_READ, SITE_WRITE,
DEFAULT_SE). In those instances, the value to enter is UNAVAILABLE

At any time, you can <CNTL-C> out of the script and no updates will be applied.


Preset information you are not prompted for
--------------------------------------------

These variables are preset at installation and cannot be changed:
OSG location
Globus location
User-VO map file
gridftp.log location


Information about your site in general
--------------------------------------
Group:      The monitoring group your site is participating in.
             - for the integration testbed, use OSG-ITB.
             - for production, use OSG.

Site name:  The name by which the monitoring infrastructure
            will refer to this resource.

Sponsors:   The VO sponsors for your site.
            For example: usatlas, ivdgl, ligo, uscms, sdss...
            You must express the percentage of sponsorship using
            the following notation.
              myvo:50 yourvo:10 anothervo:20 local:20

Policy URL: This is the URL for the document describing the usage policy /
            agreement for this resource

Specify your OSG GROUP [OSG-ITB]:
Specify your OSG SITE NAME [UNAVAILABLE]: FNAL_64_TEST
Specify your VO sponsors [UNAVAILABLE]: fermilab
Specify your policy url [UNAVAILABLE]: http://fermigrid.fnal.gov/policy.html

Information about your site admininistrator
-------------------------------------------
Contact name:  The site administrator's full name.
Contact email: The site adminstrator's email address.

Specify a contact for your server (full name) [UNAVAILABLE]: Steven Timm
Specify the contact's email address [UNAVAILABLE]: timm@fnal.gov

Information about your servers location
----------------------------------------
City:    The city your server is located in or near.
Country: The country your server is located in.

Logitude/Latitude: For your city. This  will determine your placement on any
         world maps used for monitoring.  You can find some approximate values
         for your geographic location from:
            http://geotags.com/
         or you can search your location on Google

         For USA: LAT  is about   29 (South)       ...  48 (North)
                   LONG is about -123 (West coast) ... -71 (East coast)

Specify your server's city [UNAVAILABLE]: Batavia
Specify your server's country [UNAVAILABLE]: USA
Specify your server's longitude [UNAVAILABLE]: 41.8412
Specify your server's latitude [UNAVAILABLE]: -88.2546

Information about the available storage on your server
------------------------------------------------------
GRID:       Location where the OSG WN Client (wn-client.pacman) has
            been installed.
APP:        Typically used to store the applications which will run on
            this gatekeeper.  As a rule of thumb, the OSG APP should be on
                - dedicated partition
                - size: at least 10 GB.
DATA:       Typically used to hold output from jobs while it is staged out to a
            Storage Element.
            - dedicated partition
            - size: at least 2 GB times the maximum number of simultaneously
                    running jobs that your cluster's batch system can support.
WN_TMP:     Used to hold input and output from jobs on a worker node where the
            application is executing.
            - local partition
            - size: at least 2 GB
SITE_READ:  Used to stage-in input for jobs using a Storage Element or for
            persistent storage between jobs.  It may be the mount point of a
            dCache SE accessed read-only using dcap.
SITE_WRITE: Used to store to a Storage Element output from jobs or for
            persistent storage between jobs.  It may be the mount point of a
            dCache SE accessed write-only using dcap.

Specify your OSG GRID path [UNAVAILABLE]: /usr/local/grid
Specify your OSG APP path [UNAVAILABLE]: /usr/local/app
Specify your OSG DATA path [UNAVAILABLE]: /usr/local/data
Specify your OSG WN_TMP path [UNAVAILABLE]: /local/stage1
Specify your OSG SITE_READ path [UNAVAILABLE]: dcap://fndca1.fnal.gov:24125//pnfs/fnal.gov/usr/
Specify your OSG SITE_WRITE path [UNAVAILABLE]: srm://fndca1.fnal.gov:8443/

Information about the Storage Element available from your server
----------------------------------------------------------------
A storage element does NOT exist for this node.

This is the Storage Element (SE) that is visible from all the nodes of this
server (CE). It may be a SE local or close to the CE that is preferred as
destination SE if the job does not have other preferences.

Is a storage element (SE) available [n] (y/n): y
Specify your default SE [UNAVAILABLE]: gsiftp://fndca.fnal.gov:2811

Information needed for the MonALISA monitoring.
-----------------------------------------------
MonALISA services are NOT being used.

If you do not intend to run MonALISA for monitoring purposes, you can
skip this section.

Ganglia host: The host machine ganglia is running on.
Ganglia port: The host machine's port ganglia is using.
VO Modules:   (y or n) If 'y', this will activate the VO Modules module
              in the MonALISA configuration file.

Are you running the MonALISA monitoring services [n] (y/n): y
Are you using Ganglia [y] (y/n): y
Specify your Ganglia host [fermigrid0.fnal.gov]:
Specify your Ganglia port [UNAVAILABLE]: 8649
Do you want to run the OSG VO Modules [y] (y/n): y

Information about the batch queue manager used on your server
-------------------------------------------------------------
The supported batch managers are:
  condor pbs fbs lsf sge

For condor: The CONDOR_CONFIG variable value is needed.
For sge:    The SGE_ROOT variable value is needed

Specify your batch queue manager OSG_JOB_MANAGER [UNAVAILABLE]: condor
Specify installation directory for condor [UNAVAILABLE]: /opt/condor
Specify the Condor config location []: /opt/condor/etc/condor_config

#####  #####  ##### #####  #####  #####  ##### #####
Please review the information below:

***********************************************************************
################# Configuration for the OSG CE Node ###################
***********************************************************************

Preset information you are not prompted for
--------------------------------------------
OSG location:     /usr/local/vdt-1.3.10
Globus location:  /usr/local/vdt-1.3.10/globus
User-VO map file: /usr/local/vdt-1.3.10/monitoring/grid3-user-vo-map.txt
gridftp.log file: /usr/local/vdt-1.3.10/globus/var/gridftp.log

Information about your site in general
--------------------------------------
Group:       OSG-ITB
Site name:   FNAL_64_TEST
Sponsors:    fermilab
Policy URL:  http://fermigrid.fnal.gov/policy.html

Information about your site admininistrator
-------------------------------------------
Contact name:   Steven Timm
Contact email:  timm@fnal.gov

Information about your servers location
----------------------------------------
City:       Batavia
Country:    USA
Longitude:  41.8412
Latitude:   -88.2546

Information about the available storage on your server
------------------------------------------------------
WN client: /usr/local/grid

Directories:
  Application: /usr/local/app
  Data:        /usr/local/data
  WN tmp:      /local/stage1
  Site read:   dcap://fndca1.fnal.gov:24125//pnfs/fnal.gov/usr/
  Site write:  srm://fndca1.fnal.gov:8443/

Information about the Storage Element available from your server
----------------------------------------------------------------
A storage element exists for this node.

Storage Element: gsiftp://fndca.fnal.gov:2811

Information needed for the MonALISA monitoring.
-----------------------------------------------
MonALISA services are being used.

Ganglia host: fermigrid0.fnal.gov
Ganglia port: 8649
VO Modules:   y

Information about the batch queue manager used on your server
-------------------------------------------------------------
Batch queue:     condor

Job queue:       fermigrid0.fnal.gov/jobmanager-condor
Utility queue:   fermigrid0.fnal.gov/jobmanager

Condor location: /opt/condor
  Condor config: /opt/condor/etc/condor_config
PBS location:
FBS location:
SGE location:
    SGE_ROOT:
LSF location:


##################################################
##################################################
Is the above information correct (y/n)?: y

</pre>


This configure script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file (and grid3-info.conf that is a link to it). 
This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.

---++Reconfiguration

An already installed OSG site can be reconfigured by editing =osg-attributes.conf=, or by rerunning the =configure-osg= script.

---++Examples
These examples make assumptions about relationships between what is visible from inside of the CE (from the jobs) and what is available from the outside (SE, !GridFTP) for staging operations.
Your configuration may differ considerably.

---+++Single Computer OSG site 
Let's say you want to install a complete OSG site onto a single computer, e.g. as a testbed. You thus have CE, SE, and batch system on the same piece of hardware -- no clustering or multiple pieces of hardware.

For this example, let's assume we are using a partition mounted as  =/osg= (if you prefer, each of =/osg/grid=, =/osg/wntmp=, =/osg/app=, =/osg/data=, =/osg/wngrid= can be on a different partition; anything in between is fine also).  Users are mapped to different unix accounts but all belong to the same =griduser= unix group. 
 
   * Create the subdirectories =/osg/grid=, =/osg/app=, =/osg/data=, =/osg/wngrid=, =/osg/wntmp=
   * Change their group ownership to =griduser=
   * Change their permissions (1770 on /osg/data, /osg/wngrid, /osg/app, /osg/wntmp)
   * install the server (OSG) in =/osg/grid= and the user client (OSG-WN-Client) in =/osg/wngrid=
   * Configure variables (=GRID=/osg/wngrid=, =APP=/osg/app=, =DATA=/osg/data=, =WN_TMP=/osg/wntmp=)
<!--   * Point the gridftp server to =/osg/data= (gsiftp://myserver.domain/mydir/ -> =/osg/data=) -->

Here you will have a summary of =osg-configure.sh= (=osg-attributes.conf=) that looks like:  
<pre>
Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:    <i>/osg/grid</i>
OSG WN Client:   <i>/osg/wngrid</i>
Application:     <i>/osg/app</i>
Data:            <i>/osg/data</i>
Site read:       UNAVAILABLE
Site write:      UNAVAILABLE
WorkerNode Temp: <i>/osg/wntmp</i>
Default SE:      <i>gsiftp://myheadnode.athome.edu:2811</i>
JOB Manager:     <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

The specification for the "Default SE" should be such that one can concatenate it with the setting for "Data"
to arrive at a valid location SURL without any additional, or extraneous "/".
The nodename "myheadnode.athome.edu" is the nodename of the single headnode, and the port number is the port number
at which the gsiftp server is listening.

In addition, the OSG CE expects a home directory (=/osg/users/$USER=) for every user where the proxy and gass-cache are written to as jobs are submitted to it.

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.

---+++Single Headnode Cluster
Let's say you have a cluster with a batch system, and you want to add a single OSG headnode to it that instantiates a CE and SE
on one piece of hardware in the simplest possible way. We do not suggest this as a production installation as it couples CE and SE
and thus does not provide a very reliable OSG site.

Start by following the "Single Computer OSG Site" instructions above.

Mount the file systems mentioned on the single headnode as follows:
<pre>
/osg/grid    read/write
/osg/wngrid  read/write
/osg/app     read/write
/osg/data    read/write
/osg/users   read/write
/osg/wntmp   read/write
</pre>

Make sure that the following file systems are read/write accessible via the gridftp server:
<pre>
/osg/app
/osg/data
</pre>

Mount the file systems mentioned on all worker nodes on the cluster as follows:
<pre>
/osg/wngrid  read only
/osg/app     read only
/osg/data    read/write
/osg/users   read/write
/osg/wntmp   read/write
</pre>

In this configuration, WN_TMP (/osg/wntmp) is not a shared file system. It should be local to each and every worker node.
(In fact, it could be local to each and every batch slot.)

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.
However, we do not suggest this configuration unless the cluster is quite small.

---+++Suggested Minimal Configuration
The minimal set of headnodes for an OSG site (site = cluster & headnodes) to be reliable is two:
one for the CE and one for the minimal SE configuration.

To deploy such a configuration, follow the "Single Headnode Cluster" configuration above but separate
components as follows:
<pre>
Headnode 1:
Install the CE

Headnode 2:
Install the gridftp server
Export all file systems
</pre>

In this configuration you must point "myheadnode.athome.edu" in the specification of "Default SE" to the nodename of 
Headnode 2, the one that instantiates the gsiftp server. The CE installation will also install a gridftp server. However, that gftp server will not be advertized in any way shape or form. It is required because the condor-g client uses it to retrieve those files that are specified in the jdl as output files. Condor-g implicitly assumes that the host that runs the GRAM also runs gsiftp.

The intended use case here is for large scale data movement to go via the "Default SE" while small output files (O(a few MB)) that are retrieved by condor-g directly may be transfered via the CE.

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.

---+++SRM style CE
There are two versions of SRM covered here: SRM/DRM and SRM/dCache.
The intention here is not to provide a comprehensive description of the configuration of these products,
but to give an overview of how either one of these are incorporated into the OSG infrastructure once deployed.

In principle, replacing =$OSG_DATA= in the "Minimal Suggested Configuration" with either one of these SRM solutions is an
OSG-compliant configuration as specified at LocalStorageRequirements#3_OSG_GRID_OSG_APP_OSG_DEFAULT_S.
However, in practice, too many applications are not yet ready to benefit from SRM, and we thus suggest
sites to deploy SRM in addition to the "Minimal Suggested Configuration" for the time being. A reasonable configuration
might provide a small =$OSG_DATA= area (~100GB) in addition to a large multi-TB SRM space.

SRM/dCache:<br>
For applications that are able to use dcap, SRM/dCache replaces /osg/data.
An ideal workflow is the one described at 
LocalStorageRequirements#4_OSG_GRID_OSG_APP_OSG_SITE_READ . For applications that are not yet able to use dcap
LocalStorageRequirements#3_OSG_GRID_OSG_APP_OSG_DEFAULT_S might be suitable. 
However, in that case the data a job needs must fit
into WN_TMP which often is quite small (~10GB). As a result, it is highly advisable for applications to learn about dcap
if they want to benefit from the large storage available in SRM/dCache.

A site that chooses this configuration needs to configure "Site Read", "Site Write", and "Default SE" as follows:
<pre>
Site read:       <i>dcap://mydcapnode.athome.edu:22136//pnfs/athome.edu</i>
Site write:      <i>srm://mysrmnode.athome.edu:8443/</i>
Default SE:      <i>gsiftp://myheadnode2.athome.edu:2811</i>
</pre>

Note that a file that is copied into the Default SE can not be read from "Site read" as these are two entirely 
different storage elements in this site configuration!

The specifications for "Site read" and "Site write" are chosen such that a user who knows their path inside the
dCache pnfs logical namespace can add that path behind "Site read" or "Site write" without any additional or extraneous
forward slashes.

SRM/DRM:<br>
References to documentation will be forthcoming in future OSG releases.

---++ GIP configuration

See the Generic Information Providers section in the GenericInformationProviders.

<!-- ---++Contributed Examples
Here administratirs can add tips or links to descriptions of their installation. These may include specific references that may not apply to any other CE.
-->

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->



*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.MarcoMambelli - 11 Nov 2005
-- Main.FkW - 25 Apr 2006

%STOPINCLUDE%