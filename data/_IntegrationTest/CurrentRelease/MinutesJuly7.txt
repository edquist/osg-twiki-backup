%META:TOPICINFO{author="KyleGross" date="1225985980" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="MeetingMinutes"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


---++Introduction
Minutes of the Integration meeting, July 7, 2005. 

   * Last meeting, MinutesJune23
   * Previous meetings: MeetingMinutes


The main theme of tomorrow's meeting will be to discuss what is intended for ITB 0.1.7.  


---++Coordinates

2:30pm Central, 1-510-665-5437 #1212

---++Attending

Kent, Leigh, Rob, Steve Timm, C.J. Barker, Anhok Adiga, Razvan, Ransom, Shoawon, Vivek, Stu Martin, Mike Wilde, Greg, Marco, Karthick, Gabriele, John Weigand, 


---++Globus GT4

Current focus will be on GT4 GRAM readiness.

   * See the WSGramServiceReadinessPlan off the  GlobusOSG page.
   * WS-GRAM uses a web service container that holds the service, a few GT services are using this now (gridftp is still separate). 
   * So, gatekeepers would need to run a tomcat container.  There is a GT4 java container as well.  What about using the VDT supplied apache service.  Leigh points out that with GUMS uses this approach.
   * Stu -- will discuss this with Alain Roy. 
   * Could run both concurrently - side by side.  Condor-G submissions to GT4 GRAM is in a release of Condor.  There have been improvements. This should simplify integration of applications.
   * How is logging done?  We have dependencies on gatekeeper logs.  With the container, there is a standard logging method (log4j file?).  Levels of swithes are available.  *We need to identify our dependencies.*.
   * What about PRIMA callout? Work is underway to implement code in the GT4 container, a security handler, to do this.  Callout currently is c-code, will have to change obviously.  James Moore is working on this from the Globus team.  Need to handshake with the Privilege working group.  Stu will follow-up. 
   * Need to connect with MIS group.  Rob will contact Mark.
   * Gabriele: at BNL changes were made for LSF and Condor job manager changes were made for GT2.  Changes for GT4?  Stu: Same job manager perl submit and cancel scripts are used.  For monitoring - JM reads the scheduler's log file (big change).  PBS, Condor, LSF are provided.  Should be fine out of the box.  SGE is also available.  Fermilab batch system will have to be accommodated (Fermilab is mostly going to Condor).  
   * Stu - already have tests available for GT4. Will provide links.
   * By end of day will have first pass service readiness plan.

---++Leigh's List

Here is a serial list from Leigh - need to look at this and others...

---+++0) MIS-CI 0.2.7

Supports the SGE job manager at NERSC. 


---+++1) Grid3 information provider update for the policy URL

      Update Grid3 information provider software to version 2.0 which
      will publish the "POLICY_URL=". This URL will be displayed in the 
      site information as displayed by the GridCat instance. 
      est 2hours
      minor update/ tested on IU_iuatlas



---+++2) Update of configure-osg.sh for policy URL

      Updates to the configure-osg.sh to provide greater direction 
      for setting the Policy_ URL. 
      Need to discuss a template file for Site to use as for the Policy URL.
      est 2hours
      minor update verbage only 


   * Is there agreement on what is in Policy.  Batch queue policy, VirtualOrganizations/VOInfo supported, etc.  Should be both human and machine readable.
   * Leigh will develop a template. John will supply Leigh with an earlier proposal.


---+++3) Addition of SRM client tools (srm-cp)

   * See further, http://computing.fnal.gov/ccf/projects/SRM/.  Following described by SRM 1.0.
   * Robert Kennedy provided a tar image of the code.  There are a couple of issues.  Liscense agreement needs to be reviewed. 
   * Java application, but doesn't try to use an installed Globus, but uses its own environment, for example, tcp port range.  Will there be firewall issues.  
   * Will add to OSG cache as part of the CE install. 
   * Can use dCache. Question about interoperability of the client.  Will need to test.
   * Is this the same as the one being used in the LCG service challenge? (Yujin: 1.1.17 being used in service challenge. Leigh: same version.)


---+++4) Updating and testing Monalisa VirtualOrganizations/VOInfo modules

   * Iosif has re-written VirtualOrganizations/VOInfo jobs and VirtualOrganizations/VOInfo IO.  Also, VirtualOrganizations/VOInfo storage.  Iosif has added new metrics for CPU. 
   * Three additional lines in the configuration file.  Auto-update also possible.  For storage, only those that have the monalisa user has read access are monitored.
   * Concern is introduction of a new cron job running as root.  (Avoid by making the directories world readable.)
   * MIS-CI runs a cron job, but as the user responsible for running mis-ci. Is there an inconsistency here?
   * Need to encourage Iosif and Sudhir Borra to work the issue in MIS-CI.
   * Sudhir will update the twiki for instructions for evaluation in the ITB, which we should do on a few sites to provide feedback to Iosif.
   * Iosif's mail, for reference:

<verbatim>

Iosif Legrand wrote:

Hello,

We developed new  VO modules for MonALSIA, to have
a better monitoring  for the vo specific parts  and to provide
accounting  information.

It will help us a lot to test and validate the new modules if
several OSG-ITB sites,  will run this  new version .

To run the new version :

1) login as the user from which Monalisa is running ( e.g. monalisa)

2) in Service/VDTFarm  ,  edit vdtFarm.conf  and add these lines :

#New VO_IO
*osgVO_IO{monOsgVO_IO, localhost, }%180
#New VoStorage - performs a measurements every 12 hours!
*osgVoStorage{monOsgVoStorage, localhost, }%43200
#New VoJobs module
*osgVO_JOBS{monOsgVoJobs, localhost, }%180


3) cd Service/CMD
   ./ML_SER  update


This will update MonALISA to version 1.2.36 .  There are three new
vo modules :

1) osgVO_IO .  It provides a more efficient  procedure to report
    grid ftp transfers. It reports rates and also provides information
    about transfers between sites ( source and destination as domains)

2) osgVOStorage.   This module is used to collect  information about
    storage space used by each VO.  It is based on the module developed by
    Matteo.  In this version the diskUsage.py script is included in monalisa
    distribution, but I think it should be installed by root in cron by the VDT instaltion.
    The main reason to run this script as root is to be able to read all the files. If it  is used
    from  the monalisa account , it may not be able to read all directories.

3) osgVO_JOBS . This new module collects  detail information about all  the
    running jobs and   the resources they use from the  batch queuing systems.
    Information is provided also as   rates and allows to generate accounting information
    on any time interval.
    This new jobs module supports Condor, PBS and SGE .     It does not have yet LSF support. I do not have  access to an LSF installation
    to test it.  Perhaps someone can help with development / testing on LSF .

Thanks

Best Regards
ioji

</verbatim>



   * Notes added post meeting by Mark Green:

<pre>
Please add these comments to number 4 below for this meeting:

The storage module seems quite problematic in my opinion has it been tested within the following configuration:

For both local and NFS mounted $TMP, $DATA, $APP directory tree structures
       - greater than 750GB total usage with small to medium file size, in other words not just very large files consuming the space
        - a large number of nested directory tree structures
        - a nested $TMP, $DATA, and $APP directory tree structure will it report the correct usage
   2. Root access is extremely unattractive
   3. Report the time, machine load, and network load created by walking the local and especially the NFS mounted directory tree (in my experience this will crush the system)
   4. Is this meant to be a administrative information tool or VO specific information report, will this be obsolete when managed storage is available? What is the ultimate goal here?

I guess in general this is a significant “cost” associated with obtaining this information (I know because I have tested methods in getting this information) and I am not sure of the “benefit” derived within the current OSG cache deployed.

Mark
</pre>

---+++5) Changing DOEGrids certificates location

   * During installation, three options for cert install are given. 
   * Should certificates be in a shared directory, rather than a shared /etc/root directory.
   * Are there security issues here?  These are CA certificates for signing policies, so it shouldn't be an issue (no keys are included).
   * Clarification on location of these certificates needs to be spelled out in the deployment requirements document. 
   * Feedback from all is needed here.  
   * Leigh will collect and email to osg-int.


---+++6) Update to the default GIP configuration

   * Need more information here. Shouwen/Ransom/VDT; no time est.
   * Shouwen - are developing a scope of work for GIP, in context of MIS.  Mark and Bockjoo, and Ransom had discussions.  Both MIS and Interoperabilty groups have received the list.  Prioritized the tasks.
      * Automate configuration of GIP; Laurence Field is looking into this, no progress yet.  Discussed a starting script that could be customized.
      * There is twiki web developed by Ransom describing.
      * Schedule? Will discuss with Ransom and Laurence.

---+++7) Rename "Site Name" in GridCat

    Need more information from the Naming proposal focus group from
    the TG-MIG 
    no time est.


---++New version of GUMS - Gabriele

   * No functionality differences, minor changes to improve installation and configuration.
   * Discussion with VDT for integration into a future VDT release.
   * Some improvements for flexibility; note - would there be a higher level tool for setting up an OSG site?
   * Follow up at next meeting.



%STOPINCLUDE%