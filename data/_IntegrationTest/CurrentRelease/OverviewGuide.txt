%META:TOPICINFO{author="ElizabethChism" date="1466010175" format="1.1" version="1.6"}%
%META:TOPICPARENT{name="DocumentationTable036and037"}%
<!-- This is the default OSG Trash/Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


---++ Introduction

The goal for the Open Science Grid software stack is to provide a uniform interface to sets of scientists organized into virtual organizations for doing science that requires High Throughput Computing across many independently managed computing and storage clusters.

As administrator responsible for deployment of the OSG software stack, your goal will be to make your existing computing and storage cluster available to some or all registered virtual organizations in the OSG. For you to understand what you are in for, it is probably a reasonable start to describe the expectations of the scientists that will use your cluster via the OSG software stack.

Scientists will send "jobs" into your cluster's batch system via something called a "Compute Element" (CE). Some of these scientists will require non-negligible amounts of data as input, or generate non-negligible amounts of output data. They will need to store that data in something called a "Storage Element" (SE). 

At present, the CE is fairly unambiguously specified, with few choices left to you as site administrator. The standard installation includes a GRAM and gftp interface, both of which are
presently required on the same CE host in order for the condor_g client to successfully stage out
(smallish) output files that are defined as part of the job's jdl. The host for the CE needs to include a sizeable local disk (~100GB or more) as scientists tend to specify a few (smallish) input and
output files as part of their jdl. Those files are spooled onto your cluster via the CE. In addition, stdout and stderr of all jobs submitted via the CE are spooled via the CE out of your cluster.

The main CE related choice for you as site admin is to decide on your security policy with regard to group accounts, dynamic accounts for all users, in short, uid management on the cluster. Other choices are the OS (most if not all OSG clusters are some RHEL derivative), batch system (condor, pbs, lsf, and sge are presently supported), and the network architecture (default assumption is public/private with NAT --- you will need to advertize your architecture by changing some settings by hand if yours isn't like this) of your cluster. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster.

The CE also hosts information provider(s) and monitoring services, most of which are configured
correctly by default. If you care to see the utilization of your cluster via the OSG monitoring pages at the GOC, we suggest you deploy MonALISA on the CE. However, this continues to be an optional installation. If you do deploy MonALISA we expect you to verify that the information it reports is correct. If you don't deploy it then we expect you to send daily utilization numbers via email to miron at cs.wisc.edu .

The SE is much less well defined. Scientists expect three types of functionality:<br>
   * an area where they can install application software releases into via the CE, and  that is then read-only accessible from all batch slots on your cluster.
   * an area where they can stage data to using gftp that is then readable from all batch slots.
   * an area where they can stage output files to from all batch slots, for later asynchronous retrieval using gftp.

In addition, two other types of disk space must be available at each batch slot:<br>
   * a set of "client tools" that are part of the OSG software stack. These can be either exported from the CE host, a seperate host, or be deployed locally on each compute node.
   * an area that is strictly local to each batch slot, from within which they run their job(s) during the time they have "leased" the batch slot. You or your batch system is expected to clean this area after the scientists release the batch slot to you.

Requirements, configurations, and expected use are discussed at:
   * http://osg.ivdgl.org/twiki/bin/view/Trash/Trash/Integration/LocalStorageRequirements
   * http://osg.ivdgl.org/twiki/bin/view/Trash/Trash/Integration/LocalStorageConfiguration
   * http://osg.ivdgl.org/twiki/bin/view/Trash/Trash/Integration/LocalStorageUse

Some implementation choices, and related information can be found at:
   * http://osg.ivdgl.org/twiki/bin/view/Trash/Trash/IntegrationStorageElementAdmins

Last but not least, a word about policy:<br>
We expect you to specify your site's policy via some web page that becomes part of your site registration, and is available via gridcat at the GOC. This policy should clearly specify your policies with regard to resource access. We suggest that you allow all virtual organizations registered with the OSG at least "opportunistic use" of your resources. This may mean that you preempt those jobs when higher priority jobs come around. The scientists using the OSG generally prefer having access to your site and be subject to preemption over not having any access at all.

---++ Current Documentation Set

   * Provided here: DocumentationTable036and037


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.RobGardner - 22 Mar 2006<br>
-- Main.FkW - 19 Apr 2006<br>

%STOPINCLUDE%