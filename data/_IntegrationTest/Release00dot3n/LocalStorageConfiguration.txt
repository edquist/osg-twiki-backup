%META:TOPICINFO{author="FkW" date="1136394466" format="1.0" version="1.11"}%
%META:TOPICPARENT{name="LocalStorageRequirements"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


---++Introduction

This document describes how CE administrator can configure "CE storage" (LocalStorageRequirements) during the installation and after (if he needs to change the layout of his CE). 

LocalStorageRequirements describes what the different CE storage are and LocalStorageRequirements#Minimum_requirements specifies the  minimum requirements for being OSG compliant. NB: It is NOT mandatory to provide all the CE storages.


---++Installation

This content should replace:
http://osg.ivdgl.org/twiki/bin/view/Integration/OSGCEInstallGuide#Configuring_the_Grid3_Information_Provider

Refer one document with the other as it seems more convenient and avoids duplicates.

---+++Configuring the OSG Attributes
 (former Configuring the Grid3 Information Provider)

This section covers the configuration of information about your CE that you have to let know to OSG. These will be published as pat of the GLUE schema using the GIP (or the Grid3 schema) and used directly or indirectly by other OSG applications (Monalisa, ACDC, GridCat, ...) and users submitting jobs.
The current version provides also backward compatibility with applications/users using Grid3 conventions.
The core piece is a standard config file (<tt>$VDT_LOCATION/monitoring/osg-attributes.conf</tt>), that can be edited or reviewed directly. There is a configuration script (<tt>$VDT_LOCATION/monitoring/configure-osg.sh</tt>) which automates much of the configuration.

The meaning and purpose of the various elements of the attributes in that file are documented further in LocalStorageRequirements and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. New resource admins may want read that information carefully and determine how to map those elements onto their Resource before proceeding. Guidance on the basic elements and common defaults is provided below. 

---++++!!Gather configuration information

OSG strives to make the minimum requirements on a resource, however, to provide a basic execution environment that applications can build upon, certain information about file/filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. To this purpose, some special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node or gatekeeper node and also available, using the exact path, on each of the worker nodes or simply shared filespace.

* %RED%OSG Directory (OSG_LOCATION)%ENDCOLOR%

	 This directory is where OSG Software will be installed. This directory will contain the OSG specific software and probably also the Globus middleware and other middleware applications. It should be writable for the user root. This directory contains server and client utilities used by the system (server installation). Users will use a different client installation.


* %RED%OSG Grid%ENDCOLOR%

	 This directory is where OSG Client Software will be installed.  See WorkerNodeClient for description. This directory will contain the client software for the Grid middleware: the Globus clients and other applications like srmcp. It should be writable for the user root and readable by all the users. This directory contains client utilities used by the users and will be accessible in both gatekeeper and worker nodes (it can be the same installation in a shared filesystem or different installations done on local disk using the same path)

* %RED%Temporary Directory%ENDCOLOR%

	 There will be one temporary directory  local to the worker node. The local temporary directory will be used by applications as working directory. At least 10G per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs, that can run up to 4 jobs, will have to have 40GB). 

* %RED%Data Directories%ENDCOLOR%

	 The data directories will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10G byte of space should be allocated per worker node. One VO would like 100G+ per worker node. <br>
Different options are possible: 
	* OSG_DATA: shared directory with read-write access for all users
	* OSG_SITE_READ: shared directory with read only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
	* OSG_SITE_WRITE: shared directory with write only access for all users (data may be staged-out by the administrator or using a SE pointing to the same space)
A CE can provide OSG_DATA, both OSG_SITE_READ and OSG_SITE_WRITE or none of them if it has a local SE specified in OSG_DEFAULT_SE. The keyword to say that one hierarchy is not provided is UNAVAILABLE.

* %RED%Default Storage Element%ENDCOLOR%

	 The Default Storage Element, OSG_DEFAULT_SE, is a SE close and visible from all the nodes of the CE (workernodes and headnode). Usually it is local to the CE and accessible from outside with the same or a different URL. The value to be specfied in OSG_DEFAULT_SE is the full URL including method, host/port and path of the base dir. 
If the CE has no Dfault SE it can use the value UNAVAILABLE for OSG_DEFAULT_SE. 

* %RED%Application Directory%ENDCOLOR%

	 If required or desired this will be the location of the OSG application software. Only users in the application VO will have write privileges to these directories. At least 10G byte of space should be allocated per application. 


One of the questions enquires about the <b>VO sponsor</b> of this site.
This attempts to determine the VOs paying for the resources of the cluster. The notation
incorporates a VOname followed by a percentage so that clusters are able to note multiple VO partners. 

---++++!!Execute the configuration script

Run the following script as root to execute the configuration script.

<pre>
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
</pre>

For a typical installation, the script will look something like this:

<pre>
Please specify your OSG SITE NAME [_hostname.domain.tld_]: <b><i><a href="#UniqueName">UNIQUE_NAME</a></i></b>
Please specify your OSG LOCATION (BASE DIR) [/usr/local/grid]:
Please specify your OSG GRID path [Dir_with_grid_client_sw]: 
Please specify your OSG  APP [Dir_to_install_VO_applications]: 
Please specify your OSG  DATA [UNAVAILABLE]: 
Please specify your OSG  SITE_READ [UNAVAILABLE]: 
Please specify your OSG  SITE_WRITE [UNAVAILABLE]: 
Please specify your OSG  WN_TMP [Working_dir_for_jobs]: 
Please specify your OSG  DEFAULT_SE [UNAVAILABLE]: 

Examples of possible VO Sponsors are usatlas, ivdgl, ligo, 
uscms, sdss...
You can express the percentage of sponsorship using
the following notation:<br/>
 "myvo:50 yourvo:10 othervo:20 local:20"  <font color=red>Please do not use single quotes. It interferes with the database query statement</font>
Please specify the VO sponsor of this site [iVDGL]:

Enter the URL which will describe the policy for this resource
Please specify the Policy URL [POLICY_URL]: 

Please specify the Batch Queuing to be used [condor]:

Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/usr/local/grid</i>
OSG WN Client:	<i>/share/gridclient</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/tmp</i>
Default SE:		UNAVAILABLE
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

This configure script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file (and grid3-info.conf that is a link to it). 
This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.

---++Reconfiguration

If you feel confident, you can adjust an already installed OSG site by editing =osg-attributes.conf=, else you can rerun the configure-osg script.
Both procedures are described in the previous section (about installation).

---++Examples
These examples present also relationships between what's visible from the inside of the CE (from the jobs) and what is available from the outside (SE, !GridFTP) for staging operations.
Your configuration may be different from any of these.
These examples do not match the list of LocalStorageRequirements#Minimum_requirements for being OSG compliant
unless explicitly noted.

---+++Single Computer OSG site 
Let's say you want to install a complete OSG site onto a single computer, e.g. as a testbed. You thus have CE, SE, and batch system on the
same piece of hardware. No clustering of multiple pieces of hardware whatsoever.
[add a picture]

<br>
For sake of brevity, let's say the partition is =/osg= (If you prefer each of =/osg/grid=, =/osg/app=, =/osg/data=, =/osg/wngrid= can be on a different partition. Anything in between is fine also); users are mapped to different unix accounts but all belong to the same =griduser= unix group. The space available in =/osg= has to be at least...
 
<br>
	* Create the subdirectories =/osg/grid=, =/osg/app=, =/osg/app/etc=, =/osg/data=, =/osg/wngrid=. 
	* Change ownership =:griduser=
	* Change permissions (group write+sticky on data, ...)
	* install the server (OSG) in =/osg/grid= and the user client (OSG-WN-Client) in =/osg/wngrid=
	* Configure variables (=GRID=/osg/wngrid=, =APP=/osg/app=, =DATA=/osg/data=, =WN_TMP=/osg/data=)
	* Point the gridftp server to =/osg/data= (gsifpt://myserver.domain/mydir/ -> =/osg/data=)

Here you will have a summary of =osg-configure.sh= (=osg-attributes.conf=) that looks like:  
<pre>
Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/osg/grid</i>
OSG WN Client:	<i>/osg/wngrid</i>
Application:	  <i>/osg/app</i>
Data:				<i>/osg/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/osg/data</i>
Default SE:		UNAVAILABLE
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

In addition, the OSG CE expects a home directory ( /osg/users/ ) for every user where the proxy as well as the gass-cache is written to as jobs are submitted to the OSG-CE.

This installation implements LocalStorageRequirements#1_GRID_APP_DATA_WN_TMP and is thus OSG compliant.

---+++Single Headnode Cluster
Let's say you have a cluster with a batch system, and you want to add a single OSG headnode to it that instantiates a CE and SE
on one piece of hardware in the simplest possible way. We do not suggest this as a production installation as it couples CE and SE
and thus does not provide a very reliable OSG site.

Start by following the "Single Computer OSG Site" instructions above.

Mount the file systems mentioned on the single headnode as follows:
<pre>
/osg/grid	 read/write
/osg/wngrid  read/write
/osg/app	  read/write
/osg/data	 read/write
/osg/users	read/write
</pre>

Make sure that the following file systems are read/write accessible via the gridftp server:
<pre>
/osg/app
/osg/data
</pre>

Mount the file systems mentioned on all worker nodes on the cluster as follows:
<pre>
/osg/wngrid  read only
/osg/app	  read only
/osg/data	 read/write
/osg/users	read/write
</pre>

The WorkerNode Temp ( /osg/data ) is not a shared file system. It should be local to each and every worker node.
In fact, it could be local to each and every batch slots.

This installation implements LocalStorageRequirements#1_GRID_APP_DATA_WN_TMP and is thus OSG compliant.
However, we do not suggest this configuration unless the cluster is quite small.

---+++Suggested Minimal Configuration
The minimal set of headnodes for an OSG site (site = cluster & headnodes) to be reliable is two, 
one for the CE and one for the minimal SE configuration.

To deploy such a configuration follow the "Single Headnode Cluster" configuration above but seperate
components as follows:
<pre> 
Headnode 1:
install the CE

Headnode 2:
install the gridftp server
export all file systems
</pre>

This installation implements LocalStorageRequirements#1_GRID_APP_DATA_WN_TMP and is thus OSG compliant.

---+++SRM style CE
There are two versions of SRM covered here: SRM/DRM and SRM/dCache.
The intention here is not to provide a comprehensive description of the configuration of these products,
but to give an overview of how either one of these are incorporated into the OSG infrastructure once deployed.

In principle, replacing "DATA" in the "Minimal Suggested Configuration" with either one of these SRM is an
OSG compliant configuration as specified at LocalStorageRequirements#3_GRID_APP_DEFAULT_SE_WN_TMP .
However, in practice, too many applications are not yet ready to benefit from SRM, and we thus suggest
sites to deploy SRM in addition to the "Minimal Suggested Configuration" for the time being. A reasonable configuration
might provide a small DATA area (~100GB) in addition to a large multi-TB SRM space.

SRM/dCache:<br>
For applications that are able to use dcap, SRM/dCache replaces /osg/data.
An ideal workflow is the one described at 
LocalStorageRequirements#4_GRID_APP_SITE_READ_DEFAULT_SE . For applications that are not yet able to use dcap
LocalStorageRequirements#3_GRID_APP_DEFAULT_SE_WN_TMP might be suitable. However, in that case the data a job needs must fit
into WN_TMP which often is quite small (~10GB). As a result, it is highly advisable for applications to learn about dcap
if they want to benefit from the large storage available in SRM/dCache.

SRM/DRM:<br>
???


Missing here:<br>
Information on how to configure the GIP once you have deployed everything else.


---++Contributed Examples
Here administratirs can add tips or links to descriptions of their installation. These may include specific references that may not apply to any other CE.

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->



*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.MarcoMambelli - 11 Nov 2005

%STOPINCLUDE%

