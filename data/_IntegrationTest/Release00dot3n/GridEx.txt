%META:TOPICINFO{author="ElizabethChism" date="1466010178" format="1.1" version="1.14"}%
%META:TOPICPARENT{name="ItbRel015"}%
---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%

<center>
*UNDER CONSTRUCTION!*
</center>

---++Introduction
The Grid Exerciser (GEx) is a grid diagnostic and monitoring tool.  The GEx is currently run by the [[http://www.cs.wisc.edu/condor/ Condor Project]] at the University of Wisconsin on [[http://www.ivdgl.org/grid3/ Grid3]] and the OSG [[http://www.ivdgl.org/osg-int/ Trash/Integration Testbed]] (ITB), though the GEx can be run from any site against any grid.

The GEx has these goals:

   * By providing a continuous stream of jobs for all sites on a grid, the GEx provides a continuous diagnostic capability for identifying errors, primarily when jobs don't run at all, or jobs abort with errors.

   * The GEx provides a continuous _backfill_ load upon all sites in the grid.  When sites are configured correctly, GEx jobs run at the lowest priority. and the accumulated GEx run time directly indicates the unused computing capacity for a site.  Ideally, the GEx should accumulate a low level of successful run time.  A net run time of zero indicates a problem, either with the GEx or the site.  A high net run time indicates the site is either lightly loaded, or a has problems prioritizing GEx jobs.

---++Site Loading Algorithm
Currently the GEx attempts to maintain a load of _N_ submitted jobs at each site, where _N_ is the minimum of the number of CPUs reported for the site by the [[http://osg-itb.ivdgl.org/gridcat/ GridCat]], or 10.  The latter is the current arbitrary limit for the maximum number of simulaneous jobs per site. The GEx job itself is essentially a script wrapper around "sleep 900", and outputs other useful environment information to stdout and stderr.  The GEx job itself should result in little to no CPU load.

As of June 6, 2005, GEx jobs are only submitted to "active" OSG-0.1.6 sites.

---++Authenticating, Authorizing GEx
The GEx runs with the certificate subject
<verbatim>
/DC=org/DC=doegrids/OU=People/CN=Jeff Weber (Grid Exerciser) 365339
</verbatim>
The OSG recommended local user id is _gridex_ .

   * For sites using a static grid mapfile, the entry is
<verbatim>
"/DC=org/DC=doegrids/OU=People/CN=Jeff Weber (Grid Exerciser) 365339" gridex
</verbatim>

   * For sites using edg-mkgridmap, the edg-mkgridmap.conf entry is
<verbatim>
group vomss://grid03.uits.indiana.edu:8443/edg-voms-admin/vos?/vos/gridex gridex
</verbatim>

   * For sites using PRIMA and GUMS, you must configure your gums.config file to talk to the appropriate VirtualOrganizations/VOInfoMS server. The VirtualOrganizations/VOInfoMS Admin web service URL is
<verbatim>
https://grid03.uits.indiana.edu:8443/edg-voms-admin/vos/services/VOMSAdmin
</verbatim>
and the subgroup is
<verbatim>
/vos/gridex
</verbatim>

---++Configuring Site Policy for GEx Jobs
It is intended that each sites configures GEx jobs to run as backfill jobs.  This is done by configuring GEx jobs to run at the _poorest_ possible priority.  To accomplish this, all GEx jobs should be mapped to user ids that are _only_ associated with the GEx.  Bear in mind that [[http://grid.racf.bnl.gov/GUMS/guide_architecture.html GUMS ]] may map GEx jobs to multiple user ids.

Here are suggestions for accomplishing this for the various OSG job managers: (I'll stand by the user prioritization for Condor pools, but am requesting feedback on non Condor pools.)

---+++Condor
GEx jobs are submitted to remote Condor batch systems with the
<verbatim>
nice_user = True
</verbatim>

job attribute.  Thus, GEx jobs automatically set themselves to a poor user priority for remote Condor systems.  When the PREEMPTION_REQUIREMENTS configuration macro is properly configured, the Condor Negotiator will preempt all GEx jobs for other jobs with a better user priority.  To ensure that this happens immediately, verify PREEMPTION_REQUIREMENTS contains the clause to immediately preempt "nice" user jobs
<verbatim>
PREEMPTION_REQUIREMENTS = ... || (MY.NiceUser == True) 
</verbatim>

where the ellipses ... indicate any other preemption requirements appropriate for the site.  Note that some sites define PREEMPTION_REQUIREMENTS in terms of the intermediate macro UWCS_PREEMPTION_REQUIREMENTS.  If this is the case, the above <nop>NiceUser clause should be included in the UWCS_PREEMPTION_REQUIREMENTS definition.

The Condor Negotiator has access to the <nop>NiceUser job attribute when it is made visible via the machine (Condor Startd) running the GEx job.  This is enabled in the STARTD_JOB_EXPRS configuration macro.  STARTD_JOB_EXPRS contains a comma delimited list of job attributes to be advertised in the machine ad.  Verify this macro contains the <nop>NiceUser job attribute
<verbatim>
STARTD_JOB_EXPRS = ... NiceUser
</verbatim>

where the ellipses ... indicate other job attributes to advertise in the machine ad, appropriate for the site.

---+++PBS
Put the following in your maui.cfg file:
<verbatim>
  # use fair-share (track dedicated processor seconds)
  FSPOLICY              DEDICATEDPS
  # Favor fair-share component of overall job priority
  FSWEIGHT      1
  # Favor credential component weight for job priority
  CREDWEIGHT    1
  # Insert the user priority info for the gridex user (yes they can be negative)
  USERCFG[gridex]  PRIORITY=-1000
  # Insert a local priority for site specific VOs
  GROUPCFG[uscms]  PRIORITY=2000
  # User weight for everyone else
  USERCFG[DEFAULT]  PRIORITY=1000
</verbatim>

Something else that can be done is to enable preemption such as:
<verbatim>
  # Define all gridex jobs to be preemptable for real work
  USERCFG[gridex]  FLAGS=PREEMPTEE
  # Define everyone else as able to preempt jobs from gridex
  USERCFG[DEFAULT] FLAGS=PREEMPTOR
</verbatim>

More information about maui/moab configuration can be found at: [[http://www.clusterresources.com/products/maui/docs/]]
---+++LSF

This LSF user priority configuration wisdom has been provided courtesy of
[[mailto:matteom@slac.stanford.edu Matteo Melani]],
[[mailto:yangw@slac.stanford.edu Wei Yang]], and
[[mailto:neal@slac.stanford.edu Neal Adams]]
of [[http://osgserv01.slac.stanford.edu/ SLAC]].  Thankyou!

In LSF, user priority is configured via the USER_SHARES definition.
User priority definitions are defined relative to one another.  The higher the user priority value, the better the relative priority.  Thus, user priority for the _gridex_ user should be set to a _lower_ value than for all other users.  In LSF, there are several alternative methods for defining user priorities:

---++++Host Based User Priority
In lsb.hosts file, define the gridex user to have a poorer priority than all other users:
<verbatim>
  Begin HostPartition
  HPART_NAME =GlobalPartition
  HOSTS = all
  USER_SHARES = [user1, 10] [user2, 10] [gridex, 1]
  End HostPartition
</verbatim>

---++++Queue Based User Priority
In lsb.queues file, define the gridex user to have a poorer priority than all other users:
<verbatim>
Begin Queue
QUEUE_NAME      = idle
PRIORITY        = 5
NICE            = 20
USERS           = user1 user2 gridex
FAIRSHARE       = USER_SHARES [[user1,5] [user2,8] [gridex,1]]
HOSTS           = host1 host2
End Queue
</verbatim>

---++++User Based User Priority
In lsb.users file, define the gridex user to have a poorer priority than all other users:
<verbatim>
Begin UserGroup
GROUP_NAME      GROUP_MEMBER    USER_SHARES
...
OSG             (user1 gridex)     ([user1 5] [gridex 1])
End UserGroup
</verbatim>

---+++SGE
_Help.  SGE feedback needed._

---++Most Recent OSG-ITB GEx Report
GEx reports are emailed daily to OSG-INT@OPENSCIENCEGRID.ORG.  Additionally, an [[http://www.cs.wisc.edu/~grid-ex/ge/osg-itb/ archive]] of previous reports is available.

---++Interpreting GEx Reports
See the [[http://www.cs.wisc.edu/condor/tools/exerciser/reading_report.html guidelines]] for readying GEx reports.

---++Troubleshooting
TBD

---++Links
Additional GEx information is available [[http://www.cs.wisc.edu/condor/tools/exerciser/ here]].

---++Readiness Plan
TBD

---++Pending GEx Changes:
   * Modify GEx to dynamically restart each TBD period, and read GridCat at the start of each run.  Currently, site list is queried by script from GridCat, GEx is (re)started by hand.

   * Add verification of job stdout and stderr

   * Increase maximum number of simultaneous jobs per site.  The current limit of up to 10 simultaneous jobs per site is an arbitrary starting point.  This limit is
arguably quite small when compared to sites with hundreds or thousands of CPUs, and  hampers the GEx goal of providing a realistic backfill load upon each site.  However, this limit has still caused issues with some site.  Until these issues are all understood and remedied, the current limit will remain unchanged.


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.RobGardner - 16 May 2005

%STOPINCLUDE%