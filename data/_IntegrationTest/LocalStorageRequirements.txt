%META:TOPICINFO{author="SaulYoussef" date="1123784718" format="1.0" version="1.5"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 
---+ Local Storage Requirements
%TOC%
%STARTINCLUDE%


---++ $APP

This area is intended for VO-wide software installations.

It is required that relative paths resolve consistently between gatekeeper and worker nodes even if the $APP variable differs between the two.  It is strongly recommended that the variable and paths are the same as well, or most legacy software will not function properly.  This area may be read-only for a subset of users: there is no guarantee that every user will have write access.  $APP must point to a POSIX-compliant filesystem for software installation. 

---+++Saul:

I think that we should give slightly more specific instructions which are no less convenient and could avoid major disruptions.  It could be something like this:

(a) Choose any absolute location on your gatekeeper file system for APP.
(b) On each worker node arrange by mount or symlink that APP has the same path as on the gatekeeper node.

This would avoid essentially all of the problems that I described in my postings.  However, this is only true if software is only installed by the gatekeeper node.  If software is installed in $APP from the worker nodes, problems may still arrise.  For this reason, (and perhaps just for simplicity), just requiring the same absolute address on all nodes may be worth considering.

---++ $DATA

This area is intended to hold readable datasets for jobs executing on the worker nodes.

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This area may be read-only for a subset of users: there is no guarantee that every user will have write access.
$DATA may point to a filesystem or gsiftp URL.


---+++Marco:

This area is intended to hold data shared among different applications running on different worker nodes and/or data that has to outlive the execution of the jobs on the worker nodes.
Examples are: datasets for jobs executing on the worker nodes, datasets produced by the jobs executing on the worker nodes, shared daata for MPI applications. Data staged in or waiting to be staged out.

$DATA has to be POSIX accessible (open/read/write) by regular programs (NFS, dcap, drm are OK)
It may be as well accessible through a gsiftp URL (but not exclusively using GSIftp, else it would be a SE)

---+++Terrence:

(open/read/write) capability does not require posix. Posix compatible implies a laundry list of features like the ability to modify file meta data (permissions) and special file creation (pipes, sockets, links). These are not features that are necessary from compute nodes which are not guaranteed to have write access to $data.  

The use of $data as a writable area from compute nodes is one of the most significant performance bottlenecks in an OSG cluster. Use of $data as the "hold all read/write area" should be discouraged. 

The dataflow that is behind the original requirements is as similar to the dataflow in the SE as possible to maintain consistency for users as well as scaleable and reliable data access. 

	* Data is written to $data via gridftp or if necessary fork (unpack tarballs etc). 
	* Job is staged into the cluster
	* Job copies its data to the compute node or reads data sequentially from $data if the data is read once. This is a significant performance issue if many random reads are necessary on typical network file systems and this should be avoided. It is worth noting that random data access over large data sets is where grid storage shows its potential. Its distributed nature is better suited for handling that type of data access scaleably and reliably.
	* Job output is placed in $wn_tmp (a fully posix capable file system that supports links, sockets, pipes and other special files as required)
	* At end of job the results from $wn_tmp are packaged, staged to $tmp and picked up through gridftp or other mechanism. Alternatively srmcp could be used directly from the nodes to a remote respository. 



---++ $TMP

This area is intended as a temporary work area, and cache location for staging files in and out.

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  This area must be read-write for all users.  $TMP may point to a filesystem or gsiftp URL.  Files placed here are guaranteed not to be purged for at least 24 hours barring extraordinary cirucmstances; the precise purge policy is determined by site administrators.

---+++Marco:

This area has functionalities similar to $DATA, but it is a temporary work area, meaning that there is no guarantee that the data will be preserved once the jobs working on it are terminated (e.g. if there are some files in $TMP belonging to ATLAS users and no ATLAS job running, all those files can for sure be removed) 

$TMP has to be POSIX accessible (open/read/write) by regular programs (NFS, dcap, drm are OK). No gsiftp access is required. Files currently open in a running process will not be removed. File not touched for at least 24 hours may be purged but a system purging only files not used by running programs would be better (e.g. checking the scheduler id of the job that created a file and if that is running - I don't know how feasible this is).


---+++Terrence:

$tmp should be clearly seperated in the requirements from $data. $data is writable by specific VO users, and readable by VO users. $tmp is a read write area with a much shorter data life span. This makes it suitable for limited input and output from jobs after job completion. 

Posix is not required to ensure that the core requirements, (open/read/write) are met. That functionality is available to a wide variety of file access systems none of which meet the requirments of full posix compatiblity. The one area where $tmp exceeds $data requirements is that it must be writable from the compute nodes. However even this requirement is narrow in that by the time the data is being written to $tmp for stageout it should already be packaged and compressed as a single archive for fast writing and easy retrieval. These requirements are not just for admins, they are also to promote efficient data handling on the part of the users.

Historically in Grid3 $tmp was rarely used if at all. Instead $data was used as a kind of kitchen sink for data often taking writes directly from compute nodes running jobs. This is not a scalable approach. By defining $data and $tmp in this narrow fashion we actually start to develop a model of data flow that is closer to that of the SE and encourages users to use the space available more efficiently and effectively. 


---++ $WN_TMP

This area is a temporary work area that may be purged when the job completes.

It is required that $WN_TMP points to a POSIX-compliant filesystem.

---+++Marco:
A temporary directory created empty for the job, with a well defined quota and removed after the job completes would be ideal.

$WN_TMP has to be local to the worker node where the job is executing or have similar performances.

---+++Terrence:

This is the one directory that does require Posix compliance so that special files can be created as necessary. This is also one file system where quota are likely counterproductive. 

On many clusters the user on the CE will be the same user as on the worker node. It does not make sense to quota based on user as that one user may only visit that file system occasionally. That is a lot of quota information to maintain for a file system that can at most have only as many users simultaneously as there are queue slots on that node. 4 queue slots is fairly common for example while the number of user quotas would likely extend to the thousands. 

Instead of per user you could quota based on group, but there is another problem. Say I set up a group quota where I gave atlas 1/4 of the space and CMS 1/2. The problem is that if 4 atlas jobs get queue to that node I am effectively wasting 3/4 of the temporary space as it sits inaccesible to atlas. After all atlas is restricted to 1/4 the total space on $wn_tmp. CMS on the other hand gets 1/2 but still has the original problem of not getting access to the whole disk even if they are the only user on the node. Also by setting CMS to 1/2 quota offers nothing in terms of protection. The scenario is likely that CMS could use 1/2 the space while 3 nonCMS jobs want 3/4 the space. The file system is then oversubscribed by 1/4. The single quota per user also suffers from this inefficient use of $wn_tmp problem on top of being more complex. 

If quotas are desirable then the best approach is to have 1 static user per job slot that is independent of the user id mapping on the CE. In this model the static compute node job slot user maps to the static disk quota every time. This configuration accomplishes the goal of restricting any one job from overflowing into another jobs $wn_tmp space while at the same time allowing all jobs on that compute node to make maximum use of the space available. Mapping users to queue slot users at the compute node is a more complex configuration than is currently widely deployed for OSG. 

If you do want to define quotas for $wn_tmp is a siteadmin choice but quotas should not be in the requirements as they simply do not fit well with many cluster configurations that would nonetheless suscessfully and reliably execute jobs. 

As for removal it would be preferred if the users were shown that it is their primary responsibility for cleaning up after themselves or their access may become restricted or revoked. Siteadmins can then additionally deploy a solution if they feel users of the system are simply not trustworthy. 

---+++Terrence Overall:

The goal of this requirements document should be to provide some narrow minimums that can be used as a starting point. A baseline that all users can expect. By making the requirements relatively narrow it does not preclude a site admin from allowing more generous access. It does give a better understanding though what you can expect as a user. 


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.BurtHolzman - 04 Aug 2005

%STOPINCLUDE%

