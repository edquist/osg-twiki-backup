%META:TOPICINFO{author="MarcoMambelli" date="1134055311" format="1.0" version="1.5"}%
%META:TOPICPARENT{name="LocalStorageRequirements"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%


---++Introduction

This document describes how CE administrator can configure "CE storage" (LocalStorageRequirements) during the installation and after (if he needs to change hte layout of his CE). 

LocalStorageRequirements describes what the different CE storage are and LocalStorageRequirements#Minimum_requirements specifies the  minimum requirements for being OSG compliant. NB: It is NOT mandatory to provide all the CE storages.


---++Installation

This content should replace:
http://osg.ivdgl.org/twiki/bin/view/Integration/OSGCEInstallGuide#Configuring_the_Grid3_Informatio

Refer one document with the other as it seems more convenient and avoids duplicates.

---+++Configuring the OSG Attributes
 (former Configuring the Grid3 Information Provider)

This section covers the configuration of information about your CE that you have to let know to OSG. There will e published as pat of the GLUE schema using the GIP (or the Grid3 schema) and used directly or indirectly by other OSG applications (Monalisa, ACDC, GridCat, ...) and users submitting jobs.
The current version provides also backward compatibility with applications/users using Grid3 conventions.
The core piece is a standard config file (<tt>$VDT_LOCATION/monitoring/osg-attributes.conf</tt>), that can be edited or reviewed directly. There is a configuration script (<tt>$VDT_LOCATION/monitoring/configure-osg.sh</tt>) which automates much of the configuration.

The meaning and purpose of the various elements of the attributes in that file are documented further in LocalStorageRequirements and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. New resource admins may want read that information carefully and determine how to map those elements onto their Resource before proceeding. Guidance on the basic elements and common defaults is provided below. 

---++++!!Gather configuration information

OSG strives to make the minimum requirements on a resource, however, to provide a basic execution environment that applications can build upon, certain information about file/filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. To this purpose, some special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node or gatekeeper node and also available, using the exact path, on each of the worker nodes or simply shared filespace.

* %RED%OSG Directory (OSG_LOCATION)%ENDCOLOR%

	 This directory is where OSG Software will be installed. This directory will contain the OSG specific software and probably also the Globus middleware and other middleware applications. It should be writable for the user root. This directory contains server and client utilities used by the system (server installation). Users will use a different client installation.


* %RED%OSG Grid%ENDCOLOR%

	 This directory is where OSG Client Software will be installed. This directory will contain the client software for the Grid middleware: the Globus clients and other applications like srmcp. It should be writable for the user root and readable by all the users. This directory contains client utilities used by the users and will be accessible in both gatekeeper and worker nodes (it can be the same installation in a shared filesystem or different installations done on local disk using the same path)

* %RED%Temporary Directory%ENDCOLOR%

	 There will be one temporary directory  local to the worker node. The local temporary directory will be used by applications as working directory. At least 10G per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs, that can run up to 4 jobs, will have to have 40GB). 

* %RED%Data Directories%ENDCOLOR%

	 The data directory will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10G byte of space should be allocated per worker node. One VO would like 100G+ per worker node. <br>
Different options are possible: 
	* OSG_DATA: shared directory with read-write access for all users
	* OSG_SITE_READ: shared directory with read only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
	* OSG_SITE_WRITE: shared directory with write only access for all users (data may be staged-out by the administrator or using a SE pointing to the same space)
A CE can provide OSG_DATA, both OSG_SITE_READ and OSG_SITE_WRITE or none of them if it has a local SE. The keyword to say that one hyerarchy is not provided is UNAVAILABLE.

* %RED%Application Directory%ENDCOLOR%

	 If required or desired this will be the location of Grid3 the application software. Only users in the application VO will have write privileges to these directories. At least 10G byte of space should be allocated per application. 


One of the questions enquires about the <b>VO sponsor</b> of this site.
This attempts to determine the VOs paying for the resources of the cluster. The notation
incorperates a VOname followed by a percentage so that clusters are able to note multiple VO partners. 

---++++!!Execute the configuration script

Run the following script as root to execute the configuration script.

<pre>
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
</pre>

For a typical installation, the script will look something like this:

<pre>
Please specify your OSG SITE NAME [_hostname.domain.tld_]: <b><i><a href="#UniqueName">UNIQUE_NAME</a></i></b>
Please specify your OSG LOCATION (BASE DIR) [/usr/local/grid]:
Please specify your OSG GRID path [Dir_with_grid_client_sw]: 
Please specify your OSG  APP [Dir_to_install_VO_applications]: 
Please specify your OSG  DATA [UNAVAILABLE]: 
Please specify your OSG  SITE_READ [UNAVAILABLE]: 
Please specify your OSG  SITE_WRITE [UNAVAILABLE]: 
Please specify your OSG  WN_TMP [Working_dir_for_jobs]: 
Please specify your OSG  DEFAULT_SE [UNAVAILABLE]: 

Examples of possible VO Sponsors are usatlas, ivdgl, ligo, 
uscms, sdss...
You can express the percentage of sponsorship using
the following notation:<br/>
 "myvo:50 yourvo:10 othervo:20 local:20"  <font color=red>Please do not use single quotes. It interferes with the database query statement</font>
Please specify the VO sponsor of this site [iVDGL]:

Enter the URL which will describe the policy for this resource
Please specify the Policy URL [POLICY_URL]: 

Please specify the Batch Queuing to be used [condor]:

Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/usr/local/grid</i>
OSG WN Client:	<i>/share/gridclient</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/tmp</i>
Default SE:		UNAVAILABLE
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

This configure script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file (and grid3-info.conf that is alink to it). 
This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.

---++Reconfiguration

If you feel confident, you can adjust and already installed OSG site by editing =osg-attributes.conf=, else you can rerun the configure-osg script.
Both procedures are described in the previous section (about installation).

---++Examples
These examples present also relationships between what's visible from the inside of the CE (from the jobs) and what is available from the outside (SE, !GridFTP) for staging operations.
Your conficuration may be different from any of these.

---+++Very very simple CE
Lets say I have one or two computers, I want to install OSG using a disk partition on one of them and bring up a compliant CE that users can use for tests. I want also a SE provided bi a GridFTP server on the same machine.
[add a picture]

<br>
For sake of brevity, let's say the partition is =/opt= (If you prefer each of =/opt/grid=, =/opt/app=, =/opt/data=, =/opt/wngrid= can be on a different partition. Anything inbetween is fine also); users are mapped to different unix accounts but all belong to the same =griduser= unix group. The space available in =/opt= has to be at least...
 
<br>
	* Create the subdirectories =/opt/grid=, =/opt/app=, =/opt/app/etc=, =/opt/data=, =/opt/wngrid=. 
	* Change ownership =:griduser=
	* Change permissions (group write+sticky on data, ...)
	* install the server (OSG) in =/opt/grid= and the user client (OSG-WN-Client) in =/opt/wngrid=
	* Configure variables (=GRID=/opt/wngrid=, =APP=/opt/app=, =DATA=/opt/data=, =WN_TMP=/opt/data=)
	* Point the gridftp server to =/opt/data= (gsifpt://myserver.domain/mydir/ -> =/opt/data=)

Here you will have a summary of =osg-configure.sh= (=osg-attributes.conf=) that looks like:  
<pre>
Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/opt/grid</i>
OSG WN Client:	<i>/opt/wngrid</i>
Application:	  <i>/opt/app</i>
Data:				<i>/opt/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/opt/data</i>
Default SE:		UNAVAILABLE
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>


---+++Grid3 style CE
These CE have a mechanism to share directories, a GridFTP or SRM server visible from aourside (but not mandatory from the inside)

---+++LCG style CE

---+++SRM style CE

---++Contributed Examples
Here administratirs can add tips or links to descriptions of their installation. These may include specific references that may not apply to any other CE.

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->



*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.MarcoMambelli - 11 Nov 2005

%STOPINCLUDE%

