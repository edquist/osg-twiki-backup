%META:TOPICINFO{author="StevenTimm" date="1134497646" format="1.0" version="1.169"}%
%META:TOPICPARENT{name="WebHome"}%
---+!!<nop>OSG ITB CE Install Guide

%TOC%
%STARTINCLUDE%

---++ Introduction

This document is intended for administrators responsible for installing and configuring an OSG Compute Element (CE) version 0.3.3 onto the OSG Integration !TestBed (ITB). It is not meant as an all-inclusive guide to Grid computing or even all the options for configuring a CE. Instructions for installing a CE intended for the OSG Production grid can be found [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=67&Itemid=65][here]].

%RED% Preparations are being made to support Web Services GRAM (WS-GRAM) as the default access method for future versions of OSG.
As part of these preparations, it is now possible to install either the pre-ws or the ws-enabled CE package, or both.
The default recommendation at this time is to install the pre-WS cache (for example, osg-0.3.3) if you do not want to configure WS-GRAM access,
or the ws cache (for example, osg-0.3.3-ws) if you wish to explore web services.  It is also possible to install each of these sequentially,
in the above order (osg-0.3.3 answering "y" to the web services question, followed by osg-0.3.3-ws) if you wish to have both the pre-ws gatekeeper
and the ws container active at the same time on your system.	See also the note below on multiple cache installations into the same directory area.
 %ENDCOLOR%


---+++ Operating Systems

The base software from VDT 1.3.9 is supported on:
<pre>
	 * RedHat 7
	 * RedHat 9
	 * Debian 3.1 (Sarge)
	 * RedHat Enterprise Linux 3 AS
	 * RedHat Enterprise Linux 4 AS
	 * Fedora Core 3
	 * Fedora Core 4
	 * ROCKS Linux 3.3
	 * Fermi Scientific Linux 3.0
	 * RedHat Enterprise Linux 3 AS ia64
	 * SuSE Linux 9 ia64
	 * RedHat Enterprise Linux 3 AS amd64 
</pre>
It may work on other Linux releases but we do not guarantee it.

Additionally, some software packages are starting to drop support for !RedHat 7.  

As of OSG-ITB integration workshop, 12/1/05, PRIMA was not yet compiled for x86_64 version.

The VDT does not support SELinux though some of our supported platforms (Fedora Core 4) come with SELinux enabled by default. Be sure to disable SELinux in /etc/sysconfig/selinux before installing the VDT. 

See SciLinuxFermi4Hints for potential issues of installing on Scientific Linux Fermi 4.

See [[http://vdt.cs.wisc.edu/releases/1.3.9/requirements.html]] for latest OS version support
information.


---+++Installation Method

The installation and configuration method is based on <a href="http://physics.bu.edu/pacman/">Pacman</a>.  Pacman is a package manager like RPM or dpkg, but is able to work on and support multiple platforms.  Specific Pacman directions are given below, and in the pre-installation procedures.

---+++ Conventions used

The following conventions are used in these pages:
	* =monospaced text= indicates terminal output 
	* ==bold monospaced text== indicates terminal input (by the user)
	* =<i>monospaced text in italics</i>= indicates variable data that may differ on your installation
	* the =&gt;= prompt indicates commands that do not require a specific shell or root access
	* the =#= prompt indicates commands that require root access
	* the =$= prompt indicates sh- or bash-specific commands
	* the =%= prompt indicates csh- or tcsh-specific commands

---++ Pre-installation Checklist
---+++ System or Cluster Already Setup including a functioning batch system
It is assumed that your hardware is already running one of the operating systems previously mentioned. If your system is a cluster, the batch queuing system should be likewise previously installed and configured. If you intend to use the [[http://www.cs.wisc.edu/condor/][Condor]] system, you should install it with pacman before you install the computing element software below.  It is preferred to install it 
in a different location from the rest of the VDT.  The Personal Condor is not installed as 
part of the OSG CE any more.  See CondorHints for installing a condor outside of the $VDT_LOCATION.

The installation will ask for local storage space for 3 separate purposes: OSG applications, data, and temporary caching. It is recommended that you have separate, dedicated partitions for each purpose. For details on this storage use and configuration, please reference the section on "Configuring the Grid3 Information Provider" (URL).

---+++ Preserving a pre-existing Condor installation?
If you have installed Condor as your batch queueing system, you are required to set two environment variables so the Condor path is correctly configured. 

	* *VDTSETUP_CONDOR_LOCATION* - the location of your Condor installation  (e.g. _/opt/condor_). The Condor _bin/_, _bin_, _etc/_, _lib/_... directories should be directly under this location.
	* *VDTSETUP_CONDOR_CONFIG* (optional): The location of your Condor configuration file (if non-standard). Default is _$VDTSETUP_CONDOR_LOCATION/etc/condor_config_.

This will result in:
	* VDT middleware services pointing to appropriate locations in your external Condor installation.

For example, if you wish to preserve a Condor installed in the standard location,

==$ export VDTSETUP_CONDOR_LOCATION=$CONDOR_ROOT==

will suffice (provided $CONDOR_ROOT is set, per the usual Condor config)

---+++ Network setup?
It is assumed that your hardware has a working network connection through which packages may be retrieved and from which services may contact your system.  The full OSG CE packages is ~700MB so you don't want to be a the far end of a modem!

---++++!! Firewall?
If your installation is inside a site firewall or has a host firewall setup (look at =host.allow/deny=), you'll need to review the [[OSGCEInstallGuide#Firewalls][Firewall section]] of this guide and arrange for appropriate Internet access.

---++++!! Time synchronization (NTP) 
Each system should be setup to support network time protocol. Lack of synchronization generally complicates troubleshooting and can cause problems with the security infrastructures evaluation of eg. proxy lifetimes. 

---++++!! Reverse name lookups (DNS)
For the middleware to correctly function both the forward and reverse lookups as configured through a local DNS service are required for the IP address of the system.

#UniqueName 
---+++ You'll need a unique OSG resource name
Each OSG resource needs a *unique name* by which services and resources may refer to the resource. This name will be displayed on the Site Catalog and used in tables for other monitoring and accounting.  For example, the site at University of New Mexico has a unique name of UNM_HPC; the site at University of Buffalo is Buffalo_CCR.

---+++ Previous OSG or Grid3 Installations?
If there is a _previous installation_ of the OSG or Grid3 environment or other Globus middleware, please _stop the processes_ which are currently running. This includes the Globus Resource Information Service (GRIS), MonALISA and other services configured to start upon initialization on your system. Also, if you've sourced the $VDT_LOCATION/setup.[c]sh on your previous VDT install, just log out and log back in to clear out your environment.


More information is provided in [[OSG Shutdown Guide]].
---+++ Creation and setup of local user accounts for VOs for OSG ITB
UNIX user accounts need to be created by the system administrator for the VOs.  
You will need to create at least one local user account (with the appropriate configuration) for each VO to which you wish to provide resources.  The uid and name of the account can be locally determined. You will be asked to provide a mapping from local accounts to VOs later in the installation, but there are assumed defaults detailed in OsgVoAccounts. Those defaults are assumed for examples used in this installation guide.

The accounts are: 
	* globus  -- for WS-GRAM
	* cdf
	* fermilab
	* grase
	* fmri
	* gadu
	* mis
	* sdss
	* ivdgl
	* star
	* usatlas1
	* uscms01
	* ligo
	* sam
	* samgrid
	* dosar
	* des
	* glow
see GridEx for instructions on how to enable GridEx user, currently in ivdgl VO which 
is not yet part of osg-0.2.1 VO package that is installed in default VO's of OSG-0.3.3.
ivdgl VO also needed for OSG-ITB gridcat.



---+++ Do you have a user certificate?
You will need a personal grid certificate to run validation tests.  If you don't have a personal certificate, or don't know how to generate a grid proxy (grid-proxy-init, etc), contact your VO support center. (List of VO Support Centers at 
[[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support]])
---+++ Assistance during the installation

If you get stuck or have questions during the installation of this software the
mailing list osg-general at opensciencegrid dot org is available for you to submit
questions.  More information about the mailing lists is available at OsgMailingLists .

---++ Installing and Setting up Pacman
Instructions for installing and setting up Pacman for your specific platform can be found at PacmanInfo. 


---++ Installing OSG CE Services

Now you're ready to start installing the Compute Element services.

---+++ Choose an installation directory 
This is typically something like <tt>/usr/local/grid</tt>, but whatever suits the local resource structure is fine. If you are installing on a
cluster, this directory must be available on all the nodes. If this directory is not shared additional installation of
software will be required on each of the worker nodes. Nearly all of the software to be installed is from the VDT.  %RED% (This is out of sync with the LocalStorageConfiguration which suggests
that it is only the WorkerNodeClient software that has to be available to all worker nodes
and that this will in general be a different installation and directory from the main
OSG CE server software  S. Timm 12/9/05 ) %ENDCOLOR%

For example (shown for sh, bash),
<pre><b>
 $  export VDT_LOCATION=/usr/local/grid
 $  cd $VDT_LOCATION
</b></pre>

Note regarding pacman installations into a given directory area: 
One should be very careful about installing two separate pacman-obtained packages (two invocations of <tt>%pacman -get cache:<i>package</i></tt>) in the same directory.  
It is OK to do the pacman -get of an OSG CE software package as described below and then add one or more optional single packages (such as Globus-Condor-Setup) on 
top of this base, but don't try to install multiple complex package caches such as the OSG client or the OSG Worker
node package in the same directory as the OSG CE package. It is also explicitly OK to install the -ws package on top of the pre-WS plain package (for example pacman -get iVDGL:osg-0.3.x followed by pacman -get iVDGL:osg-0.3.x-ws where x is the minor point version) if you want to enable both GRAM methods; this is the preferred way to do this.  
Finally, please note the additional batch adapter package, such as Globus-WS-PBS-Setup or other as appropriate for your system, that will be required to complete the installation for a CE that has a local batch scheduler.

---+++Installing the OSG CE software package

The installation described here is done as root.  However, non-root installs are supported. (%RED% There should probably be some caveat or supplemental reference here otherwise why have the "as root" default ? It IS dangerous if unnecessary. %ENDCOLOR%) Not all services will run as root; Condor and the GRIS will run as the user "daemon".  The VDT will configure the daemons appropriately.  _Verify that the umask is set to "0022" prior to installation.  Failure to do so may render the installation unusable._

A few questions regarding trust of the caches from which the software is downloaded will be displayed. Please answer *y* (yes) so that the
software can be retrieved. (Note that it may be necessary to set your platform if you are installing a Linux version based upon Red Hat Enterprise Linux 3. See further [[https://uimon.cern.ch/twiki/bin/view/Atlas/GetPacman][Pacman Notes]]).  Finally, make sure there are no non-standard versions of perl, tcsh, or bash in your $PATH.

There are different Pacman caches to use depending on whether you are installing the OSG production version
or the ITB development release.


<pre>
&gt; <b>pacman -get iVDGL:osg-0.3.3</B> 
...
</pre>

A list of available caches for the can be determined by inspection of the [[http://pacman.uits.indiana.edu/pacman/][production releases area maintained by the Grid Operations Center]] 
or the equivalent [[http://hep.uchicago.edu/ivdgl/][Integration Test Bed releases area]] for ITB releases.

See [[PacmanInfo#Unsupported_Platforms][here]] for options to perform installations on unsupported pacman platforms.
VDT unrecognized platform errors are indicated by:
<pre>
... the VDT is unable to recognize your platform.  This version of the VDT is supported on several Linux releases:

	 RedHat 7
	 RedHat 9
	 RedHat Enterprise Linux 3
	 Fedora Core 3
	 Debian 3.1

The VDT may work on other platforms, but it's not guaranteed.  If you
encounter problems and send mail to the VDT team for support please mention
that you are installing the VDT on an unsupported platform.

Do you wish to continue? [y/n]
</pre>
(Answer <tt><b>y</b></tt> to go ahead and try anyway).


You will be asked if you trust the various caches?  It is hopefully safe to answer yes to these:
<pre>
  Do you want to add [http://hep.uchicago.edu/ivdgl/] to [trusted.caches]?
  Do you want to add [http://vdt.cs.wisc.edu/vdt_139_cache] to [trusted.caches]?
</pre>

In this release of VDT, there is only one other question you will be asked and it is related to
globus web services which should not even be coming down now with this pre-ws version. <br>
You should answer 's' (Skip) to this.  Do not answer 'n'.
%RED% (This question doesn't happen in VDT 1.3.9  S. Timm 12/12/05)%ENDCOLOR%
<pre>
  Would you like to enable Globus web-services container to run automatically?
</pre>
In VDT 1.3.9 currently there is a question: 
<pre>
Would you like to enable the globus gatekeeper to run automatically?
</pre>
Answer 'y' (Yes) to this question.


<b>This will take between 10 and 60 minutes to complete, depending
upon the system and network connection.</b> During this time you may open a second 
terminal and watch the progress by monitoring the <tt>$VDT_LOCATION/vdt-install.log</tt> file.
The installation should complete with the following message. 
  <pre>Downloading [srmclient-1.20.tar.gz] from [hep.uchicago.edu]...
		  6/6 MB downloaded...
  </pre>



---+++Setup the OSG environment
 Assuming the Pacman install completed without fatal errors, you should now be able to source the OSG setup environment.
	 <pre> 
  $ <b>source setup.sh</b>
			or
	% <b>source setup.csh</b>
</pre> 

---+++Optional packages for Condor, PBS, LSF, FBSNG, or SGE
An extra package may be required to setup access to an existing Condor, PBS, LSF, FBSNG, or SGE installation. *Ensure that that the command-line utilities for your batch system are in your path*, then install the appropriate package (for Condor, PBS, LSF, FBSNG, or SGE respectively), making sure you 
are in $VDT_LOCATION at the time.  Note that for Web Sevices (WS-GRAM) installations, you should use the "-WS" version of the appropriate adapter package, for example <pre>Globus-WS-Condor-Setup</pre>, etc., rather than the pre-ws ones listed below.
<pre><b>
> pacman -get iVDGL:Globus-Condor-Setup
> pacman -get iVDGL:Globus-PBS-Setup
> pacman -get iVDGL:Globus-LSF-Setup
> pacman -get iVDGL:Globus-FBSNG-Setup
> pacman -get iVDGL:Globus-SGE-Setup
</b></pre>

---+++Post install README
Read the <tt>$VDT_LOCATION/post-install/README file</tt>.  This twiki page and others that it links to will take you through any steps that the file may provide instructions for. Most error messages in that file are not fatal. We suggest that you read the file for information only, and follow the instructions here in the twiki, rather than those in the README file.

---++Configure the Public Key Infrastructure
Minimally, you will need a host certificate and private key to identify of your CE.

---+++ Setup and maintenance of Certificate Authority connection

Configure the DOEGrids Certificate Authority (CA) to be used by default by running the utility below. 
<I>If there is an option "<b>choose the option
 which matches "1c3f2ca8 : ...</b>", then enter "<b>q</b>" at the prompts.</I>  ( %RED% have to add =--ca=1c3f2ca8= on command line. - from Alan Sill; Alain Roy: fixed in VDT 1.3.8 %ENDCOLOR%) 
		  <pre>&gt; <b>$VDT_LOCATION/vdt/setup/setup-cert-request</b>
Reading from /g3dev/globus/TRUSTED_CA
Using hash: 1c3f2ca8
Setting up grid-cert-request
Running grid-security-config...
<br>...<br></pre>

A <a href="http://www.cs.wisc.edu/vdt/releases/1.3.5/certificate_authorities.html">list of CAs</a> were added as authorized CAs on your system. Please review the list of authorized CAs and modify the set in /etc/grid-security/certificates as needed to match your local policy.  

The daemon <b>edg-crl-upgrade</b> should be running at all times in the background to refresh the CRLs from these CAs.  If CRLs are not kept current, incoming connections will fail. 
To check that it's running:

<pre> <b>
> ps axwww | grep edg-crl-upgrade
</b></pre>

If not running, do

<pre> <b>
> /etc/init.d/edg-crl-upgraded start
</b></pre>


The CertScripts package can assist you with choosing Certificate Authorities to trust and and periodically checking that the CRLs (Certificate Revocation Lists) have not expired.

---+++ Request and Install Host Certificate
To authorize this resource for use, a host certificate needs to be obtained from an appropriate Certificate Authority. Currently the 
<a href="http://igoc.ivdgl.indiana.edu/grid-install/documentation/www.doegrids.org"> DOEGrids CA </a>is the most common CA in use for OSG participants.
iVDGL and PPDG project instructions for getting a certificate are available on the <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/">iVDGL RA</a> and <a href="http://www.ppdg.net/RA">PPDG RA</a> sites (RA is Registration Authority). 
	 
Here is a brief guide to the process assuming you have a valid User Certificate. 
 
	* Run grid-cert-request to generate your host's private key (<tt>hostkey.pem</tt>) and the certificate request.
<pre>
	&gt; <b>cd $VDT_LOCATION</b>
	&gt; <b>. ./setup.sh</b>
	&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i></b>

Using configuration from <I>/usr/local/grid</I>/globus/etc/globus-host-ssl.conf
Generating a 1024 bit RSA private key
.++++++
.........................................................++++++
writing new private key to '<I>/usr/local/grid</I>/globus/etc/hostkey.pem'<br>-----<br>...<br>
</pre> 

	* Copy the <tt>$VDT_LOCATION/globus/etc/hostkey.pem</tt> to the <tt>/etc/grid-security</tt> directory <i>and preserve the read only for root permissions.</i>

	* The certificate request is stored as the file <tt>$VDT_LOCATION/globus/etc/hostcert_request.pem</tt> .
The important part (referred to as the PKCS#10 request) will look similar to the following:
<pre>-----BEGIN CERTIFICATE REQUEST-----
MIIBmzCCAQQCAQAwWzETMBEGCgmSJomT8ixkARkTA29yZzEYMBYGCgmSJomT8ixk
ARkTCGRvZWdyaWRzMREwDwYDVQQLEwhTZXJ2aWNlczEXMBUGA1UEAxMObXlob3N0
ZS5pdS5lZHUwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBALMM/3jhhkGUA3uz
5h4dzUzluDbCdBonv3UjeWbBFyh1JUA3d2148WBTtHAfQuB+f61tRcia2j5eEQEg
TwQnx86VaG8mAOk6gKe/zZlfhVkYR6OY1ssmc3dtkFoIprVoUUJCviq9DUp3DRO/
57/AtDAJsPhAl/UY5d2jFWl9jYGtAgMBAAGgADANBgkqhkiG9w0BAQQFAAOBgQAr
ZS88q8AgsOJ3n0EJoiAvr6dws4mr94Xg9uohNogmGsdjsdM3LG6Q+Qb22YdPPEH9
3HeLtcBlsjEnBq/a+M4IsfnPCn/6hvJs0HS7PxSR98bE116Ik4zuM1dtQ1Ce8Q/h
<nop>YtfstZj7FRZPZP1Lg2uftazDst4dkuu0msBP7dUAZw==</pre>
<pre>-----END CERTIFICATE REQUEST-----</pre>

Connect to the DOEGrids CA server and paste this into the
area provided in the form at the URL &lt;<a href="https://pki1.doegrids.org/ManServerEnroll.html">https://pki1.doegrids.org/ManServerEnroll.html</a>&gt;.
		  <i> All of the information must be filled out and in the
additional comments
section add the <b><a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/rastaff.php">
institutional sponsor</a></b></i>

	* Email will arrive indicating a link to retrieve the host certificate <tt>hostcert.pem</tt> .  Goto the link and download the certificate to <tt>/etc/grid-security/hostcert.pem</tt> on the host.

	* The following command will verify the certificate is readable.
<pre>&gt; <b>openssl x509 -text -noout -in /etc/grid-security/hostcert.pem</b>
</pre>
The output is something like:
<pre>
Certificate:<br>	 Data:<br>		  Version: 3 (0x2)<br>		  Serial Number: 1578 (0x62a)<br>		  Signature Algorithm: sha1WithRSAEncryption<br>		  Issuer: DC=org, DC=DOEGrids, OU=Certificate Authorities, CN=DOEGrids CA 1<br>		  Validity<br>				Not Before: Mar 19 14:48:32 2004 GMT<br>				Not After : Mar 19 14:48:32 2005 GMT<br>		  Subject: DC=org, DC=doegrids, OU=Services, CN=worldgrid.iu.edu<br>.....<br></pre>
	 
---+++ Get an LDAP service certificate to run authenticated MDS (optional)
MDS is the monitoring and directory service. The GRIS (Grid Resource info service, a sensor that collects site data and publishes it to MDS) is configured to allow only authenticated and authorized access. Access requires an LDAP service certificate (two files), owned by the MDS owner (if you installed as root, the owner is probably daemon -- check).
		  <pre>&gt; <b>cd $VDT_LOCATION</b>
&gt; <b>. ./setup.sh</b>
&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i> -service ldap</b></pre>
And follow the instructions, which are very similar to the host cert instructions, except that you are creating <b>ldapkey.pem</b> and <b>ldapcert.pem</b>. More info on service certificates can be found at <a href="http://www.cs.wisc.edu/vdt/releases/1.3.9/installation_post.html#servicecert">http://www.cs.wisc.edu/vdt/releases/1.3.9/installation_post.html#servicecert</a>

---++ Configuration and Setup of OSG CE Services

---+++!!Globus Configuration
Globus has been pre-configured for this installation. If you wish to reconfigure
it, there are separate scripts now to configure the globus_gatekeeper, globus_ws, and gridftp.
They are at <tt>$VDT_LOCATION/vdt/setup/configure_globus_gatekeeper, $VDT_LOCATION/vdt/setup/configure_globus_ws</tt>, and <tt> $VDT_LOCATION/vdt/setup/configure_gridftp</tt> 
respectively. Usage can be found by invoking the script with option --help.  
In particular, =$VDT_LOCATION/vdt/setup/configure_globus_gatekeeper --server y= will let you
enable the Globus Gatekeeper if you didn't enable the globus gatekeeper automatically during
the VDT location.  Documentation for configure_globus_gatekeeper is found at 
[[http://vdt.cs.wisc.edu/releases/1.3.9/config/configure_globus_gatekeeper.html]]

Review the configuration files <tt>/etc/xinetd.d/globus-gatekeeper</tt> and
<tt>/etc/xinetd.d/gsiftp</tt> (or in <tt>/etc/inetd.conf</tt>). Additionally 
these services must be listed in the <tt>/etc/services</tt> file.  When you are happy,
restart the xinetd (or inetd) daemon to pick up the configuration changes:
<pre><b>
[root@mysite grid]# /etc/rc.d/init.d/xinetd restart</b>
Stopping xinetd:														 [  OK  ]
Starting xinetd:														 [  OK  ]
</pre>

To verify that the gatekeeper is running at this point, you should be able to telnet to the public IP address of your site on port 2119 and get a response. Run <tt>telnet <i>hostname port</i></tt>.  It should return <tt>Connected to...</tt>. The same should be true of the gsiftp port, 2811 by default.  There is only 
one gsiftp in GT4.0/VDT1.3.9.

---+++(Optional:) Start the Web Services Container

BEFORE DOING THIS, READ AND FOLLOW THE INSTRUCTIONS IN THE post-install/README ON CONFIGURING WS CONTAINER ACCESS USING visudo!  Once this is done, you should be able to start the web services interface, if you have selected to install the -ws package, by doing:

==/etc/init.d/globus-ws start==

---+++Configuring the Grid3 Information Provider

Grid3 developed a set of conventions about information to provide for a Computing Element. OSG is continuing to use those conventions to provide backward compatibility and as an interrim solution until consensus on a more dynamic information system is achieved. The Grid3 Information Provider collects that information and makes it available, via a standard config file (<tt>$VDT_LOCATION/monitoring/grid3-info.conf</tt>), to other applications/services.
There is a configuration script (<tt>$VDT_LOCATION/monitoring/configure-osg.sh</tt>) which automates much of the configuration.

The prompts in configure-osg.sh have recently changed, see LocalStorageConfiguration for new
directory descriptions and instructions.

The meaning and purpose of the various elements of the Grid3 schema are documented further in the GridThreeSchema
pages. New resource admins may want read that information carefully and determine how to map those elements onto their Resource before proceeding. Guidance on the basic elements and common defaults is provided below.

---++++!!Gather configuration information

OSG strives to make the minimum requirements on a resource, however, to provide a basic execution environment that applications can build upon, certain information about file/filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. To this purpose, four special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node or gatekeeper node and also available, using the exact path, on each of the worker nodes or simply shared filespace.

* %RED%Grid Directory%ENDCOLOR%

	 This directory is where the Grid environment will be installed. This directory will contain the Globus middleware and other middleware applications. It should be writable for the user root. This directory contains both server and client utilities for the middleware and therefore should be shared between gatekeeper and worker nodes.

* %RED%Temporary Directories%ENDCOLOR%

	 There will be two temporary directories one will be local to the worker node and the other shared. The shared temporary directory will be the current working directory of a running application and persist only as long as the job is executing. The files in this directory should be managed by the running application Automated clean up may be required to be handled by the site administrator. All users should 
be able to use this directory for writing and reading files. At least 10G byte of space should be allocated per worker node. The local temporary directory may be used by applications to reduce latency to data. At least 10G should be available in this directory. 

* %RED%Data Directory%ENDCOLOR%

	 The data directory will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10G byte of space should be allocated per worker node. One VO would like 100G+ per worker node. 

* %RED%Application Directory%ENDCOLOR%

	 If required or desired this will be the location of Grid3 the application software. Only users in the application VO will have write privileges to these directories. At least 10G byte of space should be allocated per application. 


One of the questions enquires about the <b>VO sponsor</b> of this site.
This attempts to determine the longterm average allocation between VOs of the current cluster. The notation
incorperates a VOname followed by a percentage so that clusters are able to note multiple VO partners. 

---++++!!Execute the configuration script

Run the following script as root to execute the configuration script.

<pre>
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
</pre>

For a typical installation, the script will look something like this:

<pre>
Please specify your OSG SITE NAME [_hostname.domain.tld_]: <b><i><a href="#UniqueName">UNIQUE_NAME</a></i></b>
Please specify your OSG BASE_DIR [/usr/local/grid]:
Please specify your OSG APP_DIR [/app]:
Please specify your OSG DATA_DIR [/data]:
Please specify your OSG TMP_DIR [/scratch]:
Please specify your OSG TMP_WN_DIR [/tmp]:

Examples of possible VO Sponsors are usatlas, ivdgl, ligo, 
uscms, sdss...
You can express the percentage of sponsorship using
the following notation:<br/>
 "myvo:50 yourvo:10 othervo:20 local:20"  <font color=red>Please do not use single quotes. It interferes with the database query statement</font>
Please specify the VO sponsor of this site [iVDGL]:

Please specify the Batch Queuing to be used [condor]:

Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
Grid3 Location:  <i>/usr/local/grid</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Shared Temp:	  <i>/scratch</i>
WorkerNode Temp: <i>/tmp</i>
JOB Manager:	  <i>condor</i>

Is this information correct (y/n)? [n]: <b>y</b>
#</pre>

This configure script creates the $VDT_LOCATION/monitoring/grid3-info.conf file. 
This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.

---+++Job Policy and the Batch Queuing System 
The configuration of a production queue manager is beyond the scope of this document. Since there will be periodic validation submitted by DNs in the "mis" VO, it is recommended sites provide at least two levels of priority and assign the lowest priority to user mapped to the "mis" VO.  You should start the 
batch system if you have not done so at this time.

The commands
<pre># <b>CONDOR_CONFIG="$CONDOR_LOCATION/condor/etc/condor_config"</b>
# <b>export CONDOR_CONFIG</b>
# <b>$CONDOR_LOCATION/condor/sbin/condor_master</b></pre>
will start the personal-condor batch installation with 
default queuing, if you followed the instructions in CondorHints above.

There is a file $CONDOR_LOCATION/post-install/condor which can be copied
to /etc/rc.d/init.d for auto startup of personal condor if you wish.
Also note that the CONDOR_CONFIG and CONDOR_LOCATION variables are set correctly 
if you have sourced $VDT_LOCATION/setup.sh above, and the condor_master and all
other condor binaries will be in the path.

---+++ Authorizing Users
---++++!!Test Configuration (using local grid mapfile)

The first step is to configure the CE to allow access using your own Grid credentials. Once that's done you'll be able to run local tests and verify operation of your CE locally.

Make sure you have a Grid proxy for yourself. 
(Run grid-proxy-init or voms-proxy-init are the usual methods)  

<pre>
> grid-proxy-info
</pre>
should return something like 
<pre>
subject  : /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane
issuer	: /DC=gov/DC=fnal/O=Fermilab/OU=Certificate Authorities/CN=Kerberized CA
identity : /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane
...
</pre>

Take the subject string and prepend it to the file /etc/grid-security/grid-mapfile and assign it to a local user account (you can use any of the VO accounts you've created at the beginning to test). So the grid-mapfile should have at least one entry like:
<pre>
 > cat /etc/grid-security/grid-mapfile
"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane" usatlas1
</pre>

This is enough to enable the rest of the installation testing.

---++++!!Osg CE Authorization (using VOMS/GUMS/PRIMA)

OsgCEAuthorization contains further instructions on how to configure the CE to allow others to access it. You can come back to that step after verifying basic operations of your CE. 

---++Monitoring Setup

---+++ Configure MIS-CI (CoreMIS)
The Monitoring and Information Services Core Infrastructure (MIS-CI) provides 
information on the site environment and computing resources. The OSG-CE package includes MIS-CI. 
This section describes how to configure MIS-CI if you wish to enable it.

The $VDT_LOCATION/MIS-CI/configure-misci.sh script performs the configuration. It
creates or adds a crontab for the MIS-CI information collectors. (If a GUMS maps the GridCat DN to a different account, not ivdgl, see below.) As root, execute:
<pre>
  <b># cd $VDT_LOCATION</b> 
  <b># . setup.sh</b> 
  <b># $VDT_LOCATION/MIS-CI/configure-misci.sh</b> 
			  Editing site configuration...
				...
			  Would you like to set up MIS-CI cron now? (y/n) <b>y</b>
			  At what frequency (in minutes) would you like to run MIS-CI ? [10]
			  ...
			  Would you like to add MIS-CI crontab to this ? (y/n) <b>y</b>
			  ...
			 configure--misci Done
			  Please read $VDT_LOCATION/MIS-CI/README 
</pre>
If a GUMS maps the GridCat DN to a different account, not ivdgl, then notice that
<tt>$VDT_LOCATION/MIS-CI/configure-misci.sh</tt> has the option <tt>--choose_user</tt> .
If a different user is preferred to the default user which is chosen by MIS-CI (ivdgl),
please configure MIS-CI using this option instead of above command:
<pre>
<b># $VDT_LOCATION/MIS-CI/configure-misci.sh --choose_user</b> 
</pre>
%RED%Don't you need to specify the user in the above command?%ENDCOLOR% No, it will ask you.

After finishing configuring the MIS-CI, a few checks might be necessary:

1. Check if crontab is created as root, as ivdgl user, or as any other user chosen, e.g.:
  <pre><b> > crontab -u ivdgl -l</b></pre>

2. If you want to force an MIS-CI table update (due to fresh install or update), then,
as the MIS-CI user, execute:
<pre>
<b> > $VDT_LOCATION/MIS-CI/sbin/run-mis-ci.sh</b> 
</pre>


3. As a non-root user, check if at least one table is filled
	(If you chose not to force an update, it might take 10 minutes or so before the tables are filled with current information.)
<pre>
<b> > grid-proxy-init</b>
<b> > globus-job-run &lt;hostname&gt;/jobmanager-mis /bin/sh "siteinfo"</b>
		(Here &lt;hostname&gt; is the CE hostname.)
</pre>


---+++ Configure MDS (GRIS)

The Globus information system is called MDS and is preconfigured to read the grid3-info.conf information file.	

The configuration script (VDT_LOCATION/vdt/setup/configure_mds) is executed automatically during the initial
download with default values.

To test the GRIS:
	* first you need an <a href="#Get_an_LDAP_service_certificate">LDAP service certificate</a> installed in /etc/grid-security/ldap
	* both the directory and certificate files should be owned by the user daemon.
			 <pre><b>chown -R daemon /etc/grid-security/ldap</b></pre>
	* One may (optionally) start the Globus information service daemon (GRIS) with the following command (as root):
			 <pre><b>/etc/init.d/gris start</b> </pre> 
	* Then, using <u>your</u> user account, craete a grid proxy using the  <b>grid-proxy-init</b> command
	* Then, run the grid-info search commnd (you should see a whole list of output regarding your service.
			 <pre><b>grid-info-search -h your.fully.qualified.hostname</b></pre>
	* MDS should be configured for anonymous bind), so the following should work as well
			<pre><b>grid-info-search -x -h your.fully.qualified.hostname -p port_number</b></pre>


---+++ Activate Your Site

CS sites added to GridCat are presumed to be inactive if site state bit is not set
to be 1 (see below).
Inactive CS sites will have the site status dot with the grey color. Once the site becomes active, the site status dot will become either green or red, depending on the GridCat test results.
Since the default site state is presumed to be inactive, the CS site admin has to proactively switch the site state to be active. The switching can be done by modifying the file <tt>$VDT_LOCATION/MIS-CI/etc/grid-site-state-info</tt>.

If the value of <tt>grid_site_state_bit</tt> is 1, the site commences its active state, i.e.,
the content of <tt>$VDT_LOCATION/MIS-CI/etc/grid-site-state-info</tt> will look like:
<pre>
export grid_site_state_bit=1
</pre>
<font color='magenta'>It might take up to 2 hours until this change 
takes effect in the GridCat display.</font>

If the site decides to become inactive for various reasons, e.g., site maintenance, the site admin can set the value of <tt>grid_site_state_bit</tt> to be other than 1. 
<br/>
Here is an example <tt>grid_site_state_info</tt> file:
<br/>
<a href="http://www.ivdgl.org/MIS-CI/grid-site-state-info">grid-site-state-info</a>

---++ Configuring optional components
---+++ [[MonALISA][MonALISA]]
---+++[[GenericInformationProviders][Generic Information Providers (GIP)]]
---+++[[DiscoveryServiceInstallation][Discovery Service Installation]]

---++ Site Verification
---+++site-verify
Now you're ready to run the full CE site verification test suite. All elements of this test should now pass.

(NB: To run the site verify script you should not be *root* )

<tt><b>
$ cd $VDT_LOCATION<br>
$ source ./setup.sh<br>
$ grid-proxy-init <br></b>
Enter "<i>Your Passphrase</i>"<br><b>
$ cd verify<br>
$ perl site_verify.pl --host=<i>hostname.domain.tld</i><br>
</b></tt>

---++ Authorizing Users: Operational Configuration
The earlier test case only authorized yourself as a local user.  You should now go to OsgCEAuthorization and authorize other users before registering your service (otherwise no one but you will be able to access it!) 

---++ OSG Registration



To register the site with the OSG Grid Operations Center and into the Grid Catalog please use the [[http://www.opensciencegrid.org/index.php?option=com_wrapper&Itemid=68&elMenu=Grid%20Support][webform]] located under the OSG [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=47&Itemid=65][Administrator Support page]]. If you are registering into the ITB, be sure to check the appropriate box for which grid catalog you are registering with. You should receive an email response automatically back from the GOC to the operations contact you supplied. If this response doesn't arrive within a reasonable delay, please resubmit your registration.

A minimal amount of information is needed for the OSG Grid Operations Center (GOC) to publish a site to the monitoring and operational infrastructure. This includes organization name, organization manager or designated representative name and email, security contact name and email, resource URL, support center, and form submitter. 

While this minimal information will allow the GOC to publish your information to the monitoring tools, more information is requested to make site and support center communication easier. Please take time to fill out the form completely.

---++ Firewalls

Grid components are distributed throughout the network, and services 
such as gatekeepers and data movement utilties are required to be 
accessible to the dynamic cloud of clients and peer services.
This ditributed and dynamic requirement places the burden
of the security on the implementation of the application.

Due to the discovery of significant vulnerabilities in recent years, network-based applications are untrusted by default. To solve the application
problem effort has focused on developing and deploying firewalls which
restricts full and free network access.  (You might say that this is analogous to building 
a house with no doors. Is it safe? Yes. Is it useful? No.)

Some essential network-based applications have been
"hardened," such as mail relay services, web servers, and secure
shell daemons. These are further protected further by IP address 
filtering to prevent access from unknown hosts or domains.

Grid components which are located behind network firewall 
face special challenges for Grid setup and operations. 

There are two styles of firewalls usually encountered. 

	* A network firewall which is upstream from your server 
(usually centrally maintained). This blocks all traffic to your 
host. 

	* Host-based firewalls which are setup and maintained 
by individual host administrators. This is usually setup and configured
by the <b>iptables</b> program which filters incoming network packets which 
arrive for the host.

 In addition to host-based firewalls, hosts can choose to 
 implement host based access rules (usually setup with the <b>tcp_wrapper</b>
  or <b>hosts_allow</b> utilties.

Network traffic can be blocked at the firewall for both incoming and outgoing dataflow
depending on hostnames, ip addresses, ports and protocols. 


A common setup is to allow any outgoing connection,
while significantly (if not completely) restricting 
incoming connections.  The Globus project provides thorough documentation on this subject which will not be repeated here.  It is strong encouraged that you read the document <A HREF="http://www.globus.org/security/v2.0/firewalls.html">Globus Toolkit Firewall Requirements</A> to avoid issues which may arise from firewall configuration.


IP port usage for OSG services may require the following firewall updates:

	* MDS: 2135/tcp
	* GRAM: 2119/tcp
	* GridFTP: 2811/tcp
	* GRAM callback: contiguous range if tcp ports as specified by definition <b>GLOBUS_TCP_PORT_RANGE=start,stop</b> .  A minmun range of 100 for a small site. 
	* Monalisa: 9000/udp (for ABping measurements).  These are specified in the file <B>$VDT_LOCATION/MonaLisa/Service/<I>cluster_name</I>/ml.properties</B>
	* For web services: 9443 in current versions of the osg-0.3.x package.  (May be changed back to 8443 if the VDT changes from a container to a Tomcat module in the future.)
	
GRAM and GridFTP need to know the port range that you've opened up. You need to set two environment variables, GLOBUS_TCP_PORT_RANGE and GLOBUS_TCP_SOURCE_RANGE. Some people set these in the xinetd configuration, but you can also set it in $VDT_LOCATION/vdt/etc/vdt-local-setup.sh. Then these will be used by GRAM, GridFTP, and any clients that require them.

By the way, the VDT is working on a better solution for setting these variables, and it will hopefully be in an upcoming VDT release. 

These ports and protocols <i>must be open</i> to all grid clients and
server machines participating in the grid in order to provide minimal
functionality.

You also may need to open the following optional ports for additional Grid services:

	* GIIS: 2136/tcp
	* GSISSH: 22/tcp
	* MyProxy: 7512/tcp
	* VOMS: 8443/tcp
	* RLS server: 39281/tcp

Here is a sample of the a <tt>/etc/hosts.allow</tt> with the GLOBUS services opened:
<verbatim>

# cat /etc/hosts.allow
#
# hosts.allow	This file describes the names of the hosts which are
#					allowed to use the local INET services, as decided
#					by the '/usr/sbin/tcpd' server.
#
sshd: 129.79.6.113 
ALL : localhost
vdt-run-gsiftp.sh : ALL
vdt-run-gsiftf2.sh : ALL
vdt-run-globus-gatekeeper.sh : ALL
</verbatim>

*For RH9, RHEL3 or compatible iptables systems*

The default firewall configuration for Red Hat's iptables sets the system up with a stateful packet filter. This is different than some legacy RH7 systems as by default no ports that are not explicitly opened by the iptables script will be open. This includes high numbered ports that are often used by grid services. 

If your preference is to leave as much of the stateful packet filtering in place but enable just those grid services you want to deploy then you can use the following instructions. 

Two changes need to be made to an OSG gateway with a host based iptables stateful firewall. 

First is the configuration of the firewall itself. On RHEL or similar systems this is done in /etc/sysconfig/iptables

The Chain RH-Firewall-1-INPUT is a default chain for RHEL3. This chain is also sometimes called INPUT. Make sure the following rules use the chain that other rules in /etc/sysconfig/iptables do. 

Note: For GSISSH this port is often already open for systems. You can use either this rule or the default rule setup at install time if you selected custom firewall and enabled ssh. 

<verbatim>
# Globus: Requires addition configuration in /etc/xinetd.d/globus-gatekeeper
# set: env = GLOBUS_TCP_PORT_RANGE=40000,50000
# This allows up to 10,000 ports and matches the globus config.
# How globus is configured to use these ports is subject to change in an upcoming
# release
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 40000:50000 -j ACCEPT
# Monalisa, grabs 3 ports from the following range
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 9000:9010 -j ACCEPT
# Gridftp
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2811 -j ACCEPT
# MDS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2135 -j ACCEPT
# GRAM
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2119 -j ACCEPT

# Optional Services
# RLS Server
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 39281 -j ACCEPT
# MyProxy
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 7512 -j ACCEPT
# GSISSH/SSH
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT
# GIIS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2136 -j ACCEPT
# GUMS/VOMS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 8443 -j ACCEPT
</verbatim>


Second is the configuration of globus to use the above port range. _This may change in upcoming VDT releases, see above_


/etc/xinetd.d/globus-gatekeeper
<verbatim>
service globus-gatekeeper
{
			socket_type = stream
			protocol = tcp
			wait = no
			user = root
			instances = UNLIMITED
			cps = 400 10
			server = $VDT_LOCATION/vdt/sbin/vdt-run-globus-gatekeeper.sh
			env	 = GLOBUS_TCP_PORT_RANGE=40000,50000
			disable = no
}
</verbatim>

You should probably also add the port range to <tt>$VDT_LOCATION/globus/etc/globus-job-manager.conf</tt>, 
to be sure it's always picked up, by adding the following line:

<verbatim>
		  -globus-tcp-port-range 40000,50000
</verbatim>

_Note: $VDT_LOCATION should be set by the pacman installer_

After editing the above files run the following commands

<verbatim><b>
# /etc/rc.d/init.d/iptables restart</b>
Flushing firewall rules:											  [  OK  ]
Setting chains to policy ACCEPT: filter						  [  OK  ]
Unloading iptables modules:										  [  OK  ]
Applying iptables firewall rules:								  [  OK  ]
<b># /etc/rc.d/init.d/xinetd reload</b>
Reloading configuration:											  [  OK  ] 
</verbatim>

---++Troubleshooting Guide
As you install, monitor the $VDT_LOCATION/vdt-install.log.  

	* If pacman tries to retrieve something from a website that's having problems, you'll get an error message that's unrelated to the real problem because pacman can't recognize 404 errors when downloading tarballs.  For example, when the PRIMA download site was down, it told us the file wasn't in the correct format:
<pre>
vdt-untar is untarring prima-0.3.x86_rh_9.tar.gz
gzip: stdin: not in gzip format 
</pre>
---++Shutdown Guide
Please see the [[OSG Shutdown Guide]].


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->

%STOPINCLUDE%

%META:TOPICMOVED{by="DaneSkow" date="1115753408" from="Integration.OSGCoreInstallGuide" to="Integration.OSGCEInstallGuide"}%
