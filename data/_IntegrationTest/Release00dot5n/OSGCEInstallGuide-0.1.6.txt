%META:TOPICINFO{author="BockjooKim" date="1113836719" format="1.0" version="1.40"}%
%META:TOPICPARENT{name="WebHome"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%
%STARTINCLUDE%

---++ Introduction
This document is intended for site administrators responsible for OSG 
middleware installation and configuration. It is not meant as an all-inclusive install guide.  The instructionsassume the following Linux distributions:

	* Red Hat 7.x
	* Red Hat 9.0
	* Red Hat Enterprise Linux 3
	* Fedora Core 3 
	* Debian Linux 3.1 (Sarge)

Installation may be successful with other Linux distributions, but they have not been tested.  

The proposed installation and configuration method is based on <a href="http://physics.bu.edu/pacman/">Pacman</a>.  Pacman is a package manager like RPM or dpkg, but is able to work on and support multiple platforms.

---+++ Conventions used
The following conventions are used in these pages:

	* =monospaced text= indicates terminal output 
	* ==bold monospaced text== indicates terminal input (by the user)
	* =<i>monospaced text in italics</i>= indicates variable data that may differ on your installation
	* the =&gt;= prompt indicates commands that do not require a specific shell or root access
	* the =#= prompt indicates commands that require root access
	* the =$= prompt indicates sh- or bash-specific commands
	* the =%= prompt indicates csh- or tcsh-specific commands


---++ Pre-installation Checklist
---+++ System or Cluster Already Installed
It is assumed that your hardware is already running one of the operating systems previously mentioned. If your system is a cluster, the batch queuing system should be likewise previously installed and configured, unless you intend to use the Condor system that will be installed with the VDT.	

---+++ Preserving a pre-existing Condor installation
If you would like to preserve your Condor installationyou can setup the installer to recognize it by setting
two environment variables.  Doing so will produce the following effects:
	* Condor will be installed but not configured. 
	* The VDT middleware will point to the necessary locations in your external Condor installation.

To specify an external Condor installation you need to set the following two environment variables before starting the installation:
	* *VDTSETUP_CONDOR_LOCATION* - the location of your Condor installation  (e.g. _/opt/condor_). The Condor _bin/_, _bin_, _etc/_, _lib/_... directories should be directly under this location.
	* *VDTSETUP_CONDOR_CONFIG* (optional): The location of your Condor configuration file (if non-standard). Default is _$VDTSETUP_CONDOR_LOCATION/etc/condor_config_.

For example, if you have Condor installed in the standard location, the following should be sufficient to let the VDT installation scripts know to preserve your existing installation:

<pre>[root@mysite root]# env | grep CONDOR 
CONDOR_IDS=501.501
CONDOR_CONFIG=/opt/condor/etc/condor_config
CONDOR_ROOT=/opt/condor
[root@mysite root]# export VDTSETUP_CONDOR_LOCATION=$CONDOR_ROOT</pre>

It is NOT necessary to execute the setup.sh from any previously existing VDT installation in order to do an upgrade.

---+++ Existing reliable network
It is assumed that your hardware is connected to a reliable network connection by which the software may be retrieved and from which
services may contact your system.

---+++ Time synchronization (NTP) and reverse name lookups (DNS)

Each system should be setup to support network time protocol. Lack of synchronization generally complicates troubleshooting and can cause problems with the security infrastructures evaluation of eg. proxy lifetimes.

Also for the middleware to correctly function both the forward and reverse lookups as configured through a local DNS service are required for the IP address of the system.

---+++ Unique OSG Site Name
Each OSG site will need to designate a *unique name* by which services and resources may refer to the site. This name will be displayed on the Site Catalog and used in tables for other monitoring and accounting.  For example, the University of New Mexico has a unique name of UNM_HPC; the University of Buffalo is Buffalo_CCR.

---+++ Previous OSG or Grid3 Installations
 If there is a _previous installation_ of the OSG or Grid3 environment or 
 other Globus middleware please _stop the processes_ which are currently running.
 This includes the Globus Resource Information Service (GRIS), MonALISA and other 
 services configured to start upon initialization on your system.
 More information is provided in OSGShutdownGuide.


---+++ Creation and setup of VO accounts
VO accounts need to be created by the system administrator.  The accounts are: 
	* cdf
	* grase
	* fmri
	* gadu
	* btev
	* uscms01
	* uscms02
	* usatlas1
	* usatlas2
	* sdss
	* lsc01
	* ivdgl
	* star
	* osg

Ensure that the users are enabled to use the batch queuing system.
Note, this list of users is an implicit policy statement, should have link
to what OSG policy is on this, if it exists.



---++ Getting Pacman
The Pacman tool downloads a local copy and then installs the software needed for the installation. During the installation, scripts are run
to specially configure the local node. More about using Pacman can be found from the <a href="http://physics.bu.edu/pacman">Pacman documentation</a>. 



	* *Download Pacman*
<a href="http://physics.bu.edu/%7Eyoussef/pacman/sample_cache/tarballs/pacman-latest.tar.gz">Pacman 3.11</a> is the most recent version
  tested with VDT 1.3.5. For Pacman version 3 ensure the correct python version 
  is installed and available (2.3.x or newer).  (E-mail from 
Saul Youssef says that python 2.3.x is not actually required, 
2.2 versions are fine.).
  
	<pre><b>wget http://physics.bu.edu/pacman/sample_cache/tarballs/pacman-latest.tar.gz</b></pre>


	* *Unpack the tarball* 
Note that Pacman does not need to reside in your VDT directory.

  ==tar xzvf pacman-latest.tar.gz==


	* *Setup your environment to use Pacman*
 
  ==cd pacman-*==

For sh and bash:

<pre>$ <b>source setup.sh</b></pre>
For csh and tcsh:
<pre>% <b>source setup.csh</b></pre>
</pre>

	* Note that for Pacman 3.11 and some Scientific Linux
distributions, such as SLF304, you need to do the command:
<pre>
%<b>pacman -allow unsupported-platforms</b>
</pre> 
(should be fixed in Pacman 3.12).



---++ Installing the Core OSG Infrastructure

	* *Decide on an installation directory for the software*
This is typically <b>/usr/local/grid</b>, but whatever suits the
local resource structure is fine. If you are installing on a
cluster, this directory must be available on all the nodes. 
If this directory is not shared additional installation of
software will be required on each of the worker nodes. Nearly all of
the software to be installed is from the Virtual Data Toolkit (VDT).

The installation described here is done as root.  However, non-root
installs are supported.  Not all services will run as root; Condor, 
MonALISA and the GRIS will run as the user "daemon". If you prefer to run
MonALISA and/or Condor with its own UID, you need only to create a 
<b>monalisa</b> and/or <b>condor</b> user before proceeding with installation the VDT will configure the daemons appropriately.  _Verify that the umask is set to "0022" prior to installation.  Failure to do so may render the installation unusable._

A few questions regarding trust of the caches from which the software is
downloaded will be displayed. Please answer *y* (yes) so that the
software can be retrieved. (Note that in Pacman 3.11
it may be necessary to do the following if you are installing on Scientific Linux Fermi.  Pacman 3.11 doesn't support
SLF3.0.x although it's help text says it does.)  Also
make sure there are no non-standard versions of perl, tcsh, or
bash in your $PATH

=%<b>pacman -pretend-platform:SLC-3</b>=


<pre>
&gt; <b>pacman -get iVDGL:osg-0.1.5</B> 
Do you want to add [iVDGL] to [trusted.caches]? (y or n): <b>y</b>
Package [osg-0.1.5] found in [iVDGL]...
Package [osg-auto-0.1.5] found in [iVDGL]...
Package [VDT_135] found in [iVDGL]...
Do you want to add [http://www.cs.wisc.edu/vdt/vdt_133_cache] to [trusted.caches]? (y or n): <b>y</b>
...
...
...
</pre>
If you plan to use Condor, answer <b>y</b> (yes) to the question posed regarding setting up the Condor job manager.  
By default, a personal-condor instance is installed to facilitate testing.<br>

<pre>If you would like, we can set up a Globus jobmanager interface
to Condor.

This will allow Globus Gatekeeper to run jobs on Condor.

Would you like to enable Globus jobmanager for Condor?
Choices: y (yes), n(no), s (skip this question) <b>y</b>
</pre>

 Other questions may be asked it is safe to answer <b>n</b> (no).


<b>This will take between 10 and 60 minutes to complete, depending
upon the system and network connection.</b> The installation should complete
with the following message. During this time you may open a second 
terminal and watch the progress by monitoring the <b>vdt-install.log</b> file.

	* *Set up your environment*  
Assuming the Pacman install completed without
fatal errors, you should now be able to source the OSG setup environment.
<pre>  $ <b>source setup.sh</b>
</pre> 
or
<pre>  % <b>source setup.csh</b>
</pre> 

	* *Optional extra packages for PBS, LSF, or FBSng setup*

	  An extra package may be required to setup access to an
existing PBS, LSF, or FBSng installation. If you plan on using condor no action is necessary.
<b>Ensure that that the command-line utilities for your batch system 
are in your path,</b> then install the appropriate package (for PBS, LSF, or 
FBSng, respectively): 
<pre>&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/vdt_133_cache:Globus-PBS-Setup</b><br><br>
&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/vdt_133_cache:Globus-LSF-Setup</b><br><br>
&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/vdt_133_cache:Globus-FBSNG-Setup</b><br>	</pre>

---+++ Compute Element Middleware Setup

---++++Globus configuration
Globus has been pre-configured for this installation. If you wish
reconfigure it, please review the <a href="http://www.cs.wisc.edu/vdt/globus_config.html">
Globus Configuration Script</a> document on using
the <b>$VDT_LOCATION/vdt/setup/configure_globus.sh</b> script. 

Review the configuration files <b>/etc/xinetd.d/globus-gatekeeper</b> and
<b>/etc/xinetd.d/gsiftp</b> (or in <b>/etc/inetd.conf</b>). Additionally 
these services must be listed in the <b>/etc/services</b> file.  When you are happy,
restart the xinetd (or inetd) daemon to pick up the configuration changes:
<pre>
[root@mysite grid]# /etc/rc.d/init.d/xinetd restart
Stopping xinetd:														 [  OK  ]
Starting xinetd:														 [  OK  ]
</pre>

To verify that the gatekeeper is running at this point, you should be able to telnet to the public IP address of your site on port 2119 and get a response.  The same should be true of the gsiftp port (2811 by default, or 2812 for the new server).


---++++Configuring the Grid3 Information Provider
The setup of Grid3 Information Provider is partly accomplished with the
assistance of the configuration script <b>configure-grid3.sh</b> and partly
by modifying a few files with detailed information. Enter the 
<a href="#unique_name"><b>unique name</b></a> when asked for the <B>GRID3 SITE NAME</B>.

If you are upgrading your site, save *$APP/etc/grid3-locations.txt* 
before running the *configure-grid3.sh* script. Please restore it after completing the configuration.

Change into the VDT root directory.
=&gt; <b>cd $VDT_LOCATION</b>=
Source the environment; for sh and bash:
=$ <b>. ./setup.sh</b>=
For csh and tcsh:
=% <b>source ./setup.csh</b>=
The *configure-grid3.sh* script is located in the <b>monitoring</b>
directory.  (It must be run as root.)

<pre>
# <b>cd monitoring</b>
# <b>./configure-grid3.sh</b>


Please specify your GRID3 SITE NAME [_hostname.domain.tld_]: <b><i>UNIQUE_NAME</i></b>

Please specify your GRID3 BASE_DIR [/usr/local/grid]:

Please specify your GRID3 APP_DIR [/app]:

Please specify your GRID3 DATA_DIR [/data]:

Please specify your GRID3 TMP_DIR [/scratch]:

Please specify your GRID3 TMP_WN_DIR [/tmp]:

Please specify your GRID3 VO [iVDGL]:

Please specify the Batch Queuing to be used [condor]:


Please review the information:
Grid Site Name:  <i>UNIQUE_NAME</i>
Grid3 Location:  <i>/usr/local/grid</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Shared Temp:	  <i>/scratch</i>
WorkerNode Temp: <i>/tmp</i>
JOB Manager:	  <i>condor</i>

Is this information correct (y/n)? [n]: <b>y</b>
#</pre>

Start the information service daemon (optional):
<pre># <b>/etc/init.d/gris start</b>
</pre> 

---++++Job Policy and the Batch Queuing System 
A queuing manager is used to execute the incoming Grid jobs.
The configuration of a queue manager is beyond the scope of this
document.
The command
<pre># <b>CONDOR_CONFIG="$VDT_LOCATION/condor/etc/condor_config"</b>
# <b>export CONDOR_CONFIG</b>
# <b>$VDT_LOCATION/condor/sbin/condor_master</b></pre>
will start the default personal-condor batch installation with 
default queuing, if you chose the default condor installation earlier.

---+++ Initial Site Verification
  
To verify many of the installed OSG software components
execute the <b>site_verify.pl</b> (Perl) script.  

The script itself is available in the $VDT_LOCATION/verify/ directory.
Documentation can be found at &lt;<a href="http://griddev.uchicago.edu/download/grid3/doc.pkg/WIP/site_verify_pl.html">http://griddev.uchicago.edu/download/grid3/doc.pkg/WIP/site_verify_pl.html</a>&gt;.

To run the site verify script you should not be *root* :

<tt>
$ cd $VDT_LOCATION<br>
$ source ./setup.sh<br>
$ grid-proxy-init <br>
Enter "<i>Your Passphrase</i>"<br>
$ cd verify<br>
$ perl site_verify.pl --host=<i>hostname.domain.tld</i><br>
</tt>

---+++Monitoring Setup

---++++Configure Monalisa
The setup of MonALISA is done with the assistance of the
configuration script. However there are two files which need to be edited.
The <b>$VDT_LOCATION/MonaLisa/Service/CMD/site_env</b> script must be modified 
manually to set the location of the batch queue.  In addition, the 
<b>$VDT_LOCATION/MonaLisa/Service/VDTFarm/vdtFarm.conf</b> file must be updated
for the new <b>TracePath</b> module.
  
	* Run the configuration script <a href="http://www.cs.wisc.edu/vdt/releases/1.3.1/configure_monalisa.html">$VDT_LOCATION/vdt/setup/configure_monalisa.sh</a> to set up MonALISA.
<i>The name <b>OSG-ITB</b> group should be used when queried for the "monitor group name."</i>
<br>
<pre>&gt; <b>./configure_monalisa.sh</b>


Please specify user account to run MonaLisa daemons as: [daemon]: 

This is the name you will be seen by the world, so please choose
a name that represents you. Make sure this name is unique in the 
MonaLisa environment.

Please specify the farm name [<i>hostname.domain.tld</i>]: <b><i>UNIQUE_NAME</i></b>

Your Monitor Group name is important to group your site correctly
in the global site list. Grid3 users should enter "grid3".

Please enter your monitor group name [<i>hostname.domain.tld</i>]: <b>OSG-ITB</b>
...
...
...</pre>

Fill in your site specific information. *NOTE: Check your latitude and longitude entries!  
If you are in the Americas, for example, your longitude will be a negative number.	
Typical continental U.S. latitudes range from 25 to 55 degrees and longitudes range from 
-60 to -120 degrees.  If you get it wrong, your site will not show up properly in MonALISA.*

<pre>
Please review the information:

MonaLisa user: daemon 
Farm name: <i>UNIQUE_NAME</i>
Monitor group: OSG-ITB
...
...
...

Is this information correct (y/n)? [n]: <b>y</b></pre>
	 
	* Edit the *site_env* file
			To have MonALISA interact with the local batch queuing system, 
			the file <b>site_env</b> will need to specify the location of the local 
			installation directories. 
<pre>&gt; <b>cd $VDT_LOCATION/MonaLisa/Service/CMD</b>
&gt; <b>vi site_env</b></pre>

	  Make sure the location variable for your batch system is setup and 
	  exported (*PBS_LOCATION*, *LSF_LOCATION*, or *FBNSG_LOCATION*,
	  as appropriate.)  E.g.:
 <pre><b>PBS_LOCATION="/usr/local/pbs";
export PBS_LOCATION;</b></pre>

	* Edit the vdtFarm.conf file 
 <pre>&gt; <b>cd $VDT_LOCATION/MonaLisa/Service/VDTFarm</b>
&gt; <b>vi vdtFarm.conf</b></pre>

	  To have MonALISA use the *Tracepath* module the vdtFarm.conf
	  will need to be edited with the following line added:

 ==*Tracepath{monTracepath, localhost, " "}==

	* Start Monalisa

	  <tt># <b>/etc/init.d/MLD start</b></tt> 


---++++Configure CoreMIS
For OSG-ITB 0.1.5, MIS-CI release is 0.2.5. If this version of MIS-CI is installed.
Two pactches should be applied. These patches will not be necessary from 0.2.6.
The procesure to patch these patches is as follows:
<pre>
			 cd $VDT_LOCATION/MIS-CI
			 wget http://gdsuf.phys.ufl.edu:8080/releases/mis-ci/uninstall-misci.sh.0.2.5.patch
			 patch -p0 < uninstall-misci.sh.0.2.5.patch
			 cd $VDT_LOCATION/MIS-CI/etc/misci
			 wget http://gdsuf.phys.ufl.edu:8080/releases/mis-ci/mis-ci-functions.0.2.5.patch
			 patch -p0 < mis-ci-functions.0.2.5.patch
</pre>

In order to configure CoreMIS, you should follow the instructions located at 
[[http://osg.ivdgl.org/twiki/bin/view/Integration/CoreMIS#MIS_CI_Post_Install]] or run the command below.

			 Precheck 1: ivdgl account valid ?
<pre><b>			 ls -al $VDT_LOCATION/MIS-CI/share/sqlite/MIS-CI.db</b></pre> should look something like:<br>
<pre>			 -rw-r--r--	 1 ivdgl grid	 28672 Apr 15 10:40 /usr/local/osgint-0.1.5/MIS-CI/share/sqlite/MIS-CI.db<br></pre>

			 Precheck 2: 'crontab -u ivdgl -l' works ?

<pre><b>			 # $VDT_LOCATION/MIS-CI/configure-misci.sh</b> 
					 Editing site configuration...
					 ...
					 ...
					 ...
					 Would you like to set up MIS-CI cron now? (y/n) <b>y</b>
					 At what frequency (in minutes) would you like to run MIS-CI ? [10]
					 ...
					 ...
					 ...
					 Would you like to add MIS-CI crontab to this ? (y/n) <b>y</b>
					 ...
					 ...
					 ...
					 configure--misci Done
					 Please read $VDT_LOCATION/MIS-CI/README </pre>

---++ Host Certificates

---+++ Setup and maintenance of Certificate Authority connection
	  The default Certificate Authority should be configured to be
the DOEGrids CA. You will find a <a href="http://www.cs.wisc.edu/vdt/releases/1.3.1/certificate_authorities.html">list of CAs</a>
which were added as authorized CAs on your system. 
Please review the list of authorized CAs.  
The VDT installed
the daemon script <b>/etc/init.d/edg-crl-upgraded</b>
which should be running at all times. This program checks for and
updates the certificate revocation lists (CRLs) for each of the
CAs installed. If CRLs are not kept current
incoming connections will fail. 
The CertScripts package can assist you with choosing CA's to trust and
and periodically checking that the CRLs have not expired.

Configure the DOEGrids CA to be used by default by running
 the utility below. <I>If there is an option "<b>choose the option 
 which matches "1c3f2ca8 : ...</b>", then enter "<b>q</b>" at the prompts.</I>
		  <pre>&gt; <b>$VDT_LOCATION/vdt/setup/setup-cert-request</b>
Reading from /g3dev/globus/TRUSTED_CA
Using hash: 1c3f2ca8
Setting up grid-cert-request
Running grid-security-config...
<br>...<br></pre>

---+++ Request and install the host certificate for the resource

	 To authorize this resource for use, a host certificate needs to
be
requested from an appropriate Certificate Authority. Currently the
<a href="http://igoc.ivdgl.indiana.edu/grid-install/documentation/www.doegrids.org"> DOEGrids CA </a>is in use.
iVDGL and PPDG project instructions for getting a certificate are available on the <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/">iVDGL RA</a>
and <a href="http://www.ppdg.net/RA">PPDG RA</a> sites. 
	 Here is a brief guide to the process assuming you have a valid
User Certificate. 
 
	* Run grid-cert-request to generate your host's private key 
(<b>hostkey.pem</b>) and the certificate request.
		  <pre>&gt; <b>cd $VDT_LOCATION</b>
&gt; <b>. ./setup.sh</b>
&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i></b>
Using configuration from <I>/usr/local/grid</I>/globus/etc/globus-host-ssl.conf
Generating a 1024 bit RSA private key
.++++++
.........................................................++++++
writing new private key to '<I>/usr/local/grid</I>/globus/etc/hostkey.pem'<br>-----<br>...<br></pre> 

	* Copy the <b>hostkey.pem</b> in <b>$VDT_LOCATION/globus/etc</b> to the
<b>/etc/grid-security</b> directory <i>preserving
the read only for root permissions.</i>


	* The certificate request is stored as the file <b>hostcert_request.pem</b>
in <B>$VDT_LOCATION/globus/etc</b> .
The important part (referred to as the PKCS#10 request) will look similar to the follwing:
<pre>-----BEGIN CERTIFICATE REQUEST-----
MIIBmzCCAQQCAQAwWzETMBEGCgmSJomT8ixkARkTA29yZzEYMBYGCgmSJomT8ixk
ARkTCGRvZWdyaWRzMREwDwYDVQQLEwhTZXJ2aWNlczEXMBUGA1UEAxMObXlob3N0
ZS5pdS5lZHUwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBALMM/3jhhkGUA3uz
5h4dzUzluDbCdBonv3UjeWbBFyh1JUA3d2148WBTtHAfQuB+f61tRcia2j5eEQEg
TwQnx86VaG8mAOk6gKe/zZlfhVkYR6OY1ssmc3dtkFoIprVoUUJCviq9DUp3DRO/
57/AtDAJsPhAl/UY5d2jFWl9jYGtAgMBAAGgADANBgkqhkiG9w0BAQQFAAOBgQAr
ZS88q8AgsOJ3n0EJoiAvr6dws4mr94Xg9uohNogmGsdjsdM3LG6Q+Qb22YdPPEH9
3HeLtcBlsjEnBq/a+M4IsfnPCn/6hvJs0HS7PxSR98bE116Ik4zuM1dtQ1Ce8Q/h
YtfstZj7FRZPZP1Lg2uftazDst4dkuu0msBP7dUAZw==</pre>
<pre>-----END CERTIFICATE REQUEST-----</pre>

Connect to the DOEGrids CA server and paste this into the
area provided in the form at the URL &lt;<a href="https://pki1.doegrids.org/ManServerEnroll.html">https://pki1.doegrids.org/ManServerEnroll.html</a>&gt;.
		  <i> All of the information must be filled out and in the
additional comments
section add the <b><a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/rastaff.php">
institutional sponsor</a></b></i>

	* Email will arrive indicating a link to retrieve the host
certificate <b>hostcert.pem</b> .  Goto the link and download it onto the 
host.

	* After obtaining the host certificate install it into the directory
<b>/etc/grid-security</b> as <b>hostcert.pem</b> . Use the following <b>openssl</b> command
to verify the certificate.
<pre>&gt; <b>openssl x509 -text -noout -in /etc/grid-security/hostcert.pem</b>
Certificate:<br>	 Data:<br>		  Version: 3 (0x2)<br>		  Serial Number: 1578 (0x62a)<br>		  Signature Algorithm: sha1WithRSAEncryption<br>		  Issuer: DC=org, DC=DOEGrids, OU=Certificate Authorities, CN=DOEGrids CA 1<br>		  Validity<br>				Not Before: Mar 19 14:48:32 2004 GMT<br>				Not After : Mar 19 14:48:32 2005 GMT<br>		  Subject: DC=org, DC=doegrids, OU=Services, CN=worldgrid.iu.edu<br>.....<br></pre>
	 
	* Or, to get an LDAP service certificate for MDS:
		  <pre>&gt; <b>cd $VDT_LOCATION</b>
&gt; <b>. ./setup.sh</b>
&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i> -service ldap</b></pre>
And follow the instructions, which are very similar to the host cert instructions, except that you are creating <b>ldapkey.pem</b> and <b>ldapcert.pem</b>. More info on service certificates can be found at <a href="http://www.cs.wisc.edu/vdt/releases/1.3.4/installation_post.html#servicecert">http://www.cs.wisc.edu/vdt/releases/1.3.4/installation_post.html#servicecert</a>
	 
  

---++ Authorizing Virtual Organization (VO) users
Each of the Virtual Organizations (VOs) maintains a secure membership
service which may be queried with the utility
<b>$VDT_LOCATION/edg/sbin/edg-mkgridmap</b> .
To create a <b>gridmapfile</b> containing an entry for every
member of the VO, first you need to move and start the <b>edg-gridmapfile-upgraded</b> daemon.
<pre># <b>cp $VDT_LOCATION/post-install/edg-gridmapfile-upgraded /etc/init.d/</b>
</pre>
<pre># <b>/etc/init.d/edg-gridmapfile-upgraded start</b>
</pre>

<h3> Setup and testing access to VOMS services</h3>

  After the installation has finished, email must be sent to the 
  VO administrator to request that your site be given access to the 
  VOMS database. This access depends upon knowledge of the Distinguished 
  Name (DN) of the host and the CA which issued the host certificate.  

 The site should test the <b>$VDT_LOCATION/edg/sbin/edg-mkgridmap</b> script to ensure
there are no connection errors to the VOMS services. This needs to be
executed as root so that the host certificate can be used.
<pre>&gt; <b>cd $VDT_LOCATION/edg/sbin</b>
&gt; <b>./edg-mkgridmap --verbose --output=test.out</b>
</pre>
	
  The most common error is "Internal Server Error" which indicates 
  that access to the VOMS DB has not been completed.

---+++ Setting up edg-mkgridmap for local users

The *edg-mkgridmap* script uses a configuration file to dictate 
  its behavior. This is distributed with the Grid3 parameters
  *$VDT_LOCATION/edg/etc/edg-mkgridmap.conf*.

  The *edg-gridmapfile-upgrade* daemon has a similar configuration
  file *$VDT_LOCATION/edg/etc/edg-gridmapfile-upgrade.conf*.

  One of the parameters specifies a _local_ *grid-mapfile* that
  will be added to the output of the *edg-mkgridmap* command.

There are two steps to make this work:

	* Set the *GRIDMAP_LOCAL_FILE* variable in 
	 *$VDT_LOCATION/edg/etc/edg-gridmapfile-upgrade.conf*:

		==GRIDMAP_LOCAL_FILE=$VDT_LOCATION/edg/etc/grid-mapfile-local==

	* Add a new directive to the end of the *$VDT_LOCATION/edg/etc/edg-mkgridmap.conf*:

  Now local *grid-mapfile* entries to this file will be 
  added every time the script is executed.

<tt><b>gmf_local <i>/usr/local/grid</i>/edg/etc/grid-mapfile-local</b></tt>

---++ Debugging the VOMS - GUMS - Gatekeeper authentication chain

A page describing the steps to follow in testing and debugging privilege components has been started under the topic GoldenChainDebugging.

---++ Monalisa and its Information Providers

Launch the MonALISA client tool from the URL &lt;<a href="http://gocmon.uits.iupui.edu:8888/index.html">http://gocmon.uits.iupui.edu:8888/index.html</a>&gt;.
The client is launched from the "Start MonALISA Client" button on the
top of the left hand menu. On the running MonaALISA (ML) client, check
that your site is reporting to ML and that it is correct geographical
location (mouse over the dot and it will display the site name, make
sure it is the name you picked out that conforms to the naming
convention). 

To check that the various information providers are
reporting to ML go to the TabPan view (on the left hand menu bar) and
check that all of the table entries for your site are providing
information, compare with other sites. If there are "Unknown" entries
in the fields then most likely there is error in the ML configuration
files you modified.&nbsp; Please go through the instructions and
contact iGOC if you need help. 

---++ Final Site Verification

To run the site verify script you should not be *root* :

<tt>
$ cd $VDT_LOCATION<br>
$ source ./setup.sh<br>
$ grid-proxy-init <br>
Enter "<i>Your Passphrase</i>"<br>
$ cd verify<br>
$ perl site_verify.pl --host=<i>hostname.domain.tld</i><br>
</tt>

---++ OSG-ITB Registration

To register the site into the Grid Catalog please 
compose an email which contains the following information
and send it to igoc@ivdgl.org - copy/paste the text
below in the body:
<pre>

  Information about you and your machine(s)

  Short_Site_Name (a.k.a UNIQUE Name)
  Host_Name
  SystemAdmin email
  System Admin phone

  If System Admin is a person rather than a mail list System Admin Name

	Location information for installation:
  $app path name
  $data path name 
  $tmp path name
  $wntmp path name

</pre>

---++ Firewalls

Grid components are distributed throughout the network, and services 
such as gatekeepers and data movement utilties are required to be 
accessible to the dynamic cloud of clients and peer services.
This ditributed and dynamic requirement places the burden
of the security on the implementation of the application.

Due to the discovery of significant vulnerabilities in recent years, network-based applications are untrusted by default. To solve the application
problem effort has focused on developing and deploying firewalls which
restricts full and free network access.  (This is analogous to building 
a house with no doors. Is it safe? Yes. Is it useful? No.)

Some essential network-based applications have been
"hardened," such as mail relay services, web servers, and secure
shell daemons. These are further protected further by IP address 
filtering to prevent access from unknown hosts or domains.

Grid components which are located behind network firewall 
face special challenges for Grid setup and operations. 

There are two styles of firewalls usually encountered. 

	* A network firewall which is upstream from your server 
(usually centrally maintained). This blocks all traffic to your 
host. 

	* Host-based firewalls which are setup and maintained 
by individual host administrators. This is usually setup and configured
by the <b>iptables</b> program which filters incoming network packets which 
arrive for the host.

 In addition to host-based firewalls, hosts can choose to 
 implement host based access rules (usually setup with the <b>tcp_wrapper</b>
  or <b>hosts_allow</b> utilties.

Network traffic can be blocked at the firewall for both incoming and outgoing dataflow
depending on hostnames, ip addresses, ports and protocols. 


A common setup is to allow any outgoing connection,
while significantly (if not completely) restricting 
incoming connections.  The Globus project provides thorough documentation on this subject which will not be repeated here.  It is strong encouraged that you read the document <A HREF="http://www.globus.org/security/v2.0/firewalls.html">Globus Toolkit Firewall Requirements</A> to avoid issues which may arise from firewall configuration.


IP port usage which may require firewall updates:

	* MDS: 2135/tcp
	* GRAM: 2119/tcp
	* GridFTP: 2811/tcp
	* GRAM callback: contiguous range if tcp ports as specified by definition <b>GLOBUS_TCP_PORT_RANGE=start,stop</b> .  A minmun range of 100 for a small site. 
	* Monalisa: 9000/udp (for ABping measurements).  These are specified in the file <B>$VDT_LOCATION/MonaLisa/Service/<I>cluster_name</I>/ml.properties</B>
	
These ports and protocols <i>must be open</i> to all grid clients and
server machines participating in the grid in order to provide minimal
functionality.

You also may need to open the following optional ports for additional Grid services:

	* GIIS: 2136/tcp
	* GSISSH: 22/tcp
	* MyProxy: 7512/tcp
	* VOMS: 8443/tcp
	* RLS server: 39281/tcp




<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.RobGardner - 25 Mar 2005
-- Main.RobQ - 15 Apr 2005

-- Main.BockjooKim - 18 Apr 2005
%STOPINCLUDE%

