%META:TOPICINFO{author="MarcoMambelli" date="1131733233" format="1.0" version="1.35"}%
---+!! Local Storage Requirements

%TOC%

---++ Introduction

The goal of this page is to provide a recommendation for the definitions and environment variables for the Local Storage accessible to Compute Elements (the headnodes and the worker-nodes) on OSG. It will mainly cover issues like definition of these spaces and correspondance to external schemas (e.g. GLUE, Grid3/OSG).

There are some suggestions on how to push the users towards correct use and on deployment (how to make this information available), but in general this is the responsibility of other groups in OSG.

Possible technologies to deploy CE storage include (but are not limited to):
	* variables defined in the environment that resolve to the correct path or URL
	* path or URLs consistent across the CE (headnodes and WN), published using an information system

*why do you call this CE storage? I am expecting that sites will want to seperate CE and storage in order to minimize overloads on one affecting the other. As a result, I predict that sites will seperate the jobmanager that leads to batch submissions from the one that allows disk space access because the two have no reason to be on the same hardware.(fkw)*

This page is divided in 2 sections. The first one is a work in progress and will result into the final document. Every contributor is encouraged to modify the content directly and change/overwrite the current version. The second one is for discussion and explanations and the default way of editing it should be appending comments to the current content.

In the following Table there is a name matrix:
	* CE Storage are the names used in this document, please change the section headers accordingly
	* CN is the common name frequently used in emails or discussions
	* GLUE is the attribute name as in GLUE Schema 1.2 (The same attribute may appear in more than one place in the Schema)
	* Grid3/OSG is the LDAP attribute name as in Grid3 Schema
	* OSG Storage is the name used in the OSG Storage day

| *CE Storage*				  |*CN*|*GLUE*|*Grid3/OSG*|*OSG Storage*|
| APP	| $APP	 | CE.Info.ApplicationDir (!CE.Info.ApplicationDir) (*1)  | Gri3AppDir	 | $APP			  |
| DATA  | $DATA	| CE.Info.DataDir (!CE.VOView.DataDir) (*1)				  | Gri3DataDir	|	|
| SITE_WRITE |	 | Location.Path (*2) |	| $SITE_WRITE  |
| SITE_READ  |	 | Location.Path (*2) |	| $SITE_READ	|
| -na-  | $TMP	 | CE.Cluster.TmpDir (!CE.SubCluster.TmpDir)		 | Grid3TmpDir	| $TMP	  |
| WNTMP | $WNTMP  | CE.Cluster.WNTmpDir (!CE.SubCluster.WNTmpDir)	|	|	|
| DEFAULT_SE |	 | CE.Info.DefaultCE (!CE.VOViewDefaultCE)	|	 |	 |
<!-- |  |	 |	 |	 |	 |-->

*1. As visible from the table above, GLUE provides the possibility to have multiple values for some of the CE storage, depending on the VO and the Role (VOMS FQAN). In OSG these are currently sitewide information.<br>
*2. GLUE Schema does not have an attribute specific for SITE_WRITE or SITE_READ, but it provides the location entity (Name/Version/Path sets) to accommodate additional CE local storage. In order to accommodate them two locations will have to be defined (this will be done by the Information Provider (GIP?)  using some configuration info):
	1. LocalID: SITE_WRITE+OSG, Name:SITE_WRITE, Version: OSG, Path: <value of SITE_WRITE>
	1. LocalID SITE_READ+OSG, Name:SITE_READ, Version: OSG, Path: <value of SITE_READ>

No other assumption about the CE is made. It is unsafe to make assumptions about the existence and characteristics (size, being shared, ...) of the $HOME directory.

*I am confused by this statement. Access to the users proxy is a requirement in order to use the srmcp client from the worker node. It would be good to make this clear somewhere! E.g., this could be made clear in the context of what's refered to as $GRID in the discussion part of this page if such a $GRID was adopted explicitly.(fkw)*

The following sections describe the different  storage areas that may be declared local to a CE.
Each section includes:
	* brief description
	* detailed description
	* use cases (informal)
	* notes

---++ APP

This area is intended for VO-wide software installations.


It is required that relative paths resolve consistently between gatekeeper and worker nodes even if APP itself (the base dir) differs between the two.  It is strongly recommended that the base dir itself is the same as well, or some legacy software (*1) will not function properly.  This area may be writeable only by a subset of users and read-only for all the others: there is no guarantee that every user will have write access.  APP must point to a POSIX-compliant filesystem for software installation. <br>
The current model, where all users can write to APP (because some VO have no different roles and CEs have no access mechanisms in place) is a security threat because any VO users could (accidentally or intentionally) damage or modify the applications of his VO, or even install trojans. 
In the longer term having APP writable only via specific VO roles, and only to a small subset of pre-authorized users that operate directly on the server serving the aplication disks to the cluster (*2), would greatly improve security and robustness. <br>

Tipical uses of this area are:
	* install and run VO application

Notes (*#):
	1. Pacman resolves variables an symbolic links saving the full real path. A suggested procedure to avoid problems with it is:
		* Choose any absolute location on your gatekeeper file system for APP.
		* On each worker node arrange by mount or symlink that APP has the same path as on the gatekeeper node.
		* Install applications only using the gatekeeper (use only the application from other nodes)
	1. The 'sticky bit' enabled on APP (recommended for all shared CE storages) will protect against accidental mistakes but do little to help against mlintetionated people.
 Furthermore NFS or similar systems are trusting the integrity of the clients. A compromised client could compromise the whole APP if NFS write access is allowed. The mechanisms to recognize a sw manager role (FQAN) and send only it to the APP server with the possibility of installing software are currently missing in OSG and will require some software development.

---++ DATA

This area is a transient storage shared between jobs executing on the worker nodes.

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This area is intended to hold data shared among different applications running on different worker nodes and/or data that has to outlive the execution of the jobs on the worker nodes.
It has to allow open/read/write operations by regular programs (that may use it transparently as a local disk space).  (NFS, dcap, drm, AFS, CIFS are OK). This is a subset of a POSIX compatible fs, not including features like special file creation (pipes, sockets, locks, links) the ability to modify file meta data (permissions)(*1).<br>
Gridftp or 'SE like' access from outside of the cluster would allow an efficient use of DATA as staging area (*2).<br>
Since the allocation of this space is transient, it is important that users remove unused data and/or that a simple mechanism to allow cleanups (*3) is added.
The use of DATA as a writable area from compute nodes is one of the most significant performance bottlenecks in an OSG cluster. Use of DATA as the "hold all read/write area" (working area for the jobs) is discouraged.
A suggested dataflow using DATA and providing more scalable and reliable data access is the following:
	* Data is written to DATA via gridftp or if necessary fork jobs (unpack tarballs, etc...). 
	* Job is staged into the cluster
	* Job copies its data to the compute node (WN_TMP) or reads data sequentially from DATA if the data is read once. This is a significant performance issue if many random reads are necessary on typical network file systems and this should be avoided. It is worth noting that random data access over large data sets is where grid storage shows its potential. Its distributed nature is better suited for handling that type of data access scaleably and reliably.
	* Job output is placed in WN_TMP (see below for desctiption)
	* At end of job the results from $WM_TMP are packaged, staged to DATA and picked up through gridftp or other mechanisms. Alternatively srmcp can be used directly from the nodes to a remote respository. 
Functions covered by DATA are almost equivalent to those provided by SITE_READ and SITE_WRITE together (see below). This latter solution is forcing the separation between inputs and outputs and may be more efficient for big production sites with specialized hardware while DATA may be easier to deploy for small sites.<br>
If you plan to remove DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another DATA de facto (*4).

Typical uses of this area are:
	* input datasets for jobs executing on the worker nodes
	* datasets produced by the jobs executing on the worker nodes
	* shared data for MPI applications. 
	* data staged in or waiting to be staged out.


Notes (*#):
	1. The ability to modify file permissions may become something to support. It is also available in SRMv2
	1. The DEFAULT_SE entry (see below) may be used to publish the GSIftp URL of the SE viewing that space.
	1. An example of a simple space managment solution could be a file in each directory ( _.keep_) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested) all files in that directory and subdirectories may be removed. This is a gentlemans agreement. Keep in mind that none of the data in a transient storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
	1. E.g. The current Condor jobmanager is using a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As mentioned above it is not safe to assume that the $HOME dir is shared between gatekeeper and worker nodes. Furthermore it is not worth  removing a shared space like DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads)


---++ SITE_READ
This area is a transient storage visible from all worker nodes and optimized for high performance read operations

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This read-only area is intended to hold input data shared among different applications running on different worker nodes.
It allows normal file random access read operations (open, seek, read, close) by regular programs (that may use it transparently as a local disk space). This is provided through a grid file access library and the users not have to know the storage location and its the underlying implementation (the use will be uniform across OSG).<br>
Users have no write access to this area: files will be placed there by site administrators or by writing from the Grid side of the SE.  (*1).<br>
If the gatekeeper cannot write to this area there may be problems with jobmanagers using a shared directory to transfer the executable or some data (*2). 

This is the LCG model. This last option may cause problems to many current applications that count on normal file access to a shared space and it may require non trivial changes to them to use special client programs to access the DEFAULT_SE. Furthermore it may cause problems with jobmanagers using a shared directory to transfer the executable or some data. A dataflow example could be: 

Its features cover the read part of DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.


Typical uses of this area are:
	* input datasets for jobs executing on the worker nodes
	* data staged in 

Notes (*#):
	1. The DEFAULT_SE entry (see below) may be used to publish the GSIftp URL of the SE able to write in SITE_WRITE.  
	1. E.g. The current Condor jobmanager is using a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As mentioned above it is not safe to assume that the $HOME dir is shared between gatekeeper and worker nodes. Furthermore it is not worthy to remove a shared space like DATA for performance issues to reintroduce it as $HOME (that probably will hevr even more performance problems because of other loads)


---++ SITE_WRITE
This area is a transient storage visible from all worker nodes and optimized for high performance write operations

It is a write only (or mostly write) transient sorage.
This area is intended to hold the output of jobs running on different worker nodes (that has to outlive the execution of the jobs on the worker nodes) to combine it with other outputs or to allow its stage out.
It allows normal random access file write operations: open, set flags, write (sequential, with multiple write operations), close. It may not be possible to modify a file once closed. This is provided through a grid file access library hiding peculiarities, specific for the underlying storage, and programs may use it transparently (almost *1) as a local disk space.<br>
Users may have no read access to this area; in such cases files will be accessed with the help of the site administrator, or through mechanisms external to the CE. E.g. a SE can read from that area (*2).<br>
Its features cover the write part of DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.

Typical uses of this area are:
	* storage of datasets produced by the jobs executing on the worker nodes
	* data waiting to be staged out.

Notes (*#):
	1. Limitations will have to be avoided if possible and be clearly stated.
	1. The DEFAULT_SE entry (see below) may be used to publish the GSIftp URL of the SE able to write in SITE_WRITE.  


---++ TMP (REMOVED!)
This area was is intended as a shared temporary work area. Because of its similarities with DATA, the reduced interest in multimode applications (that anyway can use DATA), and the possibility of the application abusing of a shared space, we propose to remove this storage from those defined within OSG 


---++ WN_TMP
This area is a temporary work area that may be purged when the job completes.
It is generally not possible for two jobs running on as cluster to read from (or write to) each others WN_TMP areas. The WN_TMP area is thus fundamentally different from all other areas discussed here in that it is job specific.

It is required that WN_TMP points to a POSIX-compliant filesystem that supports links, sockets, locks, pipes and other special files as required. Good I/O is essential for job performance, so a local FS is suggested.
Ideally it should be a temporary directory (or partition) assigned empty for the job, with a well defined amount of space (current jobs require at least 1-2 GB of disk space *1) and removed (or cleared) after the job completes. The space provided should be dedicated and isolated: jobs overusing their space should not affect each other or affect the OS.<br>
In most cases it is a space shared among all the jobs that are currently executing at the WN, so it is recommended that a jobs creates a unique subdirectory of WN_TMP (e.g. WN_TMP/somestring$JOBSLOT) that becomes its working area and that should be removed by the job itself before ending (*2).


Typical uses of this area are:
	* working directory for jobs (running on the worker nodes)

Notes (*#):
	1. Add link to VO space requirements (which is the recommended/required size?)
	1. A mechanism to help automatic cleanup would be the introduction of a 'lock' directories with files named according the temporary directory, containing the PID of the job. If that process is terminated, the directory can be removed


---++ DEFAULT_SE
This area is a SE closely related to the CE

It is accessible only using SE access methods (Gridftp, SRM). It is accessible from within the cluster (visible from the worker nodes). If accessible from outside, it is the preferred SE for the CE and should be used when doing 2(or more)-step copy of input datasets to the CE (e.g. 3rd party transfer to the Default SE, copy to the workdir using a client program).<br>
In simple cluster this could be a SE visible from both inside and outside and serving an internally shared space like DATA (*1). In sites with no shared spaces this would allow to get data in and out using grid tools.

Typical uses of this area are:
	* staging in of big input files (e.g. datasets)
	* staging out of big input files (e.g. datasets)


Notes (*#):
	1. This is not guaranteed. To describe better the SE connected to CE storages like DATA, SITE_READ, SITE_WRITE a better mechanism will have to be defined. The GLUE CE-SE binding schema could be a starting point.


---++ Minimum requirements
An OSG site is not forced to provide all the areas described above as far as it makes clear what is provided (according to this classification presented and using the mechanisms provided by OSG *1), what it is not providing (the value '-na-'/'na' could be a marker for the missing storage, to distinguish it from entries missing in the information system) and satisfies the minimum requirements.

Mandatory set options:

	1. DATA, WN_TMP - This is the Grid3 model. A dataflow example is provided above, in the section about DATA<br>
OR
	2. SITE_READ, SITE_WRITE, WN_TMP. A dataflow example could be: 
		* site admin intervention or external transfer 
		* cp: SITE_READ->WN_TMP 
		* job execution 
		* cp:WN_TMP->SITE_WRITE 
		* external stage-out
<br>
OR
	3. DEFAULT_SE, WN_TMP - This is the LCG model. This last option may cause problems to many current applications that count on normal file access to a shared space and it may require non trivial changes to them to use special client programs to access the DEFAULT_SE. Furthermore it may cause problems with jobmanagers using a shared directory to transfer the executable or some data. A dataflow example could be: 
		* site admin intervention or externall transfer 
		* srmcp: DEFAULT_SE->WN_TMP
		* job execution
		* srmcp:WN_TMP->DEFAULT_SE
		* external stage-out

OR
	4. SITE_READ, DEFAULT_SE, WN_TMP - This is the SRM/dCache model. In this model read access is via dcap, while write access is via srmcp only. Write access via dcap is in principle possible, and thus would in principle provide SITE_WRITE. However, in practice files do not get consistently written to in an open/write/close/open/write/close sequence. dCache may serve different replicas of a file for two consecutive open. As there is no mechanism inside dCache that would enforce synchronization of the writing to one of the physical files after opening a given logical file, this generally leads to different file contents for different physical files that a referenced as the same logical file. This is obviously unacceptable and thus prohibits the use of dCache as a SITE_WRITE solution. The same is avoided if writes are allowed only via srmcp (i.e. DEFAULT_SE) while reads may use either srmcp (DEFAULT_SE) or dcap (SITE_READ). SRM implementation in dcache does not allow writing of logical files that already exist. It thus guarantees that all physical file replicas of the same logical file remain the same. 
 A dataflow example could be: 
		* site admin intervention or externall transfer 
		* job execution (open/seek/read from SITE_READ using dcap)
		* srmcp:WN_TMP->DEFAULT_SE
		* external stage-out

The set of CE storages provided by a CE must include at least one of these four sets.

Off course providing a wider selection of CE storages would allow the jobs to select the most proper for their needs. But it could also allow the jobs to adopt inefficient execution models that could affect negatively the performance of the whole cluster.

Notes (*#):
	1. Alain Roy? should be coordinating an activity working on how to make storage information available: environment variables, MDS information providers, ... 



<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.BurtHolzman - 04 Aug 2005
-- Main.MarcoMambelli - 23 Sep 2005

%STOPINCLUDE%

