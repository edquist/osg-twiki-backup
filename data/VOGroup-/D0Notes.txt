%META:TOPICINFO{author="MarciaTeckenbrock" date="1219705488" format="1.1" version="1.1"}%
%META:TOPICPARENT{name="UserGroupMeeting20080821"}%
---+ D0 Notes & Plans
---+++ 8/22/2008
---+++ *Immediate goal:*
<pre>Improve efficiency of D0's utilization of OSG 
Facility. At the moment, Joel agrees, it is not good.

Improvements will most likely materialize in a longer-term range 
(1-2 Quarters). We are moving forward at a steady pace. Joel, Andrew, 
Tanya, and we discussed a common initial plan. We are 
grateful for D0's help, but we need more D0 offline computing momentum 
with us. E.g., having effort at FNAL from D0 SAM Operations will help.

</pre>
---+++ *D0 'grid logistics'*
<pre>Production jobs typically take about 12 hours of wall clock time. 
Merge jobs typically take 1-2 hours wall clock time.

[1] During OSG utilization, at a site's worker-node, a D0 job pulls 
out bootstrap file from remote SAM cache (often at FNAL). 
[2] This bootstrap, when it has arrived at WN, sets up the workflow. 
[3] Next, application file itself is requested and pulled out of remote SAM 
caches (FNAL or distributed worldwide). 
[4] Then, as part of stage-in, runtime environment file is pulled out. 
[5] Then, actual stage-in data is pulled out. 
[6] Job executes, data is merged, job completes. 
[7] Stage-out data is written to a remote SAM cache (FNAL or 
distributed worldwide). 
[8] Successful job completion is recorded.

There is one main known cause, i.e., failure mode -- Due to D0's and 
SAMGrid's own internal latencies, any step that has a dependency on a 
remote SAM cache, there is liability of too much delay within SAM and 
over WAN, and thus, a job workflow timeout. This esp. affects 
[3][4][5]. Also, there is central dependency on FNAL SAM servers, 
which can affect [1][7] once in a while. When latencies are 
substantial, timeouts occur, jobs fail in succession, and the entire 
batch fails.

This happens on almost all sites being used by D0 on OSG Facility.

To ameliorate, an option is to have SRM-based SE local to a site. (In 
long term, since not all CE sites can furnish such SE's, there will be 
a need for D0 to modify workflow and be able to use remote SRM-based 
SE's.) With use of an SRM-based SE, however, there is a side-effect. 
D0 job submission rate, and workflow itself, is very high frequency. 
Many requests for input data are made in quick succession. An 
SRM-based SE, especially in opportunistic storage mode, furnishes very 
limited disk I/O capacity - being able to serve files at a slow steady 
frequency, serving transfer requests only in a FIFO mechanism. I 
think, currently, D0's transfer request frequency outpaces SRM-based 
SEs - rendering use of SRM almost ineffective.

In addition, related to [4], using SRM SE for reading out an 
environment file - from only a dozen or so such files - for every job 
creates a severe side-effect. Since it is the same file, but SRM SE 
considers every new transfer request on it afresh, SRM can only stack 
all requests. This further creates long waiting mover queues on a 
dCache pool disk, where the file actually resides, in turn making many 
D0 jobs wait or timeout. Result is a bottleneck on overall read/write 
throughput; or even, unsustainable high load crashing these specific 
pool nodes. This has now happened frequently at UCSD, and I suspect, 
also at other SRM sites such as Nebraska.

At the moment, only a few SRM SE's are in use by D0 - Nebraska, 
MWT2_IU, UCSD. Purdue is now ready. MIT is a special case site, with 
SE pools / CE WNs behing a NAT. Due to SRM protocol design, 
workarounds are needed for SRM-to-SRM transfers in such cases, and 
Andrew mentioned he is close to finding a solution for D0 - so that D0 
can use such special-case SE as well. MIT SE still has technical 
configuration issues; it's being worked upon by OSG Storage.

</pre>
---+++ Near term recommendations
<pre>(i) We should downsize job submission to only a select few sites, 
known to be efficient. This will help us focus on sites with SEs, 
plus, a few sites with no SEs but with better SAM WAN transfer 
history. </pre><pre>(ii) Reading out environment file per job is a severe 
bottleneck with SRM SEs, and using another mechanism, e.g. Squid, 
wget, is required. </pre><pre>(iii) There is a need to lower throttles within a 
D0 job, i.e., to reduce frequency within the workflow to have it 
'drive slower'. </pre><pre>(iv) We can soon start using SRM SEs at Purdue and 
MIT, the latter using Andrew's new solution.</pre>