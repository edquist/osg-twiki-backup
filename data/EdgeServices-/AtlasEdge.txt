%META:TOPICINFO{author="KyleGross" date="1225985937" format="1.1" version="1.10"}%
%META:TOPICPARENT{name="WebHome"}%
---+!!ATLAS Requirements for OSG Edge Services

%TOC%

---++ Introduction

Collect here and point to other locations where ATLAS requirements for edge services can be found.  This will come principally from the distributed data management system (DDM), the new US ATLAS production system (Panda), the ability to dynamically deploy grid-enabled conditions and geometry databases.

---++ Requirements from Distributed Database Services
   * [[http://atlas.web.cern.ch/Atlas/GROUPS/DATABASE/project/services/][DDS homepage]]

As a first step, only the database-resident conditions data payload is considered. The Use Case of the large conditions data payloads outside of the database (stored in external files) has to be considered later and handled together by the DDS and the DDM. Alternatively we anticipate the need for squid for web based caching of conditions data (assuming we use !FroNTier or
similar deployed at Tier1 and the corresponding Athena POOL/CORAL/FroNTier plugin), with space requirements that have yet to be quantified.

----+++Scenario 1 - Virtual Machine
Both grid-enabled !MySQL server software and replica data are pre-installed in the VM image shipped to the Edge Services node by the Globus WMS.

----+++Scenario 2 - Gatekeeper on the Real Node
Instead of the Virtual Machine the real node is used. Standard ATLAS software installation account and access is used to write the grid-enabled !MySQL server software and the replica database content to the $DATA area on the site. The globus-job-run command launch the !MySQL server on the real Edge Services node.

----+++Scenario 3 - User Account
A non-priviledged VirtualOrganizations/VOInfo user account on the Edge Service VM or the real node
is used to login via gsi-ssh to install locally the server software and the replica data content. E.g. using wget or pacman.

----+++Technical issues

In case of VM the site $DATA area could be nfs-mounted from the VM node, complicating Scenario 3. In Scenario 3 it is not possible to do the clean logout after launching the daemon process from the shell of the non-priviledged VirtualOrganizations/VOInfo user account. Thus, in case of the VM technology a preferred solution is Scenario 1.

----+++Security issues

It is not secure to have the login access for the accounts that run the daemon processes. Thus, for the use on the real node the Scenario 2 (gatekeeper) and in case of the VM the Scenario 1 (image install) are preferred.

---+++Summary for DDS:
   1 The Edge Server node or VM must have the inbound connectivity to the site. An extra outbound connectivity will ease the grid-enabled !MySQL server maintenance, monotoring and updates. 
   1 In case of the real Edge Services node must have the OSG gatekeeper running on the node with the read access to the $DATA ATLAS area on the site. A less secure alterantive is to have the non-priviledged VirtualOrganizations/VOInfo user account with gsi-ssh access too launch the daemon process.
   1 In case of the virtual Edge Services node the VM image should have pre-installed software and data. A considerable DDS development effort is required for that.

---++ Requirements from DDM

---+++ Summary

<pre>
right now - on LCG - we install Apache and MySQL ourselves (non-root). So we
need a host or a service certificate for Apache SSL, mod_gridsite. LCG
generates it from the host certificate and renews it. The requirements on
host certificates can change a bit if Apache is already there installed as
root -- BUT we'd prefer to install it ourselves (Apache & MySQL) not to
depend on versions that might have problems with the rest of our system. To
make sure we are using "safe" versions, we are of course willing to upgrade
on security upgrades - right now we follow POOL MySQL releases which does
the update whenever there's some issue with a version. For Apache we don't
follow anyone but we should setup a procedure there..

Besides that, GSI, GridFTP client libs, Certicate Authority revocation lists
and certificates renovation (automatic renewal) - LCG does it as well using
a cron job.. I think VDT does it as well?

SLC3 as the O/S - because we have C code that binds to GridFTP and SRM so we
just ship the binaries. Also, POOL is SLC3 (not sure what other flavours).
Our core part of the code is Python, so we should be safe to switch to other
platforms (just have to recompile GridFTP, SRM) *IF* POOL supports it.

Disk space: 10 GB (POOL? Can be greatly reduced if we can get POOL to
install only Pool FC interface). Also, remember databases are running there
and log files are kept for a while but shouldn't be too much.. (make it
safe.. 10 or 20 GB - should be trivial these days to have more)

Connectivity: 8000, 8443 (Apache), 3306 (MySQL)

Edge service hostname not changing often. If machine has an attached host or
service certificate that already implies it shouldn't change too often.

To install services: login (or send job) and run installation script that
does the installation - a 'big' tarball.
To manage services: start/stop -> using cron jobs; monitoring -> exposing
log files onto Apache (web visible)

That's all so far.

</pre>

---++ Other information

Follows is an initial list of edge services (or VirtualOrganizations/VOInfo services) expected from Miguel, 6/21/2005. See
also the links

   * https://uimon.cern.ch/twiki//bin/view/Atlas/DDM

<pre>
-> These are preliminary requirements for discussion within our  group!! <-

VO site services:

- VO-specific services from LCG: gLite FTS, LRC with POOL FC  interface [Expect to use LFC]
- Messaging system: TBD - common with other experiments [Jabber ?  We've used it successfully in the past]
- Experiment-specific S/W versions are not defined yet: ATLAS will  provide them later to SC3
- ATLAS requires set of experiment agents/services/daemons running at  each site
- These agents require inbound and outbound connectivity; Restricted  ports ok; No data transfer in these services; Designed to be “TCP  traced”: XML or "plain-text" messages. Minimal traffic
- Being designed to be easily deployed: a tarball plus "source"  script; (currently they run as cron jobs under a user account on a  gateway machine); BUT require a persistent area (a database for our  local catalogs *besides* the LRC) which can be:
   - directory on machine for config files
   - MySQL server? SQLite on local file system?
   - [We can profit from our MySQL deployment mechanism by Sasha  that can run as a "job"]
- Machine must have similar base software/operating system as an LCG  UI or LCG WN. Current WNs are insufficient due to connectivity  restrictions
- Must securely delegate credentials to the VO box (e.g. have MyProxy  service available?)
- Experiment should control deployment (e.g. “special job” that stays  running indefinitely; SSH/GSI login?)
- Experiment requires that ALL sites provide the same installation  model for deploying the agents
- Disk space requirements: few megabytes for experiment s/w + AA s/w  (POOL) (around 20 GB also for logging information)
- No requirement to access s/w installation area at the site from VO- box (VO-box can be “independent” from the cluster setup as long as it  is at the site with in/outbound connectivity)
- Services/agents running on box can start simple socket server: e.g.  python HTTP Server
- WNs at site can access these services (e.g. there is a published  environment variable at the WN with the hostname of the VO-box; also  it can be published in the information system)
- Services running centrally on ATLAS machines access these services  on the VO-box as well
- Purpose of services: management of datasets, data management- specific bookkeeping; Avoids outbound connectivity requirements for  WNs on DDM; greatly improves flexibility for the experiment to design  a Production infrastructure; Jobs running local have all required  information available at the site
</pre>


---+++Summary for DDM:
   1 Clean environment
   1 gridftp (srm clients we install ourselves) available
   1 Ability to install and run apache and mysql db
   1 Abilty to install pool file catalog interface - comes from this.
   1 No root requirement.
   1 Host, service certificates need to be available.
   1 Certain ports need to open (8000, MySQL db port).


---++ Requirements from Panda
   * [[https://uimon.cern.ch/twiki/bin/view/Atlas/Panda][Panda twiki home]]
   * [[http://lists.bnl.gov/pipermail/usatlas-prodsys-l/][Archive]]

---+++Summary for Panda:
   1 Initial implementation - will just use the DDM service.
   1 Require WN http access outside site, could do via proxy.
   1 Could be running panda services on the edge machine in the next machine.
   1 Monitoring services may run as an ES - communication via http and mysql on backend.
   1 Accounting information


---++ Links from the LCG VirtualOrganizations/VOInfo Box activity

   * [[https://edms.cern.ch/document/639856/][LCG/EGEE Document on "Ad Hoc Services..."]]
   * [[http://agenda.cern.ch/fullAgenda.php?ida=a054503][Discussion at LCG Authorization Workshop]]
   * Initial [[https://uimon.cern.ch/twiki/bin/view/Atlas/DDMSc3][ATLAS requirements for LCG Service Challenge 3]] and more specifically, [[https://uimon.cern.ch/twiki/bin/view/Atlas/DDMSc3#DDM_requirements_for_VO_site_box][here]]. These have evolved a bit, and could be updated. 


---++ Other sources: Prodsys generally and distributed analysis
   * [[https://uimon.cern.ch/twiki/bin/view/Atlas/ProdSys][Prodsys meetings]]
   * From David Adams:
<pre>
We may want to install ATLAS-specific services to handle "interactive" analysis jobs. 
Even if external users or agents don't directly interact with running jobs on the worker nodes, 
the need to have a high submission rate and low latency may drive us to run a service that can 
submit directly to the local batch system.</pre>

   * From Marco Mambelli:
<pre>
Even if not required yet, it could be useful to provide Edge Services like and HTTP Proxy 
(requiring outbound connectivity, and inbuond for better maitaneance; inbound access from WN) and a cache 
(e.g. Squid, requiring same connectivity and fast disk access and a space of about 5-10 GB, at least 2 GB 
to be effective)
Furthermore a local job monitoring service could be useful to provide information and/or aggregate 
WN messages and it would require:
- inbound connectivity (for queries)
- outbound connectivity (for updates and central reporting)
- I/O connectivity to WN
- some transient storage space on disk
</pre>

---++ Summary
Table sumarizing the requirements: 'd' means desired feature, 'r' means required feature, 'ra' means required if no other alternative is available. See the sections above for more details and a precise description. The last column aggregates the requirements (the highest between those of the components).

|*Feature*              |*DDS*|*DDM*|*Other*|*Requirement*|
| Connectivity outbound |  d  |  r  |       |  r  |
| Connectivity inbound  |  r  |  r  |       |  r  |
| Connectivity to WN    |     |  r  |       |  r  |
| Connectivity from WN  |  r  |  r  |       |  r  |
| VM                    |  ra  |   |  |  |
| gsi-ssh (non-root)    |  ra  |   |  |  |
| gatekeeper installed  |  ra  |  |  |  |
|  |  |  |  |  |
|  |  |  |  |  |
|  |  |  |  |  |
|  |  |  |  |  |


-- Main.RobGardner - 09 Sep 2005