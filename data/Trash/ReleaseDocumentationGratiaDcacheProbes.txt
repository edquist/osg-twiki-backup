%META:TOPICINFO{author="TanyaLevshina" date="1278004280" format="1.1" reprev="1.19" version="1.19"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *Installation of %SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="3"}%
---++ About This Document

%ICON{hand}% This document is intended for System Administrators installing the Gratia dCache  transfer or storage probes.

The probes report storage related information to [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaSiteCollector][the central Gratia collector]]. There are two types of probes:
   * The dCache-transfer probe reports to Gratia the details of each file transfer into or out of a dCache file server. 
   * The dCahche-storage probe is responsible for reporting storage capacity and storage usage to the central Gratia collector. The information reported is:
      * The storage capacity and amount used for each dCache pool.
      * The storage capacity and amount used for each SRM Space reservation.
---+++ Applicable Versions
The Gratia dCache probes can be downloaded  from  [[http://vdt.cs.wisc.edu/components/dcache.html][VDT dCache homepage]]. They are packaged as rpms and are included in vdt-dcache distribution. The rpms could be also downloaded directly from the VDT dCache homepage. 

The Gratia dCache probes also depend on 
%RED%
*osg-version 1.2.3* 
%ENDCOLOR%
(or higher) and 
%RED%
*vdt-version  2.0.0*
%ENDCOLOR%
(or higher).

---+++!!Engineering Considerations

<div id="indent">
It is recommended that the Gratia dCache probes are installed on a dCache admin node, where a dCache billing database is located. The dCache transfer probe must perform queries on the billing database so if the probe is installed somewhere else  you will need to configure database access accordingly. The dCache storage probe doesn't require to be installed on an admin node, it is polling information from its dCache Information Provider.
</div>
The installation procedure depends on your current dCache installation. You can do one of the following actions:
   * Install  vdt-dcache (fresh install) 
   * Upgrade existing vdt-dcache installation (full upgrade)
   * Upgrade just the Gratia dCache probes without upgrading vdt-dcache (partial upgrade)
   * Install the Gratia dCache probes on top of dCache installed from [[http://www.dcache.org][dCache site]] (installation without vdt-dcache)

---+++!!Help!
If a problem occurs during the installation or the verification of the service, see [[#DebugInfo][Debugging Information]].


---++Checklist

   1. dCache probes depend  on the pyOpenSSL package. Please installed it using yum (or find an appropriate rpm package). See [[#DebugInfo][Debugging Information]] for help.
   1.  [[PacmanInstall][pacman]] version >=%PACMAN_VERSION% is required in order to create OSG User-VO Mapping. Please read  [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/OsgSupportedVos][this page]] about files that are required to accurately collect grid resource usage and metrics by a VO for transfer submitted using grid proxies or where voms proxy information is not available. 

---++Installation Procedure
---+++  Installation of the Gratia dCache probes with vdt-dcache installation (fresh install)

The Gratia dCache probes are part of vdt-dcache distribution and could be installed simultaneously with dCache. The whole installation procedure is described at [[http://vdt.cs.wisc.edu/extras//2.4.8/InstallingDcacheForOSG.README.html][VDT-dCache Installation]].

%ICON{"warning"}% Please read [[http://vdt.cs.wisc.edu/extras//2.3.1/InstallingDcacheForOSG.README.html][ITB version vdt-dcache Installation]].

Before installing vdt-dcache you will need to  modify ==siteinfo.conf== file under _vdt-dcache-<OS>_<ARCH>-<VERSION>/install_ directory (e.g. ==vdt-dcache-SL4_32-2.4.8/install_directory==).To enable the probes find and modify the following entry:

<pre class="file">
# Would you like to install/use Gratia dCache storage and transfer probes?
# Options: yes or no
INSTALL_DCACHE_GRATIA_PROBES="yes"
</pre>

In order to configure the Gratia dCache probes you will need to modify the parameters listed listed under section #3 of the file (named "Gratia dCache storage and transfer probes"). You have to change at least the folowing parameters:
<pre class='file'>
<verbatim>
GRATIA_SOAP_HOST="<gratia-collector-host>:<gratia-collector-port>"
GRATIA_SITE_NAME="<site-name>"
GRATIA_GRID_NAME="<grid-name>"
</verbatim>
</pre>

See more options in [[#DebugInfo][Debugging Information]] for more options.

Your entries depend on the type of your site:

 _gratia-collector-host_:_gratia-collector-port_ - gratia-osg-transfer.opensciencegrid.org:80 for OSG or gratia-osg-itb.opensciencegrid.org:80 for ITB, if your site is running its own collector you should specify the host name and port of that collector

_grid-name_ - OSG or OSG-ITB

When you proceed with vdt-dcache installation _install.py_ script will unpack the Gratia probes on admin node and will modify the configuration files.             

To continue with the installation please go to  [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes#Setting_index_on_dCache_billing][Setting index on dCache billing database]] section.

---+++  Upgrade existing vdt-dCache installation (full upgrade)

If you already installed vdt-dcache and are in the process of upgrading it you should follow instructions provided in [[http://vdt.cs.wisc.edu/extras//2.4.8/UpgradeDcacheForOSG.html][Upgrade Procedure Document]]. 

%ICON{"warning"}% Please read [[http://vdt.cs.wisc.edu/extras/3.0.1/InstallingDcacheForOSG.README.html][ITB version of Install/Upgrade Document]].

If you had configured and enabled Gratia probes during initial installation go to  [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes#Setting_index_on_dCache_billing][Setting index on dCache billing database]] section.

If you didn't enable Gratia dCache probe last time you have installed dCache, please proceed to the [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes#Enabling_Gratia_dCache_Probes_Manually][Enabling Gratia dCache Probes]] section.


---+++  Upgrade gratia dCache probes without upgrading vdt-dCache (partial upgrade) or Install  without vdt-dcache
If you already installed vdt-dcache and just want to upgrade Gratia  dCache probes or  you want to install Gratia  dCache probes on top of "native" dCache installation you have to do the following steps on admin node:
  1. download the new release (Check the right architecture, OS and postgres version):
<pre class="screen">
<verbatim>
cd /usr/local/vdt-dcache-<OS>_<ARCH>-<VERSION>/RPMS
wget http://vdt.cs.wisc.edu/software/dcache/gratia/<VERSION>_post_8.3.7/gratia-probe-dcache-<OS>_<ARCH>-<VERSION>_postgres_8.3.7.tar.gz
</verbatim>
</pre>

  where  <OS> could  be SL4 or SL5,  <ARCH> could be  "32" or "64" and <VERSION> is the current production or ITB version of the gratia probes.
             
For example:
<pre class="screen">
wget http://vdt.cs.wisc.edu/software/dcache/gratia/1.06.15d-1_post_8.3.7/gratia-probe-dcache-1.06.15d-1_SL5_64_postgres_8.3.7.tar.gz
</pre>

  2. Untar file:
<pre class="screen">
<verbatim>
tar xvfz gratia-probe-dcache-<OS>_<ARCH>-<VERSION>_postgres_8.3.7.tar.gz
</verbatim>
</pre>

  3. Upgrade all rpms that have been created during the previous step:
<pre class="screen">
<verbatim>
rpm -Uvh gratia-probe-dCache-transfer-itb-<VERSION>.noarch.rpm   
rpm -Uvh  gratia-probe-dCache-storage-itb-<VERSION>.noarch.rpm  
rpm -Uvh  gratia-probe-common-<VERSION>.noarch.rpm
rpm -Uvh  gratia-probe-extra-libs-<VERSION>.noarch.rpm
rpm -Uvh  gratia-probe-extra-libs-arch-spec-<VERSION>.i386.rpm
#or
rpm -Uvh  gratia-probe-extra-libs-arch-spec-<VERSION>.x86_64.rpm
</verbatim>
</pre>


To configure and enable the Gratia dCache probes during initial installation go to  [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes#Setting_index_on_dCache_billing][Setting index on dCache]] section.

If you didn't enable the Gratia dCache probe last time you have installed dCache, please proceed to the [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation//GratiaDcacheProbes#Enable_Gratia_dCache_Probes_Manually][Enable Gratia dCache Probes]] section.

---++Enabling Gratia dCache Probes Manually 
%ICON{"warning"}% If you are doing upgrade of the Gratia dCache probes keep in mind that _ProbeConfig_ has been changed!!!. The new file will be _ProbeConfig.rpmnew_. You will need to edit _ProbeConfig,rpmnew_ file incorporating all the modifications you have done in old _ProbeConfig_, then move _ProbeConfig.rpmnew_ to _ProbeConfig_.

If you have not enable the  Gratia dCache probes you  have to modify probes configuration files manually. In order to do so, you have to login on admin node and modify __ProbeConfig__ files for storage and transfer probes. The ProbeConfig files for storage and transfer probes are  located under ==/opt/d-cache/gratia/probe/dCache-storage== and ==/opt/d-cache/gratia/probe/dCache-transfer== directories.
You have to edit both files. On dCache admin node do the following:
<pre class="screen">
cd /opt/d-cache/gratia/probe/dCache-transfer/
edit ProbeConfig   
</pre>

The minimum set of parameters that have to be  modified for transfer probe:
<pre class="file">
<verbatim>
CollectorHost="<gratia-collector-host>:<gratia-collector-port>"
SiteName="<site-name>"
Grid="<grid-name>" 
OnlySendInterSiteTransfers="false"
EnableProbe="1"
DCacheServerHost="<dcache-admin-node>"
EmailServerHost="smtp.domain>"
EmailFromAddress="dCacheProbe"
EmailToList="<email1@domain,email2@domain>"
</verbatim>
</pre>

For the storage probe you have to change directory:
<pre class="screen">
cd /opt/d-cache/gratia/probe/dCache-storage/
edit ProbeConfig   
</pre>
and modify storage _ProbeConfig_ file:

<pre class="file">
<verbatim>
CollectorHost="<gratia-collector-host>:<gratia-collector-port>"
SiteName="<site-name>"
Grid="<grid-name>" 
InfoProviderUrl = "http://<dcache-admin-node>:2288/info"
</verbatim>
</pre>

Where 
 
_gratia-collector-host_:_gratia-collector-port_ - gratia-osg-transfer.opensciencegrid.org:80 for OSG or gratia-osg-itb.opensciencegrid.org:80 for ITB, if your site is running its own collector you should specify the hostname and port of that collector

_grid-name_ - OSG or OSG-ITB

_smtp.domain_     -  your  smtp server

_email1@domain,email2@domain_ - list of email addresses that will receive email when probe server start/stop

_dcache-admin-node_ - fqn of dCache admin node

---++ Setting index on dCache billing database

If you already set index on your billing database skip this section. Index should be set on the datestamp and transaction fields, otherwise the probe will put significant load on your DB! 

In order to set index you have to log in to the billing DB using the following command:
<pre class="screen">
psql -U postgres billing
\d doorinfo
\d billinginfo
# If you don't see an index, execute the following commands:

create unique index transaction on doorinfo(transaction);
create index dates_di on doorinfo(datestamp);
create index initiator on billinginfo(initiator);
create index dates on billinginfo(datestamp);
</pre>
 
The above commands may take awhile before they finish, especially if your billing database is large.

---++ Creating  OSG User-VO Mapping 

Please read  [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/OsgSupportedVos][this page]] about files that are required to accurately collect grid resource usage and metrics by VO for transfer submitted using grid proxies or where voms proxy information is not available. 

You will have to install VO-Map-Utilities from VDT. In order to so you will need first to download and install pacman:
<pre class="screen">
wget http://physics.bu.edu/pacman/sample_cache/tarballs/pacman-3.28.tar.gz
tar xzf pacman-3.28.tar.gz
cd pacman-3.28
. setup.sh
</pre>


Then you have to decide if you are going to use gridmap-file or GUMS for user-vo mapping and answer appropriately questions asked during package installation. If you are using GUMS, setup VDT_GUMS_HOST:

<pre class="screen">
<verbatim>
export VDT_GUMS_HOST=<GUMS hostname>  
</verbatim>
</pre>

The next step is to install vo-map-utilities:

<pre class="screen">
<verbatim>
cd /opt
mkdir  osg_vo_map
cd osg_vo_map
pacman -get  %CACHE%:VO-Map-Utilities
. setup.sh
</verbatim>
</pre>

 Pacman will ask whether you want to trust the cache (=yall=).

The install procedure will print out a warning:
<verbatim>
========== IMPORTANT ==========
Most of the software installed by the VDT *will not work* until you install
certificates.  To complete your CA certificate installation, see the notes
in the post-install/README file.
</verbatim>

The information provided in the README is out of date. Please setup the CA certificates in the following way instead:
<pre class="screen">
source setup.sh
$VDT_LOCATION/vdt/bin/vdt-ca-manage setupca --location local -url osg
</pre>

This command will download certificates distributed by the OSG to =$VDT_LOCATION/globus/share/certificates= and create a symlink from =$VDT_LOCATION/globus/TRUSTED_CA= to that location. [[VDTCAManage][Other options]] are also available.

To reflect the changes update the environment and run the post installation script:
<pre class="screen">
vdt-post-install
</pre>

You can verify that the version installed is the version you expected by executing =vdt-version=. To see all services available use =vdt-control -list=
<pre class="screen">
vdt-version
vdt-control --list
</pre>

You will have to enable and start all the services related to user-vo mapping:

   * if you are using GUMS
<pre class="screen">
vdt-control -enable gums-host-cron
vdt-control -on gums-host-cron
touch $VDT_LOCATION/monitoring/osg-attributes.conf
$VDT_LOCATION/gums/scripts/gums-host-cron
</pre>

   * if you are using edg-grid-map file
<pre class="screen">
vdt-control -enable edg-mkgridmap
vdt-control -on edg-mkgridmap
</pre>

To enable and start crl download , certificate update do the following:

<pre class="screen">
vdt-control -enable vdt-update-certs fetch-crl
vdt-control -on vdt-update-certs fetch-crl
</pre>

Check that osg-user-vo-map.txt is present and not empty, eg:
<pre class="screen">
ls -l   $VDT_LOCATION/monitoring/osg-user-vo-map.txt
-rw-r--r--  1 root root 6224 Apr 14 14:23 /opt/vdt_vomap/monitoring/osg-user-vo-map.txt
</pre>

As a last step you will need to modify _ProbeConfig_ files  to add location of osg-user-vo-map.txt.
<pre class="file">
<verbatim>
UserVOMapFile="<VDT_LOCATION>/monitoring/osg-user-vo-map.txt"
</verbatim>
</pre>

---++ Starting/Stopping Gratia dCache Probes

To start/stop the Gratia dCache-transfer probe that should be running on dCache admin host do the following:
<pre class="screen">
/etc/init.d/gratia-dcache-transfer start/stop
</pre>

The Gratia dCache-storage probe is a cronjob that is run by default every hour. The cron file gratia-probe-dcache-storage.cron is located in /etc/cron.d
 
---++ Sanity Check

If you turned on the Gratia dCache probes you should be able to see the accounting information by accessing your Gratia collector. Keep in mind that storage probe collection is executed by a cron job, so check the time the cron job will be executed:
<pre class="screen">
[root@gwdca04 ~]# cat /etc/cron.d/gratia-probe-dcache-storage.cron 
22 * * * * root "/opt/d-cache/gratia/probe/dCache-storage/dCache-storage_meter.cron.sh"
</pre>

Also, it is possible that it will be some delay due to Collector heavy load, so be patient. 

To access the information about Gratia dCache-transfer probe, go to http://<gratia_host>:<gratia_port>/gratia-reporting/, click on "Custom SQL Query" on the left site menu frame, and enter the following query into provided text box:
<pre class="file">
<verbatim>
select * from MasterTransferSummary where ProbeName like 'dcache-transfer:<dcache_admin_host_name>';
</verbatim>
</pre>
click on "Execute Query" and you will see the total number of transfer per user.


To access the information about the Gratia dCache-storage probe, go to http://<gratia_host><gratia_port>/gratia-reporting/, click on "Custom SQL Query" on the left site menu frame, and enter the following query into provided text box:
<pre class="file">
<verbatim>
select * from StorageElement where ProbeName like 'dcache-storage:<dcache_admin_host_name>';
</verbatim>
</pre>

click on "Execute Query" and you will see the storage information. 


To check ITB Gratia collector click here [[http://gratia-osg-itb.opensciencegrid.org/gratia-reporting/][ITB Gratia]].

To check OSG Gratia collector click here [[http://gratia-osg-transfer.opensciencegrid.org/gratia-reporting/][OSG Gratia]].

---++ Known issues

---++ Support
   If you cannot find answers, please send all your questions to osg-storage@opensciencegrid.org  
#DebugInfo
---++ Debugging Information
---+++!!File Locations
You could find log and configuration files for each of the probes in the following location:
|*Probe Name* |  *Configuration files* | *Log files* | 
|Storage|/opt/d-cache/gratia/probes/dCache-storage/ProbeConfig|/opt/d-cache/gratia/var/logs  | 
|Transfer|/opt/d-cache/gratia/probes/dCache-transfer/ProbeConfig|/opt/d-cache/gratia/var/logs |


---+++Screen Dump of the Complete Install Process 
%TWISTY%
<pre class="screen">
yum list pyOpenSSL
Loading "kernel-module" plugin
Setting up repositories
Reading repository metadata in from local files
Excluding Packages from SL 4 base
Finished
Installed Packages
pyOpenSSL.i386                           0.6-1.p23              installed       
[root@gwdca06 ~]# yum install pyOpenSSL.i386
</pre>

The following rpms will be installed:
   * gratia-probe-common-&lt;version&gt;.noarch.rpm
   * gratia-probe-dCache-storage-itb-&lt;version&gt;.noarch.rpm          
   * gratia-probe-dCache-transfer-itb-&lt;version&gt;.noarch.rpm         
   * gratia-probe-extra-libs-&lt;version&gt;.noarch.rpm
   * gratia-probe-extra-libs-arch-spec-&lt;version&gt;.&lt;arch&gt;.rpm
   * gratia-probe-services-&lt;version&gt;.&lt;arch&gt;.rpm          
%STOPINCLUDE%
%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.TanyaLevshina - 13 Apr 2009 %BR%
%REVIEW% Main.SuchandraThapa - 21 Jul 2009 %BR%
%REVFLAG% %X% %BR%
%ENDTWISTY%
---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = TanyaLevshina

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = TedHesselroth
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = SuchandraThapa
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %NO%
############################################################################################################
-->
