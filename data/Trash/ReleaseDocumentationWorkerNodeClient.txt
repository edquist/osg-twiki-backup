%META:TOPICINFO{author="SuchandraThapa" date="1274470738" format="1.1" version="1.49"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%


---++What is the worker-node client and why do you need it?

The worker node client package contains a set of executables used by jobs running on the cluster.   It is a required package for OSG so that, for example, jobs requiring access to storage elements and the like have the necessary client tools available.    No host certificate is required for the compute node, but an up-to-date list of certificate authority files and certificate revocation lists (CRLs) is required.

---++ Installation options
There are three installation options:
   1. _Suggested Installation:_ Install the wn-client package on a file server separate from the CE; for example, the one that serves $APP.  You will be installing the fetch-crl program onto this node, which requires root access, and adds a script into cron.  The updates require outgoing network access from the node it is installed on.  This directory should be accessible from the CE as well.    
   2. _CE installation:_ Install the wn-client package on your CE. While we do not suggest you do this, we provide instructions, and support this installation as an option.
   3. _Local installation:_  Install the worker node client software on each node.  *%RED%This installation method is required if you plan to use gLexec software.%ENDCOLOR%*  There are required modifications to the default fetch-crl configuration so that either Squid or rsync is used to distribute the fetch-crl files.  Instructions to set up both of these methods are given below.

In all cases you need to choose a path that is common to all worker nodes and the CE.  You will be prompted for this path when running the =configure-osg.py= script on the gatekeeper in order to set the =$OSG_GRID= variable.

---++ How to install
Install using: 
<pre class="screen">
pacman -get %CACHE%:wn-client
source setup.sh
vdt-post-install
</pre>

---++ Full log of installation
A full log of an installation is given [[WorkerNodeClientInstallLog]], followed by comments on why each choice is made.

---++ Comments on installation questions
---+++ !!Caches and licenses
The installation will first ask if you trust the Pacman caches, and then if you agree to the VDT licenses.  Answer =y= to both.

---+++ !!CA's and CRL's
New VDT versions don't install the CA's and CRL's. You have to explicitly decide what to do.

---++++!!Option 1 - Default install
If you install this worker node client package on the file server that serves $APP, and you have no previous installation of the fetch-crl cron script, 
enable and turn on the CA and CRL fetchers.
<pre class="screen">
vdt-ca-manage setupca --location local --url osg
vdt-control --enable vdt-update-certs
vdt-control --enable  fetch-crl
vdt-control --on
</pre>

---++++!!Option 2 - CE install
If you are installing this worker node client on the OSG CE, just enable the CEs with
<pre class="screen">
vdt-ca-manage setupca --location local --url osg
</pre>
and do nothing else.

You will change the symlink to point to the main VDT location, =$VDT_LOCATION/globus/share/certificates= once the OSG CE is installed.

---++++!!Option 3 - Suggested WN install
---+++++!!3a: Local installation plus squid for CA/CRL's
For local installation on worker nodes, you should always use a squid proxy to reduce the network load.
This is done 
by creating the file /etc/sysconfig/fetch-crl with contents
<pre class="file">
export http_proxy=fermigrid4.fnal.gov:3128
</pre>
where fermigrid4.fnal.gov would be replaced with whatever the name of your
site squid server is.  
Then enable the CA and CRL fetching scripts:
<pre class="screen">
vdt-ca-manage setupca --location local --url osg
vdt-control --enable vdt-update-certs
vdt-control --enable  fetch-crl
vdt-control --on
</pre>

---+++++!!3b: Rsync for CA/CRL's
If you want to centrally handle the CAs and CRLs, then just setup the CAs with
<pre class="screen">
vdt-ca-manage setupca --location local --url osg
</pre>

Then download
the tarball at http://fermigrid.fnal.gov/files/csync/csync-0.9.0.tgz and follow
the instructions on how to set up the rsync server and clients to 
get your CA certs. This package has been contributed to the VDT and will 
be part of the VDT in future versions.

---+++  !!Log rotation
Answer =n= to whether or not to rotate the VDT logs

---+++ !!Automation
The Worker Node Client installation process can be automated for any 
of the scenarios above, whether installing on the head node or on a bunch of worker nodes., so that you don't have to type in the letters.

<pre class="screen">
VDTSETUP_AGREE_TO_LICENSES=y
export VDTSETUP_AGREE_TO_LICENSES
VDTSETUP_INSTALL_CERTS=l
export VDTSETUP_INSTALL_CERTS
VDTSETUP_EDG_CRL_UPDATE=y
export VDTSETUP_EDG_CRL_UPDATE
VDTSETUP_ENABLE_ROTATE=n
export VDTSETUP_ENABLE_ROTATE
pacman -trust-all-caches -get %CACHE%:wn-client
</pre> 

---++ Enabling VDT services
If you answered "y" to rotate the logs, or run the fetch-crl daemon, then you should
enable it with the following command
=vdt-control --on=

---++ Installing gLexec 

gLexec is an optional package new to VDT 1.8.0 and greater which, if installed,
should be installed with the software on the worker node client.  gLexec is designed
for authentication of pilot jobs.  Several VO's submit pilot jobs under the DN of one user 
which then call back to the VO submit host, e.g. Panda or !GlideWMS, to get a job
from a different user.  gLexec is designed to be installed on every worker node
and authenticate the user who is submitting the real job. It does this by calling 
a configurable callout (either LCG's lcas/lcmaps or GUMS/PRIMA).

%IMPORTANT% Instructions on how to install and configure gLexec can be found at [[GlexecInstall]]. 

---++ Post-install checks
If the OSG WN client is installed along with some other VDT package on any given node, the CE for instance, as in option 2, you should do the following steps afterwards.

Look at =crontab -l= for root, make sure that the fetch-crl that is there is running from the correct area. Make sure that it's sourcing the correct location.

For example, if the OSG CE is installed in =/usr/local/vdt= , and the OSG WN Client is installed in =/usr/local/grid=

Look at the =/usr/local/grid/globus/TRUSTED_CA= link. It should point to the real area where your certificates will live, in this example =/usr/local/vdt/globus/share/certificates= . If it doesn't, change it to that location.


<pre class="screen">
[root@fgtest3 globus]# <b>pwd </b>
/usr/local/grid/globus
[root@fgtest3 globus]# <b>ls -l TRUSTED_CA </b>
lrwxrwxrwx  1 root root 41 Mar 16 15:44 TRUSTED_CA -> /usr/local/grid/globus/share/certificates
[root@fgtest3 globus]# <b>rm TRUSTED_CA </b>
[root@fgtest3 globus]# <b>ln -s /usr/local/vdt/globus/share/certificates TRUSTED_CA </b>
</pre>

%NOTE% If Condor is used as the job manager, you should also make a symlink from =/etc/grid-security/certificates= to =$VDT_LOCATION/globus/share/certificates= .

---++ Notes
%IMPORTANT% If you are installing this package on a head node / compute element where an OSG CE install will also be done afterwards, it is recommended that you log out and log back in before continuing with the main OSG CE Install, otherwise all the wn-client stuff will be in your path and confuse everything.
<!--
long-standing GLOBUS_LOCATION problem now fixed in vdt 1.8.1/osg 0.7.0 and
greater.  S. Timm 10/4/07
%IMPORTANT% A side effect of the worker node client as currently configured, is that $GLOBUS_LOCATION can and does wind up in a different location on the head node than it does on the worker node.  The environment at the beginning of the job will give you $GLOBUS_LOCATION on the head node.  You have to source =$OSG_GRID/setup.sh= to get where $GLOBUS_LOCATION is available to you on the worker node. %BR% %BR%   To avoid this, it is required to add an appropriate softlink on every worker node such that $GLOBUS_LOCATION as defined on the head node points to the place where it is located on the worker node after the wn client installation. E.g., CMS analysis jobs submitted via Resource Broker will presently fail without this softlink. %RED% It has been claimed that in OSG 0.8.0/VDT 1.8.x, the vdt-job-environment routines which are called by the Globus jobmanager.pm to 
insert all the OSG variables into the job environment, will fix the $GLOBUS_LOCATION too, but in fact the problem still persists.%ENDCOLOR%.
-->

<!--

Installation of the worker node client package does imply maintenance of the certificates that it installs.  
It is possible to rsync either the full worker node client package to the worker nodes, or just the
CA certificate directory found in $VDT_LOCATION/globus/TRUSTED_CA.  This eliminates the 
need to do multiple pacman updates every time a new CA-Certificates package comes out, which 
is on the average of every couple of months.  

It is not enough to rsync the OSG CE installation, or NFS-mount it to the worker nodes.  The Worker 
Node client package now contains software that is not in the base OSG CE installation and VO's are
expecting it to be there, so it must be made available somewhere.

Doing anything automatically on the worker nodes requires care.  An example script that looks to see what worker nodes on a cluster using pbsnodes is attached to this page.  It can be edited or replaced with a more general version to match your specific cluster conditions.  Note that the example script does not use the batch system itself, but uses a feature of the local batch scheduler to avoid trying to push certificates to worker nodes that are down and to build a list of operating nodes to receive the update.  To use this script with the greatest amount of integration into existing certificate update procedures, it can be called just after the edg-fetch-crl call in the edg/sbin/edg-crl-upgrade script that should be in the edg/sbin area of your OSG VDT installation.  Example code to do so would look like:
%RED% These instructions are obsolete from osg 0.6.0 and greater because they refer to adding code to the now-obsolete edg-crl-upgrade script.  A new wn-cert-sync package is in 
preparation currently and should be released soon %ENDCOLOR%
<pre class="programlisting">
# Add section to upgrade the worker nodes:
    if [ -x $VDT_LOCATION/edg/sbin/wn-cert-sync ]; then
        $VDT_LOCATION/edg/sbin/wn-cert-sync -l $CRL_UPGRADE_PATH -q
    else
        echo "In edg-crl-upgrade: Worker node certificates not synchronized! "
    fi
</pre>

where =wn-cert-sync= is a soft link to your particular cluster's worker node certificate synchronization script, as in the example attached. (Customize one to your cluster configuration.)
   * This issue deserves further discussion and repair in the current OSG releases.
-->

%BR%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = SuchandraThapa

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = RobGardner
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = IgorSfiligoi
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->

%META:TOPICMOVED{by="ForrestChristian" date="1166058606" from="Integration.WorkerNodeClient050" to="Integration/ITB_0_5.WorkerNodeClient"}%
