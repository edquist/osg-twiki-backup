%META:TOPICINFO{author="StevenTimm" date="1285882430" format="1.1" reprev="1.13" version="1.13"}%
%SHOW_DOC_STATUS_TABLE%

---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Purpose
      To enable the !PBS batch scheduler to work with the !OSG software stack

---++ Word To The Wise
   * Install the !PBS server (or flavours thereof) on a different node than the !OSG gatekeeper node to avoid problems in the gatekeeper node bringing down the batch scheduler
   * You can have multiple gatekeepers talking to a single !PBS server/scheduler (eg: Production and !ITB) with different scheduling rules
   * However, Gratia accounting system and WS-GRAM both assume that the PBS master is on the same node as the gatekeeper.  To get around this, all you need to do is to NFS-export your PBS server logs and accounting logs from the PBS master to the gatekeeper.  On a cluster where the top level directory is in /var/spool/pbs, you would have to export the directories /var/spool/pbs/server_priv/accounting and /var/spool/pbs/server_logs.  

---++ !PBS Configuration
 !PBS *does not* stream stdout and stderr. It maintains stderr and stdout at the local cluster node and copies back these files at the end of the job.
   * Copy back, *by default*, is done using the =pbs_rcp= command which might not be supported or might not want to be supported on most clusters.
   * If your home directories are mounted across cluster nodes, use the =$usecp= directive to manage copy back.

&nbsp;&nbsp;&nbsp;&nbsp;Edit =/var/spool/pbs/mom_priv/config= and add
      <verbatim>
      $usecp *.my.sub.domain:/home /home
      </verbatim>

---++ Gatekeeper Configuration

 =$VDT_LOCATION/globus/lib/perl/Globus/GRAM/JobManager/pbs.pm= is the !Perl script that the preWS gatekeeper uses to translate RSL to the !PBS submit script.  The PBS script, by default, does not insert any queue, it assumes you will use the default queue.  It is possible to get PBS directives applied to all the jobs you submit by editing the pbs.pm file.

---+++Making a default queue
I modify the pbs.pm to add the following line #PBS -q osg.  This means that any 
job that does not have a queue defined by default will get sent to the osg queue.
If the user specifies the (queue=xxxxx) command in the RSL, then it will go to whatever
queue is specified.
<pre>
#! /bin/sh
# PBS batch job script built by Globus job manager
#
#PBS -S /bin/sh
#PBS -q osg
EOF
</pre>
---+++Making a dynamic OSG_WN_TMP directory
Under PBS it takes two steps to do this:  a prologue to actually create the directory 
and a modification to pbs.pm where OSG_WN_TMP is set:
<pre>
#!/bin/bash
## Create a /scratch/jobid dir for every job run, and copy a user's keytab file
from the
## PBS head node.
#
# Updated October 31  2006 by sether@fnal.gov, added lots and lots of error chec
king.
#
# 11/06/2006 - Jason Allen <jallen@fnal.gov> Modified script to use a separate s
cratch filesystem for each job
#
# 12/15/2006 - JA - Write critical errors to ERROR_FILE to mom health script
# will pick it and set the node offline

JOBID=$1
USERNAME=$2
JOB_EXEC_GRP=$3
JOB_NAME=$4
RESOURCE_LIMITS=$5
QUEUE=$6
JOB_ACCOUNT=$7

SCRATCH_DIR="/scratch"
KRB_DIR="/var/adm/krb5"
JOBS_DIR="/var/spool/pbs/mom_priv/jobs"
HOME_MNT_POINT="/RunII/home"
ERROR_FILE="/var/spool/pbs/ERROR"

# Sanity checks
if [ ! ${JOBID} ]; then
        logger "PBS prologue: JOBID not specified"
        echo "JOBID not specified"
        exit 1;
fi

if [ ! ${USERNAME} ]; then
        logger "PBS prologue: USERNAME not specified"
        echo "USERNAME not specified"
        exit 1;
fi


if [ ! -d ${SCRATCH_DIR} ]; then
        err="$SCRATCH_DIR does not exist"
        logger "PBS prologue: $err"
        echo $err | tee $ERROR_FILE
        exit 1;
fi


# Job info
echo "---------------------------------------------------------"
echo "Start CAB Prolouge `date`"
printf "%-15s %s\n" "jobid:" $JOBID
printf "%-15s %s\n" "username:" $USERNAME
printf "%-15s %s\n" "job name:" $JOB_NAME
printf "%-15s %s\n" "limits:" $RESOURCE_LIMITS
printf "%-15s %s\n" "queue:" $QUEUE

#$SHORT_JOBID=${JOBID%%.*}

# Clean-up scratch
# Only running jobs should be using space.
for job_id in $(ls ${SCRATCH_DIR} | egrep "fnal.gov$"); do
        short_job_id=${job_id%%.*}
        j_cnt=$(ls $JOBS_DIR | egrep -c "^$short_job_id")
        if [ "$j_cnt" -eq  0  -a "$short_job_id" != "" ]; then
                rm -rf ${SCRATCH_DIR}/${job_id}
                logger "PBS prologue: Cleaning-up old scratch dir ${SCRATCH_DIR}
/${job_id}..."
        fi
done

# do we have scratch filesystems setup
sfs_cnt=$(ls ${SCRATCH_DIR} | egrep -c "_fs$")
if [ $sfs_cnt -lt 1 ]; then

        if [ -x "/var/spool/pbs/bin/mkscratch_fs.sh" ]; then
                err="Creating scratch filesystems. This will take awhile.."

                # Send errors to job output and to error file
                # so health check script will see it and set
                # set node offline.
                echo $err | tee $ERROR_FILE

                logger "PBS prologue: $err"
                /var/spool/pbs/bin/mkscratch_fs.sh
                exit 1;

        else
                err="Could not create scratch filesytems..."
                echo $err | tee $ERROR_FILE
                logger "PBS prologue: $err"
                exit 1;
        fi
fi

# Make sure scratch filesystems are mounted
i=0
while [ $i -lt $sfs_cnt ]; do
        scratch_mnt=${SCRATCH_DIR}/scratch_${i}
        scratch_mnt_info=$(mount | awk -v mnt="$scratch_mnt" '{ if ($3 == mnt) p
rint $0 }')
        if [ "$scratch_mnt_info" == "" ]; then
                mount -oloop ${scratch_mnt}_fs  ${scratch_mnt}
                if [ $? -ne 0 ]; then
                        err="Unable to mount $scratch_mnt. Mount cmd returned co
de $?."
                        echo $err | tee $ERROR_FILE
                        logger "PBS prologue: $err"
                        exit 1;
                fi
        fi
        i=$(($i+1))
done

# Link the job id to a free scratch area
i=0
linked=0
while [ $i -lt $sfs_cnt ]; do
        ls_out=$(ls -l ${SCRATCH_DIR} | egrep "\-> scratch_${i}")
        if [ "$ls_out" == "" ]; then
                cd $SCRATCH_DIR
                # clean-up if necessary
                rm -rf $SCRATCH_DIR/scratch_${i}/*
                ln -s  scratch_${i} $JOBID
                linked=1
                break
        fi
        i=$(($i+1))
done

if [ $linked -ne 1 ]; then
        err="Unable to link $JOBID to scratch dir..."
        logger "PBS prologue: $err"
        echo $err | tee $ERROR_FILE
        exit 1;
fi


# Make sure home dirs are mounted
HOME_MNT_INFO=$(mount | awk -v mnt=$HOME_MNT_POINT '{ if ($3 == mnt) print $0 }'
)
if [ "$HOME_MNT_INFO" == "" ]; then
   echo "Home directories not mounted!!"
   logger "PBS prologue: Home dirs not mounted. Rebooting."
   /sbin/shutdown -r now
fi

chmod 777 ${SCRATCH_DIR}/${JOBID}

echo "End CAB Prolouge `date`"
echo -e "---------------------------------------------------------\n"

</pre>
<pre>
#   Steve Timm, 5/4/07
#   OSG_WN_TMP now set dynamically on this cluster to /scratch/${PBS_JOB_ID}
    print JOB "OSG_WN_TMP_OLD=\${OSG_WN_TMP}\n";
    print JOB "export OSG_WN_TMP_OLD\n";
    print JOB "OSG_WN_TMP=/scratch/\${PBS_JOBID}\n";
    print JOB "export OSG_WN_TMP \n";
</pre>

---++ Troubleshooting
   *  Oftentimes a globus-job-run will fail against a PBS cluster because it is missing an RSL statement.  There are three types of jobtype, single, multiple, and MPI.  The default is multiple.  Unless you have configured your cluster with ssh key-pairs so that all the parts of the multiple job can be launched with the gatekeeper, the job will fail and you will get this error:

<pre>
Host key verification failed.
/var/spool/pbs/mom_priv/jobs/302232.gaia.SC: line 87: [: too many arguments
</pre>

   *  The correct syntax is =globus-job-run <hostname>/jobmanager-pbs -x '(jobtype=single)' <command>=
   *   Note that if a PBS job goes over a resource such as resources_default.mem, it will get killed
   *  Note also that if you are submitting to a PBS queue that is already full to its queue limit, the submission will fail and the job will be held with Globus error 17.



%BR%


---++ *Comments*
| The resource limits are resources_default.mem, resources_default.walltime, etc, not resources_max.  Resources_max is the maximum resource levels that a jobs is allowed to request.  If a job does not explicitly request a higher resource limit, the limits in effect come from resources_default. | Main.CharlesWaldman | 15 Jun 2010 - 21:13 |
| Charles&#39; comment fixed 9/30/10.  Steve&#60;br /&#62; | Main.StevenTimm | 30 Sep 2010 - 21:33 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = StevenTimm

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = SarahWilliams
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


 
-->

%META:TOPICMOVED{by="AnneHeavey" date="1192997611" from="Integration/ITB_0_7.PBSBatchSystemHints" to="Integration/ITB_0_7.PbsBatchSystemHints"}%
