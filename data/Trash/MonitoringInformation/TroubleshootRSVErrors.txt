%META:TOPICINFO{author="ArvindGopu" date="1215604182" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="RSV"}%
---+ !RSV Troubleshooting Guide

%TOC%

---++ !RSV version 2 (!VDT 1.10.x; !OSG 1.0)

---+++ Related to !RSV infrastructure - (!VDT 1.10.x cache, !OSG 1.0)

---++++ !RSV probe jobs stop running because Condor-Cron's lock file is removed by tmpwatch 

In the initial VDT 1.10.1d release (!OSG 1.0), there's an issue on sites that run tmpwatch or similar tools to clean old files from /tmp. !RSV might stop running periodicallyuntil the condor-cron service is restarted because of the way condor-cron is configured to use a lock file in /tmp.

The Condor developers will fix the problem for a future release (as of 2008-07-10), but for now, there is a one-line fix that admins can make to avoid the problem. In =$VDT_LOCATION/condor-cron/local.*/condor_config.local=, there is a line that looks similar to this:
<pre class='programlisting'> LOCK = /tmp/condor-lock.$(HOSTNAME)0.505446661803038</pre>

Change that to this, and restart Condor-cron and osg-rsv: <pre class='programlisting'> LOCK = $(LOG)</pre>

<p> *Note:* The LOG directory should _ not _ be on NFS. It is configured to be in condor-cron/local.*/log. If that is NFS, then you will need to move the local.* directory elsewhere and symlink to it or change the references to it in the config files. 

---++++ Condor-Cron does not start because of IP address binding issue, and consequently osg-rsv service fails to start

Under certain setups, =osg-rsv= might not start because =condor-cron= has failed to start but =vdt-control= has not caught that problem.

If =osg-rsv= fails to start, then first, type =condor_cron_q=, you must see an empty queue. If you do not see an empty queue, then

   * Check =$VDT_LOCATION/condor-cron/local.*/log/MasterLog= for the following error: <pre> 6/24 21:44:20 Failed to bind to command ReliSock 6/24 21:44:20 (Make sure your IP address is correct in /etc/hosts.) 6/24 21:44:20 ERROR "BindAnyCommandPort() failed" at line 8390 in file daemon_core.C</pre> 
   * If you see the above error, then you need to add the following to your =$VDT_LOCATION/condor-cron/etc/condor_config file= <pre> NETWORK_INTERFACE <condor-master-ip-address> </pre> 

---++++ On 0.8 !RSV upgrades, osg-rsv startup fails half way through, consequently condor_cron_q has some probe jobs

Affects *only* RSVv2 installed on !OSG 0.8; Does not affect !RSV on !OSG 1.0 installs.

Some initial versions of configure_osg_rsv that was distributed with !RSV version for !OSG 0.8 resources had a bug, that inserted double-quotes in condor-cron job submission files. If you see an error related to submission of srm* probes then it's likely because of this error. If so, please contact the rsv-dev team for an easy fix. [Will add fix here later]

---+++ Related to !RSV probes (!VDT 1.10.x cache, !OSG 1.0)

---++ !RSV version 1 (!VDT 1.8.x; !OSG 0.8)

---+++ Related to !RSV infrastructure - (!VDT 1.8.1 cache, !OSG 0.8)

If !RSV jobs do not appear to start, and/or they're going to your production condor instance, then do the following to trouble-shoot:

   * If you've got OSG-RSV and/or condor-devel running, stop them. <pre> vdt-control --off osg-rsv condor-devel </pre> 

   * Then start condor-devel <pre> vdt-control --on condor-devel </pre> 

   * 
      * You should see the following processes: condor_master, condor_schedd, condor_procd <pre> # ps -ef | grep condor daemon 20482 1 0 11:24 ? 00:00:00 /osg/0.8.0-RSV/condor-devel/sbin/condor_master daemon 20483 20482 0 11:24 ? 00:00:00 condor_schedd -f root 20484 20483 0 11:24 ? 00:00:00 condor_procd -A /tmp/condor-lock. . . . .</pre> 

   * Then start osg-rsv <pre> vdt-control --on osg-rsv</pre> 

   * 
      * Apart from the above condor processes, you should see one or more condor_starter processes <pre> # ps -ef | grep condor_starter daemon 20698 20483 0 11:31 ? 00:00:00 condor_starter -f -job-cluster 193 . . . daemon 20700 20483 0 11:31 ? 00:00:00 condor_starter -f . . . -schedd-addr <xx.yy.xx.zz:nnn> daemon 20714 20483 0 11:31 ? 00:00:00 condor_starter -f -job-cluster 195 . . . </pre> 

   * 
      * At the same time, you should be able to see !RSV jobs on condor-devel's condor_q: <pre> # source $VDT_LOCATION/vdt/etc/condor-devel-env.sh # condor_q -- Submitter: host.domain.edu : : . . . ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 176.0 <em><rsvuser></em> 4/3 11:27 0+00:06:03 R 0 0.0 probe_wrapper.pl / . . . several of these probe_wrapper.pl jobs (i.e the RSV probe) . . . . . . 193.0 <em><rsvuser></em> 4/3 11:27 0+00:27:12 R 0 9.8 html-consumer 194.0 <em><rsvuser></em> 4/3 11:27 0+00:27:10 R 0 9.8 nagios-consumer ht 195.0 <em><rsvuser></em> 4/3 11:27 0+00:27:08 R 0 9.8 gratia-script-cons 196.0 <em><rsvuser></em> 4/3 11:27 0+00:00:00 I 0 9.8 rotate_html_files.</pre> 

   * *Conflicting system-wide condor_config settings:* If you have a system wide CONDOR_CONFIG setting in /etc/profile or some such file, then ensure the RSVuser gets to use condor-devel by setting in its ~/.*rc file: <pre> CONDOR_CONFIG=$VDT_LOCATION/condor-devel/etc/condor_config</pre> 
      * If you think this is the problem, then make the user-level setting, restart the osg-rsv service using vdt-control. 

---

---+++ Related to probes (!VDT 1.8.1 cache, !OSG 0.8)

---++++ Classads-valid-probe for resources running OSG 0.8 (VDT 1.8.1)

   * If you see this error in the probe output file or its verbose error file: <pre>detailsData: UNKNOWN: Cannot find CE host foo.bar.edu in ReSS collector osg-ress-1.fnal.gov</pre> then it could one of the following: 
      1 Probe cannot find condor_status, and is bailing immediately - check the probe's error log file: $VDT_LOCATION/osg-rsv/logs/probes/red.unl.edu__classad-valid-probe@org.osg.general.classad-valid.err 
      * To check this remotely (mainly for !GOC staff), do this: <pre> RHOST=; REMOTE_VDT_LOCATION=`globus-job-run $RHOST /bin/env | grep OSG_LOCATION | cut -f2 -d=`; echo 'About to test this host' $RHOST 'with OSG_LOCATION=' $REMOTE_VDT_LOCATION; globus-job-run $RHOST /bin/cat $REMOTE_VDT_LOCATION/osg-rsv/logs/probes/${RHOST}__classad-valid-probe@org.osg.general.classad-valid.err</pre> 
         * If that's indeed the case, then there should be an error message that says "condor_status not found" -- something like this: <pre>CMD: [/bin/sh /. . . /classad-valid-probe-worker.sh --glue-ce-host=foo.bar.edu --collector=osg-ress-1.fnal.gov] . . . line 96: condor_status: command not found</pre> 
         * Ensure probe can find !RSV's own condor_status, and also the condor_status the probe uses is not polling a local condor server (bug reported by Charles Waldman during initial VDT 1.8.1 release timeframe). An easy fix is to add the following line: <pre> source ${VDT_LOCATION}/vdt/etc/condor-devel-env.sh</pre> . . . right before the following line (around line 96) . . . : <pre>condor_status -l -pool ${collector} -constraint "GlueCEInfoHostName == \"$glueCEHostName\"" | grep "isClassadValid =" | sort | uniq > $tmpFileTestAttributes</pre> 
 

   * 
      * The resource could indeed be not reporting to !ReSS; for this to happen, the resource needs to have !CeMON and !GIP working properly. 
         * First, do steps outlined in [[https://twiki.grid.iu.edu/twiki/bin/view/ResourceSelection/CEMonTroubleshootingGuide][!ReSS trouble shooting guide]]; 
         * Then look in [[http://home.fnal.gov/~garzogli/ReSS/ReSS-prd-History.html][!ReSS Central Services History]] to see if resource is reporting. 
         * If you still can't figure this one out, then create GOC ticket and assign to ress-ops. 
 

-- Main.ArvindGopu - 09 Jul 2008
