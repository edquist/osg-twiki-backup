%META:TOPICINFO{author="FkW" date="1214576684" format="1.1" reprev="1.6" version="1.6"}%
This is a dirty page full of random thoughts to think about with regard to year 3 wbs planning for osg.

---+ Scalability and Reliability

---++ For Compute Element
   * the stuff we emailed about with Ruth, Toni, fkw. This is reasonably well understood.

---++ For Storage Element
   * this needs a lot more thought. The basic reason for putting it here is the following (excerpt from fkw's SciDAC 2008 paper):
<pre>
In grand total, slightly more than 50\% of the 1.5 Million jobs submitted were recorded as successfully completed.
To understand the origin of the failures, we did a controlled experiment during the month of May 2008..
We identified eight datasets, one from each Tier-1 plus CERN, and asked all Tier-2's to download at least one of them.
We then verified write access to storage at each Tier-2 for output storing. A standard, well tested application was then run at the 40
participating Tier-2. A total of 230k jobs succeeded throughout the month, as compared to 44k failures.
When analyzing the logfiles of all jobs, we found that the failures are predominantly due to storage problems at sites.
In most but not all cases, those problems are detected and fixed by the site admins within 24 hours. However, during those 24 hours,
many more jobs fail than could possibly succeed during the same timeframe because a job that fails in its storage access is very
short. When ignoring storage failures, the success rate in this exercise is better than 99\% .
</pre>

Here's an attempt at listing some of the issues:
   * goal to increase the scalability of srm/dcache by factor 2 as far as number of requests per hour is concerned. This would get it to 2Hz as a goal for writes.
   * take stock of where we are at with the "operations toolset"
      * what's the appropriate frequency of toolset releases?
      * what functionality of tools is out there, but not yet in the toolset?
   * how do we arrive at a more reliable SE ?
      * standard procedure at the sites is to restart all services in sight when something goes wrong.
         * there is no follow-up on problems because they are rare enough to be hard to re-produce.
         * there is limited interest at the sites to really put effort into reporting and figuring out problems beyond getting the site back in working order.
      * would centralized logging of errors allow for better diagnosis?
      * would dedicated testing at reasonable scale allow for finding these problems and diagnosing them.
          * this would mean that OSG runs its own SE and works out the kinks as they find them.
      * would improved logging and error messages be something we should put effort in?
         * Greg Sharp had started down on that. This is gone now that he's left.

A line of approach idea:
Should we (e.g. jointly CMS and OSG) consider a data analysis campaign a la CCRC08. Make that a once every N month deal. During that campaign, 
do a root-cause analysis of every
failure encountered. From such a one week campaign, make a list of these root-cause analyses, and determine how to design improvements.
The improvements are then implemented over the time to the next such campaign. The following campaign then establishes the effectiveness of the
improvements.

Other issues not directly related scalability and reliability:
   * It's a royal pain in the neck to run a dcache installation once it is getting full. I'd say the effort to operate it increases significantly once it is more than 85% full.
   * There's lot's of monitoring that we have but don't install as part of OSG. 
      * statistics module
      * the special srm monitoring
      * the transfer accounting
      * space accounting
   * Nebraska ran into a particularly difficult to debug problem last thanksgiving where corrupted metadata on a few files negatively affected the performance
      of the whole system, leading to roughly 25% job failure during the thanksgiving exercise. Might be worthwhile talking with Brian about details here.

---+ Functionality Follow Through
There's a few loose ends we have started in prior years that need to be completed.

---++ Opportunistic Storage
Find a customer, and push this through to satisfy that customer.
If we can't find a serious customer, drop it.

---++ Accounting
   * We are still lacking the local Gratia repository. Instructions exist, so I'm told, but deployment doesn't.
      * Is this worth it? And if not, why not?
   * Transfer accounting. We don't have it deployed. It might make a lot of sense to have this in the context of trying to understand
      why CPU/wall clock is so poor, and ruling out SE performance as a cause of it.
   * There's a WLCG requirement to have space usage accounting. I'm told that none of what exists actually is the least bit reliable.
      We should follow up on this, and first take stock of what exists, then figure out how to make it reliable.
      * At the CMS computing management meet it was clear that cms wants this to be reliable, and working, and relies on the grids to make it that way.

---++ Storage Authz
   * When we designed this we envisioned something that we never achieved because the underlying filesystem (pnfs) didn't support it.
      We could revisit this with Chimera
      * start with a performance and functionality evaluation.
      * based on eval, decide if this is ready, or should be dropped because it's too much effort, still.

---++ Operations
   * We need to be able to electrnically determine if a site is up or down, based on RSV and downtime announcement.
   * The site admins need to be educated about the tools available, and if those tools aren't up to snuff, then we need to improve them.
      * independent announcement of SE vs CE downtimes.

---++ Release process
   * OSG 1.0 roll-out wasn't very smooth. Is there something we learn from this experience?
      * can we have a more automated process for the VTB roll-out
      * such an automated process would include a list of unit tests
      * should we use RSV in this context of unit tests? 
   * Has the installation of OSG 1.0 become too complicated for small sites?
   * Should we have a simple version of the documentation for small sites?
      * have the default simple install
      * have the lengthy procedure and documentation for a complex site
   * Continuous test of srm using the various clients in OSG release like the existing bestman testing
      * this is not necessarily a release issue only.
      * it's important to test clients against all the versions of storage deployed, and not just the latest one in the release.


-- Main.FkW - 14 Jun 2008
