%META:TOPICINFO{author="BrianBockelman" date="1311784935" format="1.1" reprev="1.2" version="1.2"}%
---+Blueprint Meeting Topics/Items

---+++Administrative Topics
   * Formalize process for technology/blueprint ideas to be considered. How to get items on this list?

---+++ Feature Requests/Problem Fixes/ Scalability Tweaks
   * Global OSG BDII (Miron, Burt, VO-reps?) 
   * VM-based external testing nodes at GOC. (Rob Q.)
   * Request to add caching with TTL to GUMS to better handle load expected from widespread deployment of glexec. (John H.)
   * Issue with non-authenticated RSV and failure of one type of RSV probe causing sites to appear down as if other probe types had failed. (?)
   * Small files into SRM. Use HTTP 1.1 with bulk posts? (Brian?)

---+++ Changes Without (Obvious) Architectural Implications
   * Alternate GIP-based info system (Eliminate CEMon) (Brian, Burt?).  May include switching the data transport layer to Gratia transport.
   * Accounting issues with newer campus grids DHTC pushes.  (Requested by Chander, Gabriele)

---+++Changes With Minor Architectural Implications
Data Management:
   * Widely adopting and supporting CVMFS.
   * Integration of out-of-band data stage-in (via URI syntax) in Condor to facilitate Squid usage.  (This may already exist via plugins--just need to include working http:// handler in base distribution.) (Condor Team)
   * Data value metric to balance storage costs vs. re-copy.
   * Detailed storage accounting. (?)
   * WN-based cluster storage via glide-ins (Hadoop?, xrootd?)? What about authentication/authorization?

Accounting:
   * How to handle Gratia reporting from pilot-based systems (Panda, glideinWMS, others?). (Mats/engage)

Networks:
   * Improved integration of Internet2 network tools into the OSG Production Grid.

Testing:
   *  Work out the typology/taxonomy and approaches to Worker Node testing. E.g. all nodes, any 1 node, each node, etc? Do we need a CE-based framework for submitting tests to particular WNs?

Other:
   *  Add relevant info to WN environment about limits the job has to abide by (wallclock, memory, number of CPUs, etc.) (Igor S.)
   *  "As the demand for access to the resources increases, we will help the users with the dynamic resource allocation services and assist VOs in planning for their use." (Chander) I read this as noting the increasing relevance of Grid-as-Cloud and dynamic, temporary site provisioning.
   *  Adding a new glidein-factory in Europe?
   * Creation of a Globus Online "data mover".(?)


---+++Changes with Major Architectural Implications

   * Taxonomy of distinct areas where OSG intersects Cloud Computing. 
 	  * Elastic clusters. 
 	  * Dynamic OSG sites. 
 	  * Cloud APIs on OSG sites. 
How are these reflected in the list of demonstrators/investigation targets? 

   * Directions in Identity Management and Federation. Shibboleth. Instant X509 certificates. Etc. Review of Mine's ID Mgmt implications. Note BNL is not setting up IdP, *RACF* is. And CERN already does have IdP opening up.

   * Related to 2 previous entries-- auth for cloud resources/APIs. Cloud provider platforms typically use SSH keys or username/pass, while Grid resources use X509. Need transparent bridging between these domains. Does this already exist?

   * Storage allocation

---+++ Meeting Agenda
There's two parts to today's agenda:

1   At one hour before time runs out, we will cut off discussion and switch to the Blueprint document.  Link: http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=18 .

1   For each item it would be good to answer these questions:

      * What is its status? Any new developments or insights?
      * Do we fully understand what it means with respect to current practice/systems?
      * Where does it go next?
         * Is it done?
         * Do we discard it? Shelve it?
         * Do we need to add it as a technology investigation?
         * Does effort toward it need to be officially allocated?
      * Who should take primary responsibility for it. 

