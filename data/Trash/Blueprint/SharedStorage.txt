%META:TOPICINFO{author="KyleGross" date="1476284648" format="1.1" version="1.6"}%
%META:TOPICPARENT{name="WebHome"}%
This is information for the Trash/Blueprint Meeting and related meetings for 11 -12 February. The Trash/Blueprint starts at Noon on 11th and runs until 3pm  on 12th. There are side meetings that we are planning given the opportunity that people 

Space Management Initial thoughts -  [[http://osg-docdb.opensciencegrid.org/0009/000923/001/OSG%20Space%20Management.pdf][OSG Document 923]]

Storage Appliance Thoughts [[#StorageAppliance][here]]

Trash/Blueprint discussion about Storage Appliance: [[#BluePrint][ the notes of whiteboard]]

Thursday 11th 
|9:00-10:40 |Briefing  on status of Gratia  |Wh9W||
|11:00-12:00 |Subgroup discussion of US LHC operations support issues ||
|12:00-1.00 |Lunch ||
|1:00-5:00 |Space Management, shared storage on OSG, |Wh7E, Bullpen ||
|5:00 |PI, Project Mgr meeting||

Friday 12th
|8:00-12:00 |Continuation of discussions, |WH7XO, Racetrack||
|12:00 |Lunch ||
|1:30-3:00 |Next steps and action items.||

<hr>
<hr>

<b>Goals Include:</b>
   1. discuss consistent OSG policy for providing non-major VOs with public storage on OSG sites
   1. direction we should take  to provide the organized files transfer between OSG sites for non-major VOs 
   1. Discuss ideas for Storage Appliances offer to sites

#StorageAppliance <b>Storage Appliance Thoughts and Information</b>
<verbatim>
from Tanya, Dan, Brian:  The goal might be to provide the following:
 
  * Any VO can use public storage without prior arrangements
  * No per- VO limit is necessary: community space
  * It is crucial that the space is  correctly advertised in the OSG information systems
  * File cleanup could be done at any time (the VO notification is desirable)
 
 We have described one of the possible solution in this document (https://osg-docdb.opensciencegrid.org:440/cgi-bin/RetrieveFile?docid=923&version=1&filename=OSG%20Space%20Management.pdf). The following steps would be provided by OSG staff:
 
       1. Facilitate in the creation of public spaces.  The OSG-ET will work with the larger resource providers (USCMS and USATLAS) to determine what sites will provide public spaces and how large these spaces will be.  
       2. Assist VOs in understanding and using the technologies (OSG-Storage and Engagement).  OSG-Storage will develop a survey to help the OSG understand how VOs would like to use space and advice vo on reasonable storage utilization.
 
 Pros:
  In principal this approach doesn't change anything for site admins in current way of operating storage if they already provide some opportunistic storage. They may need to allocate more space and configure it so doesn't have time expiration. Also, they may add more VOs  they would like to access public storage.  
 
 Cons: 
  No enforcement mechanism
  No real verification mechanism until member with a particular voms certificate tries to execute the transfer
  VO user should adopt to various storage solution (e.g use different option depending on what type of storage they are trying to connect)
  Any "bad" user behavior may bring down SE 
 
 The other approach has been discussed just recently. The idea is to provide an storage appliance for a participating sites. 
    1. OSG-ET providex means to acquire the suitable hardware. According Brian's estimate it could cost 
            A. Costs ~ $2,700
             5TB  for an OS install  (RAID 1)
             4x 2TB 5400 rpm disks for the 'data' (RAID5)( for the 2TB disks. Would give just over 5TB usable formatted. 
            B. Costs ~ $3,200
              same as above but with more space
              6x 2TB disks, ~9TB usable space
           C. Costs  ~ $3,850
             with  MD1000 vault with 8x 1TB disks leaving 7x slots free for potential future expansion. with RAID6 ~ 5TB usable space that could be expended to 25 TB
     2. OSG storage group provides image of SE
     3. One storage type is used 
     4. Storage provides only public access
     5. Site admins provide physical space, network and power
     6. Site admins are responsible for  maintenance, hardware support, kernel upgrade
     7. OSG storage staff have admin right to configure/monitor/update software
 
 Pros:
    Site admins do not need to worry about public storage
    Ease of use: VOs users will use the homogeneous set of command to access storage throughout osg 
    Site admins do not need to monitor , troubleshoot storage
    Simplify support because of one type of supported storage 
     Any problem with public storage can not bring down site owned SE
 
 Cons:
    Site admins still need to be involved:
            installation, network, etc 
            maintenance, security upgrade, etc
            GIP configuration
    More load on OSG storage group  
            support load
            need to worry about monitoring, cleaning tools
 
These two solutions could co-exist.

</verbatim>

#BluePrint <b> Public Storage Discussion</b>

The note of the whiteboard:
<verbatim>
OSG owns X TB at N sites
•	Centrally know the status of disk space at all sites
•	Centrally make an allocation to any VO
•	Centrally rescind an allocation
2-3 depends on monitoring and “social pressure” instead of technical controls
•	Access is co/commit rather then fopen
•	No bandwidth control or load control of any kind
•	A VO has all the same capabilities as Dan, within its allocation
•	All VOs can see everything Dan sees
•	Notification of allocation changes via ticketing system


Storage configuration, tools:

Allocation = top directory = DN+Role
Dan has access to all tool that has power to removes files
Need new mechanism for local bookkeeping of directory vs deletion map
Create directory at allocation time and tie DN of the owner to uid via GUMS

VO /prod. Manager actions:
Registration  to allocation service (accept the credential)
Request allocation
Granted allocation (decision, implementation, verification)
Rescinding  allocation (announcement of future rescind, deletion of policy, verification)


User Interface: 
       Put, get, ls, du, rm
      Accounting – space  allocation within the gratia
      Space available /consumed – gratia 


User:
Number of files stored ~ 10,000 , number of file per directory 1,000
Total space per allocation ~ 1TB

Rates: file /per sec (Io/sec) (1HZ)  LAN – 1 MB/S  per core max 10 MB/s per appliance aggregate 1Gbps  

Usage model:
    put via WAN or LAN
    get via WAN or LAN

extra bonus – use it from the worker ~ 5TB 
write the proposal                                                                                     

no  replication, raid 
             

</verbatim>

<b>Alain's notes on the NEES file transfer work</b>

<verbatim>
Archiving for NEES 
From Purdue to Fermilab, initially. Goes to tape
May also do Purdue to UW, but that would be disk.

Need test account so we can verify that we can do it.

COPY_FERMI(File, Credential)
	0. Begin Transaction
	1. Checksum File
	2. Copy to Fermilab(File, Credential)
	((3. Checksum remote file))
	3. Verify file is at Fermi correctly (but may not be on tape)
	4. Bookkeeping at Purdue
	4a. Bookkeeping needs to be machine-readable, so we can verify file existence and checksum later.
	5. End Transaction

Tanya & Alain are contacts
Miron will give some UW effort to run this as a DAG.

Ruth has ownership of document and will get back to NEES.

Need full backup by end of March.

Good to know earlier that the file is transferred, before the checksum is created. 

On top of COPY_FERMI, might need to pass in directory, and "tar" it up. 

Tanya will be gone March 5-12, March 29-April 2

Tanya's obligations: 
	* Steps 1-4 above
	* Provide tool that takes file name, verifies it is on top and uncorrupted
Alain/Condor teams obligations: 
	* Tool to create DAG to copy one or more files with Tanya's tools
	* Tool to decide how to copy set of files/directories.
</verbatim>


-- Main.DanFraser,  Main.RuthPordes - 03 Feb 2010