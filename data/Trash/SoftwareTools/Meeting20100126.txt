%META:TOPICINFO{author="ChrisGreen" date="1266960208" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="SoftwareToolsMeetings"}%
---+ OSG Software Tools Group Meeting

---++ Meeting Coordinates
| <b>Date</b> | Tuesday, January 26, 2010 |
| <b>Time</b> | 9:00am Central |
| <b>Telephone Number</b> | 510-665-5437 |
| <b>Teleconference ID<b> | 4321 |

---++ Agenda 
   * Discussion of development needed for Gratia

---++ Background

From Philippe on 8-January-2010:
<blockquote>
<h4>Evaluate impact of new CE interfaces - CREAM and Gram 5.</h4>

Assuming access to an existing Cream and Gram 5 installation and
assuming knowledge on how the Gratia probe acquires information about
the user's certificates, the evaluation should take about a day each.  A
priori the necessary upgrades are likely to requires less than 1FTE week
each.

<h4>Improve the automation of the Gratia Testing framework and work with the VDT team on including this in the nightly build/tests as far as possible.</h4>

Aaron Thor is currently working on this.  The need here will depend on
the outcome on his work.

<h4>Work with the !MonaLisa team to allow Gratia probe information to be injected into ML.</h4>

The most significant parts of this are deciding which information to
load into ML and writing the code to load the code in ML. Both of which
are outside of our privy (i.e. in particular we have no current
knowledge of ML). So our involvement should be limited to consulting on
the which information to load and out to extract it from the Collector
Database. This should fit under the currently allocated 'maintenance
and support' effort.

<h4>Allow data from single probe to be delivered to multiple collectors.</h4>

This will requires some knowledge of the Gratia probe library and coding
in python. This will requires touching, or at least double checking, a
large fraction of the Gratia probe library. I estimate that this should
take 2 to 4 FTE weeks to complete and test.

<h4>Ensure clean and clear separation of code modules between "Gratia, the job accounting system" and "Gratia, the data transport".</h4>

This will requires some knowledge of the Gratia probe library and coding
in python and some knowledge of the Gratia Collector code and coding in
Java. The code is already pretty well separated so I estimate the work
at only 2 to 4 FTE weeks.

<h4>Make transfer probes work at a summary level instead of per-file.</h4>

This will requires some knowledge of the way the Storage Element
(dcache, gridftp) stored their transaction information
(and coding in python). This should be quite straightforward and cost
less then 1 FTE week.

<h4>Add a layer to allow forwarding of summary data instead of raw data.</h4>

This will require deep knowledge of the Collector DB Schema and
Collector code (Java, hibernate). This has potentially deep implication
on duplicate record detection, origin tracking and thus raw data
integrity. I estimate the work at anywhere between 3 and 6 FTE months.
</blockquote>

From Chris Green on 12-January-2010:
<blockquote>
<h4>The billinginfo table on a dCache billing DB needs a high-capacity
(64-bit or better) auto-incrementing primary key.</h4>

The dCache-transfer probe currently implements all sorts of nasty, inefficient hacks (that
have loophole corner cases) to get around the problem that selecting
from this table has no reliable concept of, "after."  The timestamp is
not reliably unique; and nothing else is monotonically increasing. I'm
not even sure that's true of the timestamp, to be honest. This gives
us great trouble keeping track of what we have sent, and what we
haven't, and allowing us to limit query sizes to avoid undue delay /
DB load. Of course, if/when this feature is implemented we need to
make the probe use it (and cope with its absence in the same old,
hacky way). What steps do we need to follow to request this feature in
dCache? Conceivably it could be done only for OSG installations of
dCache rather than by requiring dCache-DESY to implement it: one would
either an an auto-incrementing column to the billinginfo table; or
have a separate (UniqueId, transaction) lookup table filled with a
trigger. The former solution would be more efficient than the latter,
of course, but have a longer upgrade time.

<h4>Coping with truncation / rotation of scraped log files.</h4>

Following a problem identified on NYSGRID-CUNY-GRID, it is evident
that those probes that scrape log files (PBS/LSF, SGE, glExec) need to
be able to cope with their logfiles being truncated / rotated in
midstream. The instant case was with the SGE probe, which simply
stores the line number of the last-processed record in a checkpoint
file. The accounting log file was truncated and the probe went silent,
waiting for the day when its length surpassed that recorded in the
checksum. This, obviously, is a data loss problem. Note I'm not
offering a simple solution to this problem -- I'm not sure there is
one.
</blockquote>

From Chris Green on 11-February 2010:
<blockquote>
<h4>Scaling / High Availability improvements.</h4>
This falls into two parts:
<ol>
  <li>Do the development / re-factoring work necessary to enable a collector to run multiple concurrent !RecordProcessor threads.</li>
  <li>Enable multiple collectors to interact with one DB, or multiple DBs using master-master replication or other clustering techniques.</li>
</ol>

<h4><nop>RecordProcessor threads compete with many replication threads.</h4>

Basically the current replication system suffers from the fact that each replication table entry gets its own thread. A large replication table can therefore squeeze out the !RecordProcessor thread(s) for time slicing, and there is no easy way of prioritizing one thread relative to another. There is a workaround and a solution:

<dl>
<dt>Workaround:</dt>
<dd>Combine as many replication entries as possible into one line using SQL wild cards in the "Probename" field. This is currently only possible by editing the table in raw SQL: the GUI does not support wild cards as it uses a pull-down menu.</dd>
<dt>Solution:</dt>
<dd>Re-factor replication functionality to run in one thread only. I have this sketched out in code but have not been able to implement it fully. Basically a single replication thread will loop through replication entries and deal with them piecemeal.</dd>
</dl>

<h4>Reports to group by =resource_group= on request.</h4>

With the impending move to Gratia !SiteName &lt;=&gt; OIM =resource=, some sites will want/require some reports to be grouped by =resource_group=. This can be obtained from OIM keyed by !SiteName as opposed to having to be encoded in the Gratia data explicitly, but will need to be implemented in GUI and emailed reports.
</blockquote>

From Chris Green on 14-February 2010:
<blockquote>
<h4>Log entries may be in UTC or local time depending on service start order.</h4>
This is a consequence of switching to having all !Date objects operate in UTC. Implementable / maintainable solution not known at this time.
<h4>Email reports must be able to cope with being run against local collectors.</h4>
<h4>Automatic maintenance of Site -- Probe and FQAN/VO -- printable VO name translation via interrogation of OIM.</h4>
<h4>Gratia is not known to handle non-standard characters safely.</h4>
Neither the probes nor the collector have been verified to handle non-standard characters safely: components should be verified / updated to handle UTF-8.
</blockquote>

From Chris Green on 22-February 2010:
<blockquote>
<h4>Gratia probes can hang if collector goes down hard at a certain stage of communication.</h4>
Need to research how to add timeout into communication from probe side.
</blockquote>
---++ Attendees