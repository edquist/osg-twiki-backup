%META:TOPICINFO{author="AlainRoy" date="1263401793" format="1.1" version="1.1"}%
%META:TOPICPARENT{name="SoftwareToolsMeetings"}%
---+ OSG Software Tools Group Meeting

---++ Meeting Coordinates
| <b>Date</b> | Tuesday, January 26, 2010 |
| <b>Time</b> | 9:00am Central |
| <b>Telephone Number</b> | 510-665-5437 |
| <b>Teleconference ID<b> | 4321 |

---++ Agenda 
   * Discussion of development needed for Gratia

---++ Background

From Philippe on 8-January-2010:
<verbatim>

* Evaluate impact of new CE interfaces - CREAM and Gram 5.

Assuming access to an existing Cream and Gram 5 installation and assuming knowledge on how the Gratia probe
acquires information about the user's certificates, the evaluation should take about a day each.  A priori the
necessary upgrades are likely to requires less than 1FTE week each.

* Improve the automation of the Gratia Testing framework and work with the VDT team on including this in the nightly build/tests as far as possible.

Aaron Thor is currently working on this.  The need here will depend on the outcome on his work.

* Work with the MonaLisa team to allow Gratia probe information to be injected into ML

The most significant parts of this are deciding which information to load into ML and writing
the code to load the code in ML.    Both of which are outside of our privy (i.e. in particular
we have no current knowledge of ML).   So our involvement should be limited to consulting
on the which information to load and out to extract it from the Collector Database.   This should
fit under the currently allocated 'maintenance and support' effort.

* Allow data from single probe to be delivered to multiple collectors

This will requires some knowledge of the Gratia probe library and coding in python.
This will requires touching, or at least double checking,  a large fraction of the Gratia probe library.
I estimate that this should take 2 to 4 FTE weeks to complete and test.

* Ensure clean and clear separation of code modules between "Gratia, the job accounting system" and "Gratia, the data transport".

This will requires some knowledge of the Gratia probe library and coding in python and some knowledge of the Gratia Collector code and coding in Java.
The code is already pretty well separated so I estimate the work at only 2 to 4 FTE weeks.

* Make transfer probes work at a summary level instead of per-file

This will requires some knowledge of the way the Storage Eleement (dcache, gridftp) stored their transaction information
(and coding in python).    This should be quite straightforward and cost less then 1 FTE week.

* Add a layer to allow forwarding of summary data instead of raw data.

This will requires deep knowledge of the Collector DB Schema and Collector code (Java, hibernate).
This has potentially deep implication on duplicate record detection, origin tracking and thus raw data
integrity.   I estimate the work at anywhere between 3 and 6 FTE months.

</verbatim>

From Chris Green on 12-January-2010:
<verbatim>
1. The billinginfo table on a dCache billing DB needs a high-capacity
(64-bit or better) auto-incrementing primary key. The dCache-transfer
probe currently implements allsorts of nasty, inefficient hacks (that
have loophole corner cases) to get around the problem that selecting
from this table has no reliable concept of, "after."  The timestamp is
not reliably unique; and nothing else is monotonically increasing. I'm
not even sure that's true of the timestamp, to be honest. This gives
us great trouble keeping track of what we have sent, and what we
haven't, and allowing us to limit query sizes to avoid undue delay /
DB load. Of course, if/when this feature is implemented we need to
make the probe use it (and cope with its absence in the same old,
hacky way). What steps do we need to follow to request this feature in
dCache? Conceivably it could be done only for OSG installations of
dCache rather than by requiring dCache-DESY to implement it: one would
either an an auto-incrementing column to the billinginfo table; or
have a separate (UniqueId, transaction) lookup table filled with a
trigger. The former solution would be more efficient than the latter,
of course, but have a longer upgrade time.

2. Following a problem identified on NYSGRID-CUNY-GRID, it is evident
that those probes that scrape log files (PBS/LSF, SGE, glExec) need to
be able to cope with their logfiles being truncated / rotated in
midstream. The instant case was with the SGE probe, which simply
stores the line number of the last-processed record in a checkpoint
file. The accounting log file was truncated and the probe went silent,
waiting for the day when its length surpassed that recorded in the
checksum. This, obviously, is a data loss problem. Note I'm not
offering a simple solution to this problem -- I'm not sure there is
one.
</verbatim>

---++ Attendees
