%META:TOPICINFO{author="AlanDeSmet" date="1271782443" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="NEESFileTransfer"}%
This is, at the moment, a completely unorganized pile of my notes and to do lists. Various parts will move to the other NEESFileTransfer pages as I go.

---++ Monitoring a run

---+++ Pausing a run

Use "condor_hold -a" to pause a run. "condor_release -a" to restart it.  This may cause problems as running processes (including archive and multivol-tar) will be terminated, then restarted when you condor_release.

---+++ Instantaneous snapshot

You can see what the run is doing with "condor_q -dag"  Typical output might look like:
<verbatim>
-- Submitter: hpc114.tech.purdue.edu : <128.210.135.144:44418> : hpc114.tech.pur
due.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD
 657.0   adesmet         3/22 14:44   0+00:15:53 R  0   7.3  condor_dagman -f -
 658.0    |-G00000       3/22 14:44   0+00:15:39 R  0   7.3  condor_dagman -f -
 659.0    |-G00001       3/22 14:44   0+00:15:34 R  0   7.3  condor_dagman -f -
 663.0    |-G00000_Arch  3/22 14:56   0+00:03:48 R  0   0.0  archive_wrapper.sh
 661.0    |-G00001_Mult  3/22 14:44   0+00:15:20 R  0   0.0  multivol-tar.sh -c

5 jobs; 0 idle, 5 running, 0 held
</verbatim>

657 is the top level DAG, managing the run as a whole.  It has been runing for almost 16 minutes.

658 and 659 are the sub-DAGs, managing groups 0 (00000) and 1 (00001) respectively.  They have been running for about 15.5 minutes each.

663 is the Archive step for group 0.  It has been running for about 4 minutes.

661 is the MultivolTar step for group 1.  It has been running for about 15 minutes.

Groups will run from 00000 through the largest number.  So if you see something like this:

<verbatim>
-- Submitter: hpc114.tech.purdue.edu : <128.210.135.144:44418> : hpc114.tech.pur
due.edu
 ID      OWNER/NODENAME   SUBMITTED     RUN_TIME ST PRI SIZE CMD
 657.0   adesmet         3/22 14:44   0+00:45:53 R  0   7.3  condor_dagman -f -
 658.0    |-G00002       3/22 15:14   0+00:15:39 R  0   7.3  condor_dagman -f -
 659.0    |-G00003       3/22 15:14   0+00:15:34 R  0   7.3  condor_dagman -f -
 663.0    |-G00002_Arch  3/22 15:26   0+00:03:48 R  0   0.0  archive_wrapper.sh
 661.0    |-G00003_Mult  3/22 15:14   0+00:15:20 R  0   0.0  multivol-tar.sh -c

5 jobs; 0 idle, 5 running, 0 held
</verbatim>
We know that groups 0 and 1 are done, and we're up to groups 2 and 3.

---+++ Reviewing logs

For the progress as a whole, check the top level dag.dagman.out.  In particular, sections like the following are interesting.

---++++ Most recent status
<verbatim>
3/22 15:47:34 Number of idle job procs: 1
3/22 15:47:34 Of 408 nodes total:
3/22 15:47:34  Done     Pre   Queued    Post   Ready   Un-Ready   Failed
3/22 15:47:34   ===     ===      ===     ===     ===        ===      ===
3/22 15:47:34     2       0        2       0     403          0        1
</verbatim>

These status updates will appear at regular intervals.  2 of the groups are done, 2 are queued up to run (and are probably running).  403 are ready to run, but aren't yet because of the 2 simultaneous group throttle.  1 group failed.

---++++ History of group starts and finishes
You might run the following command to see groups starting and finishing:
<verbatim>
egrep 'Event|failed' dag.dagman.out
</verbatim>
The output will look like this:
<verbatim>
3/22 14:44:29 Event: ULOG_SUBMIT for Condor Node G00000 (658.0)
3/22 14:44:29 Event: ULOG_EXECUTE for Condor Node G00000 (658.0)
3/22 14:44:29 Event: ULOG_SUBMIT for Condor Node G00001 (659.0)
3/22 14:44:34 Event: ULOG_EXECUTE for Condor Node G00001 (659.0)
3/22 15:07:08 Event: ULOG_JOB_TERMINATED for Condor Node G00000 (658.0)
3/22 15:07:14 Event: ULOG_SUBMIT for Condor Node G00002 (665.0)
3/22 15:07:19 Event: ULOG_EXECUTE for Condor Node G00002 (665.0)
3/22 15:29:54 Event: ULOG_JOB_TERMINATED for Condor Node G00002 (665.0)
3/22 15:29:54 Node G00002 job proc (665.0) failed with status 1.
3/22 15:30:00 Event: ULOG_SUBMIT for Condor Node G00003 (669.0)
3/22 15:30:05 Event: ULOG_EXECUTE for Condor Node G00003 (669.0)
3/22 15:47:28 Event: ULOG_JOB_TERMINATED for Condor Node G00001 (659.0)
3/22 15:47:34 Event: ULOG_SUBMIT for Condor Node G00004 (672.0)
3/22 15:47:39 Event: ULOG_EXECUTE for Condor Node G00004 (672.0)
3/22 15:59:00 Event: ULOG_JOB_TERMINATED for Condor Node G00003 (669.0)
3/22 15:59:00 Node G00003 job proc (669.0) failed with status 1.
3/22 15:59:07 Event: ULOG_SUBMIT for Condor Node G00005 (674.0)
3/22 15:59:07 Event: ULOG_EXECUTE for Condor Node G00005 (674.0)
</verbatim>

A group (known as a "Node" here) will go from SUBMIT, to EXECUTE, to JOB_TERMINATED.  If a group fails, there will be something like "Node G00003 job proc (669.0) failed with status 1.".





---++ To Do
   * Reduce RAM usage.  prepare-transfer uses about 1.2 GB of memory on the 4 TB /nees data set.  Generally speaking the VM doesn't need anywhere near this much RAM, meaning the VM is either overallocated most of the time, or prepare-transfer takes a long time while it swaps in and out of memory.
      * Stop storing the entire file tree in memory. It's largely an artifact of the development history starting with using the output of "ls -lRa", but is now unnecessary. Quick and dirty testing suggests this is the largest payoff, using about 50% of the memory.
	  * Stop storing the full group information in RAM.  When we fill a group, immediately write its configuration to disk and wipe the internal file lists. Quick and dirty testing suggests this is the second largest payoff, using about 40% of the memory.
   * listtars.sh should pass output to sort. It's just more human friendly that way.  In particular, it makes the resulting Archive logs less mysterious.
   * Document how to monitor the Condor process. condor_q -dag, looking for rescue dags, etc
   * If MultivolTar, Archive, or Clean fails, the tarballs will be left there.  Given the limited space, this is a problem and will likely lead to filling the disk and the entire system crashing.  Note that "licensing claiming" on disk would solve the problem, although the entire system will deadlock when it runs low on space until someone cleans up the aborted runs.
   * Make parallel job limits a command line argument
   * Automate checksum checking
      1. Add a "Checksum" step, probably after Clean. Just run the appropriate report
      1. Add a "Validate" step, after Checksum. It needs to wait 4 hours before starting. (Make wait duration a command line argument)  Compare to report output from previous step.  Use Condor's ability to delay a job start to accomplish this.  Because of the delay, we can not use the global 2-job throttle!  A longer discussion of this problem is its own section below as "The Throttling Problem"
   * Send email on results. - Can't just have as end node in the DAG; it will never run if there are errors.
   * Have fileobjs implemented as some sort of disk-based backing store?  Perhaps SQLite?  This would reduce RAM usage (currently at about 2 GB for the 4TB data set).  If designed correctly, it could also store the resulting tarball the file ended up in, could store the size and date and be used to identify files needing incremental updates.
   * Top level unified log.  A single log that an admin can tail to see rough progress.
   * Add priorities to the top level DAG to ensure that jobs run from group 0 through group 600 in order.  It happens to work right now, but we're relying on an implementation artifact of DAGMan.  The DAGMan team has warned that the artifact may go away in the future and jobs will run in (seemingly) random order.  While harmless to the actual results, it's nice to have the groups transfer in order for human review.
   * prepare-test should write some log output (netlogger if possible)
   * prepare-test should be willing to pull binary paths from environment (if explicitly set there), or from the path.  Current hard coded paths: rm, gnutar, storage_tools_setup (setup.sh), multivol_tar, archive, condor_submit_dag
   * Documentation: Developer. Include notes on architecture 
   * Documentation: Usage.
   * Allow specifying command line arguments in a configuration file for convenience
   * Allow specifying more than one directory to prepare-transfer
   * Retrieval script
      * User specifies list of files or directories wanted. Automatically identify tar files needed.  Retrieve tar files.  Use multivol_tar to extract specific requests. Delete tar files.  Similar to prepare-transfer, it should create a DAG and should minimize the number of tar files on local disk at once.
   * Log archiving: zip/tar.gz up log files and move to a designated directory for future review
   * Add file size, date, and possibly checksum to the manifest.  Later incremental upgrades will need this information to identify changed files.  Possible bonus: Implement as a simple filename->value database (Berkley DB or SQLite, probably. SQLite allows potentially more interesting queries like.)
   * The manifest needs to be backed up, otherwise you won't be able to easily find your files in a disaster situation! It probably needs to be merged with all manifests ever generated (in the long term incremental world)
   * As validate steps finish, write entries to manifest-verified. Same as normal manifest, but confirms that it arrived.
   * Write grouping report to disk inside the workdir, as it's useful later.  Include a handy computer parsable version with exact numbers
   * New tool. That computer parsable grouping report (previous task), and condor-user-logs. Determine bytes/sec in multivol-tar and in archive
   * Have separate throttles for multivol-tar and archive.  Thus, we might allow only 1 simultaneous multivol-tar, but perhaps 2 simultaneous archives.  The tough part is ensuring the don't end up with too many tarballs on local disk.  See "The Throttling Problem" below for more on this.

---+++ Incremental
Incremental backups are currently out of the scope of the project, but for consideration:
   * Compare local disk with name, size, and date from manifest-verified for all previous runs.  If different, include in the incremental.  (This will miss files that change, but somehow keep both the same size and date. I believe this is a reasonable risk, but if it's not, we can add the adler32 checksum to the comparison. On the down side, this requires reading all files previously transferred with the same size and date, which I anticipate will be slow.)


---++ Notes

   * Names on tape are forever. Attempting to overwrite an existing file in dCache is an error.  Given this...
      * ...if we retry the Archive step, we must rewrite the archive_in file to omit successful transfers.
      * ...if we retry the entire group, we'll need to change the tar filename.  We'll probably want to append '.try1' and so forth.


---++ The Throttling Problem
Between transferring the files (the Archive step) and the (as yet unwritten) Validate step, we need to wait 4 hours.  This causes a problem.  The current architecture limits two groups to running simultaneously.  This is fine right now as the groups are blocking on disk and network.  But adding the 4 hour wait would mean that over 4 hours only 2 groups could process.  Assuming two groups running simultaneously each take 1 hour to tar and transfer (based on tests), we should be able to do 8 groups in that time.  We can't afford a 75% slowdown.

Furthermore, we should consider: currently we only allow 2 groups running at once.  This practically means that 2 instances of MultivolTar or Archive.  It's not possible to say "Run up to 2 instances of MultivolTar _and_ up to 2 instances Archive," which might be useful for maximizing the machine's throughput.

---+++ Techniques

These are techniques which might be useful.

---++++ throttling

We need to throttle:
   * Disk usage - Directly ("No more than 50 GiB") or indirectly ("No more than 2 groups in transit at once").  This is a hard limit. Failure to observe the limit will cause failures.
   * Disk bandwidth - Doing more than, say, 2 tars at once is likely worse in speed.
   * Network bandwith - Doing more than a handful of srmcp transfers at once is likely worse in speed.

We can throttle in several ways
   * *DAGMAN_MAX_JOBS_SUBMITTED* - Current, March 19, 2010, solution. Simple, but throttle applies to all jobs in the DAG. Problematic if we want different throttles for the MultivolTar, Archive, and Validate steps.  Also doesn't cross multiple simultaneous backups (but will that ever happen?)
   * *DAGMan categories* - Pretty simple. Necessitates different nodes (jobs or subdags) for different categories.  If nodes are broken up that way, it's not possible to have one node cause an earlier one to fail without adding a parent DAG per tree of nodes.  One parent DAG per tree of nodes breaks the categories into different dagmans, at which point the throttles stop working. Doesn't cross multiple simultaneous backups (but will that ever happen?)
   * *vanilla jobs* - run a startd (and collector and negotiator) on the node. Have multiple slots, each assigned to a dedicated task.
      * Have 2 slots with Requirement=jobtype=="datatransfer".  Mark the data transfer subdags as vanilla jobs with jobtype="datatransfer".  On the down side, we lose access to DAGMan's automatic handling of rescue subdags; we'd need to re-write the functionality from scratch. (We lose the functionality because the subdag support in DAGMan forces a rewrite of the dag.condor.sub file. There doesn't appear to be a way to modify it.)  It's probably not too hard, but it is more work.
         * If we go this route, we might consider just making the subdags simple scripts. We lose a small bit in live information from condor_q, and we lose the (easyish) ability to automatically retry just MultivolTar or just Archive. But is it worth our time trying retries on that level?
      * We might do something clever with seperate slots for MultivolTar and Archive, but we can't have too many MultivolTar steps run until matching Clean steps have run to free up disk space. This might work well with the "license claiming" technique below.  The "license claiming" might be implemented by having the individual MultivolTar submit files specifiy "+DiskNeeded=10000000", having the MultivoLTar slots say "START = TARGET.DiskNeeded < DiskAvaialble"  and having an aggressive STARTD_CRON script set DiskAvailable by calculating: free_disk_space *.9 - DiskNeeded for all Running MultivolTar nodes.  This may create race conditions, but is appealingly simple.
   * *License claiming* - A given node can be queued up and free to start whenever it wants, but it's actually a shell script.  The first thing the script does it check for permission to start, in the form of a "license," similar to license management systems.  This is a fair amount of work.  For Archive, the license might be a simple counter limiting the simultaneous runs.  For MultivolTar, the license might be "is there enough free space for my anticipated disk usage + 20%"?  A license must be atomically testable and claimable; if a MultivolTar needs 13GiB, it needs to claim that space immediately, even if it doesn't actually use it. The license for the disk space might be released when Clean runs, or it might be immediately after MultivolTar finishes (we can check real disk space free or used and subtract outstanding claims to see what's left.) This is potentially complex, as it adds a bunch of potential race conditions and deadlocks.  On the up side, we can throttle on exactly what we care about (disk space).
      * DAGMan can potentially help here.  The script to claim the license could be a PRE script that blocks. Then, use DAGMAN_MAX_JOBS_IDLE to avoid having too many blocked jobs running at once.
   * *Custom scheduling system* - Write a custom scheduling system.  A _lot_ of work, and reimplements functionality already (partially) present in DAGMan.
   * Perhaps multivol-tar should block and immediately handle the srmcp and clean. steps.  This ensures that only 10GB are used by a given group at once. multivol-tar would need to either run the srmcp and clean steps directory, or would submit condor jobs and wait for them.  Open question: recovery if restarted.

---+++ Solutions

We have solutions, but none are ideal:

---++++ Categories in the top level DAG

The top level DAG current exists only to throttle groups to 2 running simultaneously.  We could put more smarts here.  Instead of one node per group, there would be two.  The first would contain the existing subdag.  The second would contain a single job (or possibly a subdag): Validate.  These two nodes would be marked as "CATEGORY G00001_datatransfer datatransfer" and "CATEGORY G00001_validate validate" groups.  We would throttle with "MAXJOBS datatransfer 2" and "MAXJOBS validate 10".

Pros:
   * Simple
   * Relatively fast to implement

Cons:
   * DAGMan can't help with recovery if Validate fails.  There is no way to say "go back to G00001_datatransfer and change it from "succeeded" to "failed."  If you resubmit the top level rescue DAG, the system will rerun the Validate steps, but not the paired datatransfer subdags.
      * We can potentially work around by rewriting things, but this is complicated.
   * If multiple runs are started at once, each one has a limit of 2 simultaneous jobs, which is bad.
      * It probably doesn't matter. Don't run multiple jobs simultaneously.

---++++ Custom queue manager/dispatcher

When the existing subdag finishes