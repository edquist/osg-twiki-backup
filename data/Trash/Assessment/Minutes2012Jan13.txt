%META:TOPICINFO{author="JamesWeichel" date="1326475045" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! <nop>%TOPIC%
%TOC%

---++ Coordinates
   * Friday, January 13, 2012, *10:00 am Central*
   * Phone: (570) 310 0130, Access code: 735188; Dial *6 to mute/un-mute.

---++ Attending
   * Dan, Rob Q, Alain, Rob G, Brian, Jim, Ruth, Michael, Lothar


---++ Production Assessment Analysis
   * Production is dynamic - hard to pin down what to monitor
      * Things will be quiet - but then there are fires, eg: GUMS compatibility with VOMS - an issue shows up.  Tricky part is how this comes into a metric.

Starting from a high level
   * Projects/services run by Production that could use metrics
      * Operations
      * HTPC (whole node scheduling)
         * Have a test to see what is available
         * Can track usage when current software is deployed
         * We do not have a target or understanding of the actual need/demand
      * Campus Grids (what are the metrics?)
         * Number of campuses that have deployed (or in production)
         * How available are these? - probe sites to find what is available at each site?
         * Number users of campus grids (This is a good metric that we really care about, but tougher to measure.)
         * Current status: A few very successful campus grids with production use (e.g. Nebraska and GLOW)
         * No large scale use at the moment.
      * Stakeholder requests (capture some real time aspect?)
      * Deployments 
         * How many or percent of sites have updated to current release (need good way to describe the current status since it is important)

   * Production Statistics
      * WLCG metrics (resource provided, reliability, availability)
      * Other VOs typically do not have their needs/goals as well specified so it is more difficult to create a metric and know what is good
      * Opportunistic availability (really tough) - probe sites to find what is available at each site?
         * Test jobs can be used to probe, measure availability. - there is a system for CMS.
         * Dan - LIGO has been used in some contexts as a measure for this.
      * Opportunistic use
         * There is a sizable effort involved. Eg the monthly report on WLCG statistics. Brian - For WLCG, it is mostly automated with a person just reviewing the result for sanity.  Now that this infrastructure is in place, it would be much less work to apply it to other VOs.  Note the WLCG metrics result from a clear definition of the measurements and goals which are lacking in most other areas.
      * Others?
   * Comments
   * Lothar: experience has been distinguishing resources are available, but then are they being used. We want to be successful in offering opportunistic resources, but then are they being used. 
   * Lothar: The tricky part is we're exposing metrics we might not be able to get very easily.  However this very useful for OSG. Not always things sites can do about it, but provides means to set expectations.Factorizing availability versus usage.  Site availability versus Usage metrics.

---++ AOB
   * New balanced scorecard template: [[%ATTACHURL%/2012.01.13-Assessment.xls][2012.01.13-Assessment.xls]]
   * 
   * 


---++ References
   * 
   * 
   * 


%BR%
-- Main.RobGardner - 03 Jan 2012

%META:FILEATTACHMENT{name="2012.01.13-Assessment.xls" attachment="2012.01.13-Assessment.xls" attr="" comment="" date="1326469790" path="2012.01.13-Assessment.xls" size="44032" stream="2012.01.13-Assessment.xls" tmpFilename="/usr/tmp/CGItemp54922" user="RobGardner" version="1"}%
