%META:TOPICINFO{author="RobGardner" date="1326473709" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! <nop>%TOPIC%
%TOC%

---++ Coordinates
   * Friday, January 13, 2012, *10:00 am Central*
   * Phone: (570) 310 0130, Access code: 735188; Dial *6 to mute/un-mute.

---++ Attending
   * Dan, Rob Q, Alain, Rob G, Brian, Jim, Ruth, Michael, Lothar


<del>---++ Production Assessment Analysis
</del><ins>---++ Production Assessment Analysis (Dan)
</ins><del>   * Production is dynamic - hard to pin down what to monitor
</del><ins>   * Today would be mostly discussion
</ins><del>   * How do the real time aspect into a metric (e.g. responsiveness to compatibility issue with GUMS)
</del><ins>   * Starting from a high level
</ins><del>   * Projects/services run by Production
</del><ins>      * Operations, production itself, and campus grids
</ins><del>      * HTPC
</del><ins>      * Things will be quiet - but then there are fires, eg: GUMS compatibility with VOMS - an issue show up.  Tricky part is how this comes into a metric.
</ins><del>      * Campus Grids (what are the metrics?)
</del><ins>      * HTPC (whole node scheduling) and Campus Grids are being run out of production - important projects that could use metrics.
</ins><del>         * Number of campuses that have deployed (or in production)
</del><ins>   * Challenge is to identify what are the meaningful metrics for CG?
</ins><del>         * How available are these? - probe sites to find what is available at each site?
</del><ins>      * Could come up with simple metrics, like # deployments
</ins><del>         * Number users of campus grids (good metric but tougher to measure)
</del><ins>      * But what about users, which is what we really care about
</ins><del>         * Current status: Other than Nebraska, very little production use
</del><ins>      * No large scale use at the moment.
</ins><del>      * Stakeholder requests (capture some real time aspect?)
</del><ins>   * Lothar: experience has been distinguishing resources are available, but then are they being used. We want to be successful in offering opportunistic resources, but then are they being used. Factorizing availability versus usage.  Site availability versus Usage metrics.
</ins><del>   * Production Statistics
</del><ins>   * Test jobs can be used to probe, measure availability. - there is a system for CMS.
</ins><del>      * WLCG metrics (resource provided, reliability, availability)
</del><ins>   * Dan - LIGO has been used in some contexts as a measure for this.
</ins><del>      * Other VOs typically do not have their needs/goals as well specified so it is more difficult to create a metric and know what is good
</del><ins>   * Lothar: The tricky part is we're exposing metrics we might not be able to get very easily.  However this very useful for OSG. Not always things sites can do about it, but provides means to set expectations.
</ins><del>      * Opportunistic availability (really tough)  - probe sites to find what is available at each site?
</del><ins>   * Communication is important however. 
</ins><del>      * Opportunistic use
</del><ins>   * There is a sizable effort involved. Eg the monthly report on WLCG statistics.
</ins><del>      * Others?
</del><ins>   * Brian - much of this is automated now.  We could reuse some of this infrastructure.
</ins>   * 
   * 

---++ AOB
   * New balanced scorecard template: [[%ATTACHURL%/2012.01.13-Assessment.xls][2012.01.13-Assessment.xls]]
   * 
   * 


---++ References
   * 
   * 
   * 


%BR%
-- Main.RobGardner - 03 Jan 2012

%META:FILEATTACHMENT{name="2012.01.13-Assessment.xls" attachment="2012.01.13-Assessment.xls" attr="" comment="" date="1326469790" path="2012.01.13-Assessment.xls" size="44032" stream="2012.01.13-Assessment.xls" tmpFilename="/usr/tmp/CGItemp54922" user="RobGardner" version="1"}%
