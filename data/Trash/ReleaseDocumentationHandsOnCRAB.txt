%META:TOPICINFO{author="EricVaandering" date="1280948710" format="1.1" reprev="1.6" version="1.6"}%
%META:TOPICPARENT{name="SiteAdminsWorkshopTutorialsAug10"}%
---+!! *<nop>%SPACEOUT{%TOPIC%}%*
%DOC_STATUS_TABLE%
%TOC{depth="3"}%

---++ Introduction

This hands on will introduce you the basic Grid tools used in CMS for user analysis. CRAB (CMS Remote Analysis Builder) is the main tool for distributed user analysis. Please run and complete each of these exercises. At the end of this exercise, you should be able to install CRAB on a properly prepared machine and submit simple jobs to run on the GRID. For more complex uses of CRAB akin to the use cases for physicists, please see the resources under "More information."

---++ Requirements

To install CRAB on your own machine, you should have installed the gLite 3.2 client UI, available from http://glite.web.cern.ch/glite/packages/R3.2/sl5_x86_64/deployment/glite-UI/glite-UI.asp I have generally found that the tarball UI is easier, or at least more foolproof, to install than the RPM based UI. Some uses of CRAB may work with the OSG client only, but this is not a recommended or supported set up. This page: http://hep-t3.physics.umd.edu/HowToForAdmins/crab.html by Malina Kirn provides very good instructions on installing the gLite client (and CRAB).

To submit jobs to the Grid, you need CRAB installed, at least one release of CMSSW installed, and a Grid certificate with the CMS VOMS membership. This hands on covers CRAB installation. CMSSW installation is covered by Patrick's section, and certificate issues should already have been dealt with in the requirements before the workshop started.

---++ More Information

   * Official CRAB documentation for users: https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCrab
   * FAQ and troubleshooting page: https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCrabFaq
   * CRAB Feedback Hypernews: https://hypernews.cern.ch/HyperNews/CMS/get/crabFeedback.html (reporting problems not solved with FAQ)
   * gLite UI and CRAB installation: http://hep-t3.physics.umd.edu/HowToForAdmins/crab.html

---++ Install CRAB (optional)

You may or may not want to install !CRAB locally for your users. It does complicate your install as you also need to install gLite-UI, but then your users can submit jobs from your local cluster. Alternatively, users can log into FNAL or CERN where CRAB is already installed.

If you do choose to do this, the CRAB install itself is quite simple:
   1 Point your browser to http://cmsdoc.cern.ch/cms/ccs/wm/scripts/Crab/
   1 Download the latest release that is not a pre-release (2.7.3_p(atch)1 at this time)
   1 =tar xvfz CRAB_XYZ.tgz=
   1 =cd CRAB_XYZ=
   1 =./configure=
That's it. CRAB is installed and should be ready to submit jobs

---++ Verify your grid certificate is OK

After you've installed your grid certificate, you need to verify it has all the information needed.

Login to *cmslpc-sl5.fnal.gov* or another machine with CRAB and gLite installed. First, set up the gLite UI. On *cmslpc-sl5* this is done by: 
<pre class="command">
source /uscmst1/prod/grid/gLite_SL5_CRAB_27x.csh
</pre>
Then intialize your proxy:
<pre class="command">
voms-proxy-init -voms cms
</pre>

Next run the following command:

<pre class="command">
voms-proxy-info -all | grep -Ei "role|subject"
</pre>
The response should look like this:
<pre class="output">
subject   : /DC=org/DC=doegrids/OU=People/CN=Eric Vaandering 123456/CN=proxy
subject   : /DC=org/DC=doegrids/OU=People/CN=Eric Vaandering 123456
attribute : /cms/Role=NULL/Capability=NULL
attribute : /cms/uscms/Role=NULL/Capability=NULL
</pre>
If you do not have the first attribute line listed above, you have not completed the VO registration above and you must complete it before continuing.

---++ Submit jobs with CRAB

This lengthy exercise is adapted from the latest !CRAB tutorial which is always in the CMS !WorkBook (https://twiki.cern.ch/twiki/bin/view/CMS/WorkBook) under !WorkBookCRABTutorial (https://twiki.cern.ch/twiki/bin/view/CMS/WorkBookCRABTutorial). This will be a better resource for you after the workshop has ended. The workbook pertains to running !CRAB at CERN and there are just a couple of minor differences regarding software locations.

NOTICE: *Always use the latest production CRAB version*.

#PreRequisites
---+++ Prerequisites to run the tutorial. 

Just a reminder of the prerequisites before starting. You must
   * have a valid Grid certificate  
   * be registered in the CMS virtual organization
   * be registered in !SiteDB     

---+++ Recipe for the tutorial
For this tutorial we will use:

   * CMSSW_3_7_0_patch4 (see Patrick's tutorial if you need to install it)
    
   * The latest version of CRAB

The example is written to use the csh shell family. If you want to use sh, replace csh with sh.

---+++ Setup the gLite environment
Login to *cmslpc-sl5.fnal.gov* or another machine with CRAB and gLite installed.

Source the gLite UI script. At FNAL, you do this by
<verbatim class="command">
source /uscmst1/prod/grid/gLite_SL5_CRAB_27x.csh
</verbatim>

---+++ Setup analysis code

If you already have a CMSSW_3_7_0_patch4 area installed, great. If not, you must create one:
<verbatim class="command">
scram project CMSSW CMSSW_3_7_0_patch4 
</verbatim>

cd into the CMSSW src directory and setup the CMS environment before proceeding. The order in which gLite, !CRAB and CMSSW are setup is important.

<verbatim class="command">
cd CMSSW_3_7_0_patch4/src
cmsenv 
</verbatim>

wget something
scram b

---+++ !CRAB setup

To setup and use !CRAB, source the script =crab.(c)sh= located in =/uscmst1/prod/grid/CRAB=, which always points to the latest version of !CRAB.  (Note: At CERN and other sites, !CRAB is located in a different directory so the command will vary slightly.)

<verbatim class="command">
source /uscmst1/prod/grid/CRAB/crab.csh
</verbatim>

Warning: in order to have the correct environment, the setup order must always be 
   * setup Grid UI env 
   * setup CMSSW software (=cmsenv=)
   * source the !CRAB setup script

---+++ Check your CMSSW code by running locally

---+++ Locate the dataset and prepare !CRAB submission

In this example we are going to the dataset =/JetMETTau/Run2010A-Jun14thReReco_v2/RECO=. Generally users will find the data you want with the DBS Discovery page, but this is beyond the scope of this tutorial.

---+++ !CRAB configuration

Modify the !CRAB configuration file =crab.cfg= according to your needs: a fully documented template is available at =$CRABPATH/full_crab.cfg=, a template with essential parameters is available at =$CRABPATH/crab.cfg= . 

*Copy crab.cfg into your local area*. 

<pre class="command">
cp $CRABPATH/crab.cfg .
</pre>

For guidance, see the [[https://cmsweb.cern.ch/crabconf/crab-v2.6.4.html#configuration_parameters][list and description of configuration parameters]]. For this tutorial, the only relevant sections of the file are  =[CRAB]=, =[CMSSW]= and =[USER]= .

Edit =crab.cfg= to make it look like this (the order is not important, so it is just a few changes).

<pre class="cfg">
[CMSSW]
total_number_of_lumis = -1
lumis_per_job = 200
pset = singlepionsData_cfg.py
datasetpath = /JetMETTau/Run2010A-Jun14thReReco_v2/RECO
get_edm_output = 1

[USER]
return_data=1

[CRAB]
scheduler              = glidein
use_server = 1
jobtype=cmssw
</pre>

#MainConfiguration
---++++ Configuration parameters
Here is a short description of the most important parameters you may specify in your crab.cfg:
   * *pset*: the CMSSW configuration file name;
   * *output_file*: the output file name produced by your pset; if the output is defined in a !TFileService, the file is automatically handled by !CRAB, and there is no need to specify it explicitly;
   * *datasetpath*: the full dataset name you want to analyze;
   * *total_number_of_lumis*, *number_of_jobs*, *lumis_per_job*: you need to specify 2 of these parameters:
      * A lumi section is 23 seconds of CMS data. The number of lumi sections processed will either be *total_number_of_lumis* or *number_of_jobs*x*lumis_per_job*. The number of jobs and lumis processed by each job will only be approximately what you ask for.
   * *return_data*: this can be 0 or 1; if it is 1 you will retrieve your output files to your local working area;
   * *copy_data*: this can be 0 or 1; if it is 1 you will copy your output files to a remote Storage Element;
   * *publish_data*: this can be 0 or 1; if it is 1 you can publish your produced data to a local !DBS; 
   * *use_server*: send your jobs to a CRAB server. The server is picked automatically
   * *scheduler*: the name of the scheduler you want to use;


#SetRunCrab
---+++Run Crab
Once your =crab.cfg= is ready and the underlying environment is set up, you can start running !CRAB.
!CRAB supports command line help which can be useful for the first time. You can get it via:
<pre class="command">
crab -h
</pre>
in particular there is a  section HOW TO RUN !CRAB FOR THE IMPATIENT USER where the base commands are reported.

#JobCreation
---++++ Job Creation
The job creation checks the availability of the selected dataset and prepares *all* the jobs for submission according to the selected job splitting specified in the crab.cfg

The creation process creates a !CRAB project directory (default: crab_0_date_time) in the current working directory, where the related crab configuration file is cached for further usage,  avoiding  interference with other (already created) projects

!CRAB allows the user to chose the project name, so that it can be used later to distinguish multiple !CRAB projects in the same directory.

<pre class="command">
crab -create
</pre>

may ask for proxy/myproxy passwords and should produce output like this:

<verbatim style="font-size: 13px" class="output">
[ewv@cmslpc16 test]$ rm -rf crab_server/ ; crab -create -cfg crab_server.cfg
crab:  Version 2.7.3 running on Wed Aug  4 11:22:56 2010 CST (16:22:56 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (use_server)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Contacting Data Discovery Services ...
crab:  Accessing DBS at: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
crab:  Requested dataset: /JetMETTau/Run2010A-Jun14thReReco_v2/RECO has 5228398 events in 1 blocks.

crab:  143 jobs created to run on 53403 lumis
crab:  Adding IsolatedTracksTreeData.root (from TFileService) to list of output files
crab:  Creating 143 jobs, please wait...
crab:  Total of 143 jobs created.

Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

#JobSubmission
---++++ Job Submission

With the submission command it's possible to specify a combination of jobs and job-ranges separated by comma (e.g.: =1,2,3-4), the default is all.
To submit all jobs of  the last created project with the default name, execute the following command:
<verbatim class="command">
crab -submit 
</verbatim>
to submit a specific project:
<verbatim class="command">
crab -submit -c  <dir name>
</verbatim>

which should produce output like this:

<verbatim  class="output" style="font-size: 13px">
[ewv@cmslpc16 test]$ crab -submit -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 11:24:27 2010 CST (16:24:27 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Registering credential to the server : glidein-2.t2.ucsd.edu
crab:  Credential successfully delegated to the server.

crab:  Starting sending the project to the storage glidein-2.t2.ucsd.edu...
crab:  Task crab_server successfully submitted to server glidein-2.t2.ucsd.edu

crab:  Total of 143 jobs submitted
Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

#JobStatusCheck
---++++ Job Status Check
Check the status of the jobs in the latest !CRAB project with the following command:
<verbatim class="command">
crab -status 
</verbatim>
to check a specific project:
<verbatim class="command">
crab -status -c  <dir name>
</verbatim>

which should produce output like this:
<verbatim  style="font-size: 13px" class="output">
[ewv@cmslpc16 test]$ crab -status -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 11:34:50 2010 CST (16:34:50 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:
ID     STATUS             E_HOST                               EXE_EXIT_CODE JOB_EXIT_STATUS
--------------------------------------------------------------------------------------------
1      Running            red.unl.edu
2      Running            egeece02.ifca.es
3      Running            sbgce1.in2p3.fr
4      Running            egeece01.ifca.es
5      Running            egeece02.ifca.es
[... edited for brevity ...]
143    Running            osg-gw-4.t2.ucsd.edu

crab:   143 Total Jobs
 >>>>>>>>> 143 Jobs Running
        List of jobs Running: 1-143

crab:  You can also follow the status of this task on :
        CMS Dashboard: http://dashb-cms-job-task.cern.ch/taskmon.html#task=ewv_crab_server_3t5rh6
        Server page: http://glidein-2.t2.ucsd.edu:8888/logginfo
        Your task name is: ewv_crab_server_3t5rh6

Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

eventually jobs will start to finish and you will see them all listed as "Done" like this:

<verbatim  style="font-size: 13px" class="output">
...
73     Done               osg-gw-4.t2.ucsd.edu                 0             0
74     Done               red.unl.edu                          0             0
75     Done               sbgce1.in2p3.fr                      0             0
76     Done               sbgce1.in2p3.fr                       
77     Done               sbgce1.in2p3.fr                       
78     Done               sbgce1.in2p3.fr                      0             0
79     Done               sbgce1.in2p3.fr                      0             0
80     Running            sbgce1.in2p3.fr
-------------------------------------------------------------------------------------------------------
81     Done               sbgce1.in2p3.fr                      0             0
82     Done               osg-gw-4.t2.ucsd.edu                 0             0
83     Done               osg-gw-4.t2.ucsd.edu                 0             0
...
</verbatim>

Note that some jobs are Done with exit code 0 and others have no exit code. The latter are not quite ready to be retrieved. Wait for them to show exit codes. As soon as you've gotten a few jobs to finish with exit code 0, you can go to the "Job Output Retrieval" step.

%BLUE%Also, you can sometimes (depending on the server you used) monitor the status of your job on the CRAB server with your web browser. Go to the "Server page" link with your web browser and paste the task name into the text field you see and press "Show".%ENDCOLOR%

---++++ Job Resubmission

Sometimes grid jobs will not complete successfully and need to be resubmitted. They will either be listed by =crab -status= as Aborted or Done with a non-zero error code. In this case, you can resubmit the jobs. If jobs 4, 6, 7, 8 failed, you can resubmit those four jobs with this command:

<pre class="command">
crab -resubmit 4,6-8
</pre>
or
<pre class="command">
crab -forceResubmit 4,6-8
</pre>
if CRAB refuses to resubmit the jobs.

#JobOutputRetrieval
---++++ Job Output Retrieval
For the jobs which are in the "Done" state you need to retrieve the files produced by the jobs. The following command retrieves the files of all "Done" jobs of the last created !CRAB project:
<pre class="command">
crab -getoutput 
</pre>
to get the output of a specific project:
<pre class="command">
crab -getoutput -c  <dir name>
</pre>

the job results will be copied in the =res= subdirectory of your crab project:
<pre  style="font-size: 13px" class="output">

[ewv@cmslpc16 test]$ crab -getoutput -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 13:22:19 2010 CST (18:22:19 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Only 142 jobs will be retrieved  from 143 requested.
        (for details: crab -status)
crab:  Starting retrieving output from server glidein-2.t2.ucsd.edu...
crab:  Results of Jobs # 1 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 2 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 3 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 4 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
...
</pre>

---+++ Analysis of output

If you are doing a full blown analysis, you want to, of course, wait for all the results to be retrieved. We're just going to look at one histogram, we can start as soon as a few files are retrieved. With this particular analysis, a lot of the histograms are empty, so we want to find the largest file to look at:

<verbatim>
cd crab_0_[blah]/res/
ls -lSr *.root
root IsolatedTracksTreeData_xyz_1_ABC.root
</verbatim>

If you've never used root before, don't worry. What we're going to do is very simple to just check the output. At the prompt, type: =new TBrowser();= and you should see a graphical window. In the right hand pane, click on "ROOT Files" -> the file name -> "IsoTracks" -> "Tracks" and then click on a histogram to view it. With luck, it will not be empty.

Were you doing an analysis, you would combine all the histogram files into a single file and use that in your analysis.



---++ Check output

---++ Presentation

To follow hands on

---++ References

---++ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = JamesWeichel

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = General

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = RobertEngel
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = IwonaSakrejda
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %YES%
############################################################################################################
-->
-- Main.EricVaandering - 03 Feb 2010
