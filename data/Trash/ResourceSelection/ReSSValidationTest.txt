%META:TOPICINFO{author="TanyaLevshina" date="1160149457" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="WebHome"}%
---++ ReSSValidationTest 
G. Garzoglio, T.Levshina, R. Matur, P. Mhashilkar, S. Timm, J. Weigand
---+++ Introduction

The Resource Selection Service (ReSS) is designed to provide a framework to reference resource attributes from the description of a job and, to select a resource according to user provided requirements and ranking expressions. 
It consists of two components: 
   * Central resource collection service that includes information gathering service (IG) which interfaces to the distributed information publishing services, and the match making service of Condor-G
   * Resource publication service (CEMonitor) that  that gathers local resource information by interfacing site's  Generic Information Providers  and pushes the locally collected information to IG

This document describes the validation test results that verify ReSS stability and scalability under stress condition.

---+++ Validation Tasks Results
   * Gather feedback from administrators on the CEMon deployment procedure. Improve installation procedure and integrate with VDT, possibly by the OSG v0.6.0 deployment.

We have installed ITB CE -0.5.0 (VDT 1.3.11a) version on our testbed and verify that CEMonitor and LCG GIP installation procedures are pretty straightforward and easy to use. We have provided an additional document that describes the changes necessary for CEMonitor to push information to IG as well as means to verify that CEMonitor is properly installed and check that the information reported by GIP is correct and is propagated to Information Gatherer (IG). Several system administrators from Fermilab and from other institutions have successfully installed CEMonitor on their hosts.

   * Validate that classads coming from different sites (i.e. different GIP configurations) are well-formed and usable for common resource selection criteria.

We have verified that IG can handle the classads coming from different sites. 

   * Study the scalability of the central services handling O(100) classad from the ~13 ITB test sites. In detail, investigate how IG handles O(10) CEMon registrations and O(100) classad processing and transferring to the condor_collector.

Currently we are using several nodes(4) with multiple installations of CEMon instances(4 per node)  to emulate these requirements. The update period for each instance of CEMonitor is 1 minute.  The IG seems to be able to handle this load without any problems.

   * Stress test study of the IG. Simulate the load of the production environment by increasing 10 times the frequency of classad publication by the O(10) CEMon's. This requires a change in the configuration of all ITB CEMon, thus the collaboration of the ITB system administrators.

We have done this test locally by using multiple CEMonitor installations per one node.   The IG seems to be able to handle this load without any problem.

   * Stress test the match making infrastructure submitting O(1) job/sec for 1 hour. In particular, evaluate the efficiency of the condor_negotiator, to match elements of an attribute list. We estimated a load of around 400 resource classads to simulate this test.

It is a common requirement from the users that, their jobs be matched to a site that contains a particular string in one of the resource attributes. We identified Condor's native function, stringlistimember() as an appropriate function to achieve the desired results for match making.

We configured the testbed with:<br> 
   1. 16 CEMons (4 nodes running 4 CEMons each) 
   1. 4 CEMons (4 Nodes running 1 CEMon each)
   1. In addition to the classads sent through IG, we pumped in dummy classads using condor_advertise to simulate 426 resources.

The results for the stress test were:<br>
Jobs Submitted = 1 job/sec for hour. <br>
Total Jobs Submitted = 3600<br>
First Job Matched = 9/8/2006 16:33:00<br>
Last Job Matched = 9/9/2006 02:05:53<br>
Resources Satisfying Jobs = 2 (1800 jobs per resource)<br>
Total Number Of Resources = 426<br>
Max Jobs Matched Per Negotiation Cycle Per Resource = 10<br>
Total Jobs Matched In One Negotiation Cycle = 20<br>
Longest Negotiation Cycle: 2 sec<br>
Shortest Negotiation Cycle: 0 sec<br>
Average Negotiation Cycle: 0.772222222222 sec<br>

The results from the test satisfied the test validation requirements. We did not notice any crashes or slow down in the negotiation cycle from condor side.

   * Validate that the machine load remains manageable when GIP are called from multiple clients (CEMon, Ldap, ...) <br>

On the production node (fngp-osg.fnal.gov) where we support 39 VOs and have SRM configured as part of GIP) each run  of glite-ce-info script, takes about 16.1 sec of cpu on average. (17-18 s of clock time). During 13 days of run the cpu utilizationis  is about one day. GIP information  is collected  every 5 minutes.

On different production node (fcdfosg1.fnal.gov) without SRM but with the same number of  VOs it takes about  11 cpu seconds per each GIP invocation. 18 hours of cpu is used by CEMonitor during 13 days.
It seems that osg-info-dynamic-condor perl script is a very cpu intensive process because it  requires multiple executions of  condor_q, condor_config_val, and condor_status commands per one invocation.




-- Main.TanyaLevshina - 06 Oct 2006
