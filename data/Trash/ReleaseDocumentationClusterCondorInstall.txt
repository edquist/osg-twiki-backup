%META:TOPICINFO{author="MarcoMambelli" date="1256450852" format="1.1" version="1.5"}%
%META:TOPICPARENT{name="GridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction
We will use the stable release of Condor (7.2.4). There are different possibilities to install Condor. Here is presented a shared installation, that is very convenient for medium/small clusters. Condor will be installed using the tar file distribution on the node which will serve as the Condor *master*, and the installation will be shared to all other nodes.

The share directory will include:
   * the Condor installation
   * the main =condor_config= file
   * a local condor_config file for each host, =condor_config.${HOST}= (easy to edit centrally)

The condor home directory will contain a link to condor config (it is a default location)

Each host has a local Condor directory =/scratch/condor= where condor is storing log files, creating spool directories for the jobs, etc. 

The only host that requires write access to =/nfs/condor= is the host used to install Condor and to edit the configuration (e.g. =tg1-ce=). All other hosts can mount that directory as read-only

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.2/3_2Installation.html


---++ Installation
It is divided in two sections:
   * installation on the head node
   * installation on the other nodes


---+++Head node installation
Condor is first installed on the host which will be the master, in this tutorial it will be *tg1-ce*, the same host as the compute element.  

Here are the steps:
   * Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl][download page]] and choose the Condor version (e.g. 2.7.4, current production version)
   * Download the correct tar file distribution for you platform (e.g. condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz, condor 7.2.4 for RHEL 5 i386 dynamically linked). Provide name and email, agree to the license, ... save the file.
   * create local Condor directories <pre>
mkdir /scratch
mkdir /scratch/condor
</pre>
   * control that the user condor exists with a shared home directory (=/home/condor=) and that =/opt/condor= is a link to =/nfs/condor/condor=
   * extract in /tmp/condor-src and install it <pre>
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xvzf ~/condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz 
mkdir /nfs/condor/condor-7.2.4
ln -s /nfs/condor/condor-7.2.4 /nfs/condor/condor
# /opt/condor should exist and be lined to /nfs/condor/condor
# Install condor in /opt/condor, local dir /scratch/condor, shared ~condor
./condor_install --prefix=/opt/condor --local-dir=/scratch/condor --type=manager
</pre>   
   * Condor will place the local configuration file in the local-dir. Here are some configuration changes to achieve the setup described in the introduction with all the local configurations in the shared directory: <pre>
vi /opt/condor/etc/condor_config      # point to the right, moved, local config (/opt/condor/etc/condor_config.(HOSTNAME); set domain to yourdomain.org)
cp /scratch/condor/condor_config.local /opt/condor/etc/
cp /opt/condor/etc/condor_config.local /opt/condor/etc/condor_config.tg1-ce
ln -s /opt/condor/etc/condor_config ~condor/condor_config
</pre>
   * Test start <pre>
/opt/condor/sbin/condor_master
/opt/condor/sbin/condor_master_off
</pre>
   * Configure the cluster installation (local setups for all the hosts). This setup facilitates central management. All hosts have to have their own condor_config.${HOST} but all worker nodes are configured the same way, so their files are a link to a generic worker node configuration (=condor_config.tg1-cXX=): <pre>
cd /opt/condor/etc/
cp condor_config.tg1-ce condor_config.tg1-cXXX
ln -s condor_config.tg1-cXXX condor_config.tg1-c001
ln -s condor_config.tg1-cXXX condor_config.tg1-c002
ln -s condor_config.tg1-cXXX condor_config.tg1-c003
cp condor_config.gs1-ce condor_config.gs1-cli
vi condor_config.gs1-cli                   # enable  master, schedd
vi condor_config.gs1-cXXX              # enable  master, startd
ls /scratch/condor/
cd /scratch/condor/
mv condor_config.local was.condor_config.local 
</pre>
   * Enable automatic startup at boot:<pre>
vi /opt/condor/etc/examples/condor.boot         # to set the correct path of condor_master, /opt/condor/sbin/condor_master
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
for in in 2 3 5; do ln -s /etc/init.d/condor /etc/rc$i.d/S99condor 
</pre>

---+++ Configuration of the other nodes
This should be executed on all other nodes where Condor should be running (worker nodes, submit host)
   * Create the local Condor directories in =/scratch/condor= (making sure that =execute= directory has the right permissions =a+rwx +t=) <pre>
mkdir /scratch
mkdir /scratch/condor
mkdir /scratch/condor/execute
mkdir /scratch/condor/log
mkdir /scratch/condor/spool
chmod a+rwx /scratch/condor/execute
chmod +t /scratch/condor/execute
</pre>
   * at the end you should see: <pre>
ls -l /scratch/condor/
total 12
drwxrwxrwt 2 condor root 4096 Oct 15 04:07 execute
drwxr-xr-x 2 condor root 4096 Oct 15 04:15 log
drwxr-xr-x 2 condor root 4096 Oct 15 04:02 spool
</pre>
   * Automatic startup<pre>
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
for in in 2 3 5; do ln -s /etc/init.d/condor /etc/rc$i.d/S99condor 
</pre>

---++ Upgrades
The Condor installation is structured to facilitate upgrades with minimal efforts, whenever needed in the future.
The Condor directory on the systems is =/opt/condor=. This is a link to the exported =/nfs/condor/condor= that is a link to the real installation directory including the version number in the name (e.g. =/nfs/condor/condor-2.7.4=).
Only one version will be used at the time but linking =/nfs/condor/condor= to a new directory, e.g. =/nfs/condor/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if unhappy simply changing a link. 

---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status=.
<pre>
[copy output of condor_status on gs1]
</pre>

To test condor you have to create  a submit file (=mytest.submit=) and submit a job
   * submit file:
<pre>
#  A test Condor submission file - mytest.submit
executable = /usr/bin/hostname
transfer_executable = false
universe = vanilla
log = test.log
output = test.out
error = test.err
queue
</pre>
   * submit with:<pre>
condor_submit mytest.submit
</pre>
   * check the jobs:<pre>
condor_q
</pre>
   * inspect the files test.log/out/err
You can submit more than one test job. A different test can have =transfer_executable=true=.

%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.RobGardner - 23 Oct 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%


<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/GridColombiaWorkshop2009/centerpageborder.css
-->