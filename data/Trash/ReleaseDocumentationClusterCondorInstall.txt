%META:TOPICINFO{author="MarcoMambelli" date="1256684945" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="GridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction

We will use the latest stable release of Condor, 7.2.4 released June 25, 2009.  Our approach will be to install the Condor binaries in =/opt/condor= on the management node of the cluster (*gc1-ce*) and export them over NFS to the compute nodes.  The share directory will include:
   * the Condor installation
   * the main =condor_config= file
   * a specific =condor_config= file for each host in the cluster.   These files appear as =condor_config.${HOST}=.  Keeping them on the management node makes it very easy to make cluster-wide configuration changes easily.

The condor home directory will contain a link to the =condor_config= file residing in the default location, =/opt/condor/etc/=. 

Each host will have a local Condor directory scratch directory,  =/scratch/condor= where Condor will store its log files and create spool directories for the jobs. 

The only host that requires write access to the exported =/nfs/condor= area is the host used to install Condor (i.e. =gc1-ce=) so that configuration changes can be made.  All other hosts should mount the directory as read-only.

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.2/3_2Installation.html


---++ Head node installation

Condor is first installed on the host which will be the master.  In this tutorial it will be *gc1-ce*, the same host as the OSG compute element.  

In the following lines some lines start with "=#=". These lines are comments, explaining what you are doing or should see. Feel free to type them if you are copying form the screen. The computer will ignore them.

Here are the steps:
   * Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl][download page]] and choose the Condor version (e.g. 7.2.4, current production version).
   * Choose the correct distribution for you platform (the one below condor 7.2.4 for RHEL 5 dynamically linked ):
      *  =condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz=  (for  x86 OS)
      *  =condor-7.2.4-linux-x86_64-rhel5-dynamic.tar.gz=  (for x86_64 OS)
   * Provide name and email, agree to the license, ... save the file.
   * Create local Condor directories <pre>
mkdir /scratch
mkdir /scratch/condor
</pre>
   * Make sure the *condor* user exists with a shared home directory (=/home/condor=).
   * Make sure  that =/opt/condor= is a soft link to =/nfs/condor/condor=
   * Extract in /tmp/condor-src and install it 
      * extract the tar file:<pre>
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xvzf ~/condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz 
</pre>
      * prepare the directory: <pre>
mkdir /nfs/condor/condor-7.2.4
ln -s /nfs/condor/condor-7.2.4 /nfs/condor/condor
</pre>
      * control that =/opt/condor= exists and is linked to =/nfs/condor/condor= (was done in the NSF setup)
      * Install condor in =/opt/condor=, using =/scratch/condor= local dir, and a shared condor home:<pre>
./condor_install --prefix=/opt/condor --local-dir=/scratch/condor --type=manager
</pre>   
   * Condor will place the local configuration file in the local-dir. Here are some configuration changes to achieve the setup described in the introduction with all the local configurations in the shared directory: 
      * Change =condor_config= to point to the right, moved, local condor configiration (=/opt/condor/etc/condor_config.(HOSTNAME)=) and set the domain to yourdomain.org:<pre>
vi /opt/condor/etc/condor_config     
</pre> 
      * change the value of these variables (look for them in the file):<pre>
LOCAL_CONFIG_FILE = /opt/condor/etc/condor_config.(HOSTNAME)
UID_DOMAIN = yourdomain.org
FILESYSTEM_DOMAIN = yourdomain.org
</pre>
      * make sure that you have (the host name could be with or without domain: =gc1-ce= or =gc1-ce.yourdomain.org=; this should be the result of installing on =gc1-ce=):<pre>
CONDOR_HOST = gc1-ce
</pre>
      * copy the local configuration file to the right location:<pre>
cp /scratch/condor/condor_config.local /opt/condor/etc/
cp /opt/condor/etc/condor_config.local /opt/condor/etc/condor_config.gc1-ce
</pre>
      * link condor_config to its default location:<pre>
ln -s /opt/condor/etc/condor_config ~condor/condor_config
</pre>
   * Test start <pre>
/opt/condor/sbin/condor_master
/opt/condor/sbin/condor_master_off
</pre>
   * Configure the cluster installation (local setups for all the hosts). This setup facilitates central management. All hosts have to have their own condor_config.${HOST} but all worker nodes are configured the same way, so their files are a link to a generic worker node configuration (=condor_config.tg1-cXX=): <pre>
cd /opt/condor/etc/
cp condor_config.gc1-ce condor_config.gc1-cXXX
ln -s condor_config.gc1-cXXX condor_config.gc1-c001
ln -s condor_config.gc1-cXXX condor_config.gc1-c002
ln -s condor_config.gc1-cXXX condor_config.gc1-c003
cp condor_config.gc1-ce condor_config.gc1-cli
</pre>
   * Edit the client to enable master and schedd (=DAEMON_LIST = MASTER, SCHEDD=):<pre>
vi condor_config.gc1-cli
</pre>
   * Edit the client to enable master and schedd (=DAEMON_LIST = MASTER, STARTD=):<pre>
vi condor_config.gc1-cXXX
</pre>
   * remove the unused local config:<pre>
ls /scratch/condor/
cd /scratch/condor/
rm condor_config.local 
</pre>
   * Enable automatic startup at boot:<pre>
vi /opt/condor/etc/examples/condor.boot         # to set the correct path of condor_master, /opt/condor/sbin/condor_master
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor
</pre>

---++ Configuration of the other nodes
This should be executed on all other nodes where Condor should be running (worker nodes, submit host)
   * Create the local Condor directories in =/scratch/condor= (making sure that =execute= directory has the right permissions =a+rwx +t=) <pre>
mkdir /scratch
mkdir /scratch/condor
mkdir /scratch/condor/execute
mkdir /scratch/condor/log
mkdir /scratch/condor/spool
chmod a+rwx /scratch/condor/execute
chmod +t /scratch/condor/execute
</pre>
   * At the end you should see: <pre>
ls -l /scratch/condor/
total 12
drwxrwxrwt 2 condor root 4096 Oct 15 04:07 execute
drwxr-xr-x 2 condor root 4096 Oct 15 04:15 log
drwxr-xr-x 2 condor root 4096 Oct 15 04:02 spool
</pre>
   * Automatic startup<pre>
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor
</pre>

---++ Upgrades
This Condor installation was structured to facilitate upgrades with minimal effort.  The Condor directory on the systems is =/opt/condor=. This is a link to the exported =/nfs/condor/condor= that is a link to the real installation directory including the version number in the name (e.g. =/nfs/condor/condor-2.7.4=).
Only one version will be used at the time but linking =/nfs/condor/condor= to a new directory, e.g. =/nfs/condor/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if unhappy simply changing a link. 

---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status=. Here is the output on =uct3-edge5= (a server in Chicago):
<pre>
> condor_status 

Name               OpSys      Arch   State     Activity LoadAv Mem   ActvtyTime

slot1@uct3-edge5.u LINUX      INTEL  Unclaimed Idle     0.000  2014  0+02:24:27
slot2@uct3-edge5.u LINUX      INTEL  Unclaimed Idle     0.000  2014  0+02:24:28
slot3@uct3-edge5.u LINUX      INTEL  Unclaimed Idle     0.000  2014  0+02:25:00
slot4@uct3-edge5.u LINUX      INTEL  Unclaimed Idle     0.100  2014  0+02:25:01

                     Total Owner Claimed Unclaimed Matched Preempting Backfill

         INTEL/LINUX     4     0       0         4       0          0        0
               Total     4     0       0         4       0          0        0
</pre>

To test condor you have to create  a submit file (=mytest.submit=) and submit a job
   * submit file:<pre>
#  A test Condor submission file - mytest.submit
executable = /usr/bin/hostname
transfer_executable = false
universe = vanilla
log = test.log
output = test.out
error = test.err
queue
</pre>
   * Submit with:<pre>
condor_submit mytest.submit
</pre>
   * Check the jobs:<pre>
condor_q
</pre>
   * Inspect the files test.log/out/err. You can submit more than one test job. A different test can have =transfer_executable=true=.

---+++ A CPU intensive test
Here is a condor test to submit 20 CPU intensive jobs.
   * submit file (save it as =mytestcpu.sub=):<pre>
########################
# Submit description file for CPU test program
########################
Executable     = /home/gsadmin/use-processor.pl
# should_transfer_files  can be YES NO IF_NEEDED
should_transfer_files = YES
# when_to_transfer_output  can be ON_EXIT ON_EXIT_OR_EVICT 
when_to_transfer_output = ON_EXIT
Universe       = vanilla 
Output         = cputest.$(Process).out
Error          = cputest.$(Process).err
Log            = cputest.$(Process).log 
Queue 20
</pre>
   * Downloat the attached file [[%ATTACHURL%/use-processor.pl.txt][use-processor.pl]] and save it in =/home/gsadmin/= and make it executable, e.g. if you have wget: <pre>
wget https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/ClusterCondorInstall/use-processor.pl.txt
cp use-processor.pl.txt /home/gsadmin/use-processor.pl
chmod a+x /home/gsadmin/use-processor.pl
</pre>
   * Submit with:<pre>
condor_submit mytestcpu.sub
</pre>
   * Check the jobs:<pre>
condor_q
</pre>
   * Inspect the files =cputest.XX.log/out/err=, where XX is a number from 1 to 20. 

---++ More information
   * Examples running Condor jobs: http://www.cs.wisc.edu/condor/quick-start.html


%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.RobGardner - 23 Oct 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%


<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/GridColombiaWorkshop2009/centerpageborder.css
-->

%META:FILEATTACHMENT{name="use-processor.pl.txt" attachment="use-processor.pl.txt" attr="" comment="" date="1256677149" path="use-processor.pl" size="508" stream="use-processor.pl" tmpFilename="/usr/tmp/CGItemp20826" user="MarcoMambelli" version="1"}%
