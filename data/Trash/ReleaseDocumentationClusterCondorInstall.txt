%META:TOPICINFO{author="RobGardner" date="1256652884" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="GridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction

We will use the latest stable release of Condor, 7.2.4 released June 25, 2009.  Our approach will be to install the Condor binaries in =/opt/condor= on the management node of the cluster (*gc1-ce*) and export them over NFS to the compute nodes.  The share directory will include:
   * the Condor installation
   * the main =condor_config= file
   * a specific =condor_config= file for each host in the cluster.   These files appear as =condor_config.${HOST}=.  Keeping them on the management node makes it very easy to make cluster-wide configuration changes easily.

The condor home directory will contain a link to the =condor_config= file residing in the default location, =/opt/condor/etc/=. 

Each host will have a local Condor directory scratch directory,  =/scratch/condor= where Condor will store its log files and create spool directories for the jobs. 

The only host that requires write access to the exported =/nfs/condor= area is the host used to install Condor (i.e. =gc1-ce=) so that configuration changes can be made.  All other hosts should mount the directory as read-only.

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.2/3_2Installation.html


---++ Head node installation

Condor is first installed on the host which will be the master.  In this tutorial it will be *gc1-ce*, the same host as the OSG compute element.  

Here are the steps:
   * Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl][download page]] and choose the Condor version (e.g. 7.2.4, current production version).
   * Choose the correct distribution for you platform:
      *  =condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz=  (for x86_64 OS)
      *  =condor 7.2.4 for RHEL 5 i386 dynamically linked (for i386 OS)
   * Provide name and email, agree to the license, ... save the file.
   * Create local Condor directories <pre>
mkdir /scratch
mkdir /scratch/condor
</pre>
   * Make sure the *condor* user exists with a shared home directory (=/home/condor=).
   * Make sure  that =/opt/condor= is a soft link to =/nfs/condor/condor=
   * Extract in /tmp/condor-src and install it <pre>
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xvzf ~/condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz 
mkdir /nfs/condor/condor-7.2.4
ln -s /nfs/condor/condor-7.2.4 /nfs/condor/condor
# /opt/condor should exist and be lined to /nfs/condor/condor
# Install condor in /opt/condor, local dir /scratch/condor, shared ~condor
./condor_install --prefix=/opt/condor --local-dir=/scratch/condor --type=manager
</pre>   
   * Condor will place the local configuration file in the local-dir. Here are some configuration changes to achieve the setup described in the introduction with all the local configurations in the shared directory: <pre>
vi /opt/condor/etc/condor_config      # point to the right, moved, local config (/opt/condor/etc/condor_config.(HOSTNAME); set domain to yourdomain.org)
cp /scratch/condor/condor_config.local /opt/condor/etc/
cp /opt/condor/etc/condor_config.local /opt/condor/etc/condor_config.gc1-ce
ln -s /opt/condor/etc/condor_config ~condor/condor_config
</pre>
   * Test start <pre>
/opt/condor/sbin/condor_master
/opt/condor/sbin/condor_master_off
</pre>
   * Configure the cluster installation (local setups for all the hosts). This setup facilitates central management. All hosts have to have their own condor_config.${HOST} but all worker nodes are configured the same way, so their files are a link to a generic worker node configuration (=condor_config.tg1-cXX=): <pre>
cd /opt/condor/etc/
cp condor_config.gc1-ce condor_config.gc1-cXXX
ln -s condor_config.gc1-cXXX condor_config.gc1-c001
ln -s condor_config.gc1-cXXX condor_config.gc1-c002
ln -s condor_config.gc1-cXXX condor_config.gc1-c003
cp condor_config.gc1-ce condor_config.gs1-cli
vi condor_config.gc1-cli                   # enable  master, schedd
vi condor_config.gs1-cXXX              # enable  master, startd
ls /scratch/condor/
cd /scratch/condor/
mv condor_config.local was.condor_config.local 
</pre>
   * Enable automatic startup at boot:<pre>
vi /opt/condor/etc/examples/condor.boot         # to set the correct path of condor_master, /opt/condor/sbin/condor_master
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
for in in 2 3 5; do ln -s /etc/init.d/condor /etc/rc$i.d/S99condor 
</pre>

---++ Configuration of the other nodes
This should be executed on all other nodes where Condor should be running (worker nodes, submit host)
   * Create the local Condor directories in =/scratch/condor= (making sure that =execute= directory has the right permissions =a+rwx +t=) <pre>
mkdir /scratch
mkdir /scratch/condor
mkdir /scratch/condor/execute
mkdir /scratch/condor/log
mkdir /scratch/condor/spool
chmod a+rwx /scratch/condor/execute
chmod +t /scratch/condor/execute
</pre>
   * At the end you should see: <pre>
ls -l /scratch/condor/
total 12
drwxrwxrwt 2 condor root 4096 Oct 15 04:07 execute
drwxr-xr-x 2 condor root 4096 Oct 15 04:15 log
drwxr-xr-x 2 condor root 4096 Oct 15 04:02 spool
</pre>
   * Automatic startup<pre>
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
for i in 2 3 5; do ln -s /etc/init.d/condor /etc/rc$i.d/S99condor 
</pre>

---++ Upgrades
This Condor installation was structured to facilitate upgrades with minimal effort.  The Condor directory on the systems is =/opt/condor=. This is a link to the exported =/nfs/condor/condor= that is a link to the real installation directory including the version number in the name (e.g. =/nfs/condor/condor-2.7.4=).
Only one version will be used at the time but linking =/nfs/condor/condor= to a new directory, e.g. =/nfs/condor/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if unhappy simply changing a link. 

---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status=.
<pre>
[copy output of condor_status on gs1]
</pre>

To test condor you have to create  a submit file (=mytest.submit=) and submit a job
   * submit file:<pre>
#  A test Condor submission file - mytest.submit
executable = /usr/bin/hostname
transfer_executable = false
universe = vanilla
log = test.log
output = test.out
error = test.err
queue
</pre>
   * Submit with:<pre>
condor_submit mytest.submit
</pre>
   * Check the jobs:<pre>
condor_q
</pre>
   * Inspect the files test.log/out/err. You can submit more than one test job. A different test can have =transfer_executable=true=.

---++ More information
   * Examples running Condor jobs: http://www.cs.wisc.edu/condor/quick-start.html


%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.RobGardner - 23 Oct 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%


<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/GridColombiaWorkshop2009/centerpageborder.css
-->
