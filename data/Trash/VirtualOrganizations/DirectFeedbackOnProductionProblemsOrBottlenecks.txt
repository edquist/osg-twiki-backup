%META:TOPICINFO{author="AbhishekSinghRana" date="1279615898" format="1.1" version="1.6"}%
---+!! Direct Stakeholder/VOs Feedback On <br> Production Problems, Issues, Or Bottlenecks
%TOC%

%STARTINCLUDE%
---++ Purpose

*This document is a compendium of direct feedback provided by a subset of stakeholder VOs on problems or bottlenecks faced during production on OSG. Extracts are being added as input into the more specialized [[Production.ProblemsEncounteredByVOsDuringJobSubmission][Document on VO Job Submission Problems]]*. 

VOs providing feedback are selected to represent a broad set of perspectives - VO domain science (High Energy Physics/Nuclear Physics/Biology/Nanotechnology/etc), VO experience (new/old), VO production assets (resource provisioning to other VOs/application analysis on other sites/etc).

For additional documents with a wider scope, please see [[VirtualOrganizations.Stakeholder_PlansNeedsRequirements][Stakeholder Plans, Needs, and Requirements]].

---++ D0 Tevatron Experiment

Our primary problem/bottleneck has been file delivery to our jobs. This has been and continues to be the greatest source of loss of job and computing efficiency. The issue of file delivery on OSG for D0 is some what complicated by D0's huge reliance on its own distributed data handling system (SAM) which has been used in production since before OSG opened shop. The successful integration of SAM with OSG has allowed D0's opportunistic use of OSG resources. The biggest improvement for D0 in terms of file delivery efficiency has come from the use of SE's to cache oft used files "locally".  This realization has caused D0 to utilize SE's where ever available.  D0 recently added SE's for its use at CIT and SPRACE, both of which are paying off in much improved job and computing efficiencies.  The ability of the SAM system to manage the space allocated on the SE's is a big plus.

The other side of the equation is our SAM infrastructure. This has improved substantially since being in the hands of a professional operations team and out of the hands of developers and physicists.

The are still some sites that D0 uses with unacceptably low efficiency. D0 would like to find a solution to improve the efficiency at these sites. 

(Additionally, please refer to D0 documents in [[VirtualOrganizations.JointTaskForces][Joint Task Forces archives]].)

---++ SBGrid and !NEBioGrid

* We would like GOC to be more proactive in identifying problems at
sites: before tickets are issued, once tickets are issued, and in the
process of following up/resolving issues.  Right now GOC operators
primarily act as a switchboard.

* We would like to have some guidelines for "basic" capabilities at
sites. This would include: rsync
installed and working, curl installed and working, scratch directory set
properly, etc. etc.

* We have been caught out be the GASS cache issue.  It would have been
nice to know about this in advance, and to have guidance on how best to
deal with it and/or alternatives.

* We have worked hard to get scheduling to work properly.  Some
guidelines on alternatives and how to do this would be greatly
appreciated.  It has taken a lot of work to get Condor, VDT, DAGMan, and
OSGMM all configured in a way that works well for our jobs.

* We have had problems related to GUMS and VO mapping.  Peter and I are
both in two VOs: SBGrid and !NEBioGrid.  We still have many sites that
don't use VOMS for VO mapping, and we have, from what I understand,
revealed bugs in GUMS that the folks at BNL are working to fix.

* We have had problems with jobs running properly at many sites, and it
has often taken a long time to resolve these, or we have simply given
up, or the site has given up on us.  This has been frustrating all
around.  Some sites have been great and persevered.  The great sites
have been UNL (Brian in particular) and FNAL (Steve T in particular). 
Sites where things have languished, often despite good effort on the
part of people at those sites: UBuffalo CCR, Notre Dame, UFL, UCSD,
Caltech, BNL, MIT.  Those are just the ones from memory.  That list
consists of lots of big sites, where SBGrid is allowed to run, but for
most of them (maybe all but UCSD and Caltech, where I think we've
*finally* fixed problems) we still can't run jobs (again, this is from
memory -- Peter can correct me).

* We still don't feel we have good guidance on how to manage data and
binary (application) staging/deployment/maintenance/configuration.  We
are figuring this out for ourselves, but surely others have gone before
us and are dealing with this now.  It would be great to have some good
advice on how to proceed, and/or tools to help us with this process.

* Whenever I look at Gratia (email reports or the web-based query
system), or look at the behavior of our jobs, I feel like I see weird
stuff going on with OSG, but it always seems like no one else is
noticing or caring.  Whenever I ask, it seems people say "Yeah, that is
weird", but then take no action.  This makes me wonder who is paying
attention to large scale operational issues?  Most recent examples are
LIGO and VOs that are consuming huge wall clock times but no CPU time,
and MIT where there are crazy eviction patterns.

(Additionally, please refer to !SBGrid documents in [[VirtualOrganizations.JointTaskForces][Joint Task Forces archives]].)

---++ !IceCube Neutrino Experiment

Here is a quick history:

1) Initial connection with OSG - establishing the connection between
   the !IceCube VO VOMS server and OSG sites. There were some technical
   issues that turned out to be related to badly formatted information
   in the !IceCube VO server database. This was resolved through normal
   troubleshooting operations

2) Dealing with the Photonics tables.

   Roughly, the photonics tables describe the propagation of light
   through the ice at the South Pole. Each simulation job will
   read through the entire set of tables (14 GB). That usage pattern
   precludes most shared filesystems (although clustered filesystems
   like Lustre work, just more slowly) and therefore works best if
   installed on the local disk of the worker node.

   This made supporting !IceCube jobs more difficult for the sites.
   To mitigate this problem we took multiple steps:

   1 Work with Miron Livny's group to refactor the jobs so that
      the steps which read from the tables can happen in parallel.
      This is facilitated by the use of DAGs.
   1 Work with Dan Bradley and Igor Sfiligoi to start using the
      UCSD glide-in factory. This enables us to run multiple jobs
      in the same glide-in. In addition, each worker node processes
      a single photonics bin (subset of the full table) so that
      multiple jobs can re-use the worker node reducing the traffic
      of copying the bin.

That test has been successful which leaves us at the point of
moving from testing to production. That requires us to build an
appropriate submit machine. I am actually working on that today
and hope to have it completed by the end of next week. I will
be working with Dan Bradley on the details.

As for improvements, we will have to see. Currently we can run on
UCSD, and Nebraska has agreed in principle to support this mode
of operation. I think there are other sites than can do that as
well.

An additional area of research for us is to replace the photonics
tables with GPU based computation. This is still an area of active
research, but it shows promise. If there are sites that provide
GPU resources, at some point we would like to make some effort 
that area.

---++ GLUE-X

---++ !NanoHUB 

(Additionally, please refer to !NanoHUB documents in [[VirtualOrganizations.JointTaskForces][Joint Task Forces archives]].)

---++ Fermilab VO 

Fermilab VO makes use of the !FermiGrid site forwarding job gateway.  This relies on accurate information being
available in ReSS (Resource Selection System) to decide where the job can run.  We have found that some of the
information that we would like to have, isn't readily available via the GLUE Schema 1.3 and thus not via ReSS.
For instance, the GLUE schema only specifies the total memory which is available on a given worker node, but
it gives no information if, for instance, the batch system will kill a job if it uses >900MB of memory.  Also we have not
found any good way to represent pre-emption policy in the batch system.  Currently there is only a binary yes or no,
with no way to tell how long you have to finish, or which VO's get pre-empted, etc.
There are some other fields not present in the GLUE schema, i.e. scratch disk space available per node, which we have just
put in our own attributes.

We have also run into several failure modes of the condor stack in which the gatekeeper condor_schedd stack will
go into meltdown if too many jobs finish at once and try to send all their output back at once.  Fermilab site policy requires gLexec
and that in turn has also illuminated several places in condor that can make a denial of service attack on the authorization
system when many jobs start at once.

To date our users have not made much use of grid-enabled storage elements, preferring instead to use $OSG_DATA, that 
has limited the number of sites at which we can run.  We are currently planning to put up a Bestman SRM Gateway
in front of our Bluearc NFS server to make it more accessible from the WAN.



-- Main.AbhishekSinghRana - 06 May 2010
