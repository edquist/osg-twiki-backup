%META:TOPICINFO{author="MatsRynge" date="1408656369" format="1.1" version="1.26"}%
%META:TOPICPARENT{name="Activities"}%
---+!! *<noop>%SPACEOUT{ "IRODS and OSG Public Storage" }%*
%TOC{depth="3"}% 
---++ Goals

One  of  the  goals  of  the  OSG  is  to  provide  the  Virtual  Organizations(VOs)  with  opportunistic  usage  of  grid   resources.  One  of  the  essential  resources  is  storage.  OSG  initiated  production-­&#8208;scale  Opportunistic   Storage  provisioning  and  usage  on  all  OSG  sites.  The  major  problem  for  all  stakeholders  is  that  the  OSG   doesn't provide efficient means to manage this storage. We have started to technology investigation to understand if iRODS can be used to provide necessary tools for data management and movement.

---++  Overview of iRODS
   [[http://www.irods.org][iRODS]]  as  resource  management  and  data  movement  services  for   the  OSG  public  storage.  The  iCAT-­&#8208;enabled  IRODS  is  capable  to  register  and  set  quota  on  public  resources   per  VO  as  well  as  per  group,  and  per  user.  It  allows  to  define  resource  allocation  policies  as  a  set  of   Rules.  Rules  trigger  a  chain  of  actions  (micro-­&#8208;services)  that  are  executed  when  resource  allocation  or   resource  availability  has  been  changed.  A  chain  of  actions  may  include  recovery  from  failures  and   notification.  
In  the  current  proposal    iRODS    is    considered  as  SaaS.    It  is  deployed  on  a  central  node  along  with  iCAT   catalog.      iCAT  catalog  contains  information  about  resources,  resource  usage,  quotas  and  users.    It  also   serves  as  metadata  catalog  for  the  VOs  data  collections.     
---++ Project Timeline
---+++ Notes and documents
   * [[http://home.fnal.gov/~tlevshin/irods_osg-1.pdf][Initial presentation to OSG EB]]
   * [[http://home.fnal.gov/~tlevshin/OSG_Public_Storage_and_iRODS.pdf][OSG Public Storage and IRODS proposal]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/iRODS_Installation.pdf][iRODS Installation Procedure]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/iRODSUser.pdf][Initial commands for creating resources, users, group ,admins]]
   * [[MangeQuota][VO Quota Enforcement on Simple Resource]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/NotesMicroservices.pdf][Detailed Notes on How to Write a Custom !MicroService]] 
   * [[%ATTACHURLPATH%/irods_diagram2.jpg][Upload files to SRM: user -> iRODS -> SRM workflow]]
   * [[%ATTACHURLPATH%/irods_diagram2.jpg][Data movement from the worker node to a user]]
   * [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=1094][Document describes the goals and outcome of the investigative phase I of the project]]
   * [[https://indico.fnal.gov/materialDisplay.py?contribId=63&sessionId=4&materialId=slides&confId=5109][Presentation at OSG AHM 22-Mar-2012, University of Nebraska-Lincoln]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_phaseI_final_report.pdf][Phase I report]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_deployment.pdf][Presentation for Operation and Production Managers]]
   * [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage20121002.pdf][Presentation for DES]]
---+++ Phase I
   1. !iRODS installation and configuration
   1. Quota enforcement
   1. Writing rules using existing microservices
   1. Writing custom microservices
   1. [[https://www.opensciencegrid.org/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_Integration.pdf][Status report on OSG/IRODS integration]] 
   1. [[https://cdcvs.fnal.gov/redmine/projects/osgrods/repository][iRODS git Repository]]
   1. [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_phaseI_final_report.pdf][Phase I report]]- June 13th, 2012

---+++ Phase II
   1. [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_deployment.pdf][Meeting with Operation and Production Managers]] - June 19th, 2012
   1. [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/IRODSOSG/OSGStorage_iRODS_phaseII_final_report_2.pdf][Progress Review]] - Jan 7th, 2013
---++ Brief Tutorial
%ICON{"warning"}% %RED%This tutorial is work in progress!!!%ENDCOLOR%


Currently irods client commands are installed on OSG-XSEDE submission node. This is a very brief tutorial how to use iRODS from xd-login.opensciencegrid.org. If you want to understand iRODS in depth, please use [[http://www.irods.org][iRODS documentation]]. In order to understand OSG-IRODS integration, please read [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=1094][this document]]. 

---+++ Prerequisite 
   * x509 certificate
   * VOMS membership (optional)
   * access to  xd-login.opensciencegrid.org (irods client could be installed on SL5,SL6 - installation guide is coming soon)
   * Request registration with iRODS. Please send email to  user-support@opensciencegrid.org  and specify the following information:
      * preferred user name
      * VO affiliation (default is "osg" VO)
      * DN of your certificate 
      * email address
  
---+++ Configure iRODS client
   * login on  xd-login.opensciencegrid.org
   * create directory ==.irods== <pre class="screen">
$cd ~
$mkdir .irods
</pre>
   * copy ==/opt/irods_client/irodsEnv.template==  <pre class="screen">
$cp /opt/irods_client/irodsEnv.template .irods/.irodsEnv
</pre>
   * modify  ==.irods/.irodsEnv== by replacing %RED%USER_NAME%ENDCOLOR% with your iRODS user's name<pre class="file">
# iRODS personal configuration file.
#
# This file was automatically created during iRODS installation.
#   Created Thu Nov 17 10:52:41 2011
#
# iRODS server host name:
irodsHost 'irods.fnal.gov'
# iRODS server port number:
irodsPort 1247

# Default storage resource name:
irodsDefResource 'diskCache'
# Home directory in iRODS:
irodsHome '/osg/home/%RED%USER_NAME%ENDCOLOR%'
# Current directory in iRODS:
irodsCwd '/osg/home/%RED%USER_NAME%ENDCOLOR%'
# Account name:
irodsUserName '%RED%USER_NAME%ENDCOLOR%'
# Zone:
irodsZone 'osg'
irodsAuthScheme GSI
irodsLogLevel  'DEBUG'
</pre>
---+++ Using iRODS
---++++ Set environment and create proxy
   * login on  xd-login.opensciencegrid.org
   * get proxy certificate. You will need voms proxy in case you want to use the OSG Storage Elements<pre class="screen">
$grid-proxy-init -valid 192:00%BLUE% 
Your identity: /DC=org/DC=doegrids/OU=People/CN=xxxxxx
Enter GRID pass phrase for this identity:
Creating proxy ........................................... Done
Your proxy is valid until: Fri Aug 10 15:38:57 2012%ENDCOLOR%
</pre> <pre class="screen">
%BLUE%
$voms-proxy-init  -valid 192:00 --voms osg
Enter GRID pass phrase:
Your identity: /DC=org/DC=doegrids/OU=People/CN=xxxx
Creating temporary proxy ............................................ Done
Contacting  voms.opensciencegrid.org:15027 [/DC=org/DC=doegrids/OU=Services/CN=host/voms.opensciencegrid.org] "osg" Done
....
Creating proxy ...................................... Done
Your proxy is valid until Wed Sep 19 15:16:12 2012%ENDCOLOR%
</pre>
   * setup iRODS<pre class="screen">
$. /opt/irods_client/setup_irods.sh
</pre>
   * verify that you can access iRODS <pre class="screen">
$iuserinfo%BLUE%
name: tlevshin
id: 10519
type: rodsuser
zone: osg
info: tlevshin@fnal.gov
comment: 
create time: 01329602644: 2012-02-18.22:04:04
modify time: 01329602680: 2012-02-18.22:04:40
GSI DN or Kerberos Principal Name: /DC=org/DC=doegrids/OU=People/CN=Tanya Levshina 508821
member of group: tlevshin
member of group: osg
%ENDCOLOR%
</pre>
---++++ Listing commands
   * use ==ils -l== to list your collection <pre class="screen">
ils -l%BLUE%
/osg/home/tlevshin:
  tlevshin          1 MIT_CMS_CE2_FTP          10485760 2012-07-23.16:56 & test1
  tlevshin          1 Nebraska                        5 2012-06-18.15:08 & test2
  tlevshin          3 SPRACE_FTP                      5 2012-07-20.15:42 & test2
  tlevshin          4 cinvestav_FTP             1048576 2012-07-27.14:44 & test_1
  tlevshin          6 SPRACE_FTP                1048576 2012-07-27.14:44 & test_1
  tlevshin          8 NYSGRID_CORNELL_NYS1      1048576 2012-07-27.14:45 & test_1
  tlevshin         10 Purdue-Steele_FTP         1048576 2012-07-27.14:45 & test_1
  tlevshin         12 MIT_CMS_CE2_FTP           1048576 2012-07-27.14:45 & test_1
  tlevshin         14 SWT2_CPB_FTP              1048576 2012-07-27.14:45 & test_1
  tlevshin         16 AGLT2_CE_2_FTP            1048576 2012-07-27.14:46 & test_1
  tlevshin         18 UTA_SWT2_FTP              1048576 2012-07-27.14:46 & test_1
  C- /osg/home/tlevshin/10MB_test  
  C- /osg/home/tlevshin/test  
  C- /osg/home/tlevshin/test1  
  C- /osg/home/tlevshin/test_files  
%ENDCOLOR%
</pre> Where columns are ==User Name==, ==Replica Number==, ==Name of the Resource==,  ==File Size==, ==Creation Date==, ==File Name==.
   * We currently support three resource groups (==osgGridFtpGroup==, ==osgAppGridFtpGroup== and ==osgSrmGroup==). ==osgGridFtpGroup==  group consists of remote gridftp servers that could be used to prestage data in $OSG_DATA areas on the OSG sites. This data could be later used by jobs running on the worker node and accessing $OSG_DATA in read-only mode. ==osgAppGridFtpGroup==  group consists of remote gridftp servers that could be used to install VO software  in $OSG_APP areas on the OSG sites. The software should be accessible by jobs running on the worker node and accessing $OSG_APP in read-only mode.  ==osgSrmGroup== consists of the remote SRM end point and could be used to upload data to/from the OSG SE. The detailed description how to use this resource group to upload data from/to a worker is descibed below in [[https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/IRODSOSG#Access_to_iRODS_from_the_worker][section]]. Use ==ilsresc-osg== to list available resources available, status, quota etc <pre class="screen">
$ilsresc-osg -h%BLUE%
ilsresc_org [-hqsml] [-g group_name][-r resc_name] [-G resourcegroup_name]
-h/--help shows help
no options - shows list of resources
-g/--group -restricted to the resources that support this group
-r/--resource -restricted to a specified resource
-G/--resourcegroup -restricted to the resources this resource group
-q/--quota shows resource quota(available and used in MB)
-s/--status shows status of the resource
-m/--meta shows resource metadata
-l/--list shows complete information about resources
%ENDCOLOR%
</pre> For example if you want to list all the resources for osgGridFtpGroup resource group that are available for osg VO you can do:<pre class="screen">
$ilsresc-osg -g osg -G osgGridFtpGroup%BLUE%
resourcegroup: osgGridFtpGroup
Group: osg Resource: MIT_CMS_CE2_FTP
Group: osg Resource: GridUNESP_CENTRAL_FTP
Group: osg Resource: diskCacheGridFtp
Group: osg Resource: USCMS-FNAL-WC1-CE3_FTP
Group: osg Resource: NYSGRID_CORNELL_NYS1_FTP
Group: osg Resource: NWICG_NotreDame_FTP
Group: osg Resource: Purdue-Steele_FTP
Group: osg Resource: UCSDT2-B_FTP
Group: osg Resource: EngageRenci
Group: osg Resource: BNL_ATLAS_2_FTP
Group: osg Resource: SPRACE_FTP
Group: osg Resource: SWT2_CPB_FTP
Group: osg Resource: AGLT2_CE_2_FTP
Group: osg Resource: FNAL_GPGRID_1_FTP
Group: osg Resource: UTA_SWT2_FTP
Group: osg Resource: UFlorida-PG_FTP
Group: osg Resource: cinvestav_FTP
%ENDCOLOR%
</pre> or <pre class="screen"> ilsresc-osg -g osg -G osgSrmGroup%BLUE%
resourcegroup: osgSrmGroup
Group: osg Resource: WT2
Group: osg Resource: BU_ATLAS_Tier2
Group: osg Resource: cinvestav
Group: osg Resource: SPRACE
Group: osg Resource: UCD
Group: osg Resource: UCSDT2
Group: osg Resource: GLOW
Group: osg Resource: CIT_CMS_T2
Group: osg Resource: NWICG_NotreDame
Group: osg Resource: FNAL_FERMIGRID_ITB
Group: osg Resource: FLTECH
Group: osg Resource: NYSGRID_CORNELL_NYS1
Group: osg Resource: Firefly
Group: osg Resource: SMU_HPC
Group: osg Resource: TTU-ANTAEUS
Group: osg Resource: UFlorida-PG
Group: osg Resource: BNL-ATLAS-ITB
Group: osg Resource: FNAL_GPGRID_1
Group: osg Resource: BNL-ATLAS
Group: osg Resource: MWT2
Group: osg Resource: Purdue-RCAC
Group: osg Resource: Nebraska
Group: osg Resource: FNAL_FERMIGRID%ENDCOLOR%
</pre>If you want all the details about a particular resource <pre class="screen">
$ilsresc-osg -g osg -r FNAL_GPGRID_1_FTP -l%BLUE%
Group: osg Resource: FNAL_GPGRID_1_FTP
Quota: 10240 MB (10737418240 bytes(s)) Used:1 MB (1048586 byte(s))
Status: up
SURL: gsiftp://fnpcosg1.fnal.gov:2811
EndPath:/grid/data/osg/irods,
%ENDCOLOR%
</pre> This command is tailored for the OSG users but the  native iRODS commands such as ==ilsresc==, ==iquota==, ==imeta== can be used as well. Use ==osgSrmGroup== in order to get information about the resources in that group.

---++++ Pre-staging file to irods-OSG
Currently there are two resource groups: ==osgSrmGroup== and ==osgGridFtpGroup== you can use to pre-stage files. ==osgSrmGroup== consists of Storage Elements available on the OSG sites that support your VO. ==osgGridFtpGroup== consists of gridftp servers that are running on the OSG Compute Elements and provide access to $OSG_DATA area. This area is usually available as READ-ONLY from the worker nodes.
   * pre-stage local file to osgGridFtpGroup<pre class="screen">
 $iput -R osgGridFtpGroup example_1
</pre> check file's location<pre class="screen">
$ils -l example_1%BLUE%
  tlevshin          1 UCSDT2-B_FTP               743873 2012-08-02.16:37 & example_1
%ENDCOLOR%
</pre> You can also pre-stage file to a specific resource if needed<pre class="screen">
 $iput -R  UCSDT2-B_FTP   example_2
</pre>
   * download file from a remote resource to the local disk<pre class="screen">
 $iget example_1 example_1_copy
</pre>
   * you can use ==-P== option to monitor progress of the download and ==-r== option if you want do recursive download and  retrieve subcollections.
   * you can used ==igetwild.sh== that allows you to iget a set of files using wild-card type matching on file names: <pre class="screen">
$ igetwild.sh /osg/home/%RED%tlevshin 1GB_test b%ENDCOLOR%
%BLUE%
   1GB_test.762945.0            1023.977 MB | 64.256 sec | 16 thr | 15.936 MB/s
   1GB_test.827518.0               0.010 MB | 9.952 sec | 0 thr |  0.001 MB/s
   1GB_test.831504.0               0.000 MB | 11.992 sec | 0 thr |  0.000 MB/s
%ENDCOLOR%
</pre> where Usage: ==igetwild.sh collection pattern b|m|e  (beginning, middle, or end)==
   * delete ALL replica of a file from remote resources<pre class="screen">
 $irm -f  example_1
</pre>
   * delete a specific replica from remote resource <pre class="screen">
 $irm -f  -n 1 example_1
</pre>
---++++ Replicating file to multiple resources
You can request file replication to all currently available remote resources (with enough space, up status etc) by executing the following command:
<pre class="screen">
$irepl-osg -f  /osg/home/%RED%username/filename%ENDCOLOR% -G %RED%resourcegroup_name%ENDCOLOR%
</pre> For example:
<pre class="screen"> 
$irepl-osg -f /osg/home/tlevshin/example_1 -G osgGridFtpGroup%BLUE%
The file example_1 has been successfully copied from resource UCSDT2-B_FTP to resource FNAL_GPGRID_1_FTP
The file example_1 has been successfully replicated to resource cinvestav_FTP
The file example_1 has been successfully replicated to resource SPRACE_FTP
The file example_1 has been successfully replicated to resource NYSGRID_CORNELL_NYS1_FTP
The file example_1 has been successfully replicated to resource Purdue-Steele_FTP
The file example_1 has been successfully replicated to resource MIT_CMS_CE2_FTP
The file example_1 has been successfully replicated to resource SWT2_CPB_FTP
The file example_1 has been successfully replicated to resource AGLT2_CE_2_FTP
The file example_1 has been successfully replicated to resource UTA_SWT2_FTP
The file example_1 has been successfully replicated to resource GridUNESP_CENTRAL_FTP
The file example_1 has been successfully replicated to resource USCMS-FNAL-WC1-CE3_FTP
%ENDCOLOR%
</pre>
This command blocks until it copies a specified  file from one remote resource to all others. 
---++++ Async Replication to multiple resources
You can use ==-a== option in ==irepl-osg== command. In this case it will send a request to replicate file to iRODS server and will notify you via email when the files have been replicated, e.g:
<pre class="screen">
$irepl-osg -a -f  /osg/home/%RED%username/filename%ENDCOLOR% -G %RED%resourcegroup_name%ENDCOLOR%
</pre>
---++++ Dealing with  tar.file uploaded to OSG_APP or OSG_DATA resource group
If you need to upload a lot of small files to OSG_APP or OSG_DATA area, the most efficient way of doing it is the following:
   1.  Create a tar file with the application or data:<pre class="screen">
$tar cfh %RED%test_application.tar%ENDCOLOR% %RED%test_application%ENDCOLOR%
</pre> %ICON{"warning"}%. You should use -h option to get rid of symbolic links.
   1.  Upload your application tarfile to some or a particular resource in osgAppGridFtpGroup:<pre class="screen">
$iput -R  osgAppGridFtpGroup  %RED%test_application.tar%ENDCOLOR%
</pre>
   1. Find out the location of the file<pre class="screen">
 $ils -l test_application.tar
 %BLUE%xxx   1 Firefly_APP_FTP             10240 2012-10-03.16:55   test_application.tar%ENDCOLOR%
</pre>
   1. Untar the file:<pre class="screen">
 $ibun-osg  -f test_application.tar -R %RED%Firefly_APP_FTP%ENDCOLOR% -G osgAppGridFtpGroup
</pre>  You can use -a option in ibun-osg command. In this case it will send a request to untar file to iRODS server and will notify you via email when the files have been registered in iRODS 
---+++ Access to iRODS from the worker nodes
---++++ Access to OSG_DATA
If you are running a job on a worker node and this job needs to access the data pre-staged in ==OSG_DATA== area you can access this file using ==$OSG_DATA/osg/irods/%RED%username%ENDCOLOR%/%RED%filepath%ENDCOLOR% where %RED%username%ENDCOLOR%== is your iRODS username. Keep in mind that you should use ONLY the sites you have managed to prestage your data. So, you should add the following line in your job submission file, e.g:<pre class="file">
+SiteList = "FNAL_FERMIGRID, cinvestav, SPRACE, NYSGRID_CORNELL_NYS1, Purdue-Steele, MIT_CMS_CE2, SWT2_CPB, AGLT2_CE_2, UTA_SWT2,GridUNESP_CENTRAL, USCMS-FNAL-WC1-CE3"
Requirements = ((stringListMember(GLIDEIN_ResourceName,SiteList) == True)...)
</pre> You can find the sites by using <pre class="screen">
$ils -l %RED%filepath%ENDCOLOR%
</pre>%ICON{"warning"}%You will need to get rid of (_FTP) to get the actual name of the site.
---++++ Access to OSG_APP
If you are running a job on a worker node and this job needs to access software prestaged in  ==OSG_APP== area you can access this file using ==$OSG_APP/irods/%RED%username%ENDCOLOR%/%RED%filepath%ENDCOLOR% where %RED%username%ENDCOLOR%== is your iRODS username. Keep in mind that you should use ONLY the sites you have managed to prestage your software So, you should add the following line in your job submission file, e.g:<pre class="file">
+SiteList = " Firefly,Nebraska,GLOW_APP,UCSDT2"
Requirements = ((stringListMember(GLIDEIN_ResourceName,SiteList) == True)...)
</pre> You can find the sites by using <pre class="screen">
$ils -l %RED%filepath%ENDCOLOR%
</pre>%ICON{"warning"}%You will need to get rid of (_APP_FTP) to get the actual name of the site.
---++++ Access to SRM resources
While running a job on the worker node that requires access to the SRM resource you may choose from either using Condor file transfer plugin or using the 'icp' command. Details of those options are listed below.

---+++++ icp

When the condor worker node is setup via glidein, the setup scripts add a console utility ‘icp’ that you may use in your bash scripts to take finer control of the input and output from and to iRODS.  Following examples show use of icp for reading and writing data from/to iRODS:
<pre class="file"> 
** These commands may be called from a wrapper script, additionally the condor job description file must contain "UsesiRODS=True" as above.

# --- copy from irods 
icp irodse://aguru@irods.fnal.gov:1247?/osg/home/aguru/sourcefolder/sourcefile.txt localtarget.txt 

# --- copy to irods 
icp localsourcefile.txt irodse://aguru@irods.fnal.gov:1247?/osg/home/aguru/targetfolder/sourcetargetfile.txt
</pre>

This is an example of your condor submission file :<pre class="file"> 
executable = %RED%icp_test.sh%ENDCOLOR%
#Project Name is relevant only to user who are using xsede allocation
+ProjectName = "OSG-Staff"
universe = vanilla
x509userproxy=/tmp/%RED%x509up_u5029%ENDCOLOR%
Arguments = $(Cluster).$(Process)
Output = $(Cluster).$(Process).out
Error = $(Cluster).$(Process).err
Log = $(Cluster).$(Process).log
+UsesiRODS=True
#On these sites we can not either download irods software or there are incompetebility between irods and globus-gsi libs installed on the worker nodes
+SiteList = "BNL-ATLAS,SMU_HPC"
Requirements = ((stringListMember(GLIDEIN_ResourceName,SiteList) == False)  && (Disk >= 25000))
Notification = Never
should_transfer_files = YES
when_to_transfer_output = ON_EXIT
queue 10
</pre>
This is an example of the executable:<pre class="file"> 
#!/bin/bash
set -x
retVal=0
#the log file  for irods command will be located in irodsplugin.log.<localuserid>.${JOB_ID}
#where JOB_ID = Cluster.Process
export JOB_ID=$1
#creates the 1GB file
dd if=/dev/urandom of=1GB_test.$1 bs=1073741824 count=1
$IRODS_PLUGIN_DIR/icp 1GB_test.$1 irodse://%RED%irods-username%ENDCOLOR%@irods.fnal.gov:1247?/osg/home/%RED%irods-username%ENDCOLOR%/1GB_test.$1
if [ $? -eq 0 ]
then
        echo "`date` icp success"
        rm -f 2GB_test.$1
else
        echo "`date` icp failure"
        retVal=1
fi
exit $retVal
</pre>
---+++++ condor transfer plugin
The condor file transfer plugin allows one to let condor handle all the input and output for the job. This is done by using   protocols ‘irods://’ or ‘irodse://’ (extended). The difference between the two protocols is the method in which file data is transferred to the storage. In case of irods:// the file data along with the metadata is first sent to the irods server and then transferred to the final storage space by the irods server. Whereas, in case of irodse:// the data is directly transferred to the storage space and only file metadata is handled through the irods server. In our tests we found that using irodse:// is more scalable since it divides the data transfer and metadata management load.
The port (e.g. 1247 below) and the target resource group (e.g. Nebraska and osgSrmGroup below) are both optional. When specified they override the default setting of the port and find the best match resource to write the file data.
The best match resource gives preference to a 'local SRM' if available at site where the job is running. 
<pre class="file"> 
+UsesiRODS=True
transfer_input_files = test.sh, irodse://aguru@irods.fnal.gov:1247?Nebraska:/osg/home/aguru/sourcefiles/SMALLF.txt
output_destination = irodse://aguru@irods.fnal.gov:1247?osgSrmGroup:/osg/home/aguru/destinationfolder/
</pre>

In the above example
   * aguru@irods.fnal.gov - specifies the user identity and the irods server
   * 1247 – is the irods server port
   * Nebraska and osgSrmGroup – refer to the resource groups where the file data must be stored
   * And the path to the source and destination are denoted /osg/home/aguru/sourcefiles/SMALLF.txt and /osg/home/aguru/destinationfolder/ respectively

-- Main.TanyaLevshina - 19 Dec 2011



%META:FILEATTACHMENT{name="NotesMicroservices.pdf" attachment="NotesMicroservices.pdf" attr="" comment="" date="1324329688" path="NotesMicroservices.pdf" size="669713" stream="NotesMicroservices.pdf" tmpFilename="/usr/tmp/CGItemp23616" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="iRODS_Installation.pages" attachment="iRODS_Installation.pages" attr="" comment="" date="1324331405" path="iRODS_Installation.pages" size="177781" stream="iRODS_Installation.pages" tmpFilename="/usr/tmp/CGItemp23552" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="iRODS_Installation.pdf" attachment="iRODS_Installation.pdf" attr="" comment="" date="1324331523" path="iRODS_Installation.pdf" size="104831" stream="iRODS_Installation.pdf" tmpFilename="/usr/tmp/CGItemp23495" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="iRODSUser.pdf" attachment="iRODSUser.pdf" attr="" comment="" date="1324333320" path="iRODSUser.pdf" size="52357" stream="iRODSUser.pdf" tmpFilename="/usr/tmp/CGItemp23320" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="irods_diagram.jpg" attachment="irods_diagram.jpg" attr="" comment="" date="1327504551" path="irods_diagram.jpg" size="76656" stream="irods_diagram.jpg" tmpFilename="/usr/tmp/CGItemp26036" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="irods_diagram2.jpg" attachment="irods_diagram2.jpg" attr="" comment="" date="1327506500" path="irods_diagram2.jpg" size="81495" stream="irods_diagram2.jpg" tmpFilename="/usr/tmp/CGItemp26153" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage_iRODS_Integration.pdf" attachment="OSGStorage_iRODS_Integration.pdf" attr="" comment="" date="1330467513" path="OSGStorage_iRODS_Integration.pdf" size="493975" stream="OSGStorage_iRODS_Integration.pdf" tmpFilename="/usr/tmp/CGItemp44868" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage_iRODS_phaseI_final_report.pdf" attachment="OSGStorage_iRODS_phaseI_final_report.pdf" attr="" comment="" date="1339553365" path="OSGStorage_iRODS_phaseI_final_report.pdf" size="182013" stream="OSGStorage_iRODS_phaseI_final_report.pdf" tmpFilename="/usr/tmp/CGItemp44176" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage_iRODS_deployment.pdf" attachment="OSGStorage_iRODS_deployment.pdf" attr="" comment="" date="1340119560" path="OSGStorage_iRODS_deployment.pdf" size="126821" stream="OSGStorage_iRODS_deployment.pdf" tmpFilename="/usr/tmp/CGItemp49481" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage20121002.pdf" attachment="OSGStorage20121002.pdf" attr="" comment="OSG Public Storage (presentation for DES)" date="1349188723" path="OSGStorage20121002.pdf" size="169192" stream="OSGStorage20121002.pdf" tmpFilename="/usr/tmp/CGItemp8852" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage_iRODS_phaseII_final_report.pdf" attachment="OSGStorage_iRODS_phaseII_final_report.pdf" attr="" comment="" date="1357576512" path="OSGStorage_iRODS_phaseII_final_report.pdf" size="92022" stream="OSGStorage_iRODS_phaseII_final_report.pdf" tmpFilename="/usr/tmp/CGItemp3186" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="OSGStorage_iRODS_phaseII_final_report_2.pdf" attachment="OSGStorage_iRODS_phaseII_final_report_2.pdf" attr="" comment="" date="1357577173" path="OSGStorage_iRODS_phaseII_final_report_2.pdf" size="115159" stream="OSGStorage_iRODS_phaseII_final_report_2.pdf" tmpFilename="/usr/tmp/CGItemp3064" user="TanyaLevshina" version="1"}%
