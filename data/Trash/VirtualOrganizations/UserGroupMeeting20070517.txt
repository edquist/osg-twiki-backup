%META:TOPICINFO{author="KyleGross" date="1225985970" format="1.1" version="1.2"}%
---+!! User Group Coordination Meeting
---+!! 2007/05/17

%TOC%

---++ Preamble
---+++ Meeting coordinates

2007/05/15 at 11am CDT.<br>
For attendance instructions please see [[UserSupportMeetings#Coordinates][main meetings page]].

---+++ Present

[[Main.AbhishekSinghRana][Abhishek Singh Rana]]; [[Main.FkW][Frank Wuerthwein]]; [[Main.ChrisGreen][Chris Green]]; [[Main.TorreWenaus][Torre Wenaus]]; Mats Rynge.

---+++ Apologies

[[Main.RuthPordes][Ruth Pordes]]; [[Main.ShaowenWang][Shaowen Wang]]; Steve Clark.

---++ Agenda
---+++ Issues from last week.

   * CHARRM: Torre worked with Tim and much better job throughput has been achieved.
   * Gratia reporting requirements: please fill in on the Gratia Reporting Requirements page under [[https://twiki.grid.iu.edu/twiki/bin/view/Accounting/ReportRequirements#New_User_Requirements][New User Requirements]].

---++++ Action

   * Chris still to do something about research into BDII/LDAP/ReSS.
   * <strike>Chris to follow up with Tim regarding PANDA config and which user claims to be running the jobs.</strike>
   * <strike>Chris still to follow up with Parag about D0 status and issues.</strike>

---+++ nanoHUB status and needs

Steve mis-understood the meeting time and didn't get a reminder -- would like to reschedule.

%RED%*Update*%ENDCOLOR%

Meeting with Steve took place Friday at noon CDT:

---++++ Discussion

   * !BioMocha (sp?) complete; but general submission tool may resurface for more studies.
   * New version of tool out next week.
   * "WIRE" application currently running test jobs.
   * In general, jobs 0.5 - 6h duration; 1 user = about 20 jobs; up to about 500 users.
   * Jobs being sumbitted with GRID proxy under a limited number of certs. No current plans to swap to =voms-proxy= or individual certs; user information is held locally for audit purposes.
   * Two issues currently open:
      * SAZ not handling parsing Purdue certs properly: ticket #2577.
      * Jobs at Chicago keep cycling through Hold mode: ticket #3060.
   * MPI: Preston Smith has done proof-of-concept for running MPI jobs through the OSG. No nanoHUB OSG jobs running MPI yet because only known MPI sites are Altix and the code apparently has a problem on same; being worked on.
   * No major issues.

---++++ Action

   * <strike>Chris to reschedule discussion with nanoHUB.</strike>
   * <strike>Chris: bump (at least) Chicago ticket to Troubleshooting team for follow-up.</strike>
   * Chris: Pass MPI information on to RENCI.

---+++ Engagement Group status and needs

---++++ Overview

Engagement operating on three fronts:

   * A meta-scheduling Condor front-end to support multiple applications using RENCI as a portal.
   * Non-MPI protein-folding application.
   * MPI applications, initially WRF, others waiting in the wings.

---++++ Current status

   * Non-MPI jobs have been running mostly very well. Two problems:
      * A LIGO site -- initial tests seemed to indicate outgoing network access from WN, but this turned out not to be the case and jobs failed. That wasn't the main problem as the test has since been corrected; but the Condor-based matching system continued to send jobs to that site after the first failures. Is there some way to have Condor keep track of failures (eg failures in the last hour) and build it into the matching?%BR%%RED%*Update*%ENDCOLOR%%BR%Chris and Mats discussed job failure a little more off-line. Problem is that through the GT2 interface, don't get status indication of whether the job failed since the status is simply that of the Globus interaction. This is fixed in GT4, but still. Decided that he would be happy with a daily query to Gratia grouping failed jobs for the previous day by site for the =%ngage%= VO(s).%BR%Something like:<pre>select J.ProbeName, T.facility_name as SiteName, sum(Njobs)
 from JobUsageRecord J, CETable T, CEProbes P
 where
  EndTime >= CURDATE() - INTERVAL 1 DAY
  and EndTime < CURDATE()
  and VOName like '%ngage%'
  and J.Status != 0
  and P.probename = J.ProbeName
  and T.facility_id = P.facility_id
 group by J.ProbeName
 order by SiteName desc;</pre>Chris will get this into a daily email or provide the means for him to make the query himself.%BR%%RED%*Update*%ENDCOLOR%%BR%Query is actually substantially more complicated than this due to job's execution status actually being carried by the =ExitCode= resource for Condor rather than the =Status= GLUE schema datum which only concerns whether Condor's of the job transport, etc, was successful.
      * UCSD jobs -- shorter jobs work just fine, but longer ones seem to run forever without completing. Mats has specified a !WallTime of 900m, so it shouldn't be a case of getting killed and restarting continuously.
   * MPI jobs are running on NERSC, but not through the OSG interface. OSG interface only recently inaugurated on an MPI cluster at NERSC (=davinci=, an Altix cluster) -- should push on this and expose technical problems as soon as possible. 

---++++ Action

   * <strike>Mats to contact Doug Olson about getting =davinci= to support engage (and Leesa in particular).</strike>
   * Chris to liaise with Doug O., John M. and Leesa on pushing to use OSG interface now it's (nominally) available.
   * <strike>Mats to use "Blue button" to follow up on UCSD trouble.</strike>

---+++ Issues from troubleshooting

None.


---+++ July workshop

---++++ Discussion

   * Decided on sessions:
      * 8.30 intro and welcome on first day;
      * 9-noon with 15 minute break at 10.30;
      * 90 mins for lunch; 
      * 1.30 - 3.30 and 3.45 - 5.45 in the afternoon.
   * When to finish on second day? 3.30 maybe?
   * Try to avoid charging a registration fee, but would like refreshments morning and afternoon -- talk to Ruth.
   * First session, 10+5 talks from troubleshooting, docs and goc with remainder of session taken up with VirtualOrganizations/VOInfoRS/BDII/ReSS.
   * VOs: definitely want to hear from Engagement, CHARMM, nanoHUB, STAR, LIGO, GRASE, GADU, CDF, DZERO, ATLAS and CMS in some capacity. Want to organize in terms of issues rather than a roll call of VOs.
  * Some issues that could make up a session:
      * Data movement
      * Meta-scheduling
      * Workflow management and ways to organize an application to get the most out of the GRID (eg DAG construction).
      * Part of above could be subsumed int a Best Practices session -- issues affecting throughput and "good gridizenship -- %RED%*Frank to expand*%ENDCOLOR%.

---++++ Action
   * ALL: email discussion to arrive at announcement email and session schedule; draft to [[mailto:osg-eb@opensciencegrid.org][EB]] by EOB Friday.
   * Chris to follow up refreshments and room allocation logistics as necessary.

---+++ Other current issues

None.

---+++ AOB.

None.

-- Main.ChrisGreen - 17 May 2007
