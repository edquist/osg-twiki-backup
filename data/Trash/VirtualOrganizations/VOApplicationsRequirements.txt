%META:TOPICINFO{author="AbhishekSinghRana" date="1197616792" format="1.1" version="1.29"}%
%META:TOPICPARENT{name="WebHome"}%
<!-- Hidden Settings
   * Set ALLOWTOPICCHANGE = Main.ChrisGreen, Main.AbhishekSinghRana
-->

%TABLE{ sort="off" tableborder="0" cellpadding="1" cellspacing="3"  headeralign="left" dataalign="left" headerrows="1" footerrows="0" databg="#00FF00"}%
|Deadline: December 12, 2007 (Closed)|

---+!! OSG 1.0 Planning: Requirements set forth by VOs

---++!! Overview and Goal

The next major middleware release, OSG 1.0, is scheduled for a February 2008 rollout. This document is a compendium of <u>%MAROON%requirements%ENDCOLOR%</u> being set forth by the VO community, in view of <u>%MAROON%needs and expectations of running VO applications on OSG 1.0%ENDCOLOR%</u>. 

Immediate goals are: (1) To provide input to the OSG Executive Board, as well as to the Middleware, Operations, and the Sites coordination groups. (2) To encourage VOs to set and convey the expectations ahead of schedule.

If needed for reference, please see [[https://twiki.grid.iu.edu/twiki/bin/view/ReleaseDocumentation/SiteValidationTable#VO_validation][archived ITB 0.7 validation table]] with results of activity immediately prior to OSG 0.8 release. 

%TOC%

---++ *ATLAS*   %Y%

*Modular packages/components*

   * Site admin can choose to install/uninstall a stand-alone component (such as GIP) without installing the whole OSG CE.                                                                                           
                                                                                                                                                 
*GUMS*                                                                                                                                                               

   * Map VOMS proxies from a particular user but different VOs to different pool accounts, even if all pool accounts are in one persistence factory.                                                                                             

*Globus Gatekeeper*                                                                                                                                                    
                                                                                                                                                                      
   * In general, improved gatekeeper scalability and fault tolerance.                                                                                                                                                                                                                                                                         
   * Load balancing w/ LVS. (?)                                                                                                                                       
   * High availability.                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                      
   * Some kind of CE throttling to prevent CE from being impacted by !DoS.                                                                                               
   * Quota based on number of jobs in batch system.     
   * Quota by user and/or VO.                                                                                                                                         
   * Throttling by CE load. (?)                                                                                                                                       
                                                                                                                                                                       
*Globus <notwiki>JobManager</notwiki>*
                                                                                                                                                                      
   * Some mechanism to add processing to condor.pm (plugin), e.g., to add custom site-specific flags based on user, group, etc.                                                                                                                
                  
*RSV/Service availability monitoring*

   * A complete set of baseline service availability probes to monitor LHC Tier 1/Tier 2 services (CE, SE/SRM, FTS, !MyProxy, BDII).  The test results can be easily integrated into site admin monitoring server (such as Nagios).                                                                                                                                                             

   * OSG availability monitoring needs to inter-operate with WLCG.  
   * a single web interface (gridview) shows WLCG/OSG/EGEE availability metrics.
   * scheduled downtime should be recognized by OSG availability monitoring, and availability metric should properly reflect the service downtime.                                                                                                                                                            

*Gratia / Accounting*                                                                                                                                                 

   * Add ability to define subclusters, and specify submission method such                                                                                             
 that an accounting record is a triple (subcluster, submit host, user-vo). Or                                                                                         
 at least have an implicit mechanism for multiple Gratia instances run on                                                                                             
 different submit hosts to report records as part of a single site.                                                                                                   

*GIP*       

   * Support SRM 1 and SRM 2 (dCache Cell information provider,  xrootd), and allow SRM admin to easily publish static/dynamic information to OSG.

*BDII and/or WLCG BDII*

   * Make plugin that allows the site admin to fine-tune reported info, e.g., reporting each Condor VM as job slot instead of dividing by 2 or 4.

*Network performance test*
                                                                                                                                                                      
   * iperf:  Authorized users can remotely start an iperf server by running globus-job-run to test the network performance.                                                                                                                      

---++ *CDF*   %Y%

*Compute services*

   * A need to have extra RSL field(s) to run only on WNs with larger than 4GB memory.

*Storage services*

   * SRM V2 for dCache.

*Wider deployment of Squid*

   * Expand Squid support across the OSG - including reporting of existence of Squid servers at each of bigger sites.

---++ *CMS*   %Y%

*Compute services*

   * Support for multiple CE's for a single site.                                                                                                                                     
   * Better logfile format for parsing.                                                                                                                                  
   * Better scalability (always).      

*GIP*

   * Advertising wall clock time and having GIP advertise this.
   * Population of the 'GlueCEStateEstimatedResponseTime' key with something reasonable.  Right now, GIP sets it to zero. 

*Packaging*

   * Alternative to Pacman for installer.                                                                                                                                
   * A possible introduction of complete RPM based installation. 

*Storage services*

   * Extensions to Replica Manager in dCache.
   * Functional space reservation in SRM v2.2.
   * Over longer run, functional quotas and leases in the combination of SRM protocol and underlying dCache filesystem. This will greatly facilitate opportunistic usage of deployed storage, further enhancing true Data-grid capabilities of OSG.

*Storage clients*

   * Functional srmcp clients that work with srm v1 as well as v2. This does not have to necessarily be one client, but could be multiple. 
   * Ability to interoperate with LCG sites' storage, i.e., FTS clients would need to be distributed.                                                                                                                                  
   * A possible inclusion of lcg-utils-lite, which would bring in the LCG's set of (high quality) client bindings for SRM, RFIO, !dCap, and !GridFTP.                                                                                    

---++ *<notwiki>CompBioGrid</notwiki>*   %Y%

*Memory requirement support*

   * Sites should advertise WN max installed memory (even if it pertains only to subset of WNs).
   * Jobs should be able to require submission to WN with specified minimum free RAM available.

*Standardize MPI support*

   * Advertise MPI support (or lack thereof).
   * Support some type of reservation policies for MPI jobs.

*"Sleeper" jobs*

   * Allow special class of "sleeper" jobs with negligible CPU time but (quasi)unlimited wall clock time.  !CompBioGrid uses such jobs on local clusters to manage/monitor job submissions to achieve transactional job control.  They facilitate messaging from active jobs and do cleanup/resubmission of crashed/failed jobs, as well as tracking checkpointing and data archival.  We run a few of these jobs on every cluster similar to daemons, usually not stopping until the respective WN reboots.  They have small memory footprint and typically are active <0.1% wall clock time, thus creating a negligible load on WNs.

   * Submission of such "sleeper" jobs/daemons could be restricted to VO admins or some other subset of privileged VO members.

*SRM compatibility*

   * Single SRM client that works against any version of SRM at any SE site.

*License support*

   * Enable control running of licensed applications - e.g. via !FlexLM support.

*Enhanced job accounting and job tracking conveniences*

   * Some facility for VO-based and user-based ability to query job status and history independently of the Condor-G client.  This should work almost always, including almost all failure cases (including authentication failures), except global failures (e.g. site down).  Ideally, it would include tracking by a !JobID attribute specified at submission time by the client, independent of any identifier(s) assigned by the site job manager. (yes, this creates some uniqueness issues, but they can be handled cleanly and consistently)

---++ *DOSAR*   %Y%

*MPI capabilities*

   * Availability of MPI capabilities on the OSG.

---++ *DZero*   %Y%

*Wider deployment of CEMon*

   * Since DZero uses !ReSS for resource selection, sites that support DZero should install CEMon (v1.7.6 or higher).

*Wider deployment of GUMS v1.1*

   * We are also planning to expand are use of role-based access, so having GUMS (v1.1 or higher) is desired.

*General preferences*

   * For the foreseeable future, we are planning to access pre-WS GRAM for job submission.
   * We also would like to use local storage at sites via SRM interfaces, rather than ad-hoc SAM storages.


---++ *Engagement*   %Y%

*Advertisement of MPI capabilities*

   * Inclusion of the MPI information provider into GIP/ReSS.

---++ *Fermilab VO*   %Y%

*CEMon and <notwiki>ReSS</notwiki>*

   * Get an up-to-date version of CEMon  (which, as is understood, the !ReSS project will be working on shortly).                                                               
                    
*NMI build related benign error messages*
                                                                                                                                                   
   * Clean up the various ugly error messages such as those in voms-proxy-init which are due to artifacts in the NMI build                                                                                                            system used by the VDT.  We get lots of user questions about these "harmless" warnings and are spending lots of time on them.                                                                                                       
   
*WS-GRAM advertisement*
                                                                                                                                                                    
   * Some way in the information system, ad-hoc though it may be, to advertise the presence of WS-GRAM.                                                                                                                                   
                                        
*GIP*
                                                                                                                               
   * A systematic way for campus grids to insert extra fields into the GIP without breaking LCG/OSG interoperability.                                                                                                                
   * Ways to at least advertise compatibility-breaking site differences such as nfs-lite, initialdir, etc.                                                                                                                                     
  
*CA and CRL certificates management*
                                                                                                                                                                     
   * Addition of the "csync" package (written and submitted by us) to the VDT for CA/CRL distribution. 
            
---++ *GUGrid*   %Y%

*Platform Support*

   * Full support for RHEL/CENTOS 5 x86-64 and Mac OSX Intel Tiger/Leopard

*Packaging*
                                                                                                                            
   * A possible introduction of complete RPM based installation. 

---++ *LIGO*   %Y%

*Wider deployment of WS-GRAM and Squid*

   * LIGO is pursuing a new analysis to run on the OSG. This analysis has experiences on the German Grid (DGRID). From our communications with the developers in Europe, we believe that WS-GRAM and SQUID will be important technologies for accelerating the successful migration of                                                                                                    this new application onto the OSG. 
   * We are aware that there are already successes with SQUID and WS-GRAM at specific OSG sites. Any possibilities for making these successes more widely available would be of interest to LIGO. We are in early phases of this application port                                                                                                   so the timeline for the next OSG release would be most reasonable for larger scale running. 

*General*

   * Possible diagnosis and resolution of NFS-lite as well as PBS related problems encountered during the ITB 0.7 validation. It is not clear if problems were specific to sites, or are more generally related to NFS-lite and, separately, PBS. This may need some effort, or perhaps become more evident as more sites supporting LIGO, using NFS-lite and PBS, and with OSG 0.8 middleware stack appear on the map.

---++ *nanoHUB*   %Y%

*Disk quota management*

   * nanoHUB jobs will cleanup after themselves when they complete. Jobs that are aborted or fail for other reasons do not necessarily achieve this goal.  Applications are run in scratch directories to avoid filling the home directory.  Globus however deposits many files and directories under home directory that are often not cleaned up upon job completion. Under these conditions it is possible that either home or scratch directories will fill up with old data. The consequence of a full directory is that new jobs cannot be started - including jobs intended to remove old dead files. Notification of such condition would be useful. More generally a real time monitor of disk usage relative to quota would also be more useful. I found that in some cases the quota command does not return any information.

*Queue prediction*

   * Join forces with TG to present a common queue prediction service. TG is currently promoting a set of software developed by Network Weather Service for predicting queue wait time and probability of completing a job by a given deadline.

*Resource discovery*

   * Join forces with TG to create a common interface for resource discovery and availability. Data provided should be provided in a timely relevant fashion and reduce the need for each VO to do their own testing (probes)

*Community Accounts*

   * Maintain community accounts eliminating the need for individual certificates. Put the burden on the Gateway not the user.

---++ *NWICG*   %Y%

*Resource Discovery*
   * Advertisement of MPI libraries and capabilities.  If OSG                                                                                                             
  cannot expand beyond serial jobs, it will remain of very                                                                                                             
  limited utility in specific disciplines.  We realize this                                                                                                            
  is fraught with complications, but it must be done.                                                                                                                  
                                                                                                                                                                       
*Packaging/Installation*                                                                                                                                               
   * More breakout of components from each other and CE possible.                                                                                                         
   * Breakout of component configuration. (Not require rerun of                                                                                                           
  master reconfig to modify one component's configuration.)                                                                                                            
   * Less redundancy in components and better integration with                                                                                                            
  system software packages already available. (Possibility of                                                                                                          
  using one, system-level !MySQL, Java, etc. and only offering                                                                                                          
  those in alternate versions for those who would have a                                                                                                               
  version issue, rather than conflict being an assumption.)                                                                                                            
   * Alternative packaging system available, such as RPM.                                                                                                                 
  (The above would seem much easier by just creating a "CE"                                                                                                            
  RPM with the right package dependencies.)                                                                                                                            
                                                                                                                                                                       
*Globus*                                                                                                                                                                
   * Better scalability, fault tolerance, and redundancy/failover.                                                                                                        
   * Peer-to-peer gatekeeper communications to enable some of this,                                                                                                       
  DNS round-robin, gatekeeper pools and forwarders, etc.           

---++ *SDSS/DES*   %Y%

*Java and storage clients*

   * If possible, there should be only one version of Java and one version of the SRM client. Right now, with OSG 0.8,  there are a couple of those when one installs the CE.

---++ *STAR*   %Y%

*General*      
                                                                                                                                          
   * Availability of myproxy clients in OSG stack everywhere.                                                                                                          
   * Standard/homogeneous error format of logs [start/end events                                                                                                  
      based for example] and standard verbosity levels in middleware                                                                                                   
      for easier troubleshooting.                                                                                                                                       
   * Globus support for Mac OS-X.                                                                                                                                      
                                                                                                                                                                       
*Storage services* 
                                                                                                                                                     
   * SRM clients V2 everywhere [critical].                                                                                                                             
   * Dynamic space reservation in SRM.                                                                                                                                 
   * VO & Role-based Space pre-allocation capability in storage                                                                                                       
     (other than SRM managed).                                                                                                                                          
   * Quota capabilities per VO at multiple levels - general space                                                                                                     
      allocation available to VO and SRM managed space should be                                                                                                       
      consistent with a usage policiy and advertised.                                                                                                                   
   * Possibility to start SRM services (server integration) on                                                                                                        
      'edge' node / gatekeeper.                                                                                                                                         
   * SRM services (server side) supporting multiple-VOs with                                                                                                          
      role based.                                                                                                                                                       
                                                                                                                                                                       
*Compute services* 
                                                                                                                                                     
   * Extended RSL (and old request) and ability to pass arbitrary                                                                                                     
      parameters to jobmanagers.                                                                                                                                        
   * Site advertisement of average pending time per VO.                                                                                                                
   * Site advertisement of average Number of slots (jobs) used                                                                                                        
      per VO and maximum claim-able number of slots [the latest                                                                                                        
      pertain to usage policies].                                                                                                                                       
   * Information of network conditions from WN, i.e., firewall                                                                                                          
      settings. Information we would find useful:                                                                                                                      
      * Are communication in/out allowed                                                                                                                               
      * If so, within which port range if restricted                                                                                                                   
      * Is http OUT allowed (is HTTP download allowed)                                                                                                              
      * Would communication from outside to inside be allowed                                                                                                          
        [is a call-back possible]                                                                                                                                      
   * Error recovery (retries of jobs for example) specification                                                                                                       
      capabilities at jobmanager / GRAM level.                           

*Interoperability* [starting experimenting with this - not sure                                                                                                        
    if already the case]                                                                                                                                               
   * Coexistence of accounting/monitoring services between LCG/OSG                                                                                                    
      sites (info consistent and can be added to one another).                                                                                                          
   * Consistent ENV variable definitions and advertisement ($APPS,                                                                                                    
      $DATA, ...).

---++ *Miscellaneous*

This is a listing of requirements gathered from internal projects of OSG, or as general feedback, i.e., <u>not</u> necessarily from VOs.

---+++!! Gratia Accounting Team 

*Inclusion of GRAM Auditing*

   * Working GRAM auditing system installed and configured by default, including pre-WS and WS and a configurable purging facility, utilizing !MySQL5 if at all possible.
   * GRAM auditing enhancements for pre-WS and WS, including but not limited to: (a) Storage of FQAN in audit record (b) Resolution of globus bugzilla reports 5712, 5713 and 5714.

---+++!! CI Team at U Chicago

*RPM based VDT installation*

   * A means to bundle the filesystem tree used for VDT packages (not just for CE, but GUMS, VOMS, etc.), and streamline a post-install procedure (e.g., defining the parameters in a text file).                                                                              
   * A simplified RPM package/post-install system would be advantageous for the following reasons:                                                                                                                                                   
      * (a) Deployable via configuration management systems such as bcfg and cfengine (b) Ease of migration between nodes, for prototyping, disaster recovery, and multi-gatekeeper environments (c) Hands-off installation using a properly-defined configuration input file (d) Snapshot package can be downloaded once and stored locally, rather than running Pacman again everywhere.                                                        


-- Main.AbhishekSinghRana - 09 Dec 2007