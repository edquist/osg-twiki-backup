%META:TOPICINFO{author="AbhishekSinghRana" date="1197511612" format="1.1" reprev="1.19" version="1.19"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! OSG 1.0 Planning: Requirements set forth by VOs

---++!! Overview and Goal

The next major middleware release, OSG 1.0, is scheduled for a February 2008 rollout. This document is a compendium of <u>%MAROON%requirements%ENDCOLOR%</u> being set forth by the VO community, in view of <u>%MAROON%needs and expectations of running VO applications on OSG 1.0%ENDCOLOR%</u>. 

Immediate goals are: (1) To provide input to the OSG Executive Board, as well as to the Middleware, Operations, and the Sites coordination groups. (2) To encourage VOs to set and convey the expectations ahead of schedule.

If needed for reference, please see [[https://twiki.grid.iu.edu/twiki/bin/view/ReleaseDocumentation/SiteValidationTable#VO_validation][archived ITB 0.7 validation table]] with results of activity immediately prior to OSG 0.8 release. 

%TOC%

---++ *ATLAS*   %Y%

*Modular packages/components*

   * Site admin can choose to install/uninstall a stand-alone component (such as GIP) without installing the whole OSG CE.                                                                                           
                                                                                                                                                 
*GUMS*                                                                                                                                                               

   * Map VOMS proxies from a particular user but different VOs to different pool accounts, even if all pool accounts are in one persistence factory.                                                                                             

*Globus Gatekeeper*                                                                                                                                                    
                                                                                                                                                                      
   * In general, improved gatekeeper scalability and fault tolerance.                                                                                                                                                                                                                                                                         
   * Load balancing w/ LVS. (?)                                                                                                                                       
   * High availability.                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                      
   * Some kind of CE throttling to prevent CE from being impacted by !DoS.                                                                                               
   * Quota based on number of jobs in batch system.     
   * Quota by user and/or VO.                                                                                                                                         
   * Throttling by CE load. (?)                                                                                                                                       
                                                                                                                                                                       
*Globus <notwiki>JobManager</notwiki>*
                                                                                                                                                                      
   * Some mechanism to add processing to condor.pm (plugin), e.g., to add custom site-specific flags based on user, group, etc.                                                                                                                
                  
*RSV/Service availability monitoring*

   * A complete set of baseline service availability probes to monitor LHC Tier 1/Tier 2 services (CE, SE/SRM, FTS, !MyProxy, BDII).  The test results can be easily integrated into site admin monitoring server (such as Nagios).                                                                                                                                                             

   * OSG availability monitoring needs to inter-operate with WLCG.  
   * a single web interface (gridview) shows WLCG/OSG/EGEE availability metrics.
   * scheduled downtime should be recognized by OSG availability monitoring, and availability metric should properly reflect the service downtime.                                                                                                                                                            

*Gratia / Accounting*                                                                                                                                                 

   * Add ability to define subclusters, and specify submission method such                                                                                             
 that an accounting record is a triple (subcluster, submit host, user-vo). Or                                                                                         
 at least have an implicit mechanism for multiple Gratia instances run on                                                                                             
 different submit hosts to report records as part of a single site.                                                                                                   

*GIP*       

   * Support SRM 1 and SRM 2 (dCache Cell information provider,  xrootd), and allow SRM admin to easily publish static/dynamic information to OSG.

*BDII and/or WLCG BDII*

   * Make plugin that allows the site admin to fine-tune reported info, e.g., reporting each Condor VM as job slot instead of dividing by 2 or 4.

*Network performance test*
                                                                                                                                                                      
   * iperf:  Authorized users can remotely start an iperf server by running globus-job-run to test the network performance.                                                                                                                      

---++ *CDF*   %Y%

*Compute services*

   * A need to have extra RSL field(s) to run only on WNs with larger than 4GB memory.

*Storage services*

   * SRM V2 for dCache.

*Wider deployment of Squid*

   * Expand Squid support across the OSG - including reporting of existence of Squid servers at each of bigger sites.

---++ *CMS*   %Y%

*Compute services*

   * Support for multiple CE's for a single site.                                                                                                                                     
   * Better logfile format for parsing.                                                                                                                                  
   * Better scalability (always).      

*GIP*

   * Advertising wall clock time and having GIP advertise this.
   * Population of the 'GlueCEStateEstimatedResponseTime' key with something reasonable.  Right now, GIP sets it to zero. 

*Packaging*

   * Alternative to Pacman for installer.                                                                                                                                
   * A possible introduction of complete RPM based installation. 

*Storage services*

   * Extensions to Replica Manager in dCache.
   * Functional space reservation in SRM v2.2.
   * Over longer run, functional quotas and leases in the combination of SRM protocol and underlying dCache filesystem. This will greatly facilitate opportunistic usage of deployed storage, further enhancing true Data-grid capabilities of OSG.

*Storage clients*

   * Functional srmcp clients that work with srm v1 as well as v2. This does not have to necessarily be one client, but could be multiple. 
   * Ability to interoperate with LCG sites' storage, i.e., FTS clients would need to be distributed.                                                                                                                                  
   * A possible inclusion of lcg-utils-lite, which would bring in the LCG's set of (high quality) client bindings for SRM, RFIO, !dCap, and !GridFTP.                                                                                    

---++ *<notwiki>CompBioGrid</notwiki>*   %ICON{bubble}%

<i> Work in progress. !CompBioGrid is in communication with User Group and is expected to send requirements soon.</i>

---++ *DOSAR*   %Y%

*MPI capabilities*

   * Availability of MPI capabilities on the OSG.

---++ *DZero*   %Y%

*Wider deployment of CEMon*

   * Since DZero uses !ReSS for resource selection, sites that support DZero should install CEMon (v1.7.6 or higher).

*Wider deployment of GUMS v1.1*

   * We are also planning to expand are use of role-based access, so having GUMS (v1.1 or higher) is desired.

*General preferences*

   * For the foreseeable future, we are planning to access pre-WS GRAM for job submission.
   * We also would like to use local storage at sites via SRM interfaces, rather than ad-hoc SAM storages.


---++ *Engagement*   %Y%

*Advertisement of MPI capabilities*

   * Inclusion of the MPI information provider into GIP/ReSS.

---++ *Fermilab VO*   %Y%

*CEMon and <notwiki>ReSS</notwiki>*

   * Get an up-to-date version of CEMon  (which, as is understood, the !ReSS project will be working on shortly).                                                               
                    
*NMI build related benign error messages*
                                                                                                                                                   
   * Clean up the various ugly error messages such as those in voms-proxy-init which are due to artifacts in the NMI build                                                                                                            system used by the VDT.  We get lots of user questions about these "harmless" warnings and are spending lots of time on them.                                                                                                       
   
*WS-GRAM advertisement*
                                                                                                                                                                    
   * Some way in the information system, ad-hoc though it may be, to advertise the presence of WS-GRAM.                                                                                                                                   
                                        
*GIP*
                                                                                                                               
   * A systematic way for campus grids to insert extra fields into the GIP without breaking LCG/OSG interoperability.                                                                                                                
   * Ways to at least advertise compatibility-breaking site differences such as nfs-lite, initialdir, etc.                                                                                                                                     
  
*CA and CRL certificates management*
                                                                                                                                                                     
   * Addition of the "csync" package (written and submitted by us) to the VDT for CA/CRL distribution. 
            
---++ *GUGrid*   %Y%

*Platform Support*

   * Full support for RHEL/CENTOS 5 x86-64 and Mac OSX Intel Tiger/Leopard

*Packaging*
                                                                                                                            
   * A possible introduction of complete RPM based installation. 

---++ *LIGO*   %Y%

*Wider deployment of WS-GRAM and Squid*

   * LIGO is pursuing a new analysis to run on the OSG. This analysis has experiences on the German Grid (DGRID). From our communications with the developers in Europe, we believe that WS-GRAM and SQUID will be important technologies for accelerating the successful migration of                                                                                                    this new application onto the OSG. 
   * We are aware that there are already successes with SQUID and WS-GRAM at specific OSG sites. Any possibilities for making these successes more widely available would be of interest to LIGO. We are in early phases of this application port                                                                                                   so the timeline for the next OSG release would be most reasonable for larger scale running. 

---++ *nanoHUB*   %Y%

*Disk quota management*

   * nanoHUB jobs will cleanup after themselves when they complete. Jobs that are aborted or fail for other reasons do not necessarily achieve this goal.  Applications are run in scratch directories to avoid filling the home directory.  Globus however deposits many files and directories under home directory that are often not cleaned up upon job completion. Under these conditions it is possible that either home or scratch directories will fill up with old data. The consequence of a full directory is that new jobs cannot be started - including jobs intended to remove old dead files. Notification of such condition would be useful. More generally a real time monitor of disk usage relative to quota would also be more useful. I found that in some cases the quota command does not return any information.

*Queue prediction*

   * Join forces with TG to present a common queue prediction service. TG is currently promoting a set of software developed by Network Weather Service for predicting queue wait time and probability of completing a job by a given deadline.

*Resource discovery*

   * Join forces with TG to create a common interface for resource discovery and availability. Data provided should be provided in a timely relevant fashion and reduce the need for each VO to do their own testing (probes)

*Community Accounts*

   * Maintain community accounts eliminating the need for individual certificates. Put the burden on the Gateway not the user.

---++ *SDSS/DES*   %Y%

*Java and storage clients*

   * If possible, there should be only one version of Java and one version of the SRM client. Right now, with OSG 0.8,  there are a couple of those when one installs the CE.

---++ *STAR*   %ICON{bubble}%

<i>Work in progress. STAR is in communication with User Group and is expected to send requirements soon.</i>

---++ *Miscellaneous*

This is a listing of requirements gathered from internal projects of OSG, i.e., not necessarily from VOs.

---+++!! Gratia Team 

   * Working GRAM auditing system installed and configured by default, including pre-WS and WS and a configurable purging facility, utilizing !MySQL5 if at all possible.
   * GRAM auditing enhancements for pre-WS and WS, including but not limited to: (a) Storage of FQAN in audit record (b) Resolution of globus bugzilla reports 5712, 5713 and 5714.

-- Main.AbhishekSinghRana - 09 Dec 2007
