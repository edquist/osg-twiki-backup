%META:TOPICINFO{author="AbhishekSinghRana" date="1265598419" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%

*Purpose*: The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.

---++ Preparation
---+++ Introduction

Hadoop Distributed File System (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:
   * An SRM interface for grid access; 
   * !GridFTP-HDFS as transport layer; and  
   * A FUSE interface for localized POSIX access.

The VDT packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs. Two YUM repositories are available: 
   * Stable repository for wider deployments and production usage.
   * Testing repository for limited deployments and pre-release evaluation.

Please note that this distribution is independent of the Pacman packaging of the VDT: it is separately versioned, and separately packaged. We expect future releases to eventually be common with the rest of the VDT, as the "rest" of the VDT begins to be packaged as RPMs. For now, the VDT distribution of Hadoop is distinct from the rest of the VDT.

The *stable YUM repository* is enabled by default through the *osg-hadoop RPM*, and contains the *golden release* supported by OSG for LHC operations. 

---+++ Architecture

---++ Installation

Please read the [[Storage.HadoopUnderstanding][planning document]] to understand different components of the system. 

Main server components can be divided in 3 categories: 
   * HDFS core: Namenode, Datanode.
   * Grid extensions: !BeStMan SRM, Globus !GridFTP, Gratia probe, and Xrootd server plugin, etc.
   * HDFS auxiliary: Secondary Namenode, Hadoop Balancer.

Main client components are FUSE and Hadoop command line client.

---+++ FUSE Kernel module

The FUSE interface to HDFS requires installation of an external FUSE kernel module. The stock RHEL kernels do not include the FUSE kernel module yet. Refer to http://vdt.cs.wisc.edu/components/hadoop.html to find link to external FUSE kernel module. It has ready-to-install module RPMs for the recent kernel versions. Upgrade your kernel as needed to match the kernel version. 

<pre class="screen">
$ yum update kernel
$ wget ftp://ftp.pbone.net/mirror/atrpms.net/OS-ARCH/atrpms/stable/fuse-kmdl-KERNEL-VER-FUSE-VER.rpm
$ rpm -Uvh fuse-kmdl-KERNEL-VER-FUSE-VER.rpm
</pre>

If needed, modify the GRUB bootloader to load the correct kernel image.

<pre class="screen">
$ vi /boot/grub/grub.conf
</pre>

If you are running a custom kernel, then be sure to enable the FUSE module with =CONFIG_FUSE_FS=m=. Building a FUSE kernel module for your custom kernel is beyond the scope of this document.

---+++ Initializer RPM

---+++ YUM install/upgrade of components 

---++++ !NameNode

---++++ !DataNode

---++++ !GridFTP

---++++ SRM

---++++ Transfer Probe

---++++ Storage Probe


---++ Validation

<pre class="screen">
srm-ping
</pre>

<pre class="screen">
lcg-cp
</pre>

<pre class="screen">
lcg-ls
</pre>

---++ References

<br />%STOPINCLUDE% 
%BR% 
%COMPLETE3% %BR% 
%RESPONSIBLE% Main.AbhishekSinghRana- 12 Sept 2008 %BR% 
%REVIEW% Main.TanyaLevshina - 21 Jul 2009 %BR%
%REVFLAG% %X% %BR%

%COMMENT{type="tableappend"}%

-- Main.AbhishekSinghRana - 05 Feb 2010
