%META:TOPICINFO{author="AbhishekSinghRana" date="1276027500" format="1.1" version="1.2"}%
---+++ *Report from VOs Coordinator* <br> June 8, 2010

---+++ Ongoing general activities with VOs

   * Biannual cycle of updates to:
      * Scientific publications resulting from substantial/beneficial use of OSG. More details are with David Ritchie (OSG Communications).
         * Looking to arrive at a closure by June 8. List currently has around 300-350 publications.
         * Contributions from: *CDF, MINOS, D0, ATLAS, CMS, STAR, GLOW community grid, !FermiGrid, LIGO, !MiniBOONE, !EIE4CI/Engagement, NYSGrid, CIGI, HCC, !NanoHUB, SBGrid*. More from *GEANT4* collaboration are expected.
      * [[https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/Stakeholder_PlansNeedsRequirements][Database of stakeholder plans/needs and requirements]], with emphasis on: (i) current production averages, (ii) future production estimates.

---+++ Facilitated VO participation for peer Areas

   * Regular Weekly VO Forum; venue to collect input from VOs, and for discussions between VOs and OSG Areas. 
   * VO Job Problems compilation with Jim W and Dan.
      * https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/DirectFeedbackOnProductionProblemsOrBottlenecks
      * https://twiki.grid.iu.edu/bin/view/Production/ProblemsEncounteredByVOsDuringJobSubmission
   * Familiarizing VOs with Software Evolution Proposals/SEPs.
   * Production resources prediction and planning with Dan 
      * https://twiki.grid.iu.edu/bin/view/Production/OSGResourcePredictions
      * Annual estimates for 2010 based on direct VO input.
   * Bi-annual collection of stakeholder scientific publications is in progress with David.
   * Working to strengthen liaison with Collective VOs Council Representatives.

---+++ Summary from At-large VOs

   * *D0* monte-carlo production remains cyclic; fluctuated between 5 to 10 Million events/week. Achieved a new annual peak of 13.3 Million Events/week. This is also the highest all-time peak noted in D0 collaboration so far. Focus has been on increasing efficiency at important sites; D0 and VOs Group analyzing site issues every week. | [[https://twiki.grid.iu.edu/twiki/pub/VirtualOrganizations/JointTaskForces/D0_Event_Production_volume_on_OSG.ppt][Peaks Tracking]]

   * *CDF* and *Fermilab-VO* production remains steady. Regular communication of operational feedback and requirements from Fermilab-VO to VOs Group, to peer Areas. Usage projections for Fermilab-VO and sub-VOs are now available for calendar years 2010, 2011, 2012.

   * Ongoing work with *SBGrid/NEBioGrid* to sustain an increased scale of production. Getting good concurrent job runs; but efficiency is not consistent from day to day. Peak 70,000 hours/day; usage in bursts; average 8-10,000 hours/day. Massive overloading issues at FNAL T1 and UNL sites (many factors: cleaning NFS ~/.globus/.gass_cache directory; using curl; recursive wrapper calls). Resolved now. Continuing to use OSG/Engage !MatchMaker for production. Feedback: need for client-side functionality to remotely diagnose job failures. E.g., whether jobs are pre-empted, and why. Starting to investigate !GlideinWMS. Plans are to also evaluate Panda; targeting 8 sites. Multiple factors evaluated/addressed; thus, progress steady but slow.

   * Moving *IceCube* from testing to production. Planning to use UCSD, UNL, GLOW. !IceCube and GLOW adding a dedicated submit infrastructure node. Using Condor DAGs, !GlideinWMS, Squid. Synopsis of accomplished changes in workflow: Jobs refactored with DAGMan to read from Photonic tables in parallel steps; Multiple jobs in the same glide-in; Each worker node processes a single photonics bin (subset of the full table); Multiple jobs can re-use the worker node reducing the traffic of copying the bin.
   
   * Facilitated usage of *GLUE-X* site UConn-OSG; now in use by Engage, GEANT4, GPN, LIGO, SBGrid, GLOW. Pointed out bugs/improvements in !MyOSG and GIP | [[https://ticket.grid.iu.edu/goc/viewer?id=8343][ticket]]. Working to reach closure. GLUE-X is interested in offering SRM/dCache opportunistic storage to other VOs; facilitating D0 now. Glideins from GPN VO were failing at GLUE-X site; Related to bug in Condor; condor team notified. Interested in trying out !GlideinWMS. Next step for GLUE-X workflow management - need to decide on a scheme for job dispatch. Abhishek & Dan discussing options with Richard Jones. Policy related feedback: Lack of OSG policy on POSIX account requirements; a site's support for multiple VOs; multiple sites' support for a specific VO. GLUE-X's own application/production to follow in coming months. 

   * Sustained *nanoHUB* production at 200-700 wall hours/day. 5+ types of nanotechnology applications. Opened some 'end-user' production jobs; 'probe' jobs ongoing as earlier. OSG accounting view -  Probe Jobs: "/CN=nanoHUB Service07", Application Testing: "/CN=nanoHUB Service06", End-users Production runs "/CN=nanoHUB Service02". nanoHUB accounting view - reporting websites that correspond to these categories are: [[http://nanohub.org/usage/gridprobe][Probe]] | [[http://nanohub.org/usage/gridapptest][AppTest]] | [[http://nanohub.org/usage/gridappprod][Production]]. Problems were noticed in DAGMan after upgrading submit/workflow infrastructure to Condor 7.4.1; recurring schedd crashes; multiple copies of the same jobs were being submitted; resulted in apparent peaks in consumption. Condor team supplied fix as a patched binary; evaluated and deployed by nanoHUB.

   * Good success with *DOSAR* and *GridUNESP*; researchers running MPI jobs through full site/VO infrastructure; running on OSG. | [[https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/DOSAR_GridUNESP_OSG][Experiences Blog]]

   * *CompBioGrid* Local site running at full capacity; continuing to provide cycles for Engage and SBGrid. Virtual Cell integration being planned. | [[https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/CompBioGrid_OSG][Experiences Blog]]

   * *GPN* and *GROW* have active sites; but scale is very moderate.
 
   * *CIGI, GRASE, NWICG* valuable partners in Consortium; but science production is moderate. Plans need to be encouraged.

   * Restarted work with *CHARMM* team. Production running on OSG now; publication expected in coming months. Running under *OSG-VO* using VOMS proxy. Actively doing production and getting results. Studying molecular dynamics simulations of mutant proteins. Have a few more models to run, then get results to JHU collaborators and get feedback. Using Panda; has been steady and reliable. VO Group in contact with Tim Miller and Maxim to track any resulting scientific publication.

   * *GEANT4* production accounting discrepancy (as reported in previous Area Coordinator meeting) in wasted wall hours. 50% wasted in OSG resource-view, 0% wasted in Geant4-view. Reason likely to be OSG-side issue: exit-code discrepancy due to Pilots. Remains an item of concern.

   * New VOs: Discussions with !DayaBay, HCC. PEGrid expected.

---+++ Continued general concerns pointed out by VOs

   * Looking for more advice on Globus 5's adoption timeline; for VOs preparedness.
   * Workload Management solutions and choices.
   * Opportunistic storage availability.
   * Accounting discrepancies - e.g., uncatched/mismatched exit codes, Pilots.
   * Lack of real-time job status monitoring.  
   * Lack of accuracy in advertisement of heterogenous subcluster parameters.
      * Fermilab VO implemented GIP configuration changes; working to get documentation; expected later this year.
   * Preemption/Eviction remains to be fully addressed | 
 [[https://twiki.grid.iu.edu/bin/view/VirtualOrganizations/PreemptionHandling_OSG][More background]].   

-- Main.AbhishekSinghRana - 08 Jun 2010
