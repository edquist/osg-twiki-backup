%META:TOPICINFO{author="IanStokesRees" date="1309460973" format="1.1" reprev="1.7" version="1.7"}%
%META:TOPICPARENT{name="Meetings"}%
---++ *VO and User Support Forum | June 30, 2011*

   * Thursdays 1:30 PM Central Time. 
   * Phone: 866-740-1260, ID: 2460335 

---++ Attending

<!-- Dan Bradley, Gabriele Garzoglio, Kyle Gross, Tanya Levshina, Marco Mambelli, John McGee, Chander Sehgal, Marko Slyz, Joel Snow,  Ian Stokes-Rees, Doug Strain, Marcia Teckenbrock, Steve Timm -->

---++ Previous Minutes 

[[VOGroupMeeting20110623][Weekly Forum: June 23]] 

---++ Announcements



---++  Presentations

---+++ Community Packaging (transition from Pacman to native packaging) -  Alain Roy

   * [[https://twiki.grid.iu.edu/bin/view/SoftwareTeam/CommunityPackagingProposal][Overview of Community Packaging Approach]]
   * [[https://twiki.grid.iu.edu/bin/view/SoftwareTeam/NPTransition][Native Packaging Transition]] (draft)
   * [[https://twiki.grid.iu.edu/bin/view/SoftwareTeam/RPMDevelopmentGuide][VDT RPM Development Guide]] (draft)


---+++ GLOW - Dan Bradley

---+++ !SBGrid - Ian Stokes-Rees

   * current usage, general status, and issues
      * operationally things are going well.  A few permission problems with UCSD (no response on ticket one week later) [[https://ticket.grid.iu.edu/goc/viewer?id=10612][#10612]].
      * we have jobs accounted as "SBGrid" and "sbgrid" - why? sbgrid
      * we ran 12k jobs last week (externally), requiring 33.6k wall-hours, but running at only 48% efficiency. THIS IS BAD!  Our jobs should be at >80% efficiency.  They are almost entirely computationally bound.
      * one of our clusters is sick (SBGrid-Harvard-East), and as it consists of 1.4 GHz AMD Opterons probably won't be revived if the CE looses its pulse.  It is small and only contributes a couple thousand hours of compute time/week in any case.
      * looks like QWCG-Harvard-OSG has been taken offline (maybe since early June), but if so it is news to me.  Maybe it just isn't running jobs or reporting them.
      * 4000 job peak at end of May, but usually we've been around 1000-1500 jobs.
      * glideinWMS Gratia probes should be installed soon.  Held up due to staffing shortages/changes and flux in responsibilities for OSG admin tasks.
      * we need accounting to split out real user utilization and success/failure, rather than clumping them all as frontend/glidein.nebiogrid.org
      * we're concerned about glideinWMS idle jobs and evictions which we get v. limited view of.

   * science that is being accomplished
      * X-ray structure determination by Wide Search Molecular Replacement portal
      * NMR model optimization by DEN refinement via portal
      * lung cancer drug delivery modelling

   * plans for the next few months (how you use OSG and changes inside the VO)
      * Getting Harvard Medical School 4000-job slot cluster connected (in or out) to OSG is still in progress.  Problems with network topology and firewall are hindering progress.

   * concerns and possible obstacles to implementing this roadmap
      * Condor, Globus, and OSG don't like NAT or firewalls.  They are a reality for us here.
      * We still struggle with X.509 certs for users, but are just about there with a fully-wrapped system that will mean users don't need them to use OSG.
      * glideinWMS is still too much of a blackbox for my liking.
      * why are we getting such low efficiency?

   * requirements for new OSG capabilities
      * data ACLs are important for us.  Knowing how this can work with SRM and BeStMan now and in the near future would be helpful.
      * user management is a big deal: VOMS does one side, GUMS another, and (for us) LDAP/FreeIPA a third.  Convergence of these would be great.  I assume that isn't going to happen, so just some good guidance on how to set things up quickly and easily and then to manage this system operationally would be really helpful.
      * user-driven sub-VOs (dynamic VOs) would be really helpful.


---++  Open Forum



---++ AOB








-- Main.MarciaTeckenbrock - 03 Jun 2011
