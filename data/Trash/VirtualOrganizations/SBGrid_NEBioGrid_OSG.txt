%META:TOPICINFO{author="KyleGross" date="1480625638" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="WebHome"}%
---+ SBGrid/NEBioGrid Production scale-up on OSG sites

Here are details based on our phone meeting Dec 16, discussions in weekly VO forums, and email exchanges in past few weeks.

We will pursue this further after holidays. Brian and Ian are in contact on 2 and 4. Mine has advised on 6 and 9.

Dan & Abhishek

-----
%TOC%


---++ 1. Production Scale Estimates

SBGrid has re-worked workflow to use Condor DAGMan, to release relatively short 5-15 minutes jobs from a pool of 100,000 jobs to OSG rather than group computations into sets of 100 and submit 1000 'grouped' jobs, that run for 8-24 hours each. In both cases the nature of the smallest computation is such that occasionally it will not converge on a solution and instead will take many hours. Need is to implement timeouts after 15 minutes.

<u>Immediate Goal</u> is to increase job success and wall efficiency, at sustained peak 3000 jobs running simultaneously. Try to increase to 6000 later. Target 1-2 times jobs queued as running. Underlying aim is to setup a long term infrastructure for SBGrid consortium users.

Approximate need is 10,000 - 20,000 hours for each run. Planning nearly 2 such runs per month, using OSG-MM.

   * October’09:  9 sites. 6,500 hours at 70% wall efficiency. 2,000 jobs at 92% success.
   * November’09:  30 sites. 90,000 hours at 62% wall efficiency. 35,000 jobs at 50% success.
   * December’09 week 1:  30 sites. 100,000 hours at 36% efficiency.13,000 jobs at 50% success.
   * December’09 week 2:  29 sites. 7,000 hours at 25% efficiency. 3,000 jobs at 67% efficiency.
   * December’09 week 3:  29 sites. 570 hours at 91% efficiency. 6,000 jobs at 97% efficiency.
   * [Problem was with longer jobs ~ 12 hours, and lack of monitoring. Now changed to smaller jobs ~ 0.5 hour]

---++ 2. Workflow Mgt of scientific applications

   * Site selection: Where to send jobs that will run 2h / 6h / 24h?
   * Job efficiency: Need to keep this high to maximize use of opportunistic wall hours across OSG sites.
   * Python at WNs: About one-third of OSG sites seem to have Python 2.3 in environment. Needed 2.4. Porting of applications ongoing to use older versions. Implemented a bash wrapper to work around. Not a long term solution.
   * Bash scripting issues: With use of OSG-MM, a side effect has been overuse of bash scripting. Trying to get away from using bash scripts. Local expertise on OSG-MM is Peter, with help from Mats as available.
   * How to get default path when jobs start at a site similar to !FermiGrid? Prefix bin areas e.g. /usr/bin/ then source OSG's setup script.
   * Application deployment Mgt: Which packages to bring along to sites? Short term: Bring Python as needed. Long term: This is a combination of need to build the right software, and to find the right mechanism for users to easily and correctly install the software into OSG_APP.
   * Job lifetime Mgt to return job to queue, or to put on hold. Try PERIODIC_HOLD expression to cancel the job from the submitter side.


---++ 3. Choice of WMS: OSG-MM or !GlideinWMS

General need is to have a document with comparative analysis of OSG-MatchMaker and !GlideinWMS. This can list differences, benefits, operational load.


---++4. Real-time Job Progress Monitoring

Need ability to see the stage of jobs, at any given time. Have adapted Condor Log Analyzer from D. Thain. But it is not fully helpful; parsing log files.

Have built custom tool which gives an aggregate view of status snapshots.

   * Code: http://abitibi.sbgrid.org/devel/projects/pycondor/pycondor
   * Depends on: http://abitibi.sbgrid.org/devel/projects/shex/shex
   * Example image: http://abitibi.sbgrid.org/se/xraid/ijstokes/phaser-full/tgase/mr/output-tgase/

General needs for Monitoring agreed by Brian and Ian:

   * Status monitoring: how many jobs are running, sitting in queue.
   * Grid level job debugging: status at WN. -- This is difficult.
   * Application level performance monitoring.
   * More visibility into why jobs are being killed, site preemption policies.


---++ 5. Preemption Handling

Need to get more visibility into the number of times a job is being preempted. E.g. Max wall-time in PBS and Condor. Long-term goal is to have ability to manage VO’s own queue, so that preferred classes of jobs are scheduled (reordered) accordingly. Can benefit from ability to predict likelihood of preemption at a site, based on old historical data on accounting.


---++ 6. Certificate Mgt for End-users (12/17/09 weekly VO Forum)

Need ability to apply for user certificates on behalf of end-users. It is possible, but may violate grid compliance standards. Use varying levels of CAs. Sites may not recognize the CA.


---++ 7. Site Infrastructure Mgt with Rocks

SBGrid site infrastructure has grown into 6 independent clusters, possibly with more on the way. These probably aren't going to be merged any time soon. Each is on its own subnet, administrative domain, completely independent, except that the same people are maintaining them. Moved to using Rocks 5.2 for the latest clusters, and have gained some good experience with this. Aware of other options such as Puppet and xCAT. Rocks latest version is poorly documented


---++ 8. Suggestions to improve Usability (11/20/09 VO Forum email discussion)

   * Improve user-focused job monitoring and metrics tools.
   * Consider if VMs are possible. It would be a huge help to us if we could package a few VMs and have those as job-prerequisites which are automatically launched by the OSG middleware as containers for our jobs.
   * Laptop-deployment: Can an OSG client (for job submission/retrieval) be setup on a laptop with only DHCP (no fixed IP or hostname), and no inbound network connections? This would be a big win for users and sites in our community.
   * Provide template guides for best practice of getting new users onto OSG and on how to prepare different classes of workflow/application for execution on OSG.


---++9. Long-term Data SecurityTeam and Privacy (11/10/09 Identity Mgt workshop)

Details available with Mine.


---++10. July 2010 Production Ramp Up using glideinWMS

We have now re-configured our systems to have a 24 GB RAM glideinWMS frontend which should be able to support >10k active jobs.  This is running Condor 7.5.3.  On Tuesday July 6th at 5pm EDT we have submitted two large DAGs (100k nodes each, expected run time of 10-30 minutes per DAG node).  This should allow us to avoid the queue draining problem and any authorization or file transfer overloads which can result from shorter jobs.  The DAGs have different priorities, so we can see if these are honored.  In 40 minutes we have ramped up to 1000 concurrent jobs across 9 sites.  These two DAGs should take 20-40k hours each to complete.

---++ Old News


---++ Accounting discrepancy

Some sites do not map SBGrid FQAN properly in Gratia configuration. Usage shows up as unknown or unrecognized.

---++ More background on past 6-8 weeks:  

Initially, jobs were targeted directly to sites. As a consequence, jobs remained sitting in queue for very long time. 1000 jobs, each runs 0.5 to 5-6 hours. Tried another application 5-100 times longer to complete. Similar in data analysis, but more complex algorithm, thus longer run time. Went back to submitting with OSG-MM. In first attempt, did not use extra features in OSG-MM of retrying jobs. As a result, log files could not be read and scoring algorithms didn't work well. So jobs got submitted again to sites where failure rate was high. 6000-8000 jobs 6-15 hours. 15-200 jobs got started, but reminder were queued - and remained in queue after timing out every 2 hours. Smallest scientific computation can complete in 0.5 to 2 minutes. 100,000 in 1 workflow. A different application 5 to 60 minutes. It may make sense to submit individual jobs. Main Problems: An inability to see the stage of jobs, at any given time. Condor Log Analyzer is not fully helpful; parsing log files. Preemption/Eviction pattern of jobs at some sites was not clear.

---++ GOC tickets:



-- Main.IanStokesRees - 06 Jul 2010