%META:TOPICINFO{author="AbhishekSinghRana" date="1260313419" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+ SBGrid !NEBioGrid Production scale-up on OSG sites

---++ Ongoing Summary
   * *Ticket:* https://ticket.grid.iu.edu/goc/viewer?id=7724
   * *Ticket:* https://ticket.grid.iu.edu/goc/viewer?id=7781
   * Python 2.4 required; discussion with Alain/OSG-Software; resolved.
   * Convergence needed on WMS: OSG !MatchMaker or !GlideinWMS. 
   * Other roadblocks: (a) possible accounting discrepancy, (b) lack of monitoring tools, (c) need for more OSG-MM expertise.
   * Phone meeting Dec 9th between Piotr, Ian, Abhishek, Dan, Mine, Igor.

---++ More Details (need prioritization)

We are regularly getting 700-2000 jobs running concurrently, and the jobs that run are going at 90% efficiency (i.e. they are number crunching).  High success rates are reported (but I don't believe these).  We're running on a number of different sites: UFL, UNL, CIT, FNAL, CCR, UCR, Cornell, and others.  I don't believe gratia reports at all (more below), and OSG lacks good user-oriented monitoring tools (also more below).

*Comments on issues we've had with OSG:*

The technical problems that we're having seem to stem more from our relative inexperience with the OSGMM and also changes to our workflow in the past few weeks which have pushed our single job run-time well over 12 hours.  A single protein structure study consists of 1000 such jobs, and we're trying to run a handful (10-20) of these structure studies at once.

Currently the OSG shortcomings we're facing are: the OSGMM does not seem to be especially well suited to our mode of computing (e.g. we'd like all of one structure study to complete before scheduling the next, and we'd like some mechanism to submit other jobs from our VO that get scheduled ahead of any of our structure study jobs).  We think glideinWMS may help with some of these, but are concerned about the complexity of setting it up -- we think we'd need to set aside 1 week to do this.

Other OSG shortcomings: we lack user-oriented monitoring tools to know what the current state OSG-accessible remote clusters are in to understand better what is going on with already-submitted jobs or to make decisions on job submission.  Similarly we lack user-oriented monitoring tools that give a good picture "now" of a large collection of jobs.  Take our structure study example: I'd like to know for each structure study a summary of what state the various jobs are in right now.  There is no good way to do this, so I am writing log parsing tools to give me this info.  Parsing Condor logs is not fun (multi-line entries, un-predictable format, meant to be read by humans not consumed by software).

Finally, when we do things like submit a batch of jobs that are going to take days to run we can do some "live" monitoring of their status but it is also commonly the case that we'll set things up and then a day or two or three later return to review what has happened.  There is no good way to see some kind of historical summary of the state of the jobs over a time window.  The Condor Log Analyzer, suitably adapted for grid jobs, may give us at least part of this, but certainly not the full picture.

*Comments on our production run of jobs on OSG:*

We have done much better over the past 2 weeks.  We're still learning how to use the OSGMM.  At the moment I'm going back to "direct targeting" of sites, since there are a few niggles we're working to resolve when we submit via the OSGMM.  John McGee has promised to get back to us on a list of questions, and Peter Doherty is increasing his understanding of how it works and how to configure it (as am I).

We had 34k wall-clock hours two weeks ago, and 56k wall-clock hours last week.  This is almost entirely due to my jobs, although we do have 3 other users who are submitting 100s to 1000s of hours of jobs (and both hope to ramp up as well in the next month or two).  As I mentioned above, we're still trying to understand all of the output of these jobs -- I am suspicious that a higher failure rate isn't reported.

Here is the gratia report for wall clock hours from 10/1 to the present (2 months):

%TABLE{ tablewidth="400" columnwidths="100%" cellpadding="4" dataalign="left" tablerules="all" tableborder="2" databg="#FFFFFF"}%
|!SiteName     |    !WallDuration|
|SBGrid-Harvard-East       |  2,631 |
|SBGrid-Harvard-Exp     |    3,843 |
|RENCI-Engagement      |   131 |
|NYSGRID-CCR-U2       |  21,146 |
|FNAL_FERMIGRID       |  2,614 |
|UCR-HEP     |    33,799 |
|NYSGRID_CORNELL_NYS1     |    57,080 |

This is certainly not complete -- we ran stacks of jobs at Caltech, University of Florida, Purdue, and UNL.  Beats me why they're missing from the gratia query.

One of the problems giving you a summary of results is that for some reason many of my jobs are not being matched to a VO properly -- I had what appear to be 5000 jobs with 55000 hours (55k) in November running at our sites alone that logged my DN properly, but didn't get the VO right (logged as "unknown").  There are also another 7700 jobs that took 39k hours across our 3 sites in November that have no DN associated with them.  These are probably jobs directly submitted to Condor (we use the same condor instance), however given the other problems with the logging, we can't be sure.  We don't know how many of our jobs running at other sites also had problems identifying the VO and/or DN (probably just the former).  I'm working now on some tools to extract this information as best possible from condor log files -- Doug  Thain from ND has given me his Condor Log Analyzer code to adapt for Condor-G logs.

Ian



-- Main.AbhishekSinghRana - 01 Dec 2009
