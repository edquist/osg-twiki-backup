%META:TOPICINFO{author="JeffDutton" date="1242666697" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *CompBioGrid* Experiences Blog<br>
%TOC%
---++ Local Site Architecture and Topology
%ATTACHURL%/UCHC_CBG-Topoplgy.jpg

*UCHC_CBG Site Topology*

*General comments:*  

The Center for Cell Analysis and Modeling at the University of Connecticut Health Center is the home of the Virtual Organization CompBioGrid and the Compute Element UCHC_CBG.   To support Open Science Grid operations at UCHC_CBG, we have created a network infrastructure that is separate from the institutional network with respect to access to the internet.  With the guidance and assistance of the University of Connecticut Network Engineer, we have purchased and set up a “Research DMZ” such that we have our own independent pipeline from our server room, to dedicated hardware that includes redundant firewalls, to the internet without using any other University of Connecticut Health Center network infrastructure.  This assists us in solving two problems.  First, we have our own 1GbE connection to the Internet that is not shared with any other entity within the institution.  This prevents other institutional traffic to the Internet from using this pipeline and similarly preventing OSG traffic from impacting the institutions pipeline to the Internet.  Second, by having our own firewalls we can control what ports are open to our internal servers without opening holes in the institutions firewall.

*Active Directory Integration*

All of the servers in our OSG infrastructure are integrated into our Microsoft Active Directory (AD) infrastructure.  Our AD infrastructure includes two different domains, ccam.uchc.edu and vcell.uchc.edu.  All of the OSG servers are all AD integrated.  This makes user management and management of ACL’s simple from an administrative point of view.  This integration has in part played a role in making the OSG Software Stack function properly (see below for specifics of the issue this presents).  In short, since we operate in a very diverse environment from the point of view of client operating systems connecting to our shared file system and some configuration changes on the shared file system conflicted with Condor and its operation in the OSG Software Stack. 

The figure representing our infrastructure is a simplified diagram distilled down to OSG relevant servers and network paths.  

*In the Researhc DMZ:*

VDTGATEWAY – This is where the CE is installed.  The server runs CentOS 5.1 x64 as the operating system.

VDGUMS – This is where GUMS us installed.  This server runs CentOS 5.1 (32-bit) and will be upgraded to CentOS 5.1 x64 in the future – at the same time we will transition to GUMS 1.3.

VDVOMS – This is where VOMRS is installed.  This server now runs CentOS 5.1 x64 as the operating system.

These are not the only Research DMZ servers.  There are also AD domain controllers for authentication and DNS servers among others.

*In the CCAM Server Room:*

OSGROCKS - We have an OSG dedicated cluster of 30 nodes with 60 CPU’s.  The cluster runs Rocks Clusters 4.3 (32-bit) with the head node as well as the compute nodes connecting to the shared file system.  The entire cluster is AD integrated with the head node acting as a slave NIS server to our VCELL primary NIS server.  All the compute nodes bind to the OSGROCKS head node instead of the primary NIS server.  
In the current architecture there is a 1GbE network connection from the OSGROCKS cluster public switch (the blue Cluster Switch) to the Core Switch for the sever room.  It is proposed that this change to either a 10GbE connection to the Clustered File System switch or a 10GbE connection to the Core Switch.  It **may** be advantageous to have a 10GbE connection to the shared file system switch versus through the core switch as another production cluster already utilizes the 10GbE connection from the Core Switch to the Clustered File System switch.

CLUSTERED FILE SYSTEM – the clustered File System is a system produced by Isilon.  It is a fourteen node IQ200 cluster with 2TB of storage per node.  Each node connects via a 1GbE connection to the clustered file systems 
switch.  New connections to the file system are handled in a round-robin fashion to each node in sequence to balance the load on the file system.

Issues with the shared file system – because of the diversity of client operating systems (Win/MAC/Linux) some patching to Samba was done by Isilon to make use of 64-bit time stamping.  The reason for this patch was to make the Isilon system time stamping and the AD time stamping compatible.  Without this patching, mirroring of files on the Isilon system to Windows-based systems meant every time the mirror job would run the entire job would run as if all files were new because of the time stamp incompatibility.  Patching Samba meant that only new files would be mirrored as it should be.     

---+++ Issues

---+++ Resolutions

---++ VCell Integration into OSG software stack

---+++ Issues

---+++ Resolutions

---++ Webpage Resources

---++ Etc.

%META:FILEATTACHMENT{name="UCHC_CBG-Topoplgy.jpg" attachment="UCHC_CBG-Topoplgy.jpg" attr="" comment="UCHC_CBG CE architecture" date="1242666257" path="UCHC_CBG-Topoplgy.jpg" size="159768" stream="UCHC_CBG-Topoplgy.jpg" tmpFilename="/usr/tmp/CGItemp13786" user="JeffDutton" version="1"}%
