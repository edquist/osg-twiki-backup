%META:TOPICINFO{author="JeffDutton" date="1249069130" format="1.1" version="1.18"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *CompBioGrid* Experiences Blog<br>
%TOC%
---++ Local Site Architecture and Topology
%ATTACHURL%/UCHC_CBG-Topoplgy.jpg

*General comments:*  

The Center for Cell Analysis and Modeling at the University of Connecticut Health Center is the home of the Virtual Organization <noWiki>CompBioGrid</noWiki> and the Compute Element UCHC_CBG.   To support Open Science Grid operations at UCHC_CBG, we have created a network infrastructure that is separate from the institutional network with respect to access to the internet.  With the guidance and assistance of the University of Connecticut Health Center Network Engineer, we have purchased and set up a “Research DMZ” such that we have our own independent pipeline from our server room, through dedicated hardware that includes redundant firewalls, to the internet without using any other University of Connecticut Health Center network infrastructure.  This assists us in solving two problems.  First, we have our own 1GbE connection to the Internet that is not shared with any other entity within the institution.  This prevents other institutional traffic to the Internet from using this pipeline and similarly preventing OSG traffic from impacting the institutions pipeline to the Internet.  Second, by having our own firewalls we can control what ports are open to our internal servers without opening holes in the institutions firewall.

*Active Directory Integration*

All of the servers in our OSG infrastructure are integrated into our Microsoft Active Directory (AD) infrastructure.  Our AD infrastructure includes two different domains, ccam.uchc.edu and vcell.uchc.edu.  This integration makes user management and file system management simple from an administrative point of view.  This integration has played a role in making the OSG Software Stack function improperly (see below for specifics of the issue this presents).  In short, we operate in a very diverse environment from the point of view of client operating systems connecting to our shared file system.  Some of the software patches supplied by the vendor (Isilon) to address a certain issue has been shown to conflict with Condor and its operation in the OSG Software Stack.  This issue that causes problems with Condor may also have implications for Globus as well but that remains to be confirmed.  *One other implication of AD integration is that there are no local accounts on any of the OSG servers.*  This may have an influence on error codes produced when components int he OSG stack fail and log the error.  It is unknown to the site administration at this point how error reporting would behave as a result of managing users in this schema.

The figure representing our infrastructure is a simplified diagram distilled down to OSG relevant servers and network paths.  

*In the Researhc DMZ:*

VDTGATEWAY – This is where the CE is installed.  The server runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

VDGUMS – This is where GUMS is installed.  This server runs <noWiki>CentOS</noWiki> 5.1 (32-bit) and will be upgraded to <noWiki>CentOS</noWiki> 5.1 x64 in the future – at the same time we will transition to GUMS 1.3.

VDVOMS – This is where VOMRS is installed.  This server now runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

(NOT SHOWN) VDTCLIENT1 - This is where the OSG client is installed for local <noWiki>CompBioGrid</noWiki> users to submit jobs from

These are not the only Research DMZ servers.  There are also AD domain controllers for authentication and DNS servers among others.

*In the CCAM Server Room:*

OSGROCKS - We have an OSG dedicated cluster of 30 nodes with 60 CPU’s.  The cluster runs Rocks Clusters 4.3 (32-bit) with the head node as well as the compute nodes connecting to the shared file system.  The entire cluster is AD integrated with the head node acting as a slave NIS server to our VCELL primary NIS server.  All the compute nodes bind to the OSGROCKS head node instead of the primary NIS server.  
In the current architecture there is a 1GbE network connection from the OSGROCKS cluster public switch (the blue Cluster Switch) to the Core Switch for the sever room.  It is proposed that this change to either a 10GbE connection to the Clustered File System switch or a 10GbE connection to the Core Switch.  It may be advantageous to have a 10GbE connection to the shared file system switch versus through the core switch as another production cluster already utilizes the 10GbE connection from the Core Switch to the Clustered File System switch.

CLUSTERED FILE SYSTEM – The clustered file system is a system produced by Isilon.  It is composed of fourteen IQ200 nodes with a capacity of 2TB per node.  Each node connects via a 1GbE connection to the clustered file systems public switch.  New connections to the file system are handled in a round-robin fashion to each node in sequence to balance the load on the file system.  There is also a private switch for restriping and other cluster maintenance functions that are performed "out of band" on the private network.

ISSUES WITH THE SHARED FILE SYSTEM – Because of the diversity of client operating systems (Win/MAC/Linux) some patching to Samba was done by Isilon to make use of 64-bit time stamping.  This is not an unusual feature as it has been present in more recent versions of Samba but not in the particular version included in the Isilon OneFS operating system at the version level we were running at the time.  The reason for this patch was to make the Isilon system time stamping and the AD time stamping compatible.  Without this patching, mirroring of files on the Isilon system to Windows-based systems meant every time the mirror job would run, the entire job would run as if all files were new because of the time stamp incompatibility.  Patching Samba meant that only new files would be mirrored as it should be.  A side effect of this patching is that if Condor does a stat call on a directory on the shared file system it can experience an unhandled exception that kills the parent process.  It has been determined that this is really a glibc issue.  Unless large file support is enabled at compliation this error can occur when running a process on a 32-bit Linux system and accessing the shared file system supporting 64-bit time stamps.  This issue can also be seen when trying to compile using gcc on the shared file system in a home directory or other shared directory while on a 32-bit machine.  Thus Condor or Globus as well as any other 32-bit application that is installed on a Linux box running a 32-bit operating system that has a Linux version more recent than say CentOS 4.3 or Scientific Linux 4.3 can experience this issue.  It seems that there may be some compliation at installation that makes it incompatible with the 64-bit file system when doing certain types of calls on the remote file system.  There are some symptoms that we have encountered.  When logging in via a gui interface on a 32-bit Linux box, a brief error will appear that reports the home directory does not exist and the session will immediately log back out.  We have seen instances when trying to compile using gcc on a 32-bit Linux box while sitting in a directory on the shared file system that an error is produced reporting the "value too large for defined data type".  There are two ways to get around this issue:  First, run a 32-bit Linux OS such as CentOS 4.3, Scientific Linux 4.3, or for clusters Rocks Clusters 4.3.  Second, we have run CentOS 5.1 and CentOS 5.2 64-bit operating systems and this behavior is not seen, as one might expect.  We wish to thank Alain Roy for his time and persistence in helping to determine the root cause of this issue.      

---+++ Issues

*09 Sep 2008* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=5533][5533]]*
   * ISSUE - Our VOMS server had a hardware failure.
   * RESOLUTION - The decision was made that the CE to be built would be run in full compatibility mode so a VOMRS server and GUMS sever were built and configured.  The issue was resolved on 27 Oct 2008.


*22 Jan 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought advice for installing the OSG:wn-client on a Rocks Cluster system
   * RESOLUTION - Advice was to install it on a shared file system that the Rocks compute nodes can access 


*04 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought information about local job managers, specifically if Condor needed to be installed for the OSG stack even it it wasn't to be used as a local job manager
   * RESOLUTION - Was advised that Condor would not need to be installed to support the OSG stack as a lite version of Condor would be installed for that purpose


*06 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - We had an issue with GUMS and the PRIMA callout with respect to the creation of correct supported VO text files
   * RESOLUTION - Determined that the local accounts were not created exactly as the OSG files contained for VO names - bad accounts were destroyed and recreated and the supported VO files were correct


*10 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE 1 - No matter what modifications to sudoers are made, when running configure-osg.py -c config.ini it always produces the error:  Modifications to the /etc/sudoers file are still required
   * RESOLUTION 1 - Was advised to ignore this - a feature request was put in to have the sudoers file checked for the proper modification to silence the error if the modification has been done

   * ISSUE 2 - When running configure-osg.py -c config.ini I always get the following error: You will need to restart the /etc/init.d/globus-ws container to effect the changes
   * RESOLUTION 2 - I was advised to ignore the errors sine vdt-control --on had not been executed yet so none of the services were running 

   * ISSUE 3 - When running configure-osg.py -c config.ini I get the following error:  Not defined: PER_JOB_HISTORY_DIR
   * RESOLUTION 3 - This was actually on the CE installation Twiki page - however, it was not clear what file to modify.  A documentation change was made for the Release and ITB documentation


*16 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE: Had trouble understanding the Release documantation process for configuring gip.conf for the correct PBS queues (vo's on whitelist - blacklist all other non-OSG queues)  
   * RESOLUTION: Received instruction on the proper configuration - remove all other gip.conf entries and just configure the PBS site entries


*18 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE 1:  Still having problems with RSV Probe authentication - this was the start of the process to identify our specific issue with the shared file system
   * RESOLUTION:  This was the result of the odd behavior between 32-bit systems accessing the 64-bit shared file system - once this was understood and fixed the problem cleared (there are more entries about this below)
 
   * ISSUE 2:  I was rceiving many errors in the $VDT_LOCATION/globus/var/container-real.log file about neing unable to read the log path in GLOBUS_LOCATION/etc/globus-pbs.conf.  That file didn't exist
   * RESOLUTION:  I was advised to create the file with the patch to the PBS logs


*05 Mar 2009* - *RESOLVED* - *GOC TICKET [[https://oim.grid.iu.edu/gocticket/viewer?id=6487][6487]]*
   * ISSUE:  Registered our site at the request of many OSG management members at the 2009 All Hands Meeting at LIGO.  Once registered we could not pass the site verify process
   * This issue was resolved once the odd behavior between 32-bit systems accessing the 64-bit shared file system were understood and corrected


*16 Mar 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE:  Reported more detailed info about the authentication issues gathered using different local job managers (fork, managed fork, pbs) to try and resolve this issue
   * RESOLUTION:  The 32-bit client to 64-bit shared file server bug was creating the odd behaviors - resolved once this was understood and fixed


*30 Mar 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE:  <noWiki>MonALISA</noWiki> was not reporting correct number of nodes on the <noWiki>MonALISA</noWiki> map for UCHC_CBG - it is understood that this is not a supported service
   * RESOLUTION:  One small change in the vdtFarm.conf file fixed the issue


*13 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6683][6683]]*
   * ISSUE - Our VOMS server in the VO Package is identified incorrectly from the hostname change back in September 2008 when the server was rebuilt and renamed
   * RESOLUTION - Rob Quick updated the VO package and it is deployed


*13 Apr 2009* - %BLUE% *ABANDONED* %ENDCOLOR% - *OSG-SITES* - *reference GOC ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6696][6696]]* 
   * ISSUE - Described in detail the behavior of PBS found during site validation - sought either corrective action to obtain better behavior or validation that the behavior is itself correct (behavior described below)
   * RESOLUTION - No advice received from the group

----

*CHARACTERIZING THE BEHAVIOR OF PBS*

Working through the steps to validate the Compute Element...

In the documentation for the “Simple Test of the Job Manager Queue” it shows the standard output came back to the client when using jobmanager-condor.  Using PBS, when the following command is run the output does not come back to the client but is dumped in the home directory the user is mapped to via GUMS and the output file has the filename of the form STDIN.pbs_job_ID.  The command runs is: 

globus-job-run vdgateway.vcell.uchc.edu/jobmanager-pbs /usr/bin/id 

There is no explanation if this is correct or incorrect output for this kind of job.

Further investigation let to the following:

The issue with the Client-side testing of WS-GRAM is that the simple command below never finishes:

globusrun-ws -submit -F vdgateway.vcell.uchc.edu:9443 -Ft PBS -c /bin/true

It seems that some process on the client-side is waiting for some other process to return from the PBS submitted job.  As a result, the submitted job never ends and because it is dumping nothing, in this case /bin/true, into the output the same situation is created as first described that I get a STDIN.xxxxx file where the xxxxx represents the PBS job ID.  In this case it makes an empty file.  And since it never finishes on its own, I have to cancel the job from the client side to get back to a bash prompt.

Retrieving the correct output file would involve the user somehow figuring out the correct PBS job ID to be able to fetch the results.  Not a simple thing unless the user has access to the PBS queue which would be undesireable. 

NOTE:  Additional info needs to be added once the ticket is resolved on the behavior of PBS with globus-job-submit, globus-job-status, and globus-job-get-output - This may be a place to improve the CE Validation documentation with regard to specific commands for sites with PBS as the local jobmanager.

----


*13 Apr 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - I was unabel to create a voms proxy 
   * RESOLUTION 1 - It was determined that the VO Package had a different hostname for our VOMS server - GOC Ticket was created to correct this (see GOC Ticket 6683 above)

   * RESOLUTION 2: Since the file system had caused so many issues to date it was suggested to reinstall on a known working OS our VOMS server.  This was done and the problem was resolved.


*16 Apr 2009* - %RED% *UNRESOLVED* %ENDCOLOR% - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6696][6696]]*
   * ISSUE: Posed the same question to the GOC as what was sent above to OSG-SITES about PBS behavior and difficulties dealing with job result collection
   * RESOLUTION:


Summary of Activites - as of 31 July 2009

1.  Initial description of PBS job behavior was submitted by Jeff Dutton  (16 Apr 2009)

2.  Kyle at the GOC reposds that he is looking into the issue  (17 Apr 2009)

3.  Additional notes were appended to the ticket with addition details about the behavior when using globusrun-ws by Jedd Dutton (20 apr 2009)

4.  Additional info on behavior and logged events were sought by the GOC  (24 Apr 2009)

5.  Additional info including the behaviors requested were added to the ticket as well as log entry info by Jeff Dutton (24 Apr 2009)

6.  Kyle responded that he had been seeking a PBS expert at PSU that had recently departed and was sucessful in enlisting help (12 May 2009)

7.  Shreyas informed me about the proper way to submit PBS jobs (*Note:  this information should be included in the CE Validation process).   (12 May 2009)

8.  The methods for jobmanager PBS and jobmanager fork that Shreyas sent were tested and results sent to the GOC via a ticket update - sought logging info location to proceed (27 May 2009)

9.  Location of config files was sent to point to the log locations - this may mean enabling some logging temporarily.  (27 May 2009)

10. Info on finding logging locations was sent by Shreyas (28 May 2009)

11.  The log files were located and the logging of sucessful jobs was enabled - testing began (29 May 2009) 

12.  A characteristic set of logs for a job submission, a globus-get-job-output job, and the relevant entries in the globus-gatekeeper log were put on our web site and the ticket updated - the logs are too big and wrapping confuses things in the ticket if pasted into the body of ticket entries.  The location of the files:  http://dropbox.vcell.uchc.edu/OSG/  (02 June 2009)

13.  Kyle checked up on the ticket - no recent action (08 June 2009)

14.  Shreyas reviewed the logs and has a hypothesis about what is causing our trouble.  Additional information was requested about the PBS version we are using and the backend batch scheduler.  (08 June 2009)

15.  The ticket was updated with our PBS version (PBS Pro v9.2.0 for 32-bit systems) and we requested addtional info on the meaning of backend batch scheduler.  

16.  The version number alone will be sufficient to help compare error codes so work will continue on the GOC side of the ticket looking at the log entries submitted. (09 June 2009)

17.  The PBS configuration for our CE used our production PBS set-up of a pair of redundant servers and a FlexLM license server.  OSG jobs submitted to this server has been identified as causing production jobs to experience issues in both the running state and queue state with timeouts.  The PBS server and all clients were updated to see if this would resolve the issue and it didn't.  (mid-June 2009)

18.  A stand-alone PBS server was set up for OSG jobs with its own queue.  The client was also updated on the cluster to the new version consistent with the server.  After debugging it produced the same results in running behavior as before.  A side benefit is that GIP is now reporting correct CPU counts, the only queue in production as opposed to all queues from the previous set-up, and available slots. (mid-June 2009)

20.  CE was reinstalled on 30 July 2009 updating it to OSG 1.2 along with a GUMS and VOMS reinstasllation to the new version as well.  CE validation will start on 03 Aug 2009.


*20 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6717][6717]]*
   * ISSUE:   Detailed data is missing on VORS 
   * RESOLUTION: With the pending deprication of VORS and the implementation of <noWiki>MyOSG</noWiki> this problem was deemed to be irrelevant and no resources will be assigned.  

  *20 Apr 2009* - %BLUE% *ABANDONED* %ENDCOLOR% - *OSG-SITES* - *reference GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6733][6733]]* 
   * ISSUE 1:   Working through GIP Validation I found errors and missing information 
   * RESOLUTION 1: Advice receved was to re-run config-osg.py to populate the missing information

   * ISSUE 2: GIP LDIF info is not being shown on <noWiki>MyOSG</noWiki>
   * RESOLUTION 2:  None


*23 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6733][6733]]*
   * ISSUE:  Same as above - GIP LDIF data not showing on <noWiki>MyOSG</noWiki>
   * RESOLUTION:  Fixing reverse DNS configuration issues resolved the inability to pull data from our CE.  Once the data was bneing pulled, it required some assistance to correct the mis-information.  Burt Holzman on 
         OSG-SITES was very helpful in providing fixes to get the data correct.


*08 June 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6992][6992]]*
   * ISSUE:  We are not showing up on daily RSV reports as down
   * RESOLUTION:  Reverse DNS was working at internally to resolve our CE hostname from its IP, but not externally.  The Instituions Network Engineer made some configuration changes and the problem was resolved.

*31 July 2009* - %RED% *UNRESOLVED* %ENDCOLOR% - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=7257][7257]]*
   * ISSUE:  CE was reinstalled on 30 July 2009 and while we show greebn for al local RSV probes, <noWiki>MyOSG</noWiki> shows almost all metrics are expired.  
 


---++ VCell Integration into OSG software stack

---+++ Issues

---+++ Resolutions

---++ Webpage Resources

---++ Etc.

%META:FILEATTACHMENT{name="UCHC_CBG-Topoplgy.jpg" attachment="UCHC_CBG-Topoplgy.jpg" attr="" comment="UCHC_CBG CE architecture" date="1242666257" path="UCHC_CBG-Topoplgy.jpg" size="159768" stream="UCHC_CBG-Topoplgy.jpg" tmpFilename="/usr/tmp/CGItemp13786" user="JeffDutton" version="1"}%
