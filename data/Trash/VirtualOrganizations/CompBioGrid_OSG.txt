%META:TOPICINFO{author="RobQ" date="1243000114" format="1.1" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *CompBioGrid* Experiences Blog<br>
%TOC%
---++ Local Site Architecture and Topology
%ATTACHURL%/UCHC_CBG-Topoplgy.jpg

*General comments:*  

The Center for Cell Analysis and Modeling at the University of Connecticut Health Center is the home of the Virtual Organization <noWiki>CompBioGrid</noWiki> and the Compute Element UCHC_CBG.   To support Open Science Grid operations at UCHC_CBG, we have created a network infrastructure that is separate from the institutional network with respect to access to the internet.  With the guidance and assistance of the University of Connecticut Health Center Network Engineer, we have purchased and set up a “Research DMZ” such that we have our own independent pipeline from our server room, through dedicated hardware that includes redundant firewalls, to the internet without using any other University of Connecticut Health Center network infrastructure.  This assists us in solving two problems.  First, we have our own 1GbE connection to the Internet that is not shared with any other entity within the institution.  This prevents other institutional traffic to the Internet from using this pipeline and similarly preventing OSG traffic from impacting the institutions pipeline to the Internet.  Second, by having our own firewalls we can control what ports are open to our internal servers without opening holes in the institutions firewall.

*Active Directory Integration*

All of the servers in our OSG infrastructure are integrated into our Microsoft Active Directory (AD) infrastructure.  Our AD infrastructure includes two different domains, ccam.uchc.edu and vcell.uchc.edu.  This integration makes user management and file system management simple from an administrative point of view.  This integration has in part played a role in making the OSG Software Stack function improperly (see below for specifics of the issue this presents).  In short, we operate in a very diverse environment from the point of view of client operating systems connecting to our shared file system and some of the software patches supplied by the vendor to address a certain issue conflicted with Condor and its operation in the OSG Software Stack.   

The figure representing our infrastructure is a simplified diagram distilled down to OSG relevant servers and network paths.  

*In the Researhc DMZ:*

VDTGATEWAY – This is where the CE is installed.  The server runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

VDGUMS – This is where GUMS is installed.  This server runs <noWiki>CentOS</noWiki> 5.1 (32-bit) and will be upgraded to <noWiki>CentOS</noWiki> 5.1 x64 in the future – at the same time we will transition to GUMS 1.3.

VDVOMS – This is where VOMRS is installed.  This server now runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

(NOT SHOWN) VDTCLIENT1 - This is where the OSG client is installed for local <noWiki>CompBioGrid</noWiki> users to submit jobs from

These are not the only Research DMZ servers.  There are also AD domain controllers for authentication and DNS servers among others.

*In the CCAM Server Room:*

OSGROCKS - We have an OSG dedicated cluster of 30 nodes with 60 CPU’s.  The cluster runs Rocks Clusters 4.3 (32-bit) with the head node as well as the compute nodes connecting to the shared file system.  The entire cluster is AD integrated with the head node acting as a slave NIS server to our VCELL primary NIS server.  All the compute nodes bind to the OSGROCKS head node instead of the primary NIS server.  
In the current architecture there is a 1GbE network connection from the OSGROCKS cluster public switch (the blue Cluster Switch) to the Core Switch for the sever room.  It is proposed that this change to either a 10GbE connection to the Clustered File System switch or a 10GbE connection to the Core Switch.  It may be advantageous to have a 10GbE connection to the shared file system switch versus through the core switch as another production cluster already utilizes the 10GbE connection from the Core Switch to the Clustered File System switch.

CLUSTERED FILE SYSTEM – the clustered File System is a system produced by Isilon.  It is a fourteen node IQ200 cluster with 2TB of storage per node.  Each node connects via a 1GbE connection to the clustered file systems switch.  New connections to the file system are handled in a round-robin fashion to each node in sequence to balance the load on the file system.

ISSUES WITH THE SHARED FILE SYSTEM – because of the diversity of client operating systems (Win/MAC/Linux) some patching to Samba was done by Isilon to make use of 64-bit time stamping.  This is not an unusual feature as it has been present in more recent versions of Samba but not inthe particular version supplied at the operating system level we were running at the time.  The reason for this patch was to make the Isilon system time stamping and the AD time stamping compatible.  Without this patching, mirroring of files on the Isilon system to Windows-based systems meant every time the mirror job would run the entire job would run as if all files were new because of the time stamp incompatibility.  Patching Samba meant that only new files would be mirrored as it should be.  A side effect of this patching is that if Condor, or any other application, does a stat call on a directory on the shared file system it can experience an unhandled exception that kills the parent process.  This is really a glibc issue.  Unless large file support is enabled at compliation this error can occur when running a process on a 32-bit Linux system and accessing the shared file system supporting 64-bit time stamps.       

---+++ Issues

*09 Sep 2008* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=5533][5533]]*
   * ISSUE - Our VOMS server had a hardware failure.
   * RESOLUTION - The decision was made that the CE to be built would be run in full compatibility mode so a VOMRS server and GUMS sever were built and configured.  The issue was resolved on 27 Oct 2008.


*22 Jan 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought advice for installing the OSG:wn-client on a Rocks Cluster system
   * RESOLUTION - Advice was to install it on a shared file system that the Rocks compute nodes can access 


*04 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought information about local job managers, specifically if Condor needed to be installed for the OSG stack even it it wasn't to be used as a local job manager
   * RESOLUTION - Was advised that Condor would not need to be installed to support the OSG stack as a lite version of Condor would be installed for that purpose


*06 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - We had an issue with GUMS and the PRIMA callout with respect to the creation of correct supported VO text files
   * RESOLUTION - Determined that the local accounts were not created exactly as the OSG files contained for VO names - bad accounts were destroyed and recreated and the supported VO files were correct


*10 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE 1 - No matter what modifications to sudoers are made, when running configure-osg.py -c config.ini it always produces the error:  Modifications to the /etc/sudoers file are still required
   * RESOLUTION 1 - Was advised to ignore this - a feature request was put in to have the sudoers file checked for the proper modification to silence the error if the modification has been done

   * ISSUE 2 - When running configure-osg.py -c config.ini I always get the following error: You will need to restart the /etc/init.d/globus-ws container to effect the changes
   * RESOLUTION 2 - I was advised to ignore the errors sine vdt-control --on had not been executed yet so none of the services were running 

   * ISSUE 3 - When running configure-osg.py -c config.ini I get the following error:  Not defined: PER_JOB_HISTORY_DIR
   * RESOLUTION 3 - This was actually on the CE installation Twiki page - however, it was not clear what file to modify.  A documentation change was made for the Release and ITB documentation


*TO BE CONTINUED - JEFF DUTTON*


*13 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6683][6683]]*
   * ISSUE - Our VOMS server in the VO Package is identified incorrectly from the hostname change back in September 2008 when the server was rebuilt and renamed
   * RESOLUTION - Rob Quick updated the VO package and it is deployed



---++ VCell Integration into OSG software stack

---+++ Issues

---+++ Resolutions

---++ Webpage Resources

---++ Etc.

%META:FILEATTACHMENT{name="UCHC_CBG-Topoplgy.jpg" attachment="UCHC_CBG-Topoplgy.jpg" attr="" comment="UCHC_CBG CE architecture" date="1242666257" path="UCHC_CBG-Topoplgy.jpg" size="159768" stream="UCHC_CBG-Topoplgy.jpg" tmpFilename="/usr/tmp/CGItemp13786" user="JeffDutton" version="1"}%
