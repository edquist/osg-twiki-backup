%META:TOPICINFO{author="ElizabethChism" date="1466010162" format="1.1" version="1.22"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *CompBioGrid* Experiences Blog<br>
%TOC%
---++ Local Site Architecture and Topology
%ATTACHURL%/UCHC_CBG-Topoplgy.jpg

*General comments:*  

The Center for Cell Analysis and Modeling at the University of Connecticut Health Center is the home of the Virtual Organization <noWiki>CompBioGrid</noWiki> and the Compute Element UCHC_CBG.   To support Open Science Grid operations at UCHC_CBG, we have created a network infrastructure that is separate from the institutional network with respect to access to the internet.  With the guidance and assistance of the University of Connecticut Health Center Network Engineer, we have purchased and set up a “Research DMZ” such that we have our own independent pipeline from our server room, through dedicated hardware that includes redundant firewalls, to the internet without using any other University of Connecticut Health Center network infrastructure.  This assists us in solving two problems.  First, we have our own 1GbE connection to the Internet that is not shared with any other entity within the institution.  This prevents other institutional traffic to the Internet from using this pipeline and similarly preventing OSG traffic from impacting the institutions pipeline to the Internet.  Second, by having our own firewalls we can control what ports are open to our internal servers without opening holes in the institutions firewall.

*Active Directory Integration*

All of the servers in our OSG infrastructure are integrated into our Microsoft Active Directory (AD) infrastructure.  Our AD infrastructure includes two different domains, ccam.uchc.edu and vcell.uchc.edu.  This integration makes user management and file system management simple from an administrative point of view.  This integration has played a role in making the OSG Software Stack function improperly (see below for specifics of the issue this presents).  In short, we operate in a very diverse environment from the point of view of client operating systems connecting to our shared file system.  Some of the software patches supplied by the vendor (Isilon) to address a certain issue has been shown to conflict with Condor and its operation in the OSG Software Stack.  This issue that causes problems with Condor may also have implications for Globus as well but that remains to be confirmed.  *One other implication of AD integration is that there are no local accounts on any of the OSG servers.*  This may have an influence on error codes produced when components int he OSG stack fail and log the error.  It is unknown to the site administration at this point how error reporting would behave as a result of managing users in this schema.

The figure representing our infrastructure is a simplified diagram distilled down to OSG relevant servers and network paths.  

*In the Researhc DMZ:*

VDTGATEWAY – This is where the CE is installed.  The server runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

VDGUMS – This is where GUMS is installed.  This server runs <noWiki>CentOS</noWiki> 5.1 (32-bit) and will be upgraded to <noWiki>CentOS</noWiki> 5.1 x64 in the future – at the same time we will transition to GUMS 1.3.

VDVOMS – This is where VOMRS is installed.  This server now runs <noWiki>CentOS</noWiki> 5.1 x64 as the operating system.

(NOT SHOWN) VDTCLIENT1 - This is where the OSG client is installed for local <noWiki>CompBioGrid</noWiki> users to submit jobs from

These are not the only Research DMZ servers.  There are also AD domain controllers for authentication and DNS servers among others.

*In the CCAM Server Room:*

OSGROCKS - We have an OSG dedicated cluster of 30 nodes with 60 CPU’s.  The cluster runs Rocks Clusters 4.3 (32-bit) with the head node as well as the compute nodes connecting to the shared file system.  The entire cluster is AD integrated with the head node acting as a slave NIS server to our VCELL primary NIS server.  All the compute nodes bind to the OSGROCKS head node instead of the primary NIS server.  
In the current architecture there is a 1GbE network connection from the OSGROCKS cluster public switch (the blue Cluster Switch) to the Core Switch for the sever room.  It is proposed that this change to either a 10GbE connection to the Clustered File System switch or a 10GbE connection to the Core Switch.  It may be advantageous to have a 10GbE connection to the shared file system switch versus through the core switch as another production cluster already utilizes the 10GbE connection from the Core Switch to the Clustered File System switch.

CLUSTERED FILE SYSTEM – The clustered file system is a system produced by Isilon.  It is composed of fourteen IQ200 nodes with a capacity of 2TB per node.  Each node connects via a 1GbE connection to the clustered file systems public switch.  New connections to the file system are handled in a round-robin fashion to each node in sequence to balance the load on the file system.  There is also a private switch for restriping and other cluster maintenance functions that are performed "out of band" on the private network.

ISSUES WITH THE SHARED FILE SYSTEM – Because of the diversity of client operating systems (Win/MAC/Linux) some patching to Samba was done by Isilon to make use of 64-bit time stamping.  This is not an unusual feature as it has been present in more recent versions of Samba but not in the particular version included in the Isilon OneFS operating system at the version level we were running at the time.  The reason for this patch was to make the Isilon system time stamping and the AD time stamping compatible.  Without this patching, mirroring of files on the Isilon system to Windows-based systems meant every time the mirror job would run, the entire job would run as if all files were new because of the time stamp incompatibility.  Patching Samba meant that only new files would be mirrored as it should be.  A side effect of this patching is that if Condor does a stat call on a directory on the shared file system it can experience an unhandled exception that kills the parent process.  It has been determined that this is really a glibc issue.  Unless large file support is enabled at compliation this error can occur when running a process on a 32-bit Linux system and accessing the shared file system supporting 64-bit time stamps.  This issue can also be seen when trying to compile using gcc on the shared file system in a home directory or other shared directory while on a 32-bit machine.  Thus Condor or Globus as well as any other 32-bit application that is installed on a Linux box running a 32-bit operating system that has a Linux version more recent than say CentOS 4.3 or Scientific Linux 4.3 can experience this issue.  It seems that there may be some compliation at installation that makes it incompatible with the 64-bit file system when doing certain types of calls on the remote file system.  There are some symptoms that we have encountered.  When logging in via a gui interface on a 32-bit Linux box, a brief error will appear that reports the home directory does not exist and the session will immediately log back out.  We have seen instances when trying to compile using gcc on a 32-bit Linux box while sitting in a directory on the shared file system that an error is produced reporting the "value too large for defined data type".  There are two ways to get around this issue:  First, run a 32-bit Linux OS such as CentOS 4.3, Scientific Linux 4.3, or for clusters Rocks Clusters 4.3.  Second, we have run CentOS 5.1 and CentOS 5.2 64-bit operating systems and this behavior is not seen, as one might expect.  We wish to thank Alain Roy for his time and persistence in helping to determine the root cause of this issue.      

---+++ Issues

*09 Sep 2008* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=5533][5533]]*
   * ISSUE - Our VOMS server had a hardware failure.
   * RESOLUTION - The decision was made that the CE to be built would be run in full compatibility mode so a VOMRS server and GUMS sever were built and configured.  The issue was resolved on 27 Oct 2008.


*22 Jan 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought advice for installing the OSG:wn-client on a Rocks Cluster system
   * RESOLUTION - Advice was to install it on a shared file system that the Rocks compute nodes can access 


*04 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - Sought information about local job managers, specifically if Condor needed to be installed for the OSG stack even it it wasn't to be used as a local job manager
   * RESOLUTION - Was advised that Condor would not need to be installed to support the OSG stack as a lite version of Condor would be installed for that purpose


*06 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - We had an issue with GUMS and the PRIMA callout with respect to the creation of correct supported VO text files
   * RESOLUTION - Determined that the local accounts were not created exactly as the OSG files contained for VO names - bad accounts were destroyed and recreated and the supported VO files were correct


*10 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE 1 - No matter what modifications to sudoers are made, when running configure-osg.py -c config.ini it always produces the error:  Modifications to the /etc/sudoers file are still required
   * RESOLUTION 1 - Was advised to ignore this - a feature request was put in to have the sudoers file checked for the proper modification to silence the error if the modification has been done

   * ISSUE 2 - When running configure-osg.py -c config.ini I always get the following error: You will need to restart the /etc/init.d/globus-ws container to effect the changes
   * RESOLUTION 2 - I was advised to ignore the errors sine vdt-control --on had not been executed yet so none of the services were running 

   * ISSUE 3 - When running configure-osg.py -c config.ini I get the following error:  Not defined: PER_JOB_HISTORY_DIR
   * RESOLUTION 3 - This was actually on the CE installation Twiki page - however, it was not clear what file to modify.  A documentation change was made for the Release and ITB documentation


*16 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE: Had trouble understanding the Release documantation process for configuring gip.conf for the correct PBS queues (vo's on whitelist - blacklist all other non-OSG queues)  
   * RESOLUTION: Received instruction on the proper configuration - remove all other gip.conf entries and just configure the PBS site entries


*18 Feb 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE 1:  Still having problems with RSV Probe authentication - this was the start of the process to identify our specific issue with the shared file system
   * RESOLUTION:  This was the result of the odd behavior between 32-bit systems accessing the 64-bit shared file system - once this was understood and fixed the problem cleared (there are more entries about this below)
 
   * ISSUE 2:  I was rceiving many errors in the $VDT_LOCATION/globus/var/container-real.log file about neing unable to read the log path in GLOBUS_LOCATION/etc/globus-pbs.conf.  That file didn't exist
   * RESOLUTION:  I was advised to create the file with the patch to the PBS logs


*05 Mar 2009* - *RESOLVED* - *GOC TICKET [[https://oim.grid.iu.edu/gocticket/viewer?id=6487][6487]]*
   * ISSUE:  Registered our site at the request of many OSG management members at the 2009 All Hands Meeting at LIGO.  Once registered we could not pass the site verify process
   * This issue was resolved once the odd behavior between 32-bit systems accessing the 64-bit shared file system were understood and corrected


*16 Mar 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE:  Reported more detailed info about the authentication issues gathered using different local job managers (fork, managed fork, pbs) to try and resolve this issue
   * RESOLUTION:  The 32-bit client to 64-bit shared file server bug was creating the odd behaviors - resolved once this was understood and fixed


*30 Mar 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE:  <noWiki>MonALISA</noWiki> was not reporting correct number of nodes on the <noWiki>MonALISA</noWiki> map for UCHC_CBG - it is understood that this is not a supported service
   * RESOLUTION:  One small change in the vdtFarm.conf file fixed the issue


*13 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6683][6683]]*
   * ISSUE - Our VOMS server in the VO Package is identified incorrectly from the hostname change back in September 2008 when the server was rebuilt and renamed
   * RESOLUTION - Rob Quick updated the VO package and it is deployed


*13 Apr 2009* - %BLUE% *ABANDONED* %ENDCOLOR% - *OSG-SITES* - *reference GOC ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6696][6696]]* 
   * ISSUE - Described in detail the behavior of PBS found during site validation - sought either corrective action to obtain better behavior or validation that the behavior is itself correct (behavior described below)
   * RESOLUTION - No advice received from the group

----

*CHARACTERIZING THE BEHAVIOR OF PBS*

Working through the steps to validate the Compute Element...

In the documentation for the “Simple Test of the Job Manager Queue” it shows the standard output came back to the client when using jobmanager-condor.  Using PBS, when the following command is run the output does not come back to the client but is dumped in the home directory the user is mapped to via GUMS and the output file has the filename of the form STDIN.pbs_job_ID.  The command runs is: 

globus-job-run vdgateway.vcell.uchc.edu/jobmanager-pbs /usr/bin/id 

There is no explanation if this is correct or incorrect output for this kind of job.

Further investigation let to the following:

The issue with the Client-side testing of WS-GRAM is that the simple command below never finishes:

globusrun-ws -submit -F vdgateway.vcell.uchc.edu:9443 -Ft PBS -c /bin/true

It seems that some process on the client-side is waiting for some other process to return from the PBS submitted job.  As a result, the submitted job never ends and because it is dumping nothing, in this case /bin/true, into the output the same situation is created as first described that I get a STDIN.xxxxx file where the xxxxx represents the PBS job ID.  In this case it makes an empty file.  And since it never finishes on its own, I have to cancel the job from the client side to get back to a bash prompt.

Retrieving the correct output file would involve the user somehow figuring out the correct PBS job ID to be able to fetch the results.  Not a simple thing unless the user has access to the PBS queue which would be undesireable. 

NOTE:  Additional info needs to be added once the ticket is resolved on the behavior of PBS with globus-job-submit, globus-job-status, and globus-job-get-output - This may be a place to improve the CE Validation documentation with regard to specific commands for sites with PBS as the local jobmanager.

----


*13 Apr 2009* - *RESOLVED* - *OSG-SITES*
   * ISSUE - I was unabel to create a voms proxy 
   * RESOLUTION 1 - It was determined that the VO Package had a different hostname for our VOMS server - GOC Ticket was created to correct this (see GOC Ticket 6683 above)

   * RESOLUTION 2: Since the file system had caused so many issues to date it was suggested to reinstall on a known working OS our VOMS server.  This was done and the problem was resolved.


*16 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6696][6696]]*
   * ISSUE: Posed the same question to the GOC as what was sent above to OSG-SITES about PBS behavior and difficulties dealing with job result collection
   * RESOLUTION:


Summary of Activites - as of 31 July 2009

1.  Initial description of PBS job behavior was submitted by Jeff Dutton  (16 Apr 2009)

2.  Kyle at the GOC reposds that he is looking into the issue  (17 Apr 2009)

3.  Additional notes were appended to the ticket with addition details about the behavior when using globusrun-ws by Jedd Dutton (20 apr 2009)

4.  Additional info on behavior and logged events were sought by the GOC  (24 Apr 2009)

5.  Additional info including the behaviors requested were added to the ticket as well as log entry info by Jeff Dutton (24 Apr 2009)

6.  Kyle responded that he had been seeking a PBS expert at PSU that had recently departed and was sucessful in enlisting help (12 May 2009)

7.  Shreyas informed me about the proper way to submit PBS jobs (*Note:  this information should be included in the CE Validation process).   (12 May 2009)

8.  The methods for jobmanager PBS and jobmanager fork that Shreyas sent were tested and results sent to the GOC via a ticket update - sought logging info location to proceed (27 May 2009)

9.  Location of config files was sent to point to the log locations - this may mean enabling some logging temporarily.  (27 May 2009)

10. Info on finding logging locations was sent by Shreyas (28 May 2009)

11.  The log files were located and the logging of sucessful jobs was enabled - testing began (29 May 2009) 

12.  A characteristic set of logs for a job submission, a globus-get-job-output job, and the relevant entries in the globus-gatekeeper log were put on our web site and the ticket updated - the logs are too big and wrapping confuses things in the ticket if pasted into the body of ticket entries.  The location of the files:  http://dropbox.vcell.uchc.edu/OSG/  (02 June 2009)

13.  Kyle checked up on the ticket - no recent action (08 June 2009)

14.  Shreyas reviewed the logs and has a hypothesis about what is causing our trouble.  Additional information was requested about the PBS version we are using and the backend batch scheduler.  (08 June 2009)

15.  The ticket was updated with our PBS version (PBS Pro v9.2.0 for 32-bit systems) and we requested addtional info on the meaning of backend batch scheduler.  

16.  The version number alone will be sufficient to help compare error codes so work will continue on the GOC side of the ticket looking at the log entries submitted. (09 June 2009)

17.  The PBS configuration for our CE used our production PBS set-up of a pair of redundant servers and a FlexLM license server.  OSG jobs submitted to this server has been identified as causing production jobs to experience issues in both the running state and queue state with timeouts.  The PBS server and all clients were updated to see if this would resolve the issue and it didn't.  (mid-June 2009)

18.  A stand-alone PBS server was set up for OSG jobs with its own queue.  The client was also updated on the cluster to the new version consistent with the server.  After debugging it produced the same results in running behavior as before.  A side benefit is that GIP is now reporting correct CPU counts, the only queue in production as opposed to all queues from the previous set-up, and available slots. (mid-June 2009)

20.  CE was reinstalled on 30 July 2009 updating it to OSG 1.2 along with a GUMS and VOMS reinstasllation to the new version as well.  CE validation will start on 04 Aug 2009.

21.  Noticed same job submission problems during CE validation as before.  Will finish CE validation of non-PBS related services and refresh the info on the GOC ticket when finished.

22.  Attempted to change the local jobmanager to Condor - due to architecture issues Condor will not work as our local jobmanager - back to PBS (Sept 2009).

23.  CE was reinstalled with PBS as the jobmanager on Sep 11, 2009.  the site functions exactly as it has been but users (Engage VO and SBGRID VO) have been successfull in using the site as it is.  

24.  On Oct 7, 2009 the ticket was closed as it seems unlikely that any further time expenditures will resolve the issues and VO's are running jobs successfully.


*20 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6717][6717]]*
   * ISSUE:   Detailed data is missing on VORS 
   * RESOLUTION: With the pending deprication of VORS and the implementation of <noWiki>MyOSG</noWiki> this problem was deemed to be irrelevant and no resources will be assigned.  

  *20 Apr 2009* - %BLUE% *ABANDONED* %ENDCOLOR% - *OSG-SITES* - *reference GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6733][6733]]* 
   * ISSUE 1:   Working through GIP Validation I found errors and missing information 
   * RESOLUTION 1: Advice receved was to re-run config-osg.py to populate the missing information

   * ISSUE 2: GIP LDIF info is not being shown on <noWiki>MyOSG</noWiki>
   * RESOLUTION 2:  None


*23 Apr 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6733][6733]]*
   * ISSUE:  Same as above - GIP LDIF data not showing on <noWiki>MyOSG</noWiki>
   * RESOLUTION:  Fixing reverse DNS configuration issues resolved the inability to pull data from our CE.  Once the data was bneing pulled, it required some assistance to correct the mis-information.  Burt Holzman on 
         OSG-SITES was very helpful in providing fixes to get the data correct.


*08 June 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=6992][6992]]*
   * ISSUE:  We are not showing up on daily RSV reports as down
   * RESOLUTION:  Reverse DNS was working at internally to resolve our CE hostname from its IP, but not externally.  The Instituions Network Engineer made some configuration changes and the problem was resolved.

29 July 2009 - The latest VOMRS release was installed and configured - no problems

30 July 2009 - The latest GUMS release was installed and configured - no problems

*31 July 2009* - *RESOLVED* - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=7257][7257]]*
   * ISSUE:  CE was reinstalled on 30 July 2009 and while we show greebn for al local RSV probes, <noWiki>MyOSG</noWiki> shows almost all metrics are expired.  
   * RESOLUTION:  pyOpenSSL was installed on the CE and the problem was solved 

04 Aug 2009 - Started CE validation - running version OSG 1.2

09 Sep 2009 - CE put into production - Engage and SBGrid VO's actively running jobs.  CompBioGrid test jobs are also running successfully.


 *21 Dec 2009* - %RED% *UNRESOLVED* %ENDCOLOR% - *GOC Ticket [[https://oim.grid.iu.edu/gocticket/viewer?id=7870][7870]]*
   * ISSUE:  An attempt was made to upgrade the CE from OSG 1.2.3 to 1.2.4 and the update failed.  Jobs submitted after the update attempt were receiving GRAM 12 and GRAM 47 errors.  
   * RESOLUTION:  

1.  Firewall on the CE is disabled - all jobs run reasonably fine with some GRAM 10 errors and "whoami" call failures

2.  Under the old architecture, all OSG related servers (CE, VOMS, GUMS), the PBS server, and all compute nodes, used a NIS server within the VCELL server room that required the OSG servers to cross the DMZ and enter the UCHC internal network.  This seems to be creating potential NIS timouts.  The CE itself was made a NIS slave and the PBS server (also the cluster headnode) was made a NIS slave.  We no longer see whoami call timouts.

3.  To further reduce latency in network calls, the CE and PBS server were made caching-nameservers and had the network services caching daemon enabled.  Still there can be as many as 50% job submission failure rate when submittiong 3X the number of job slots on the cluster.  

4.  There is also a VDT support ticket for this issue:   [[http://crt.cs.wisc.edu/Ticket/Display.html?user=guest&pass=guest&id=6196][6196]]

5.  Enabling the firewall causes immediate GRAM 12 errors to appear again.

6.  The ticket was updated on 12 Feb 2010 and further instructions on diagnosing the GRAM 10 errors or in any other way resolving this issue are eagerly anticipated.



---++ VCell Trash/Integration into OSG software stack

Summer of 2009 - Summer Intern activities

1. A summer intern started a project for automating site selection for VCELL jobs being submitted to sites supporting CompBioGrid.  

2.  The intern managed to get the project ot the point where OSG <noWiki>MatchMaker</noWiki> was implemented, in combination with some LDAP queries that he automatically parsed, to rank sites in an automated manner for site selection.

3.  This project is on hold as <noWiki>MyOSG</noWiki> continues to evolve and there are other potentially easier solutions for site selection.



November 2009 - Mine Altunay, Ion Moraru, and Jeff Dutton participated in an email exchange to sort out the proper use of DOEGrids CA certificates for Vcell application services.  it has been decided that we can use DOEGrids service certificates and that it would be an appropriate use of DOEGrids certificates for this purpose.  A number of issues were clarified:

1.  Traceability - Since we already have generic services on our own production cluster and can trace back any activity to a user that must log in with username and password, we have the ability to identify any individual with a submitted OSG job.  

2.  Isolation - any VCELL user that authenticates and uses the Virtual Cell can not alter any executables nor any data that would be used/created on an OSG site.  In fact, the user has no knowledge where current jobs are physically run nor where the software and data resides.

3.  Remote site administrator concerns -  Should there be an issue with any/all VCELL submitted job(s), a site administrator would see a the jobs as being submitted as a single user - this is perfectly acceptable.  Any jobs that have issues are the result of a problem with the executable created by our tightly controlled process.  Should there be an issue, such as a bug, that is introduced into any/all jobs would be a problem and the site administrators ability to deny one DN from running jobs could be an advantgage for the remote site.

4.  Redundant services - It is deemd that there are no issues having multiple certs per service for the sake of redundancy.



---+++ Issues

---+++ Resolutions

---++ Webpage Resources

---++ Etc.

 January 2010

1.  A graduate student at the main UCONN campus at Storrs became a <noWiki>CompBioGrid</noWiki> member and completed a project comparing an RNA sequence against a database.  The project resulting in 1,962 submitted jobs that ran for 4,182 hours.  The student is already planning another project that can utilize OSG resources.  This data will be going into a publication and the proper way to acknoledge the Open Science Grid was provided to the student.  If/when publication takes place the citation will be forwared to OSG Communications.  The success of this project is prompting us to reach out to other researchers in the UCONN Health Center (and potentially Storrs) for membership in <noWiki>CompBioGrid</noWiki> and development of grid-enabled applications within other research labs.  This should take place in February 2010.

%META:FILEATTACHMENT{name="UCHC_CBG-Topoplgy.jpg" attachment="UCHC_CBG-Topoplgy.jpg" attr="" comment="UCHC_CBG CE architecture" date="1242666257" path="UCHC_CBG-Topoplgy.jpg" size="159768" stream="UCHC_CBG-Topoplgy.jpg" tmpFilename="/usr/tmp/CGItemp13786" user="JeffDutton" version="1"}%
