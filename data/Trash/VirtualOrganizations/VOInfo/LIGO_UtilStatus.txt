%META:TOPICINFO{author="MichaelSamidi" date="1163022509" format="1.1" reprev="1.13" version="1.13"}%
%META:TOPICPARENT{name="WebHome"}%
---++++ Update on running full-blown LIGO workflow at several production clusters as of November 8 2006

LIGO just completed running a full-blown HIPE workflow at Purdue_ITaP. This workflow has 78004 DAG nodes and roughly 700 GB of input gravitational-wave files (GWF). However, Purdue_ITaP has only about 125 GB of available disk space prior to running the workflow. Therefore, we've incorporated several techniques such as workflow partitioning (Pegasus) and cleanup of input GWFs that are no longer needed (cleanup wrapper) so the workflow can be run in such a small disk space.

Besides Purdue_ITaP, LIGO is still waiting for the completion of the same workflow running at the following sites:

    * osg-gw-2_t2_ucsd_edu (UCSD)

    * OSG_LIGO_PSU

    * UWMilwaukee

---++++ Update on Atlas T2 site utilization as of September 12 2006

After fixing sites table, LIGO binary inspiral workflow was completed successfully at BU_ATLAS_Tier2 and UC_ATLAS_MWT2.

---++++ Update on Atlas T2 site utilization as of September 7 2006 (from Michael Samidi)

UT Arlington (atlas.dpcc.uta.edu) is running !OSG 0.2.1. !OSG must be upgraded to 0.4.1 in order to run !LIGO workflow (binary inspiral) at this site. 

U Indiana (bandicoot.uits.indiana.edu) - Failed running <verbatim> globus-job-run bandicoot.uits.indiana.edu/jobmanager-pbs /usr/bin/id </verbatim>
Notified Operations.

Brookhaven (gridgk02.racf.bnl.gov) - Misconfiguration. In !GridCat, the value for $DATA is <i>/usatlas/prodjob/share/</i>, but from
reading log file, the value is <i>/direct/usatlas+prodjob/share</i>. Contacted site admin, no response so far.

UC_ATLAS_MWT2 has only about 140 GB available space. A !LIGO job is sub-partitioned but the first partition requires at least 220 GB. Unfortunately, this particular LIGO workflow (HIPE) can only be partitioned into 6 parts. We're working with VDS developers to increase the number of partitions.

BU_ATLAS_Tier2 reports the following error: 

<i>
9/7 12:07:22 ERROR: aborting DAG because of bad event (BAD EVENT: job (14811.0.0) ended, total end count != 1 (2))
</i>

The source of this error is not understood.

---++++ Update on September 5 2006

All ATLAS T2 sites except OU have passed my authentication test.

<verbatim>
Brookhaven (gridgk02.racf.bnl.gov) - Passed
U Indiana (bandicoot.uits.indiana.edu) - Passed                                         
U Chicago (tier2-osg.uchicago.edu) - Passed                                     
UT Arlington (atlas.dpcc.uta.edu) - Passed
Boston U (atlas.bu.edu) - Passed
Oklahoma U (ouhep0.nhn.ou.edu) - NO LIGO support                                         
</verbatim>

Further test on the advertised job manager in GridCat, only U Indiana has failed.
For example, 
[msamidi@osg-itb-se msamidi]$ globus-job-run bandicoot.uits.indiana.edu/jobmanager-pbs /usr/bin/id
Permission denied, please try again.
Permission denied, please try again.
Permission denied (publickey,password).

The current list:

<verbatim>
U Indiana (bandicoot.uits.indiana.edu) - Failed
Brookhaven (gridgk02.racf.bnl.gov) - Passed                                        
U Chicago (tier2-osg.uchicago.edu) - Passed                                     
UT Arlington (atlas.dpcc.uta.edu) - Passed
Boston U (atlas.bu.edu) - Passed
Oklahoma U (ouhep0.nhn.ou.edu) - NO LIGO support                                         
</verbatim>

---++++ Update on September 4 2006

In response to the Atlas [[http://listserv.fnal.gov/scripts/wa.exe?A1=ind0608&L=osg-users][invitation]] (message 3) to VOs to exploit Atlas sites within the next two weeks, !LIGO is testing authentication on Atlas T2's. Here is the current status:

<verbatim>
Brookhaven (gridgk02.racf.bnl.gov) - Passed                                     
Boston U (atlas.bu.edu) - Failed                                                
U Indiana (bandicoot.uits.indiana.edu) - Passed                                 
Oklahoma U (ouhep0.nhn.ou.edu) - Failed                                         
U Chicago (tier2-osg.uchicago.edu) - Passed                                     
UT Arlington (atlas.dpcc.uta.edu) - Failed                                      
</verbatim>

!LIGO submitted a !GOC ticket to address this issue.

---++++Status as of August 8 2006

There are regular meetings between !LIGO and !VDS on Tuesday mornings 10am !PST.

Most sites now report the absolute path to the worker node location correctly. A new site-verify script is ready. It checks for kickstart-manager and other things specific to !VDS tools that !LIGO has been using. The script will be in !OSG-0.5.0. They are looking into partitioning !LIGO jobs to make it easier for sites to satisfy their storage requirements.

---++++Status as of July 11 2006

The current status of !OSG sites utilization by !LIGO is summarized here: [[%ATTACHURL%/LIGO_OSG_experience.html][LIGO_OSG_experience.html]]

!LIGO is running at 6 sites at the moment: Nebraska, Purdue-ITaP, UFlorida-IHEPA, UFlorida-PG, UWMilwaukee, and OSG_LIGO_PSU.

Here is a list of major problems encountered by !LIGO in the process of job submissions to various !OSG sites:

   * !LIGO needs to rework their applications to make them runnable on !OSG sites. !LIGO apps use interfaces historically developed for the !LIGO Data Grid and different from !OSG. Also, !LIGO apps were designed under assumption that all data are available at each site and this is not the case for !OSG sites.

   * The main technical challenge is data transfer. A !LIGO application which uses only 1-2% of available !LIGO data requires 5 !GB minimum. !CE storage is typically insufficient for !LIGO needs. !LIGO would benefit from uniform deployment of !SRM/DRM or !SRM/dcache through !OSG sites.

   * Information in !GridCat is not being maintained as !OSG production systems evolve. This forces !LIGO to monitor !OSG sites on their own. They rely on vds-get-sites, a !VDS workflow planning tool, which has been poorly supported in the recent past. In particular, it does not keep track of possible worker-node client configurations in an !OSG installation. !LIGO is planning to have regular phone meetings with !VDS to work out this issue.

   * Monalisa reported !LIGO jobs incorrectly at Nebraska - much fewer than expected. A !GOC ticket was filed and quickly resolved; now Nebraska seems to be counting their jobs correctly.

At the moment, !LIGO has just a few (2 or 3) members who are interested in running the binary inspiral application on !OSG sites. In principle, the computing needs of !LIGO members are satisfied for now by the !LIGO Data Grid.

!LIGO is working to reengineer two new applications to run on the !OSG with
the goal of carrying our scientifically meaningful runs in the next couple of weeks to months.

-- Main.IlyaNarsky - 12 Jul 2006

%META:FILEATTACHMENT{name="LIGO_OSG_experience.html" attr="h" comment="" date="1152664123" path="LIGO_OSG_experience.html" size="15048" user="IlyaNarsky" version="1.1"}%
