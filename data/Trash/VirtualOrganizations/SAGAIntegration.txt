%META:TOPICINFO{author="KyleGross" date="1476285863" format="1.1" version="1.3"}%
%META:TOPICPARENT{name="WebHome"}%
%TOC%

---+!! SAGA Interface to OSG

---++ Introduction.

SAGA and !BigJob are job management environments for multiple scientific communities. Historically, SAGA has interfaced to !TeraGrid / XSEDE resources. This is a proof-of-principle effort to demonstrate the feasibility of interfacing SAGA with OSG resources for those jobs that best fit the HDTC paradigm.

In our discussions, we have identified 2 main types of jobs that might fit this paradigm
   1. Jobs with limited I/O requirements (1 GB): these jobs are the first target for the communities using the OSG / XSEDE submission infrastructure. See the [[https://www.xsede.org/osg-user-guide][Open Science Grid User Guide for XSEDE]]  
   1. Job with larger I/O requirements (10-100 GB): we have a couple of techniques to pre-stage the data to sites for these jobs: (a) *Squid* caching: data is uploaded to a VO web server and accessed by the jobs via http through Squid; (b) [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/VirtualOrganizations/IRODSOSG][*Public Storage Management*]]: OSG User Support is deploying an infrastructure for limited production to manage public storage at sites using iRODS; data can be uploaded to iRODS and replicated to the OSG_DATA area at sites; jobs can then access the data via a semi-posix (hadoop/Fuse) or posix interface.

We believe that following application can fit these requirements.

---++ Candidate Application
Here's a quick introduction and rundown of the genome sequence alignment
application which we are trying to get working on OSG. Please forward this
to the appropriate people in OSG application support to get the process
started. 

In a typical sequence alignment scenario, a large number of short read files
(generated by sequencing machines) need to be matched agains the same
reference genome. The read files are usually stored by the sequencing
machines on a laboratory's storage server or in a shared data archive that
is accessible by multiple parties. The same is the case for the large
reference genomes. Reference genomes range from a few Gigabytes for a single
chromosome up to more than 100 Gigabyte for the full human genome. Short
read files are much smaller in size, around a few hundred Megabyte per file,
depending on the capabilities of the sequencing machine.

We are currently using a short-read aligner called 'BFAST'
(http://sourceforge.net/apps/mediawiki/bfast/index.php?title=Main_Page)
which is really just a statically linked executable which compiles without
any problems on OSG. We have prepared an experimental dataset and made it
available for download:

(1) A ~ 2GB reference genome:
http://gw68.quarry.iu.teragrid.org/experiments-SC12/data/pstar/reference/.
All files (except for the tar file which is just an archive of all files for
easier download) in this directory make up the reference genome (chromosome
21) and need to be on accessible on the machine(s) were BFAST executes.

(2) A bunch (32) of 170 MB short read files:
http://gw68.quarry.iu.teragrid.org/experiments-SC12/data/pstar/reads_170M.
These files need to be matched agains the reference genome one by one, i.e.,

<pre>
for i in number_of_shortreads
    BFAST(Ref, short_read_i)
</pre>

Typically, there are more than 32 short read files, O(1000) is not uncommon.

BFAST is executed on the command line using the following parameters:

./bfast match -f $refData -A 1 -r $readData -n1 > output.bmf

The memory requirement is somewhat dependent on the size and partitioning of
the reference genome. For above input data, it is about 1.5 GB. For a full
human reference genome, the memory requirement can go up to 6-8 GB.
Execution time varies from ~ 5 minutes (for the dataset above) to 30 minutes
for a full reference genome (obviously, dependent on the hardware). 

We tried to submit the individual BFAST tasks to glidein-WMS wrapped in a
small script. The script either downloaded the data (read + ref!) via HTTP
or SRM and executed BFAST. Obviously this is really inefficient, since the
download often takes much longer than the task execution itself. We tried to
find a way to share reference data between jobs that end up executing on the
same machine, but we did not succeed. Also, jobs often hang or failed (see
our report), but this might have been a problem with the RENCI OSG gateway. 



-- Main.GabrieleGarzoglio - 26 Jul 2012