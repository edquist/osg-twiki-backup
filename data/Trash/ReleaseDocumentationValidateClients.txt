%META:TOPICINFO{author="KyleGross" date="1223293926" format="1.1" version="1.13"}%
%META:TOPICPARENT{name="ValidatingComputeElement"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

Note:  This page refers to the ldapsearch command.  The ldapsearch command is not part of the standard OSG installation, it is commonly
on many standard linux installations. If you do not have it and would like to get it
from the VDT it is part of the VDT OpenLDAP package that can be pulled using pacman
and refer to the VDT Package 'OpenLDAP.'

%STARTINCLUDE%
%BR%
---+ _%INCLUDEHEADING%  %SPACEOUT{ "%TOPIC%" }%_
%EDITTHIS%
%BR%

---++ OSG Client

The OSG Client package provides a set of tools useful for user-access to Grid services.  Below are a series of test you can do after installing the client software via the ClientInstallationGuide and sourcing the =setup.(c)sh= script to test the operability of the software.

---+++ Obtain a Grid Proxy: *grid-proxy-init* and *voms-proxy-init*

In the following examples it is assumed that your =usercert.pem= and =userkey.pem= files are in =$HOME/.globus/=. If not, you will need to specify them explicitly via "-cert" and "-key" arguments to the xxxx-proxy-init commands.

   * Obtain a grid-proxy via =grid-proxy-init= <pre class=screen>
$ grid-proxy-init
<b>
Your identity: /DC=org/DC=doegrids/OU=People/CN=XXX XXXXX #####
Enter GRID pass phrase for this identity:
Creating proxy ....................................... Done
Your proxy is valid until: Wed Aug 22 04:57:38 2007
</b>
</pre>

   * Obtain a voms-proxy via =voms-proxy-init=:<pre class=screen>
$ voms-proxy-init -voms osg
<b>
Cannot find file or dir: /home/condor/execute/dir_14135/userdir/glite/etc/vomses
Enter GRID pass phrase:
Your identity: /DC=org/DC=doegrids/OU=People/CN=XXX XXXXX #####
Cannot find file or dir: /home/condor/execute/dir_14135/userdir/glite/etc/vomses
Creating temporary proxy ................................ Done
Contacting  voms.opensciencegrid.org:15027 [/DC=org/DC=doegrids/OU=Services/CN=host/voms.opensciencegrid.org] "osg" Done
Creating proxy .................................... Done
Your proxy is valid until Tue Oct 30 23:32:54 2007
</b>
</pre>

<b>Note</b>: the apparent error message ==Cannot find file or dir: /home/condor/.....== is not an error but an artifact of the software build used in the VDT.  You should ignore that message.

---+++ Querying Resources  

The client tools include utilities to query GRID resources. Examples here used to test the client are directed the OSG-ITB repositories.

   * Query ReSS <pre class=screen>condor_status -pool osg-ress-4.fnal.gov -format '%s\n' GlueSiteName | uniq
<b>CIT_ITB_1
ITB_INSTALL_TEST_2
ITB_INSTALL_TEST_3
CMS-BURT-ITB
...</b>
</pre>
   * Query BDII (see ValidateBDII documentation) <pre class=screen>ldapsearch -x -LLL -p 2170 -h is-itb.grid.iu.edu -b mds-vo-name=LBNL_VTB,mds-vo-name=local,o=grid
<b>dn: mds-vo-name=LBNL_VTB,mds-vo-name=local,o=grid
objectClass: GlueTop
 
dn: GlueSEUniqueID=osp1.lbl.gov,mds-vo-name=LBNL_VTB,mds-vo-name=local,o=grid
...</b>
</pre>
   * Query by LCG tools (<b>NOTE</b> =lcg-info= and =lcg-infosites= are not in ones path by default):<pre class=screen> $VDT_LOCATION/lcg/bin/lcg-info --list-ce --bdii is-itb.grid.iu.edu:2170 --vo osg
<b>- CE: cithep201.ultralight.org:2119/jobmanager-condor-osg
- CE: cms-xen1.fnal.gov:2119/jobmanager-condor-osg
....</b>

$VDT_LOCATION/lcg/bin/lcg-info --list-se --bdii is-itb.grid.iu.edu:2170 --vo osg
<b>- SE: cit-se.ultralight.org
- SE: cms-xen1.fnal.gov
...</b> 
 
$VDT_LOCATION/lcg/bin/lcg-infosites --vo osg -f ce  --is is-itb.grid.iu.edu 
<b>valor del bdii: is-itb.grid.iu.edu:2170
#CPU    Free    Total Jobs      Running Waiting ComputingElement
----------------------------------------------------------
  40      40       0              0        0    osp1.lbl.gov:2119/jobmanager-pbs-batch
   2       0       0              0        0    tb10.grid.iu.edu:2119/jobmanager-condor-osg
1042       2       0              0        0    itb.rcac.purdue.edu:2119/jobmanager-condor-osg
...</b>

$VDT_LOCATION/lcg/bin/lcg-infosites --vo osg -f se  --is is-itb.grid.iu.edu 
<b>Avail Space(Kb) Used Space(Kb)  Type    SEs
----------------------------------------------------------
125             2               n.a     osp1.lbl.gov
28824440        10638628        n.a     tb10.grid.iu.edu
37              242             n.a     itb.rcac.purdue.edu
...</b>
</pre>

---+++ Simple Client Jobs

Some of the examples here are also documented in CESimpleTest with a emphasis on observiny the CE response via the log files.  Here we simply note the response you should observe from the client.  It is assumed that you have sourced the =$VDT_LOCATION/setup.(c)sh= script , have a valid grid-proxy, and are aware of resources available to you. 

----++++ Submit a remote job: =globus-job-run=

To the default (fork) queue:

<pre class=screen>
$globus-job-run osp1.lbl.gov/jobmanager /bin/hostname
<b>osp1.lbl.gov</b>
</pre>

To the local batch system queue: (specific syntax depends on what batch system is deployed [Condor|PBS|LSF|SGE])

<pre class=screen>
$globus-job-run osp1.lbl.gov/jobmanager-pbs /bin/hostname
<b>osp3.lbl.gov</b>
</pre>

----++++ File tranfer: =globus-url-copy,srmcp,uberftp=

Copy a file using globus-url-copy from a local system to a remote gsiftp server.

<pre class=screen>
$ls -l /tmp/testdata_10.dat
<b>-rw-r--r--  1 qwerty qwerty 2121000 Nov 15 11:25 /tmp/testdata_10.dat</b>

$globus-url-copy file:///tmp/testdata_10.dat gsiftp://osp4.lbl.gov:2811/tmp/testdata_destination.dat
</pre>

Now copy it back and compare:

<pre class=screen>
$globus-url-copy gsiftp://osp4.lbl.gov:2811/tmp/testdata_destination.dat  file:///tmp/testdata_return.dat
$ls -l /tmp/testdata_return.dat
<b>-rw-r--r--  1 qwerty qwerty 2121000 Nov 15 11:27 /tmp/testdata_return.dat</b>

$diff /tmp/testdata_10.dat /tmp/testdata_return.dat
</pre>

You can repeat the above tests between two remote gsiftp servers using the syntax:
<pre class=screen>
$globus-url-copy  gsiftp://server1:2811/filesystem/filename gsiftp://server2:2811/filesystem2/filename2
</pre>

You can repeat the above tests using =srmcp= instead of =globus-url-copy=. In addition you can access a remote SRM service explicitly. For example:

<pre class=screen>
$srmcp gsiftp://osp4.lbl.gov:2811/tmp/testdata.dat srm://osg-itb.ligo.caltech.edu:8643/pnfs/ligo.caltech.edu/data/star/testdata_dest.dat
</pre>

You may also use the "-debug" flag which will generate diagnostic messages.  Also, <b>srm-v2-client</b> package is included in the OSG-client installation but the software is not added to ones path by default. 

To test =uberftp=, simply point to a known resource:

<pre class=screen>$ uberftp osg-itb.ligo.caltech.edu
<b>220 osg-itb.ligo.caltech.edu GridFTP Server 2.5 (gcc32dbg, 1182369948-63) ready.
230 User qwerty logged in.
</b>
</pre>


----++++ Submit a remote job to WS GRAM: =globusrun-ws=

Basic submission using WS GRAM is done with =globusrun-ws= analgous to =globus-job-run= with GRAM.   A wide range of tests are shown in the  ValidateGramWebServices documentation.  Here is a basic test to verify that the client tools are in order.

<pre class=screen>

$globusrun-ws -submit -F osp1.lbl.gov:9443 -Ft Fork -s -c /bin/hostname
<b>
Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:7bfd4324-93b7-11dc-8f33-00304889ddce
Termination time: 11/16/2007 20:15 GMT
Current job state: Active
Current job state: CleanUp-Hold
osp1.lbl.gov
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.
</b>
</pre>

---+++ *Condor-G submission*

OSG Client includes a condor package for use as a Condor-G submit host. To test this service 

   * verify service is running:<pre class=screen> $condor_q
 
-- Submitter: qwerty@osp3.lbl.gov : <128.3.30.238:59969> : osp3.lbl.gov
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
 
0 jobs; 0 idle, 0 running, 0 held
</pre>

   * prepare submit file for either GRAM or WS-GRAM <pre class=screen> $cat test.submit
<b>#specify resource destination
Universe            = grid
grid_resource       = gt4 https://hostname:9443 [Condor|PBS|SGE]

# specify executable
Executable          = /bin/hostname
Transfer_Executable = false
 
# copy stdout stderr to local files, referenced by job (Process) and submission id (Cluster)
output  =  mytest.out.$(Cluster).$(Process)
error   =    mytest.err.$(Cluster).$(Process)
#a single local log file for tracking Condor-g submission
log    = mytest.log
 
# do not send email notification
notification=Never

# submit 2 identical jobs: Process=0 and Process=1
Queue 2</b>
</pre>
   * Submit job <pre class=screen>$condor_submit test.submit 
<b>Submitting job(s)..
Logging submit event(s)..
2 job(s) submitted to cluster 51.</b>
</pre>
   * monitor jobs <pre class=screen>$condor_q
  
<b>-- Submitter: qwerty@osp3.lbl.gov : <128.3.30.238:59969> : osp3.lbl.gov
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  51.0   qwerty         11/15 12:29   0+00:00:00 I  0   9.8  hostname          
  51.1   qwerty         11/15 12:29   0+00:00:00 I  0   9.8  hostname          
  52.0   qwerty         11/15 12:29   0+00:00:15 R 0   0.0  gridftp_wrapper.sh
 
3 jobs; 2 idle, 1 running, 0 held</b>
</pre>
   * verify jobs completed <pre class=scree>$cat mytest.out.51.0
<b>osp3.lbl.gov</b>
</pre>


---++ Worker node client

The worker node client is a subset of client tools assumed to be needed, not by an interactive user, but by a batch job which lands on a worker node.  These tools can be checked via the methods listed above where applicable.  To validate one should test the job-submission tools (=globus-job-run= and =globusrun-ws=) and the data transfer tools (=globus-url-copy=, =srmcp= and =uberftp=) as described for the OSG Client package.



%STOPINCLUDE%
%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.JeffPorter - 25 Oct 2007 %BR%
%REVIEW%

%META:TOPICMOVED{by="RobGardner" date="1193339124" from="Integration/ITB_0_7.ValClients" to="Integration/ITB_0_7.ValidateClients"}%
