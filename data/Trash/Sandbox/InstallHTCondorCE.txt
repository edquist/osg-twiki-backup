%META:TOPICINFO{author="BrianLin" date="1418682194" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! Installing the HTCondor CE

%TOC{depth="2"}%

---# About this Document

This document is for System Administrators. It covers the installation of the HTCondor-CE software, which aims to provide an end-to-end gatekeeper technology built entirely out of core HTCondor components.  As a goal, we aim for the HTCondor-CE to be a particular "configuration" of HTCondor, and not include any non-HTCondor daemons.

This document follows the general OSG documentation conventions: %TWISTY{%TWISTY_OPTS_OUTPUT% showlink="Click to expand document conventions..."}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%
%ENDTWISTY%

---# How to get Help?
To get assistance please use the [[Documentation.HelpProcedure][this page]].

---# Requirements

---## Host and OS
   * A host to install the Compute Element
   * OS is %SUPPORTED_OS%
   * Root access

---## Users

%STARTSECTION{"Users"}%

The following users are needed by HTCondor-CE at all sites
| *User* | *Comment* |
| =condor= | The HTCondor-CE will be run as root, but perform most of its operations as the =condor= user. |
| =gratia= | Runs the Gratia probes to collect accounting data |

The above user will be added to the system automatically when the HTCondor RPM installs.  If your fabric management automatically overwrites users and groups, you will want to create this user beforehand.

%ENDSECTION{"Users"}%

---## Certificates
| *Certificate* | *User that owns certificate* | *Path to certificate* |
| Host certificate | =root= | =/etc/grid-security/hostcert.pem= <br> =/etc/grid-security/hostkey.pem= |

Find instructions to request a host certificate [[Documentation/Release3.GetHostServiceCertificates][here]].

---## Networking

%STARTSECTION{"Firewalls"}%
%INCLUDE{"Documentation/Release3/FirewallInformation" section="FirewallTable" lines="htcondorce,htcondorce_shared"}% 

Allow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.</br>

%ENDSECTION{"Firewalls"}%

---## Additional Requirements
To be part of the OSG Production Grid, your CE must be registered in OIM. To register your resource:
   * Use your user certificate.  Find instructions to request a user certificate [[Documentation.CertificateUserGet][here]].
   * Register in OIM as described in Operations.OIMRegistrationInstructions

---# Installation
%NOTE% We currently distribute the HTCondor-CE only in the OSG 3.2 repository; make sure you have used the appropriate repositories.
%INCLUDE{"Documentation/Release3.YumRepositories" section="OSGRepoBrief" TOC_SHIFT="+"}%
%INCLUDE{"Documentation/Release3.InstallCertAuth" section="OSGBriefCaCerts" TOC_SHIFT="+"}%

---## Installing HTCondor CE and Related Software

%NOTE% If you are updating from an older version of the [[SoftwareTeam/WebHome][latest OSG release]], it is recommended that you update to the latest software with =yum update= before proceeding with the installation. At a minimum, you should have =condor= >= 8.0.3 and the latest version of =osg-configure=.

<p>A complete CE installation contains the job gateway itself (the HTCondor CE job router) and a variety of other support software (such as a Gratia probe, the Squid proxy server, and OSG Configure). To simplify installation, there are special RPMs that bring along all required packages, based on your batch system. Pick one install command from the table below:</p>

%TABLE{sort="off"}%
| *If your batch system is…* | *Then run the following command…* |
| HTCondor | =yum install osg-ce-condor= |
| PBS | =yum install osg-ce-pbs= |
| SLURM | =yum install osg-ce-pbs gratia-probe-slurm= |
| LSF | =yum install osg-ce-lsf= |
| SGE | =yum install osg-ce-sge= |

%NOTE% To smooth the transition between GRAM-based CE&rsquo;s and HTCondor CE&rsquo;s, we are currently shipping both CE&rsquo;s in the =osg-ce-*= packages. The GRAM software will eventually be dropped but is still currently included. The instructions below will show you how to disable GRAM and enable HTCondor CE.

%NOTE% With the release of version 1.6-2, HTCondor CE will begin reporting your site&rsquo;s availability and capabilities back to the OSG for reporting.

---# Configuration

   * The Unix environment variables for the HTCondor-CE daemons are controlled by =/etc/sysconfig/condor-ce=.
   * You can place site HTCondor-CE configuration customizations in =/etc/condor-ce/config.d=. Any filename prefixed with "99-" will override the default files from the CE.
   * If you have an HTCondor local batch system, make sure to differentiate between the configuration that goes in =/etc/condor-ce= and the configuration that goes in =/etc/condor=.

---## Setup osg-configure
[[Documentation.Release3.IniConfigurationOptions][osg-configure]] is an automatic configuration tool that is used to set up a CE. This section only covers the HTCondor CE specific options of =osg-configure=: you will have to make other configuration changes before applying your configuration with =osg-configure=. 

   1. For sites updating from a GRAM CE, enable the HTCondor gateway and disable the GRAM gateway in =/etc/osg/config.d/10-gateway.ini=. This is the default in new installations: \
<pre class="file">
gram_gateway_enabled = False
htcondor_gateway_enabled = True
</pre> \
%NOTE% If you need to run _both_ HTCondor CE and GRAM (e.g. you need to run SAM tests), you need to enable both gateways:\
<pre class="file">
gram_gateway_enabled = True
htcondor_gateway_enabled = True
</pre>\
<p>More information about the GRAM CE can be found at Documentation.Release3.InstallComputeElement.</p>
   1. Enable your batch system by editing the =enabled= field in =/etc/osg/config.d/20-<span style="background-color: #FFCCFF;">&lt;your batch system&gt;</span>.ini=: \
<pre class="file">
enabled = True
</pre>
   1. Verify your configuration: \
<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -v
</pre>
   1. Apply your configuration: \
<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -c
</pre>

---## Setup authorization with edg-mkgridmap

<i>Alternatively, setup authorization with [[#GumsAuth][GUMS]].</i>

   1. Edit your list of accepted VO&rsquo;s in =/etc-edg-mkgridmap.conf= as specified in the [[Documentation/Release3.Edg-mkgridmap][edg-mkgridmap]] document.
   1. Under the =authorize_only= policy of =/etc/lcmaps.db=, comment out the =gumsclient= line and uncomment the =gridmapfile= line.  The resulting policy should read: \
<pre class="file">
authorize_only:
# gumsclient -> good | bad
gridmapfile -> good | bad
</pre>
   1. Specify the location of your grid mapfile in =/etc/condor-ce/config.d/01-common-auth.conf=: \
<pre class="file">
GRIDMAP = /etc/grid-security/grid-mapfile
</pre>

---## Non-HTCondor batch system configuration

The following subsections are only required if you  are *not* using HTCondor as your local batch system. 

---### Sharing the spool directory

Like GRAM, HTCondor-CE requires a shared file system between worker nodes and the CE to transfer files. Unlike GRAM, it does not write job files into the user&rsquo;s =$HOME= directory.  By default, job files are written into =/var/lib/condor-ce= and hence that directory must appear at that location on all worker nodes. We recommend setting up an NFS server on the CE dedicated to sharing the spool directory with the site&rsquo;s worker nodes instead of using a pre-existing NFS share.    

%NOTE% You can control the exported directory by setting the =SPOOL= configuration variable in =/etc/condor-ce/config.d=.  
%NOTE% Root squash must be turned off and the directory must be readable/writeable by the condor user for HTCondor CE to function correctly.

---### Disable blahp worker node proxy renewal

Turn off worker node proxy renewal in =/etc/blah.conf= by specifying the following two lines:

<pre class="file">
blah_disable_wn_proxy_renewal=yes
blah_delegate_renewed_proxies=no
</pre>

There should be no whitespace around the =&#61;=. Neither functionality is used with HTCondor-CE; enabling worker node proxy renewal will actually cause jobs to fail to refresh the proxy in some setups.

---## Optional Configuration

The following configuration steps are optional and will likely not be required for setting up a small site.

#JobRoutes
---### Setup Job Routes
     <i>Main document: [[Documentation/Release3.JobRouterRecipes][Job Router Recipes]]</i>

#GumsAuth
---### Setup authorization with GUMS

   1. Install and set up GUMS as specified in the [[Documentation/Release3.InstallGums][installation document]].
   1. Set the =authorization_method= and =gums_host= in =/etc/osg/config.d/10-misc.ini=:\
<pre class="file">
authorization_method = xacml
gums_host = <span style="background-color: #FFCCFF;">gumshost.example.com</span>
</pre>\
<p>Replacing <span style="background-color: #FFCCFF;">gumshost.example.com</span> with the hostname of your GUMS server</p>
   1. Verify your configuration: \
<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -v
</pre>
   1. Apply your configuration: \
<pre class="rootscreen">
%UCL_PROMPT_ROOT% osg-configure -c
</pre>

%NOTE% Once gsi-authz.conf is in place, your local HTCondor will attempt to utilize the LCMAPS callouts if enabled in the condor_mapfile.  If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null in the local HTCondor configuration.

---### Configuring for multiple network interfaces
If you have multiple network interfaces with different hostnames, the HTCondor CE daemons need to know which hostname to use when communicating to each other. Generally, you will want to set =NETWORK_HOSTNAME= to the hostname of your public interface in your =/etc/condor-ce/config.d/= directory with the line:

<pre class="file">
NETWORK_HOSTNAME=<span style="background-color: #FFCCFF;">condorce.example.com</span>
</pre>

Replacing <span style="background-color: #FFCCFF;">condorce.example.com</span> text with your public interface&rsquo;s hostname.

---### Configure jobs running on the CE

Local and scheduler universes are HTCondor CE&rsquo;s analogue to GRAM&rsquo;s managed fork: they allow jobs to be run on the CE itself. The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison.

   * *To change the default limit* on the number of locally run jobs (the current default is 20), add the following to  =/etc/condor-ce/config.d/99-local.conf=: <pre class='file'>START_LOCAL_UNIVERSE = <span style="background-color: #FFCCFF;">TotalLocalJobsRunning + TotalSchedulerJobsRunning < &lt;job limit&gt;</span>
START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)</pre>
   * *To only allow a specific user* to start locally run jobs, add the following to  =/etc/condor-ce/config.d/99-local.conf=: <pre class='file'>START_LOCAL_UNIVERSE = <span style="background-color: #FFCCFF;">target.Owner =?= "&lt;username&gt;</span>"
START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)</pre>
   * *To disable* locally run jobs, add the following to  =/etc/condor-ce/config.d/99-local.conf=: <pre class='file'>START_LOCAL_UNIVERSE = <span style="background-color: #FFCCFF;">False</span>
START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)</pre>

---### HTCondor accounting groups

%NOTE% For HTCondor batch systems only 

%INCLUDE{"Documentation/Release3/InstallComputeElement" section="AccountingGroups"}%

---#### /etc/osg/uid_table.txt
%INCLUDE{"Documentation/Release3/InstallComputeElement" section="uid_table"}%

---#### /etc/osg/extattr_table.txt
%INCLUDE{"Documentation/Release3/InstallComputeElement" section="extattr_table"}%

---### Batch system client tools in non-standard locations

%NOTE% For non-HTCondor batch systems only 

If your batch system binaries aren&rsquo;t in the standard location or not in your PATH, you will need to configure =/etc/blah.config= to point to the directory where you keep said binaries. If your SGE binaries live in =/opt/sge/bin=, then you would want to change the =sge_binpath= configuration variable to read:

<pre class="file">
sge_binpath=<span style="background-color: #FFCCFF;">/opt/sge/bin</span>
</pre>

---### Adding attributes to each job

%NOTE% This method adds attributes to *all* jobs that are submitted to your batch system. It is recommended to instead use [[Documentation/Release3.JobRouterRecipes][job routes]] wherever possible: you can use job routes to specify [[Documentation/Release3.JobRouterRecipes#2_6_Setting_a_Default][defaults]] for requested memory, requested number of cores, batch system queue, and walltime.

Additional attributes can be inserted into the job submit script by editing =/usr/libexec/blahp/<span style="background-color: #FFCCFF;">&lt;batch system&gt;</span>_local_submit_attributes.sh=.  This file is sourced during submit time and anything printed to stdout is appended to the job submit script.  For example, the following will set a default walltime for PBS: \

<pre class="file">#!/bin/sh

# Set walltime according to request; the batch system
# may reject this, of course!
if [ -n "$Walltime" ]; then
  echo "#PBS -l walltime=$Walltime"
else
  echo "#PBS -l walltime=24:00:00"
fi
</pre>

The environment variable =$Walltime= is set if the the attribute =remote_cerequirements= is set in the HTCondor-G job.  That attribute should follow the form:

<pre class="file">remote_cerequirements = foo == X && bar == Y && ...</pre>

to set =foo= to value X and =bar= to Y in the environment of =<span style="background-color: #FFCCFF;">&lt;batch system&gt;</span>_local_submit_attributes.sh=.  So, to set the Walltime to 1 hour with the above =<span style="background-color: #FFCCFF;">&lt;batch system&gt;</span>_local_submit_attributes.sh=, the job would need:

<pre class="file">remote_cerequirements = Walltime == 3600</pre>

---# Testing and Verification
<i>Main document: [[Documentation/Release3.InstallRSV][Install RSV]]</i>

In addition to RSV, you may find the need to manually test and verify your HTCondor CE. To do this, use the [[Documentation/Release3.TroubleshootingHTCondorCE#condor_ce_run][condor_ce_run]], [[Documentation/Release3.TroubleshootingHTCondorCE#condor_ce_trace][condor_ce_trace]], and [[Documentation/Release3.TroubleshootingHTCondorCE#condor_submit][condor_submit]] commands against your CE from a remote host. 

---# Troubleshooting
     <i>Main document: [[Documentation/Release3.TroubleshootingHTCondorCE][Troubleshooting HTCondorCE]]</i>
     
---# Services

Start and enable the following services:

   * =fetch-crl-boot= (or =fetch-crl3-boot= for EL5)
   * =fetch-crl-cron= (or =fetch-crl3-cron= for EL5)
   * =gratia-probes-cron=
   * Your batch system (=condor=, =pbs_server=, etc.)
   * =condor-ce=

---# Reference

   * HTCondor CE overview and architecture (coming soon)
   * [[Documentation/Release3.JobRouterRecipes][Configuring HTCondor CE job routes]]
   * [[Documentation/Release3.TroubleshootingHTCondorCE][Troubleshooting HTCondor CE]]