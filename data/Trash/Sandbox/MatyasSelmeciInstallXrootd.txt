%META:TOPICINFO{author="MatyasSelmeci" date="1456453542" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="MatyasSelmeciSandbox"}%
%LINKCSS%
---+!! %SPACEOUT{ "Install !XRootD" }%

<!-- Prompt rendering changes
Root's on a server, not a client.
   * Local UCL_PROMPT_ROOT = [root@server ~]#
-->

%TOC{depth="3"}%

---+ About this Document

Here we describe how to install the !XRootD redirector as part of a storage element for the Open Science Grid.
This document is intended for system administrators who wish to install !XRootD as part of a storage element or as part of a global namespace.


---+ Introduction to !XRootD

**TODO**

---+ Before Starting

Before starting the installation process, consider the following points
(consulting [[#ReferenceSection][the Reference section below]] as needed):

   * *User IDs:* If they do not exist already, the installation will create the
     Linux user ID =xrootd= (UID **TODO**)
   * *Service certificate:* The !XRootD service uses a host certificate at
     =/etc/grid-security/host*.pem=
   * *Networking:* The !XRootD service uses the standard HTTP port (80).
     For more information, see the
     [[Documentation/Release3.FirewallInformation][firewall information]] page

As with all OSG software installations, there are some one-time (per host)
steps to prepare in advance:

   * Ensure the host has [[SupportedOperatingSystems][a supported operating system]]
   * Obtain root access to the host
   * Prepare [[YumRepositories][the required Yum repositories]]
   * Install [[InstallCertAuth][CA certificates]]

---+ Installing !XRootD Server

**TODO** *description*

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install xrootd
</pre>

---++ Configuring !XRootD server

---+++ Minimal configuration


!XRootD is configured out-of-the-box to run a standalone server, which just
serves files from =/tmp= on the local file system. **TODO** *is this true?*

You may want to use this configuration to verify that your clients will be able
to talk to your server, for example. To do this, start the !XRootD server with
standalone config as described in the
[[#ManagingServices][managing services section]], and then validate with the
[[#ValidatingStandalone][standalone validation instructions]].




---+ Advanced Configuration

---++ Creating Xrootd Cluster

     <img src="%ATTACHURLPATH%/rdr.jpg" alt="rdr.jpg" width='736' height='318' />

An !XRootD cluster can make a storage installation much more scalable.  Controlling file access queries to the cluster is the "redirector" node.  This node will serve as a frontend for user requests and redirect their file accesses to the appropriate data node(s) that have the data they are requesting.  To facilitate this, two daemons xrootd (controlling file access and storage) and cmsd (controlling "cluster management services" and communications between nodes) will be started on each node once installation and configuration is completed.  Note that, for large virtual organizations, a site-level redirector may actually also communicate upwards to a regional or global redirector that handles access to a multi-level hierarchy.  This section will only cover handling one level of !XRootD hierarchy.

In the below instructions, RDRNODE will refer to the redirector instances and DATANODE will refer to the data node host.  Each instance of this below should be replaced with the FQDN, ie output of 'hostname'.

---+++ Modify =/etc/xrootd/xrootd-clustered.cfg=

You will need to modify the xrootd-clustered.cfg on both nodes.  The following example should serve as a base configuration for clustering.
Further customizations are detailed below.

<pre class="file">
all.export /tmp stage
set xrdr = RDRNODE
all.manager $(xrdr):3121
if $(xrdr)
  all.role manager
else
  all.role server
  cms.space min 2g 5g
fi
</pre>
Note: if there are other lines in the file, you can comment them out or delete them unless you are an expert user.

You will need to customize the following lines:

|*Configuration Line*|*Changes Needed*|
|=all.export /tmp stage=| Change =/tmp= to the directory to allow !XRootD access to|
|=set xrdr&#61;RDRNODE= | Change to the hostname of the redirector |
|=cms.space min 2g 5g= | Reserve this amount of free space on the node.  For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use k, m, g, or t to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively.|


Further information can be found at [[http://xrootd.slac.stanford.edu/doc]]



---+++ Starting services

Both =xrootd= and =cmsd= need to be started on all nodes:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service xrood start
%UCL_PROMPT_ROOT% service cmsd start
</pre>

---+++ Verifying cluster

Verify that you can copy file to /tmp on the server data via redirector:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% xrdcp /bin/sh  root://%RED%RDRNODE%ENDCOLOR%:1094///tmp/second_test
[xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]
</pre>

Check that the ==/tmp/second_test== is located on data server DATANODE.

---++ (Optional) Adding Simple Server Inventory  to your cluster

The Simple Server Inventory (SSI) provide means to have an inventory for each data server (See details in [[http://xrootd.org/doc/prod/cms_config.htm][XRootD CMS config]]).
In order to configure it you will need to run a second instance of xrootd daemon as well !XrdCnsd process that should run on every data server. We will configure xrootd cluster that consists of two nodes.  Note, this cnsd process is short for "composite name space" and will handle the inventory.
Host A is a redirector node that is running the following daemons:
   1. xrootd redirector
   1. cmsd
   1. xrootd - second instance that required for SSI

Host B is a data server that is running the following daemons:
   1. xrootd data server
   1. cmsd
   1. !XrdCnsd - started automatically by xrootd

We will need to create a directory on the redirector node for Inventory files.
<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir -p /data/inventory
%UCL_PROMPT_ROOT% chown xrootd.xrootd /data/inventory
</pre>

On the data server (host B) let's create the storage cache that will be different from ==/tmp==.
<pre class="rootscreen">
%UCL_PROMPT_ROOT% mkdir -p  /local/xrootd
%UCL_PROMPT_ROOT% chown xrootd.xrootd /local/xrootd
</pre>

Now, we have to change ==/etc/sysconfig/xrootd== on  redirector node %RED%hostA%ENDCOLOR% to run multiple instances of xrootd. Second instance of xrood will be name "cns" and will be used for SSI<pre class="file">
XROOTD_USER=xrootd
XROOTD_GROUP=xrootd
XROOTD_DEFAULT_OPTIONS="%RED%-k 7%ENDCOLOR% -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg"
%RED%XROOTD_CNS_OPTIONS="-k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg"%ENDCOLOR%
CMSD_DEFAULT_OPTIONS="%RED%-k 7%ENDCOLOR% -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg"
FRMD_DEFAULT_OPTIONS="%RED%-k 7%ENDCOLOR% -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg"
%RED%XROOTD_INSTANCES="default cns"%ENDCOLOR%
CMSD_INSTANCES="default"
FRMD_INSTANCES="default"
</pre>

Now we have to modify ==/etc/xrootd/xrootd-clustered.cfg== on both nodes so it looks like this:<pre class="file">
all.export /data/xrootdfs
set xrdr=%RED%hostA%ENDCOLOR%
all.manager $(xrdr):3121
if $(xrdr) &amp;&amp; named cns
      all.export /data/inventory
      xrd.port 1095
else if $(xrdr)
      all.role manager
      xrd.port 1094
else
      all.role server
      oss.localroot /local/xrootd
      ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory
      #add cms.space if you have less the 11GB
      # cms.space options http://xrootd.slac.stanford.edu/doc/dev/cms_config.htm
      cms.space min 2g 5g
fi
</pre>

Now, we can start xrootd cluster executing the following commands. On redirector you will see:<pre class="rootscreen">
%UCL_PROMPT_ROOT% service xrootd start
Starting xrootd (xrootd, default):                        %GREEN%[  OK  ]%ENDCOLOR%
Starting xrootd (xrootd, cns):                             %GREEN%[  OK  ]%ENDCOLOR%
%UCL_PROMPT_ROOT% service cmsd start
Starting xrootd (cmsd, default):                          %GREEN%[  OK  ]%ENDCOLOR%
</pre>

On redirector node you should see two instances of xrootd running:<pre class="rootscreen">
%UCL_PROMPT_ROOT% ps auxww|grep xrootd
xrootd   29036  0.0  0.0  44008  3172 ?        Sl   Apr11   0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default
xrootd   29108  0.0  0.0  43868  3016 ?        Sl   Apr11   0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-cns.pid -n cns
xrootd   29196  0.0  0.0  51420  3692 ?        Sl   Apr11   0:00 /usr/bin/cmsd -k 7 -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default
</pre>%ICON{warning}% the log file for second named instance of xrootd with be placed in /var/log/xrootd/cns/xrootd.log

On data server node you should that !XrdCnsd process has been started:<pre class="rootscreen">
%UCL_PROMPT_ROOT% ps auxww|grep xrootd
xrootd   19156  0.0  0.0  48096  3256 ?        Sl   07:37   0:00 /usr/bin/cmsd -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default
xrootd   19880  0.0  0.0  46124  2916 ?        Sl   08:33   0:00 /usr/bin/xrootd -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default
xrootd   19894  0.0  0.1  71164  6960 ?        Sl   08:33   0:00 /usr/bin/XrdCnsd -d -D 2 -i 90 -b fermicloud053.fnal.gov:1095:/data/inventory
</pre>
---+++ Testing Xrootd Cluster with SSI
   1. Copy file to redirector node specifying storage path (/data/xrootdfs instead of /tmp): <pre class="rootscreen">
%UCL_PROMPT_ROOT% xrdcp /bin/sh root://localhost:1094//data/xrootdfs/test1
[xrootd] Total 0.00 MB  |====================| 100.00 % [inf MB/s]
</pre>
   1. To verify that SSI is working execute cns_ssi command on the redirector node: <pre class="rootscreen">
%UCL_PROMPT_ROOT% cns_ssi list /data/inventory
fermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011
%UCL_PROMPT_ROOT% cns_ssi updt /data/inventory
cns_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors.
%UCL_PROMPT_ROOT% cns_ssi list /data/inventory
fermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011
/data/xrootdfs/test1
</pre>  _Note_: In this example fernilcould53.fnal.gov is a redirector node and fermicloud054.fnal.gov is a data server

---++ (Optional) Authorization

There are several authorization options in !XRootD available through the security plugins.
In this document, we will cover two options for security:

   * Simple Unix Security: Based on user accounts that the client is logged in as.
   * xrootd-lcmaps: Using the lcmaps callout to use GUMS authorization

Note: for both authorization schemes, you will need to provide an authorization file (=auth_file=).  See below.

Note: On the data nodes, the files will actually be owned by unix user =xrootd= (or other daemon user),
not as the user authenticated to, under most circumstances.  xrootd will verify the permissions and authorization
based on the user that the security plugin authenticates you to (for instance, your unix user for option 1 or your
gums id for option 2), but, internally, the data node files will be owned by the =xrootd= user.


---+++ Authorization File

In order to add security to your cluster you will need to add "auth_file" on the your data server node.
Create ==/etc/xrootd/auth_file== :<pre class="file">
# This means that all the users have read access to the datasets
u * %RED%/data/xrootdfs%ENDCOLOR% lr

# This means that all the users have full access to their private dirs
u = %RED%/data/xrootdfs/%ENDCOLOR%@=/ a

# This means that this privileged user can do everything
# You need at least one user like that, in order to create the
# private dir for each user willing to store his data in the facility
u xrootd %RED%/data/xrootdfs%ENDCOLOR% a
</pre> Here we assume that your storage path is "/data/xrootdfs" (same as in the previous example).

Change file ownership (if you have created file as root):<pre class="rootscreen">
 $ chown xrootd.xrootd /etc/xrootd/auth_file
</pre>

This file is a flat file of the following form:
<pre class="file">
idtype id path privs
</pre>

Some examples of each option.  For more details or examples on how to use
templated user options, see [[http://xrootd.org/doc/dev/sec_config.htm#_Toc317706960][Xrootd Authorization Database File]].


|idtype|Type of id - u for username, g for group, etc|
|id|Username (or groupname).  Use <noop>*</noop> for all users or <noop>=</noop> for user-specific capabilities, like home directories|
|path|The path prefix to be used for matching purposes.|
|privs|Letter list of privileges: =a - all ; l - lookup ; d - delete ; n - rename ; i - insert ; r - read ; k - lock (not used) ; w - write= |


---+++ Security Option 1: Adding Simple (Unix) Security

The first step in adding simple Unix security to validate based on username is to create the =auth_file= as in the previous section.

The next step is to modify  ==/etc/xrootd/xrootd-clustered.cfg== on both nodes:<pre class="file">
all.export /data/xrootdfs
set xrdr=%RED%hostA%ENDCOLOR%
all.manager $(xrdr):3121
if $(xrdr) &amp;&amp; named cns
      all.export /data/inventory
      xrd.port 1095
else if $(xrdr)
      all.role manager
      xrd.port 1094
else
      all.role server
      oss.localroot /local/xrootd
      ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory
      cms.space min 2g 5g
%RED%
     # ENABLE_SECURITY_BEGIN
        xrootd.seclib /usr/lib64/libXrdSec.so
        # this specify that we use the 'unix' authentication module, additional one can be specified.
        sec.protocol /usr/lib64 unix
        # this is the authorization file
        acc.authdb /etc/xrootd/auth_file
        ofs.authorize
        # ENABLE_SECURITY_END
%ENDCOLOR%
fi
%ENDCOLOR%
</pre> _Note_: change %RED%*.fnal.gov%ENDCOLOR% with appropriate domain name.

Note that, to access users directories, you will have to create them in oss.localroot
(For instance, =/local/xrootd/data/xrootdfs/username=) and make sure they are
writable by =xrootd= user (or the daemon user, if you have changed it).  Files in localroot on
the data nodes are normally owned by =xrootd= not by the authenticated username.

After making all the changes, please, restart xrootd and cmsd daemons on all nodes.

---+++ Testing Xrootd Cluster with simple security enabled
   1. Login on redirector node as root
   1. Check that user "root" still can read files: <pre class="rootscreen">
%UCL_PROMPT_ROOT% xrdcp  root://localhost:1094//data/xrootdfs/test1 /tmp/b
[xrootd] Total 0.00 MB  |====================| 100.00 % [inf MB/s]
</pre>
   1. Check that user "root" can not write files under /data/xrootdfs:<pre class="rootscreen">
%UCL_PROMPT_ROOT%  xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2
Last server error 3010 ('Unable to create /data/xrootdfs/test2; Permission denied')
Error accessing path/file for root://localhost:1094//data/xrootdfs/test3
</pre> or you may get this error:<pre class="rootscreen">
%UCL_PROMPT_ROOT% xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2
Last server error 3011 ('No servers are available to write the file.')
Error accessing path/file for root://localhost:1094//data/xrootdfs/test2
</pre>
   1. Check that user can copy/retrieve files to/from /data/xrootdfs/~/...<pre class="rootscreen">
%UCL_PROMPT_ROOT% su - %RED%user%ENDCOLOR%
-bash-3.2$   xrdcp  /tmp/a  root://localhost:1094//data/xrootdfs/%RED%user%ENDCOLOR%/test1
[xrootd] Total 0.00 MB  |====================| 100.00 % [inf MB/s]
-bash-3.2$  xrdcp    root://localhost:1094//data/xrootdfs/%RED%user%ENDCOLOR%/test1 /tmp/c
[xrootd] Total 0.00 MB  |====================| 100.00 % [inf MB/s]
</pre>

---+++ Security Option 2: xrootd-lcmaps Authorization

The xrootd-lcmaps security plugin uses the =lcmaps= package to access the =GUMS= server to authenticate and authorize users
based on X509 certificates.

*Certificate Installation*

In order to install the lcmaps, you will need a set of certificates.  The below shows how to install the =osg-ca-certs= package in order to satisfy this requirement.  After this, you will need to update the certificate revocation lists via the =fetch-crl= package.
%INCLUDE{"InstallCertAuth" section="FetchCRLInstallShort"}%

You may also run fetch-crl and start the service.
%INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlStart"}%

%INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlEnable"}%

* (other grid service running on the machine may still use it) %INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlStop"}%

* (other grid service running on the machine may still use it) %INCLUDE{"InstallCertAuth" section="OSGBriefFetchCrlDisable"}%

*Install xrootd-lcmaps*

<pre class="rootscreen">
yum install xrootd-lcmaps
</pre>
Note that the xrootd-lcmaps is usually coupled to a specific version of xrootd, so make sure that you install xrootd-lcmaps from the same repository that you install xrootd from.  Otherwise, you may have dependency issues due to differing versions of shared libraries.


*Create host certificates*

You will need to have a X509 certificate to talk to GUMS.  If you already have a host certificate, you can use a copy of that:
<pre class="rootscreen">
 mkdir /etc/grid-security/xrd
 cp /etc/grid-security/hostkey.pem /etc/grid-security/xrd/xrdkey.pem
 cp /etc/grid-security/hostcert.pem /etc/grid-security/xrd/xrdcert.pem
 chown -R xrootd:xrootd /etc/grid-security/xrd/
 chmod 400 /etc/grid-security/xrd/xrdkey.pem
</pre>
This certificate should be owned by xrootd and located in =/etc/grid-security/xrd=.

*Authorization File*

Next, create =/etc/xrootd/auth_file= using the example in the above section.

*Modify /etc/xrootd/lcmaps.cfg*

Edit =/etc/xrootd/lcmaps.cfg= to point to your authorization server:
<pre class="file">
scasclient = "lcmaps_scas_client.mod"
             "-resourcetype wn"
             "-actiontype execute-now"
             "-capath /etc/grid-security/certificates"
             "-cert   /etc/grid-security/xrd/xrdcert.pem"
             "-key    /etc/grid-security/xrd/xrdkey.pem"
             "--endpoint https://%RED%gums.fnal.gov%ENDCOLOR%:8443/gums/services/GUMSXACMLAuthorizationServicePort"
</pre>

*Modify /etc/xrootd/xrootd-clustered.cfg*

You will need to add the security plugins to =/etc/xrootd/xrootd-clustered.cfg=.
Add the following lines to the =/etc/xrootd/xrootd-clustered.cfg=.  This will need to be added to all data nodes in the section relevant to the data node server.  (For instance, in the above example(s), this should be placed in the last "else" clause after "all.role server".  See the section above on unix security for an example of where the security commands should go).
<pre class="file">
xrootd.seclib /usr/lib64/libXrdSec.so
sec.protocol /usr/lib64 gsi -certdir:/etc/grid-security/certificates -cert:/etc/grid-security/xrd/xrdcert.pem -key:/etc/grid-security/xrd/xrdkey.pem -crl:3 -authzfun:libXrdLcmaps.so -authzfunparms:--osg,--lcmapscfg,/etc/xrootd/lcmaps.cfg,--loglevel,0|useglobals --gmapopt:2 --gmapto:0
acc.authdb /etc/xrootd/auth_file
ofs.authorize
</pre>

*Restart xrootd and cmsd*

<pre class="rootscreen">
service xrootd restart
service cmsd restart
</pre>

*lcmaps notes*

For recent versions of lcmaps (1.5+), VOMS signature verification is enabled by default.
You may need to either disable it or install the =vo-client=.

Installing vo-client:
<pre class="rootscreen">
yum install vo-client
</pre>

Disabling vo verification can be done by adding a line to =/etc/sysconfig/xrootd=:
<pre class="file">
export LLGT_VOMS_DISABLE_CREDENTIAL_CHECK=1
</pre>

This feature is evolving, so stay tuned for updates.

---+++ Testing Xrootd Cluster with lcmaps security enabled

From any machine with the =xrootd-client= installed, you can test with xrdcp.
With a user that has no grid certificate installed, you should get an error:

<pre class="screen">
%UCL_PROMPT% xrdcp /bin/bash root://HOSTNAME/tmp/lcmaps_test
120327 14:31:52 10509 secgsi_InitProxy: cannot access private key file: /home/dstrain/.globus/userkey.pem
XrdSec: No authentication protocols are available.
Last server error 3010 ('cannot obtain credentials for protocol: Secgsi: ErrParseBuffer: error getting user proxies: kXGS_init: unable to get protocol object.')
Error accessing path/file for root://fermicloud121.fnal.gov/tmp/test2
</pre>

After running =voms-proxy-init= or =grid-proxy-init= to initialize your x509 certificate (usually found in =/tmp/x509up_uUID=), the =xrdcp= command should execute cleanly.  For instance, the following shows an example of a user creating a voms certificate and copying to the xrootd client and then re-executing the xrdcp command.
<pre class="screen">
%UCL_PROMPT% voms-proxy-init -voms Engage -valid 999:0
Your identity: /DC=org/DC=doegrids/OU=People/CN=Doug Strain 834323
Creating temporary proxy ..................................................... Done
Contacting  osg-engage.renci.org:15001 [/DC=org/DC=doegrids/OU=Services/CN=osg-engage.renci.org] "Engage" Done
Creating proxy .......................... Done

Your proxy is valid until Tue May  8 05:34:40 2012
%UCL_PROMPT% scp /tmp/x509up_u44678 CLIENT_HOSTNAME:/tmp/x509up_u44678
</pre>
On the =xrootd-client= node,
<pre class="screen">
%UCL_PROMPT% xrdcp /bin/bash root://fermicloud121.fnal.gov//tmp/lcmaps_test
[xrootd] Total 0.73 MB  |====================| 100.00 % [inf MB/s]
</pre>
In the above examples, make sure to change "/tmp" to a directory allowed by the =/etc/xrootd/auth_file= created in a previous section.

---++ (Optional) Adding CMS TFC support to !XrootD

For CMS users, there is a package available to integrate rule-based name
lookup using a storage.xml file.

<pre class="rootscreen">
yum install --enablerepo=osg-contrib xrootd-cmstfc
</pre>

You will need to add your =storage.xml= to =/etc/xrootd/storage.xml= and then add the following
line to your xrootd configuration:

<pre class="file">
# Integrate with CMS TFC, placed in /etc/xrootd/storage.xml
oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml%ORANGE%?protocol=hadoop%ENDCOLOR%
</pre>
Add the orange text only if you are running hadoop (see below).

See CMS twikis for more information: https://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall

---++ (Optional) Adding Hadoop support to !XrootD

Users with Hadoop-based storage under their !XrootD installation have extra steps
to configure more efficient access to Hadoop Stroage

<pre class="rootscreen">
yum install xrootd-hdfs
</pre>

You will then need to add the following line(s) to your xrootd configuration:
<pre class="file">
xrootd.fslib /usr/lib64/libXrdOfs.so
ofs.osslib /usr/lib64/libXrdHdfs.so
</pre>

For more information, see [Storage.HadoopXrootd].

---++ (Optional) Adding File Residency Manager (FRM) to Xrootd Cluster
The FRM deals with two major mechanisms:
   * local disk
   * remote servers

The description of fully functional multiple xrootd clusters is beyond the scope of this document. In order to have this fully functional system you will need a global redirector and at least one remote xrootd cluster from where files could be moved to the local cluster.

Below are the modifications you should make in order to enable FRM on your local cluster:
   1. Make sure that FRM is enabled in  ==/etc/sysconfig/xrootd== on your data sever:<pre class="file">
ROOTD_USER=xrootd
XROOTD_GROUP=xrootd
XROOTD_DEFAULT_OPTIONS="-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg"
CMSD_DEFAULT_OPTIONS="-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg"
FRMD_DEFAULT_OPTIONS="-l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg"
XROOTD_INSTANCES="default"
CMSD_INSTANCES="default"
FRMD_INSTANCES="default"
</pre>
   1. Modify ==/etc/xrootd/xrootd-clustered.cfg== on both nodes to specify options for frm_xfrd (File Transfer Daemon)  and frm_purged (File Purging Daemon).  For more information, you can visit the [[http://xrootd.org/doc/prod/frm_config.htm][FRM Documentation]] in the [[http://xrootd.org/doc/prod/frm_config.htm#_Toc298165607][frm_xfrd section]] and the [[http://xrootd.org/doc/prod/frm_config.htm#_Toc298165601][frm_purged section]]
   1. Start frm daemons on data server: <pre class="rootscreen">
%UCL_PROMPT_ROOT% service frm_xfrd start
%UCL_PROMPT_ROOT% service frm_purged start
</pre>
%ICON{warning}% Both daemons will use  /var/log/xrootd/frmd.log for logging.





---+ Using !XRootD

**TODO** *blurb*


#ManagingServices
---++ Managing !XRootD services



You will want to start services on the redirector node before starting any
services on the data nodes. If you have only installed !XRootD itself, you will
only need to start the =xrootd= service. However, if you have installed cluster
management services, you will also need to start =cmsd= on the nodes.


The instructions for starting and stopping an !XRootD service depend on whether
the service is installed on an EL 6 or EL 7 machine. You can use the same
commands, but the names of the services are different.

The services are:

%TABLE{sort="off"}%
| *Service*                   | *EL 6 service name* | *EL 7 service name* |
| !XRootD (standalone config) | =xrootd=            | =xrootd@standalone= |
| !XRootD (clustered config)  | =xrootd=            | =xrootd@clustered=  |
| !CmsD (standalone config)   | =cmsd=              | =cmsd@standalone=   |
| !CmsD (clustered config)    | =cmsd=              | =cmsd@clustered=    |


As a reminder, here are common service commands (all run as =root=):

%TABLE{sort="off"}%
| *To &hellip;* | *Run the command &hellip;* |
| Start a service | =service <em>SERVICE-NAME</em> start= |
| Stop a service | =service <em>SERVICE-NAME</em> stop= |
| Enable a service to start during boot | =chkconfig <em>SERVICE-NAME</em> on= |
| Disable a service from starting during boot | =chkconfig <em>SERVICE-NAME</em> off= |



---+ Validating !XRootD

#ValidatingStandalone
---++ Validating a standalone !XRootD Server

In order to test that xrootd is working as a stand alone data server, start the server using the commands in the previous section.  You should be able now to copy files using xrdcp command into ==/tmp==. To test do:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install xrootd-client
%UCL_PROMPT_ROOT% xrdcp /bin/sh root://localhost:1094//tmp/first_test
[xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]
%UCL_PROMPT_ROOT% ls -l /tmp/first_test
-rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test
</pre>


---+ Troubleshooting !XRootD


**TODO**


---+ Getting Help

To get assistance. please use the [[HelpProcedure][Help Procedure]] page.

---+ Reference

---++ File Locations


%STARTSECTION{"Locations"}%

| *Service/Process* | *Configuration File* | *Description* |
| !XRootD | /etc/xrootd/xrootd-clustered.cfg | Main xrootd configuration and settings|
| | /etc/xrootd/auth_file | Authorized users file |

| *Service/Process* | *Log File* | *Description* |
| !XRootD | /var/log/xrootd/xrootd.log | !Xrootd server daemon log |
| cmsd | /var/log/xrootd/cmsd.log | Cluster management log |
| cns | /var/log/xrootd/cns/xrootd.log | Server inventory (composite name space) log |


%ENDSECTION{"Locations"}%

---+ Comments

%COMMENT{type="tableappend"}%
