%META:TOPICINFO{author="ForrestChristian" date="1175553872" format="1.1" version="1.78"}%
%META:TOPICPARENT{name="Main.ForrestChristian"}%
<link rel="stylesheet" type="text/css" href="%ATTACHURLPATH%/exercises.css" />

d10 1
<div class="topnotch">
---+!! %SPACEOUT{ "%TOPIC%" }%



ForrestChristianSandboxTesting


onLoad="setPref('LOGINNAME', 'Timmy'); setPref('LOGINHOST', 'this.is.my.host.be'); 



%STARTMore%
<span id="twid_%CALC{$GET(twisty_id)}%hide" class="twistyHidden"><a href="#" class="twistyTrigger">close</a></span>
%ENDMore%


</div>


---+ dCache Services
%STARTMore%

| *Location Manager* | Supplies IP address and port of dCache domain |
| *dCache* | Routes messages between dCache services |
| *admin* | Allows login to running services |
| *pnfsManager* | Queries the pnfs service |
| *pinManager* | Prevents modification of files during transfer |
| *replicaManager* | Maintains replica count of files on pools |
| *Billing* | Maintains records and statistics of all transactions |
| *http* | Provides information on state of dCache |
| *gPlazma* | Authorizes users |

%ENDMore%

---++ Fermilab SRM clients
<pre class="screen">
srmcp srm://ligodata.uwm.edu:8443/pnfs/uwm.edu/data/testfile file:////tmp/test.tmp
srmcp srm://storage.ligo.lsu.edu.:8443/pnfs/uwm.edu/data/file.01 srm://ligodata.uwm.edu:8443/pnfs/uwm.edu/data/testfile.01 
</pre>

srmcp uses a GSI X.509 proxy, which can contain attributes for both group and role memberships. 
srmcp can specify for gridftp transfers
Port range
Number of streams
Buffer size, ...
srmls, srmrm

---++ Private Networks and Firewalls
Many dCache installations will have portions on private networks or exist behind firewalls. dCache supports both.

---++ dCache Authorization
dCache supports authorization through put GSI and kerberos. 

Host certificates on SRM, gridftp, gsidcap, gPlazma

Client proxies are delegated within dCache. This allows SRM to act as a client.

Gridftp doors must be on public network

Pools may be on private network. This requires gridFTP adapter for local to SRM transfers, although SRM to SRM transfers may be possible.

Additionally, dCache allows you to specify certain ports or port ranges for use when behind firewalls.


---+++ gPLAZMA
Our prefered authorization is through gPlazma, which allows choice of authorization methods. It support role-based authorization.

Additionally, gPLAZMA can use the GUMS server for centralized administration. This requires that storage obligations be added to GUMS permit decision. 


---+ Generic USCMS Tier2 SRM/dCache Installation Example
<table><tr><td>
Admin node:
   * lmDomain
   * poolManager
   * adminDoor
   * httpDomain 
   * utilityDomain 
   * gplazmaService
   * infoProvider
</td><td>
pnfs node 
   * pnfsManager
   * dirDomain
</td><td>
SRM node
   * Three door nodes running dcap + grid doors
   * Pools on worker nodes or RAID
</td></tr></table>

This should be around 30 terabytes with a throughput of ~20 MB/sec.


---++ Inputs Required for Installation of SRM/dCache
   * Number of storage nodes in your site
      * The installation script suggests service distribution

---+ OSG Lessons Learned and Best Practices
From Main.StevenTimm and the team at Fermilab.

---++ File systems

---+++ Four File Systems Needed
Home areas for each user
   * Quota of 50MB per user
   * Nothing permanent should be stored in home areas.
APP area for each VO's applications&#8212;quota 10GB per VO
   * Any way to do garbage collection here, or make VOs do it?
DATA area for each VO's data, 2GB per job slot
   * Again, any way to make VOs clean up after themselves?  Right now using quotas to keep any one VO from overrunning the area.
   * Even if you have a SITE_READ, SITE_WRITE storage element should have DATA too..
WN_TMP area on each worker node
   * In past, VDS has required that this be a fixed path. This will change for OSG ­0.5.0 to be able to be assigned at execute time


---++ To NFS or not to NFS
Pre-­web ­services GRAM makes pig­headed use of NFS, tolerates NFS failures badly. WS GRAM doesn't need so much NFS.
   * Small to medium cluster (<120 machines)
      * Get users' home areas on the globus gatekeeper machine itself.
	  * Make second server with APP and DATA.
   * Big cluster
      * Use NFS-­lite configuration or some global file system for home areas.
      * Bluearc NAS head has worked well for us thus far.
      * Others out there &mdash; Ibrix, Panasas, etc.

%WARNING% The condor negotiator/collector and NFS servers don't mix.


---++ Software, Pre-­Installation

---+++ Condor
   * Install first and separately out of VDT area.
   * Condor needs upgraded more than rest of VDT
   * Some versions may not be available in VDT&#8212;use RPM
   * We install rpm on every WN&#8212;helps with network trouble
   * Needed for <nop>ManagedFork even if main batch system isn't condor.

---+++ Accounts
Create accounts in advance. We suggest using the =/sbin/nologin= shell

---+++ Ganglia
Have your gmetad somewhere other than your head node. It's a big network-traffic generator &mdash; and CPU load, too.


---++ Software, during install
I install worker node client on head node and on each worker

Big load on VDT server and on CRL servers&#8212;in 0.5.0 plan to use Squid to mitigate this.

---+++ <nop>ManagedFork
This is a lifesaver.  Only way to log and control what users are doing with the jobmanager-­fork.

---+++ =$VDT_LOCATION/globus/TRUSTED_CA=
   * This is the default location for certificates. 
   * =/etc/grid­-security/certificates= must also be symlinked to this, otherwise Condor 6.7.18+ doesn't know where to get certificates.
   * Ensure that the worker node client's TRUSTED_CA points here.


   



---++ Importance of a central submit node
Have a central submit node!!
   * Discourage people from submitting from their desktops
      * Can't control the configuration of desktops
	  * Lots of ways to make it insecure
   * Can kill rogue condor-­G processes, if necessary
   * See the same condor errors they are seeing
   * Laptops turn off and jobs get held that would otherwise succeed.
   * Most serious grid work needs a bigger staging disk than a desktop will provide anyway.


---++ Importance of a central submit node
Have a central submit node!!
   * Discourage people from submitting from their desktops. You can't control the configuration of desktops, and there are lots of ways to make it insecure.
   * With central submit node, you can kill rogue condor­G processes if necessary and it allows you to see the same condor errors the users are seeing.
   * Most serious grid work needs bigger staging disk than a desktop will provide anyway
   * Laptops turn off and jobs get held that would otherwise succeed





---++ Wish List
   * Better audit trail in Condor_G
      * source job ID available to remote and vice versa
   * globus-­job-­manager reaper, kill hung processes
   * Better home area cleanup of =/home/.globus/.gass_cache/*=
   * MySQL root is open. Why?
   * Policies for VOs to clean up APP and DATA
   * Watchdog to look for broken services.
   * Hostnames
   * Paths to and sizes of your pools
   * Doors: types, number


%BR%

ForrestChristianSandboxDocScientists

------------
%INCLUDE{ "Documentation.ToolsBottomMatter" }%
-- Main.ForrestChristian - 12 Sep 200
   * [[%ATTACHURL%/dcb.css][Stylesheet for HTML file]]
   * [[%ATTACHURL%/InstallingDcacheForOSG.html][Installing dCache for the Open Science Grid]]
   * [[%ATTACHURL%/efc_css_test.css][Tasty css]] 
   * [[%ATTACHURL%/forrestSideBar.css][forrestSideBar.css]]: 
   * [[%ATTACHURL%/forrestSideBar.css][forrestSideBar.css]]: Tasty sidebar css
   * [[%ATTACHURL%/PacmanInfo.html][PacmanInfo.html]]: Printable version of pacmaninfo for testing
  

%META:FILEATTACHMENT{name="natskintest_patternskin_low.gif" attr="" autoattached="1" comment="GIF for JS test" date="1179934676" path="natskintest_patternskin_low.gif" size="80644" user="Main.ForrestChristian" version="1"}%
%META:FILEATTACHMENT{name="dcb.css" attr="" autoattached="1" comment="Stylesheet for HTML file" date="1168467137" path="dcb.css" size="3057" user="Main.ForrestChristian" version="1"}%
%META:FILEATTACHMENT{name="InstallingDcacheForOSG.html" attr="" autoattached="1" comment="Installing dCache for the Open Science Grid" date="1168468793" path="InstallingDcacheForOSG.html" size="59512" user="Main.ForrestChristian" version="2"}%
%META:FILEATTACHMENT{name="javascript_cookies.js" attr="" autoattached="1" comment="Javascript cookies js file" date="1179437627" path="javascript_cookies.js" size="2513" user="Main.ForrestChristian" version="1"}%
%META:FILEATTACHMENT{name="PacmanInfo.html" attr="" autoattached="1" comment="Printable version of pacmaninfo for testing" date="1171647730" path="PacmanInfo.html" size="17888" user="Main.ForrestChristian" version="1"}%
%META:FILEATTACHMENT{name="efc_css_test.css" attr="" autoattached="1" comment="Tasty css" date="1170432403" path="efc_css_test.css" size="2638" user="Main.ForrestChristian" version="8"}%
%META:FILEATTACHMENT{name="forrestSideBar.css" attr="" autoattached="1" comment="Tasty sidebar css" date="1175543808" path="forrestSideBar.css" size="2802" user="Main.ForrestChristian" version="28"}%
