%META:TOPICINFO{author="CarlEdquist" date="1452025920" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="MatyasSelmeciSandbox"}%
---+ Switching Software Area (/software) to !GitHub


This document details the plan for migrating the projects in the VDT SVN
software area to Git repositories located primarily on !GitHub.  This document
supersedes [[SoftwareTeam.GitMigrationProposal]].


---++ Terminology


This plan involves creating a "primary" repository and an "authoritative"
repository, per software project.  The terms are as follows:

   * An "authoritative" repository is the "one true copy" of the project repository.\
      It has the following properties:
      * If there is a conflict between an authoritative repository and *any* other repository (including the primary repository), the authoritative repository "wins".
      * If the authoritative repository goes offline or becomes inaccessible, it is considered a fire.

   * A "primary" repository is the project repository that developers will use for their daily work.\
      It has the following properties:
      * It is easy to access and update (for authorized users).
      * Developers publish their changes to the primary repository.
      * If the primary repository goes offline or becomes inaccessible, it is considered a major hassle.
      * The authoritative repository pulls data from the primary repository.

Because of the design, the authoritative repositories will also be referred to
as "AFS repositories", and the primary repositories as "!GitHub repositories".


---++ Objectives


Most of the objectives from [[SoftwareTeam.GitMigrationProposal]] remain
unchanged:

   * The OSG (or CHTC) must own and fully manage the operations of the authoritative Git repositories.
   * We must have dependable backups of the repositories.
   * Anyone should be able to submit a contribution.
   * The OSG Technology team must have sole write privileges on the authoritative repositories.
   * Team members must be able to easily review and approve submitted contributions.
   * The workflows for both contributors and team members must be clearly documented.\
     Common use cases should be efficient and easy to learn.
   * The migration process must preserve existing data, should interrupt work little, and should be reversible.

These two bullet points have been removed:

   * There should be public mirrors of our repositories (on !GitHub) that are kept up to date.
   * Anyone should be able to read the repositories or their public copies (e.g., !GitHub), including histories.

and replaced with:

   * The authoritative repositories should be kept up-to-date with the primary repositories.
   * Anyone should be able to read the primary repositories, including histories.


---++ High-Level Architecture


The architecture is similar to [[SoftwareTeam.GitMigrationProposal]].  The main
difference is that when everything is working fine, instead of developers
pushing changes to the authoritative repo, which subsequently pushes changes to
!GitHub, developers push changes to !GitHub, which are subsequently pulled down
to the authoritative repo (as long as they are safe).

---+++ Authoritative repos

The authoritative repos will be on the UW's AFS.  We'll use the UW CSL's
Kerberos for authentication and access control.  In the normal case, team
members do not touch or clone from the authoritative repositories.  If a
primary repo becomes unusable for some reason (outage, corruption, etc.),
trusted developers (i.e. those with the right AFS ACLs) will push directly to
the AFS repo until the problem is resolved.

---+++ Primary repos

The primary repos will be on !GitHub.  They will be owned by the
"opensciencegrid" organization.  Team members will need to have !GitHub
accounts and be members of the organization in order to approve contributions.
The !GitHub repos will be configured to reject "unsafe" commits (commits that
alter history, cause conflicts, or otherwise cause breakage or potential loss
of information).

---+++ Synchronization

Each AFS repo will periodically pull from the corresponding !GitHub repo.  The
properties of the AFS repo will be set so that it only accepts changes that are
"safe".

If an "unsafe" update is attempted, the developers will be notified via email.
They will have to resolve the conflict and push the changes back to !GitHub.

---+++ Backups

The AFS repos will live on an AFS volume that is backed up nightly by the CSL.
We will make a "snapshot" of the repo before each backup runs, to ensure
consistency.  This process is modeled after what we do for the VDT SVN repos.

---+++ Contributions

External collaborators or team members will fork one of the !GitHub repos and
submit their changes as pull requests; a team member will review and merge the
pull requests to incorporate the changes.


---++ Detailed Architecture


---+++ Authoritative (AFS) repos

The AFS repo for a software project will live under
<code>/p/condor/workspaces/vdt/git/software/<em>PROJECT</em>.git</code>
Once the !GitHub repo for a project has been created, making the corresponding
AFS repo is as follows:
<pre>
PROJECT=&lt;your project&gt;

cd /p/condor/workspaces/vdt/git/software/
git clone --mirror https://github.com/opensciencegrid/$PROJECT.git
cd $PROJECT.git/

# Check integrity of each object fetched:
git config transfer.fsckObjects true

# Enable the reflog (helps debugging):
git config core.logAllRefUpdates true

# Deny non-fast-forward updates:
git config remote.origin.fetch 'refs/*:refs/*'
</pre>

The AFS ACLs for the repo will be copied from
<code>/p/condor/workspaces/vdt/svn</code>.

---+++ Primary (!GitHub) repos

The !GitHub repo for a project will be
<code>https://github.com/opensciencegrid/<em>PROJECT</em></code>

We will enable
[[https://help.github.com/articles/about-protected-branches/][protected branches]]
for all branches of the project, to prevent non-fast-forward pushes.

Write access (the ability to accept pull requests or push to the repository)
will be given to members of the "opensciencegrid" !GitHub organization.

---+++ Synchronization

There will be one "fetch" cron job per each AFS repo, owned by =cndrutil=, to run
=git fetch=.  Results will be logged; the log will be rotated; any =stderr=
output or nonzero exit codes will be emailed to Madison people.

Create a file named =NOFETCH= in an AFS repo to disable fetches for that repo.
There will be one "watchdog" cron job that will run nightly, look through all
repos for files named =NOFETCH=, and send email reminders to Madison people.

(We may want certain errors from =git fetch= to automatically create a
=NOFETCH= file).

---+++ Backups

Add a rolling backup system like what we do for the SVN repos.  A single cron
job will go through each AFS repo, and do this:

   1. Check for the =NOFETCH= file; if present, skip this repo.
   1. Create a file called =NOFETCH=.
   1. Create an "update" hook that does =exit 1= (to prevent direct pushes).
   1. Wait for =git-receive-pack= processes to exit.
   1. Run =rsync= to copy the repo to a numbered directory in AFS.
   1. Remove the "update" hook.
   1. Remove =NOFETCH=.
   1. Delete the oldest backups until we have &lt;=10


---++ Migration

---+++ One-time setup

   1. Create AFS volume for =/p/condor/workspaces/vdt/git=
   1. Create authors file to map SVN user names to email addresses for Git
      (there is one at "/p/condor/workspaces/vdt/svn-access/authors.txt" --
      make sure it's up-to-date)
   1. Create "backup" cron job, run by =cndrutil= (see "Backups" section)
   1. Create "fetch" and "watchdog" cron jobs, run by =cndrutil= (see
      "Synchronization" section)

---+++ Per-project migration

   1. Schedule a switchover date with the owner.
   1. Use =subgit import= (it's nicer than =git-svn=) to create a new Git repo
      based on the data from SVN.
   1. Create the !GitHub repo under the "opensciencegrid" organization; do not
      enable protected branches yet.
   1. In the repo created by =subgit= above, do:
      1. <code>git remote add github https://github.com/opensciencegrid/<em>PROJECT</em></code>
      1. =git push --mirror github=
   1. Enable protected branches for the repo in !GitHub.
   1. Create the authoritative repo for this project (see "Authoritative Repos"
      section above).
   1. Run the "fetch" cron job by hand for the repo, to test it.
   1. Add the "fetch" cron job to <code>cndrutil</code>'s crontab on =ingwe=.
   1. View the logs for the Git repo and make sure they match the logs of the
      SVN repo.
   1. If anything fails, either debug, _carefully recording all steps_ or just
      try again.
   1. Make a file in the trunk of the SVN dir called =MOVED-TO-GIT= that
      contains the following text:<pre>
The source code for this project is now contained in GitHub at opensciencegrid/<b>PROJECT</b>
Changes committed to SVN will be ignored.</pre>
   1. =svn lock= the project in SVN.


---++ Safety


From Main.CarlEdquist:
<verbatim>
What kind of vulnerabilities are we concerned with?

1. "corruption" -- anything that destroys history; specifically, the ability
to do fast-forward fetches.

2. "garbage" -- an unauthorized or rogue user (or a rogue github) pushes
unwanted commits that do not destroy history.  ("Sufficiently-fat-fingered"
can also be considered rogue.)
</verbatim>

---+++ Preventing "corruption"

From Main.CarlEdquist:

<verbatim>
- If "corruption" happens on github, the automated fetch to the AFS repos
  will fail, and we'll be notified immediately.  Since non-fast-forward
pushes will (should) be disabled on github, this would be an indication that
somebody's not playing nice, and we may want to investigate that.

In any case, the AFS repos won't be changed, so we can force-push everything
back to github to recover.  (Temporarily re-enabling force-pushes for this
recovery.)

On the developer end, corrupted updates from the github upstream get
rejected by pulls.  New checkouts of the corrupted github repos (before our
recovery) will of course contain the corruption, so they can be discarded as
we'll direct everyone in an email.
</verbatim>

---+++ Preventing "garbage"

From Main.CarlEdquist:

<verbatim>
- If "garbage" commits make their way into the github repos; first someone
will have to notice.  If someone is trying to be sneaky and sneak something
in, it may not be obvious.  (The case seems about the same as for the UW SVN
repo.)  There are ways to protect against it (namely having everyone sign
all their own commits, and enforcing checking for this), but I suspect
that's more than we really want.

But if a bad commit is discovered (aha! gotcha!), and it's offensive enough
that we want to remove it from our repo entirely (instead of just adding a
further commit to revert it), we can 'git reset' the AFS repos to just
before the offending commit, optionally run 'git gc' to remove any offending
blobs from the object store, force-push back to github, and email everybody.

As a special case, if a 4TB garbage commit makes it to the github repos
(assuming github itself doesn't reject it!), then the automated fetches to
AFS would fail, exhausting the vdt disk quota on AFS.  In this case, the AFS
repos will not be in a corrupted state, and the garbage commit will not be
included in the repo, but a (large) incomplete temporary file will be left
behind (with an obvious name like objects/02/tmp_obj_YCIXgK).

We will have to manually delete such a temp file, and then force-push the
(unchanged) AFS repo back to github.
</verbatim>

---+++ Responding to !GitHub outages

From Main.CarlEdquist:

<verbatim>
There are two cases to consider.

1. Temporary outages.  (Kind of like when JIRA goes down  :)

2. Severe or permanent outages.  (When it's too long to tolerate...)


For temporary outages, developers can continue to commit to their local
repos, and do whatever development they want.  This includes making new tags
and cutting release tarballs to use as new upstream sources for builds.
Anyone can also clone or update from the official ones in AFS in the mean
time -- though as long as the outage is considered temporary, no one should
push back to AFS, but wait until it becomes possible to push back to github
again.

For severe or permanent outages (which someone with Say can determine), then
everyone can just switch their upstream (remote) back to AFS.

Something like:

    upstream=/p/condor/workspaces/vdt/git/software/<project>.git
    git remote set-url origin $upstream

for checkouts with direct AFS access; or for access via ssh:

    upstream=/p/condor/workspaces/vdt/git/software/<project>.git
    cshost=library.cs.wisc.edu
    git remote set-url origin $cshost:$upstream

Everyone then gets to play nice (no pull requests) and push directly to the
AFS location (like they do in FW's CONDOR_SRC git).  If someday github comes
back online, we should be able to just fast-forward push the updates to
github, and tell everybody to switch back.  (But, it's important that they
listen!)
</verbatim>

---+++ Preventing tag corruption

From Main.CarlEdquist:

<verbatim>
[2] Removing the '+' in the fetch refspec prevents non-fast-forward updates,
but tag updates are actually a separate issue.  Normally for a checkout, if
an upstream tag changes, a pull will not update the local tag.  Also, a
(non-force) push will fail for an updated tag (that is, it will refuse to
update a tag reference on a remote.)  But in our case, we have a bare repo
(not a working copy/checkout) and we do fetches, not pulls or pushes.  To my
horror, I have found no way to prevent tag updates via fetch to a bare repo.
If tags are included in your fetch, tag updates will come also.

(Again, tag updates will not get pulled into working copy/checkouts that
have the original tag, but the main issue is how to protect the tags in the
AFS repos from upstream corruption.)

I would very much welcome any suggestions on how to handle this potential
issue.  (@bb?)  So far, I have come up with two ways to protect against
this, both of which I consider a little ugly.

1. fetch to an intermediate bare repo, and then push to the main bare repo.
The fetch will update the tag, but the push will fail.  This has the
advantage of being able to detect easily when tag updates happen (which
should be forbidden, so we do want to know).  The downside is this approach
uses an extra staging repo.  (Then again, we could use that as an extra
backup...)

2. use a non-standard fetch refspec for tags; for example:

        fetch = refs/heads/*:refs/heads/*
        fetch = refs/tags/*:refs/tag-stage/*

Instead of fetching everything ('refs/*'), single out heads and tags, but
fetch tags to a non-standard location.  As a side effect, new tags will get
created in the standard location (under refs/tags), along with
refs/tag-stage, but tag updates will only make it to tag-stage, and will
(silently) not update the tags under refs/tags.  This appears to work, and
has the advantage of not needing a separate staging repo.  One disadvantage
is it does not automatically bring in 'other' types of refs (eg, github
'pull' refs), although if we really want them, we can list them explicitly
in the fetch list.  Also, this is a little less obvious to detect; there are
no failures, though if you parse the command output you can detect trouble
any time there is a tag-stage update, eg:

   8c1bcea..cec9488  v1.2.3         -> refs/tag-stage/v1.2.3

Also worrisome is that the semantics here don't seem to be clearly
documented... It would make me feel a little better if we could point to
something that says yes it's supposed to work that way.  But in any case,
I've confirmed it works in both git 1.7.12 and 2.5.0.  (So, the behavior
seems stable...)

---

Alternatively, if we are OK with just being informed of tag updates (instead
of preventing them), we can add reflogs for all tags, which under normal
circumstances will remain empty.  Making reflogs for tags automatically is
not really convenient in git; for some reason core.logAllRefUpdates works
for refs/heads but not refs/tags (which at least is documented), and
'git tag --create-reflog' only seems to create a reflog for a new tag.

We can add something like the following to the fetch script, after the fetch
command runs, to create reflogs for all new tags, and detect tag updates
from existing ones.

    # from $GIT_DIR
    for tag in $(git tag); do
        taglog=logs/refs/tags/$tag
        tagdir=${taglog%/*}
        [[ -d $tagdir ]] || mkdir -p $tagdir
        [[ -e $taglog ]] || touch $taglog
        if [[ -s $taglog ]]; then
            echo "Warning: tag $tag has a history..."
            git reflog refs/tags/$tag
            echo
        fi >&2
    done

</verbatim>

A third option is to have developers sign tags with their GPG keys.  That might
not prevent tag corruption, but it would let us know if it had taken place.


-- Main.MatyasSelmeci - 29 Dec 2015

<!-- vim:ft=twiki
-->