%META:TOPICINFO{author="MarcoMambelli" date="1330357113" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+ OSG / Ohio Supercomputer Center meeting - Feb 23, 2012 

---++ Coordinates
Phone: # 1 866-740-1260, code 8349885#

International participants dial Toll Number: 303-248-0285 or for International Toll-Free Number check: http://www.readytalk.com/intl

Join via Adobe Connect: http://osg.adobeconnect.com/osgmeeting/   

The time is Thursday 2/23 11am EST (10am CST)


---++ Initial email
From: Doug Johnson <djohnson@osc.edu>
To: Gabriele Garzoglio <garzogli@fnal.gov>
Cc: Rolf Andreassen <rolfa@slac.stanford.edu>, Marko J Slyz <mslyz@fnal.gov>,
    Gabriele Garzoglio <garzoglio@fnal.gov>, Marco Mambelli <marco@hep.uchicago.edu>
Subject: Re: Open Science Grid support of SuperB and OSC

>     At Mon, 20 Feb 2012 14:54:28 -0600,
     Gabriele Garzoglio wrote:

         Dear Doug and Rolf,

         as representatives of the Open Science Grid user support team, we are
        working with the SuperB VO to run jobs on US facilities through the Open
         Science Grid interfaces.

         We were told by our contacts (Armando Fella, Steffen Luitz, Luca
         Tomassetti) that you have an interest in supporting SuperB and had tried
         to install the OSG interfaces at OSC in the past.

         We would be happy to meet with you to discuss the OSG consortium and
         what it would take for you to support SuperB through OSG.

         Let us know if you are interested in this discussion.

> Doug Johnson wrote:

     Hi Gabriele,

     I would definitely appreciate a discussion on how to support
     SuperB/OSG at OSC.  I'm available anytime Wednesday afternoon or all
     day Thursday except for 14:00-14:30 EST.  If these times do not work,
     I can provide alternatives.

     I will mention that we are a supercomputer center with existing OS
  distributions, software, and systems management schemes.  During
    initial attempts to we encountered incompatibles installing the OSG
     software in our environment.  It would be very useful to have a
     discussion about how other large, shared resource centers that aren't
     starting with a clean slate support OSG.

Doug

---++ Minutes
Attending: Gabriele, Marko, Marco, Doug Johnson <djohnson@osc.edu>, Rolf Andreassen
<rolfa@slac.stanford.edu>

Gabriele an Marco introduced OSG (see [[%ATTACHURL%/OSG_Intro_OSC_23feb2012.pdf][OSG_Intro_OSC_23feb2012.pdf]])

Doug: production cluster w/ 700 nodes, 5000 cores and 400GB p/n scratch. OS: RH5. Will support for SuperB VO.
Installing new cluster w/ 700 nodes, 8000 cores, 800GB p/n scratch RH6.1
All FS are available on all cluster nodes, unified name space (user home directories w/ 500GB quota)
Project space (at large allocation) migrating from GPFS array to Lustre (2 large file systems 1PB an 600TB)

Standard deployment: Single shared NFS read only OS (system directories)
This imposed installing RPM by hand, not installing the OSG repo.
Stumbling blocks:
   1.  shared read-only root file system (highly customized system) that is not allowing extra installations on some nodes
   2.  OSG yum repository configuration requires to disable EPEL (found in OSG documentation) -> Misunderstanding clarified at the meeting. OSG DOES use EPEL. Marco asked for a pointer to the bad document.

Alternative.
VO and OSG software on a standalone system with some independence: superb.osc.edu 
Marco suggested to use a VM with 2GB ram to run the servers (CE/SE)

They started the installation but found a problem:
 OSG provided torque client not compatible with Torque server used in the cluster (OSG version enables munch, not configured in the production cluster)
Marco will provide pointer to custom PBS configuration allowing them to use their client (empty-pbs):
   * https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallComputeElement#2_2_If_you_are_using_Torque_or_P
   * https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallComputeElement#ConfigPBS

Another worry is user management. The would like all superb jobs using one (or few) userID. 
Superb prefers 2 accounts, one to start, one in the future will be for sw management.


OSC is supporting Alice outside on any Grid (ALIEN middleware, installed as non privileged user)

OSG will support !SuperB by joining OSG and allowing !SuperB jobs.
OSG will be a new OSG site. They are in the process of installing a CE: =superb.osc.edu= 
We did not talk about registering in OIM and VO ownership of the site.

OSC will provide to !SuperB:
   * one gatekeeper installing OSG 3 stack via RPM
   * add OSG wn-client to the worker nodes image (30MB RPM)
   * outbound connectivity from the worker nodes (behind a NAT)
   * servers with 10 Gbps connection to I2.

Schedule:
   * Certificate requests within days
   * If everything goes smoothly, hopefully done by mid March




-- Main.MarcoMambelli - 23 Feb 2012

%META:FILEATTACHMENT{name="OSG_Intro_OSC_23feb2012.pdf" attachment="OSG_Intro_OSC_23feb2012.pdf" attr="" comment="" date="1330028570" path="OSG Intro OSC 23feb2012.pdf" size="1361994" stream="OSG Intro OSC 23feb2012.pdf" tmpFilename="/usr/tmp/CGItemp34524" user="MarcoMambelli" version="1"}%
%META:TOPICMOVED{by="MarcoMambelli" date="1330357113" from="SiteCoordination.SitesCoord120127OSC" to="SiteCoordination.SitesCoord120223OSC"}%
