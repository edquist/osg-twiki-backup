%META:TOPICINFO{author="DanFraser" date="1326658718" format="1.1" reprev="1.43" version="1.43"}%
---+!! %MAKETEXT{"Campus Grids Homepage" }%
<table cellpadding="5" cellspacing="2"><tr><td width="40%">

---++ What is a "campus grid"?
A Campus Grid is a resource sharing capability that enables faculty, students, and researchers to submit jobs to multiple computational resources simultaneously using only their campus identity management credentials. A campus grid is not limited to resources on a campus but incorporates the ability to use resources directly from other campuses and can also tie into resources from the national cyberinfrastructure including XSEDE and the Open Science Grid (OSG).

---++ How does a campus grid benefit researchers and students?
Typically in today's universities and research institutions a researcher will log into a favored computational resource and run her jobs only on that resource. Researchers become very familiar with the local environment (e.g. batch schedulers, file systems, and idiosyncracies of the operating environment) and may only rarely learn to utilize other resources that are not as familiar. In some cases the research itself becomes limited to the computational abilities that researchers recognize as available to them.  A campus grid capability enables researchers to move beyond the limitations of a single resource and expand their research programs to utilize orders of magnitude larger pools of heterogeneous resources. In many cases the transition involving access to orders of magnitude more resources has proven to be transformational to the research program. 

---++ What types of research applications can benefit from a campus grid capability?
The computational model underlying the campus grid is designed to be effective for: high throughput, pleasantly or small-scale parallel applications; job runs of between one hour and a couple of days; jobs that can be check- pointed; explicit management of data movement and storage; and ensembles of jobs that can effectively run across a large number of resources. We refer to this capability as High Throughput Computing (HTC). HTC is a natural mechanism to share and distribute workloads since single processor jobs can often be distributed across multiple systems. One common HTC pattern for example is the use of parameter sweeps whereby independent jobs are submitted and each runs with different parameters, but this is one of many different patterns. HTC research applications fall in all areas of science including weather modeling; computational chemistry and chemical engineering; bioinformatics; image/data processing; physics; math; computer science; ... 

---++ Key advantages of a campus grid infrastructure:
Adoption of HTC sharing capabilities within a campus offers benefits in several key areas: 
   * Use of a larger resource pool 
   * Improved (policy driven) resource utilization 
   * Higher fault tolerance (If one system goes out, others can take up the load)
   * Reduced costs 
In short, shared campus infrastructures make researchers (and communities) more competitive by enabling access to more resources, and allowing them to get more science done faster. 

---++ How does a campus grid capability work in practice?
Faculty, researchers, and students log into a resource called a "submit host" that is connected to multiple computational resources both on and off the campus. The technology is based on a recently developed Campus Factory overlay component that leverages Condor [[http://www.cs.wisc.edu/condor/manual/v6.8/5_2Connecting_Condor.html]["flocking"]] and "glide-in" mechanisms to gather multiple resources into a resource "pool" and make them available to a submit host user. The submit host user sees a single operating environment and is thereby shielded from many of the multiple different environmental interfaces that would otherwise be requred to access each of the resources in the pool independently. These heterogeneities arise both from the resource capabilities (e.g. processor type, operating system, batch scheduler, ...) as well as in their operating and administrative environments. 

---++ Examples of Campus Grid infrastructures:
   * Condor pools are a commonly found example of campus grids that connect multiple Condor clusters using the [[http://www.cs.wisc.edu/condor/manual/v6.8/5_2Connecting_Condor.html][flocking]] capability. The Condor systems are the submit hosts that are used to transparently distribute HTC jobs across other Condor systems. Condor pools can exist primarily on a single campus such as the Baker lab or can span multiple campuses. One large working example is [[https://en.wikipedia.org/wiki/DiaGrid_%28distributed_computing_network%29][Diagrid]], a Condor based campus grid that connects multiple campuses at Purdue, Notre Dame, Indiana University, Indiana State University, the University of Notre Dame, the University of Louisville, the University of Wisconsin, Purdue's Calumet and North Central campuses, and Indiana University-Purdue University Fort Wayne.
   * More general campus grid infrastructures incorporate non-Condor resources such as the University of Nebraska that incorporates a mixture of Condor and PBS clusters, and incorporate multiple distributed campus resources from both the University of Nebraska and Lincoln. At Virginia Tech University, there are no Condor clusters -- all the resources use PBS as a batch scheduler. Both university campus grids also enable their users to use OSG resources as well.  

---++ Campus grids and the Open Science Grid (OSG)

The OSG is fundamentally invested in enabling High Throughput Computing (HTC) capabilities locally at the nations campuses in support of science, research and education locally as well as enabling these campuses to participate fully in the national cyber-infrastructure. To lower the barriers for campuses to adopt shared infrastructures the OSG recognizes that local campus credentials should be sufficient to access shared campus resources. At the same time, users should be able to use the exact same job submission framework to extend their HTC usage to submit jobs either to the OSG or to XSEDE resources.

The OSG has developed and is currently offering a Campus Factory overlay mechanism that allows multiple heterogeneous clusters to participate in a shared infrastructure (clusters can utilize PBS or LSF schedulers in addition to Condor) . This is currently being used in production at the University of Nebraska. One submit host at UNL is able to send jobs across University of Nebraska, Omaha and Lincoln campuses, Purdue university, and NERSC. Another important advantage is that it does not require root access to install the Campus Factory on a remote resource. Click the links below to see the documentation and how to try out this model on your campus. 

OSG is directly engaged with the US LHC Tier-3 sites on helping with their campus installations and broader collaborations: [[https://twiki.grid.iu.edu/bin/view/Production/USLHCTier-3Group][US LHC - OSG Tier-3 Group]]. OSG also works with other organizations on [[http://cidays.org][cross-cutting activities]] to foster and help with local organizing across the university's groups.

---++ Support for campus grids
Contact the mail list campusgrids@fnal.gov or a member of the OSG Campus Activity at for more information and support.

<p>
---++ Campus Infrastructure Links
---+++ Campus Infrastructure Documentation
   * [[Documentation.CampusFactoryInstall][Campus Grid Setup]]
   * [[CampusGrids.ConfiguringRemoteSubmissionHost][Configuring Remote Submission Host]]
   * [[CampusGrids.RunningCampusGridJobs][Running Campus Grid Jobs]]
   * [[Finding potential HTC users on your campus]]
   * [[http://twiki.grid.iu.edu/bin/viewauth/CampusGrids/OfflineClassAdFactory][Research: Using Offline Class Ads]]
   * [[CampusGrids.InstallCondorFlockSubmit][Installing a Condor Flocking host]] - Install a submit host that will flock with well-known osg submit hosts.
   * [[CampusGrids.InstallUsageMonitor][Installing OSG Usage Monitoring]] - Install the OSG Usage monitoring on a Campus Grid.

---+++ Campus Grid Meeting Notes & Presentations
   * [[http://twiki.grid.iu.edu/bin/view/CampusGrids/CampusGridMeetings][Campus Grid Notes & Presentations]]

---+++ Activities and Links prior to Sept 2010
%TWISTY{}%
   * [[AboutCampusGrids][More about the Campus Grids activity]]
      * Goals
      * Activities
   * [[http://cidays.org/index.php?title=Campus_Grids][Information about existing campus grids with whom we work]] 
   * [[WindowsPlan][On the use of Windows Resources in OSG]]
   * [[OSGcelive][OSG Live, easy install for new Campus Grids]]
   * [[CampusgridContact][Technical Contact Listing]]
   * CampusGridMeetings
   * [[CampusGridTechnology][Campus Grid Technology Overview]]
   * Latest Campus Grid talk at Western Kentucky University, April 30th 2010:

<p><iframe src="http://docs.google.com/present/embed?id=dgfk2k9z_8594gf4brc3" frameborder="0" width="410" height="342"></iframe></p>

---+++ Virtualization and Clouds
Virtualization and Clouds can help Campus grids build a resource they can share on the grid. Current efforts with the STAR VO are listed below.
They summarize how a campus can support an OSG VO via virtualization. Several other grass root efforts in Clouds and Virtualization were discussed at the 2010 All hands meeting [[http://indico.fnal.gov/sessionDisplay.py?sessionId=9&slotId=0&confId=2871#2010-03-08][VM Workshop]].

   * [[DeployingTheSTARVM][Deploying the STAR VM at Clemson]]
   * [[InstallingKestrel][Installing and Using the Kestrel Scheduler]]
   * [[UsingKVM][Starting Virtual Machines with KVM]]
   * [[http://www.cs.wisc.edu/condor/manual/v7.0/2_11Virtual_Machine.html][Condor and Virtual Machines]]
   * [[http://www.nimbusproject.org][Nimbus]]
   * [[http://www.opennebula.org][Opennebula]]

   * [[http://sourceforge.net/apps/trac/campusfactory][Old Campus Glide-in Factory Download Site (includes a description and installation guides)]]

%ENDTWISTY%

<!--
</td>
<td width="10%">
</td>
<td valign="top" width="20%">
*Campus Grids team*

[[Main.JohnMcGee][John !McGee]], RENCI <br />
[[http://runseb.googlepages.com/][Sebastien Goasguen]], Clemson University <br>
[[Main.DanFraser][Dan Fraser]] and <br>
Contributions from:<br>
[[Main.StevenTimm][Steve Timm]]<br>
[[Main.DanBradley][Dan Bradley]]<br>
and the Executive Team osg-et@opensciencegrid.org</tr>
<tr>


</tr>
-->
</table>

This 

%META:FILEATTACHMENT{name="goasguen.jpg" attr="" autoattached="1" comment="Sebastien Goasguen, Clemson University" date="1201027104" path="goasguen.jpg" size="84155" user="Main.AnneHeavey" version="1"}%
%META:FILEATTACHMENT{name="mcgee.jpg" attr="" autoattached="1" comment="John !McGee, RENCI" date="1201027074" path="mcgee.jpg" size="28977" user="Main.AnneHeavey" version="1"}%
