%META:TOPICINFO{author="DanFraser" date="1332475452" format="1.1" version="1.49"}%
---+!! %MAKETEXT{"Campus Infrastructure Homepage" }%
<table cellpadding="5" cellspacing="2"><tr><td width="40%">

---++ What is a "Campus Infrastructure"?
%DOC_STATUS_TABLE%
A Campus Infrastructure is a resource sharing capability that enables faculty, students, and researchers to submit jobs to multiple computational resources simultaneously using only their campus identity management credentials. A campus infrastructure is not limited to resources on a campus but incorporates the ability to use resources directly from other campuses and can also tie into resources from the national cyberinfrastructure such as the Open Science Grid (OSG) or XSEDE resources. (While XSEDE resources are not currently being utilized, there is no technological reason  althoughere is no technological reason why these could not also be connected to campus infrastructures.)

---++ How does a campus infrastructure benefit researchers and students?
Typically in today's universities and research institutions a researcher will log into a favored computational resource and run her jobs only on that resource. Researchers become very familiar with the local environment (e.g. batch schedulers, file systems, and idiosyncracies of the operating environment) and may only rarely learn to utilize other resources that are not as familiar. In some cases the research itself becomes limited to the computational abilities that researchers recognize as available to them.  A campus infrastructure capability enables researchers to move beyond the limitations of a single resource and expand their research programs to utilize orders of magnitude larger pools of heterogeneous resources. In many cases the transition involving access to orders of magnitude more resources has proven to be transformational to the research program. 

---++ What types of research applications can benefit from a campus grid capability?
The computational model underlying the campus infrastructure is designed to be effective for: high throughput, pleasantly or small-scale parallel applications; job runs of between one hour and a couple of days; jobs that can be check- pointed; explicit management of data movement and storage; and ensembles of jobs that can effectively run across a large number of resources. We refer to this capability as High Throughput Computing (HTC). HTC is a natural mechanism to share and distribute workloads since single processor jobs can often be distributed across multiple systems. One common HTC pattern for example is the use of parameter sweeps whereby independent jobs are submitted and each runs with different parameters, but this is one of many different patterns. HTC research applications fall in all areas of science including weather modeling; computational chemistry and chemical engineering; bioinformatics; image/data processing; physics; math; computer science; ... 

---++ Key advantages of a campus infrastructure:
Adoption of HTC sharing capabilities within a campus offers benefits in several key areas: 
   * Use of a larger resource pool 
   * Improved (policy driven) resource utilization 
   * Higher fault tolerance (If one system goes out, others can take up the load)
   * Reduced costs 
In short, shared campus infrastructures make researchers (and communities) more competitive by enabling access to more resources, and allowing them to get more science done faster. 

---++ How does a campus infrastructure work in practice?
Faculty, researchers, and students log into a resource called a "submit host" that is connected to multiple computational resources both on and off the campus. The technology is based on a recently developed Campus Factory overlay component that leverages Condor [[http://www.cs.wisc.edu/condor/manual/v6.8/5_2Connecting_Condor.html]["flocking"]] and "glide-in" mechanisms to gather multiple resources into a resource "pool" and make them available to a submit host user. The submit host user sees a single operating environment and is thereby shielded from many of the multiple different environmental interfaces that would otherwise be requred to access each of the resources in the pool independently. These heterogeneities arise both from the resource capabilities (e.g. processor type, operating system, batch scheduler, ...) as well as in their operating and administrative environments. 

---++ Examples of Campus Grid infrastructures:
   * Condor pools are a commonly found example of campus grids that connect multiple Condor clusters using the [[http://www.cs.wisc.edu/condor/manual/v6.8/5_2Connecting_Condor.html][flocking]] capability. The Condor systems are the submit hosts that are used to transparently distribute HTC jobs across other Condor systems. Condor pools can exist primarily on a single campus such as GLOW or can span multiple campuses. One large working example is [[https://en.wikipedia.org/wiki/DiaGrid_%28distributed_computing_network%29][Diagrid]], a Condor based campus grid that connects multiple campuses at Purdue, Notre Dame, Indiana University, Indiana State University, the University of Notre Dame, the University of Louisville, the University of Wisconsin, Purdue's Calumet and North Central campuses, and Indiana University-Purdue University Fort Wayne.
   * More general campus  infrastructures incorporate non-Condor resources such as the University of Nebraska that incorporates a mixture of Condor and PBS clusters, and incorporate multiple distributed campus resources from both the University of Nebraska and Lincoln. At Virginia Tech University, there are no Condor clusters -- all the resources use PBS as a batch scheduler. Both university campus grids also enable their users to use OSG resources as well.  

---++ Campus grids and the Open Science Grid (OSG)

The OSG is fundamentally invested in enabling High Throughput Computing (HTC) capabilities locally at the nations campuses in support of science, research and education locally as well as enabling these campuses to participate fully in the national cyber-infrastructure. To lower the barriers for campuses to adopt shared infrastructures the OSG recognizes that local campus credentials should be sufficient to access shared campus resources. At the same time, users should be able to use the exact same job submission framework to extend their HTC usage to submit jobs either to the OSG or to XSEDE resources.

The OSG has developed and is currently offering a Campus Factory overlay mechanism that allows multiple heterogeneous clusters to participate in a shared infrastructure (clusters can utilize PBS or LSF schedulers in addition to Condor) . This is currently being used in production at the University of Nebraska. One submit host at UNL is able to send jobs across University of Nebraska, Omaha and Lincoln campuses, Purdue university, and NERSC. Another important advantage is that it does not require root access to install the Campus Factory on a remote resource. Click the links below to see the documentation and how to try out this model on your campus. 

OSG is directly engaged with the US LHC Tier-3 sites on helping with their campus installations and broader collaborations: [[https://twiki.grid.iu.edu/bin/view/Production/USLHCTier-3Group][US LHC - OSG Tier-3 Group]]. OSG also works with other organizations on [[http://cidays.org][cross-cutting activities]] to foster and help with local organizing across the university's groups.

---++ Support for campus grids
Contact the mail list campusgrids@fnal.gov or a member of the OSG Campus Activity at for more information and support.

<p>
---++ Campus Infrastructure Links
---+++ Campus Infrastructure Documentation
   * [[Documentation.CampusFactoryInstall][Campus Grid Setup]]
   * [[CampusGrids.ConfiguringRemoteSubmissionHost][Configuring Remote Submission Host]]
   * [[CampusGrids.RunningCampusGridJobs][Running Campus Grid Jobs]]
   * [[Finding potential HTC users on your campus]]
   * [[http://twiki.grid.iu.edu/bin/viewauth/CampusGrids/OfflineClassAdFactory][Research: Using Offline Class Ads]]
   * [[CampusGrids.InstallCondorFlockSubmit][Installing a Condor Flocking host]] - Install a submit host that will flock with well-known osg submit hosts.
   * [[CampusGrids.InstallUsageMonitor][Installing OSG Usage Monitoring]] - Install the OSG Usage monitoring on a Campus Grid.

---+++ Campus Grid Meeting Notes & Presentations
   * [[http://twiki.grid.iu.edu/bin/view/CampusGrids/CampusGridMeetings][Campus Grid Notes & Presentations]]
   * [[Campus Infrastructure Project Plan]]
   * [[Campus Infrastructure Best Practices]]


---+++ Activities and Links prior to Sept 2010
%TWISTY{}%
   * [[AboutCampusGrids][More about the Campus Grids activity]]
      * Goals
      * Activities
   * [[http://cidays.org/index.php?title=Campus_Grids][Information about existing campus grids with whom we work]] 
   * [[WindowsPlan][On the use of Windows Resources in OSG]]
   * [[OSGcelive][OSG Live, easy install for new Campus Grids]]
   * [[CampusgridContact][Technical Contact Listing]]
   * CampusGridMeetings
   * [[CampusGridTechnology][Campus Grid Technology Overview]]
   * Latest Campus Grid talk at Western Kentucky University, April 30th 2010:

<p><iframe src="http://docs.google.com/present/embed?id=dgfk2k9z_8594gf4brc3" frameborder="0" width="410" height="342"></iframe></p>

---+++ Virtualization and Clouds
Virtualization and Clouds can help Campus grids build a resource they can share on the grid. Current efforts with the STAR VO are listed below.
They summarize how a campus can support an OSG VO via virtualization. Several other grass root efforts in Clouds and Virtualization were discussed at the 2010 All hands meeting [[http://indico.fnal.gov/sessionDisplay.py?sessionId=9&slotId=0&confId=2871#2010-03-08][VM Workshop]].

   * [[DeployingTheSTARVM][Deploying the STAR VM at Clemson]]
   * [[InstallingKestrel][Installing and Using the Kestrel Scheduler]]
   * [[UsingKVM][Starting Virtual Machines with KVM]]
   * [[http://www.cs.wisc.edu/condor/manual/v7.0/2_11Virtual_Machine.html][Condor and Virtual Machines]]
   * [[http://www.nimbusproject.org][Nimbus]]
   * [[http://www.opennebula.org][Opennebula]]

   * [[http://sourceforge.net/apps/trac/campusfactory][Old Campus Glide-in Factory Download Site (includes a description and installation guides)]]

%ENDTWISTY%

<!--
</td>
<td width="10%">
</td>
<td valign="top" width="20%">
*Campus Grids team*

[[Main.JohnMcGee][John !McGee]], RENCI <br />
[[http://runseb.googlepages.com/][Sebastien Goasguen]], Clemson University <br>
[[Main.DanFraser][Dan Fraser]] and <br>
Contributions from:<br>
[[Main.StevenTimm][Steve Timm]]<br>
[[Main.DanBradley][Dan Bradley]]<br>
and the Executive Team osg-et@opensciencegrid.org</tr>
<tr>


</tr>
-->
</table>

 <!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
DOCUMENT OWNER
======================
 
   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = DanFraser
 
   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|General|Integration|Monitoring|Operations|Security|SmallSite|Storage|User|VO)
   * Local DOC_AREA       = General
 
   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Any|Developer|Documenter|Scientist|Student|SysAdmin|VOManager)
   * Local DOC_ROLE       = Scientist
 
   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Team|Training|Troubleshooting)
   * Local DOC_TYPE       = Team
 
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %NO%
 
   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%
 
   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%
 
   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%
 
   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%
 
 
DOCUMENT REVIEWER
======================
 
   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = 
 
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%
 
 
DOCUMENT TESTER
======================
 
   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
 
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
 -->
 

%META:FILEATTACHMENT{name="goasguen.jpg" attr="" autoattached="1" comment="Sebastien Goasguen, Clemson University" date="1201027104" path="goasguen.jpg" size="84155" user="Main.AnneHeavey" version="1"}%
%META:FILEATTACHMENT{name="mcgee.jpg" attr="" autoattached="1" comment="John !McGee, RENCI" date="1201027074" path="mcgee.jpg" size="28977" user="Main.AnneHeavey" version="1"}%
