%META:TOPICINFO{author="DougOlson" date="1263969814" format="1.1" reprev="1.20" version="1.20"}%
%META:TOPICPARENT{name="CampusGridMeetings"}%
---+!! <nop>%TOPIC%
%TOC%

---++ Introduction
   * Date: 1/19/10 to 1/20/10
   * Location: Fermilab
   * A working meeting to prepare a technical summary document describing and comparing current CG implementations, best practices and common patterns, as well as address technical issues moving forward in four topical areas identified last time (there may be more resulting from the workshop): global storage systems at the CG, seamless user environments, role and use of vm & cloud computing technologies, as well as challenges dealing with campus provincial issues (or how technical progress might inform that, or stimulating growth of a movement/community that provide clear examples of the benefits associated with CGs)
   * Previous planning meetings: MinutesDec15, MinutesJan5

---++ Workshop organization
   * The meeting will being at 10 am, January 19 and end at 4pm January 20
   * The deliverable of the meeting will be the CG technical document signed by all participants
   * The first morning will be providing a download and inventory of current implementations, noting best practices & challenges.   A comparison table will be created summarizing this discussion.  We expect to hear about GLOW, Purdue, Fermigrid, and Chapel Hill in particular.
   * The first afternoon will be driven by topic, namely the four mentioned above (at least).  Before the end of the first day people will be assigned topics for overnight homework/R&D and will prepare more in-depth summary notes and references for input into the document. 
   *  The morning of January 20 - each person will present their topical summary to the group for discussion.
   * Afternoon January 20: *collating* of contributions into the CG technical summary document.
   * Adjourn by 4pm, with a completed document submitted to the OSG EB.

---++ Workshop logistics
   * Tuesday, 19-Jan-2010 - Fermilab, Wilson Hall 3rd Floor North West Conference Room
   * Wednesday, 20-Jan-2010 - Fermilab, Wilson Hall 1st Floor North West Conference Room

---++ Audio conferencing instructions
   * Meeting ID: 226787
   * Meeting Password: none 
   * To attend via telephone dial 510-665-5437
   * To attend via web conference,
   * 1. Go to: http://abm.es.net/a/67326ee4bb3f812d1b89d6dd66c80020
   * 2. Sign in as a Guest or with your !Cisco MeetingPlace User ID, and click on Attend Meeting.
   * 3. Click on CONNECT and enter your phone and click OK. The system will now call you. 

---++ Attendees
   * Brian Bockelman (Nebraska)
   * Dan Bradley (Wisconsin)
   * Keith Chadwick (Fermilab)
   * Steve Gallo (Buffalo)
   * Sebastien Goasguen (Clemson)
   * Rob Gardner (U Chicago)
   * John !McGee (RENCI)
   * Doug Olson (LBNL)
   * Preston Smith (Purdue)
   * Sam Hoover (Clemson)
   * Ben Cotton (Purdue, bcotton@purdue.edu)
   * Prakashan Korambath (UCLA)  , Kejian Jin
   * Bill Strossman (UC Riverside)


---++ *January 19 - Notes*
   * 10 am CST - Introductions, overview, discussion
   *  Rob: Preliminary thoughts - what we can accomplish, etc.
   *  John: interest in how do you bring new campuses that have not yet been involved in grid computing - esp those that do not have existing knowledge base, history expertise in the required technolgies.  And competing interests and budgets. *How can commercial products be leveraged* with OSG software - eg. metaschedulers, open-source cloud computing. 
   *  Preston - at PU focusing on departments that don't have an CI already setup.   What are the small school or regional campus grids.
   *  Dan - concern about having the key people in place at the campus - people who can bridge the technical-political issues.   Needs to be a *local champion* who can convince them of the local benefit.  
   *  At Purdue --> Clemson it was Jim Bottom (CIO level)
   *  Steve (leads sw eng, grid comp) - imp to have a good relationship with CIO.  at UB - have HPC and computing IT. There is a good relationship between.  Have campus condor grid.  *Outreach.* Engaging Deans, getting support.  Providing storage and condor resources - defining roles for the community - proactive in the beginning.  
   *  Ben - faculty get what they want - get them.
   *  Keith - fermigrid is the fermilab campus grid. Laboratory CIO charge - consolidate diverse cyber silos.  (cdf, d0, cms t1, farm, etc etc).   Detailed knowledge needed to maintain.  Demand for centralized services. Decreasing budget constraints.  Construct out of pieces.  Central services - voms, gums, minimal site gateway (2005).   Demonstrating *interoperability*.  Services need to be *highly available* (!FermigridHA) - load share and redundancy.  Clustered filesystem (blue arc appliance).  About to release a !MyProxyHA, security.
   * Brian: the buy-in and idea materializing in Nebraska - it was always requiring buy-in, better to buy-in the go it alone and buy your own cluster, managed by grad students.  Led to old clusters around campus. vice chancellor of technology - understands economy of scale of pooling resources.   That resulted in grid initiative.  More than grid - integration tasks (eg. username/pw, training users).  *Education* will be an important part - eg. workflows and file management. 
   * Sam - big change came w/ Jim as CIO - required commitment of VP.  Unification became possible - IT working closely with faculty on research projects. 
   * Prakashan - coming from HPC background - campus resources very decentralized.  Underutilized resources - want to find a way to distribute these, across the campus.  Users coming from various backgrounds (eg. Windows) - teaching required, then they leave.   On-going teaching - led to a unified interface, web, independent of backend schedulers.  Led to development of UC  grid interface, two kinds of users, full users.  We take care of all the software install, etc.  Metascheduling across clusters.  Some applications are costly.  Campus authentication moving towards Shiboleth identification.  Workflow interfaces for very long running tasks (~ month) requiring little manual intervention.  6 campuses - resulting in UC Grid portal across system.  Includes Teragrid.  Telephone monthly meetings.  Grid agnostic. 
   * Doug - interested from point of view of STAR - grid computing too complicated.  Using VMs to capture software environment to run on more resources, Amazon and OSG.  Magellan at NSERC and Argonne to use cloud for HPC and sci computing.  Not clear how cloud can connect with campus grids, interested in exploring this further.   Deployed a gk on one amazon node, fired up other nodes for worker nodes.  Seemed straightforward and easy.  Lots of questions of how to operate clusters in a cloud-like way.   LBL in particular - there is a central IT group that run clusters for sci computing, independent.  Has not been much interest in sharing these with grid users. Stakeholders don't see much of an advantage. 
   * Question of *incentives* to *share*, and *trade* resources.  Having a fully utilized cluster is a strong incentive.  ATLAS vs CMS - ATLAS more costly - no spare cycles.  Difference in mindset between ATLAS and CMS... it never finishes in ATLAS. 
   * UCLA experience in shared resources
   * Community - researchers pool resources, make large purchase, university puts up infrastructure and staff. 
   * Sam Hoover - same thing in place at Clemson
   * Implementations - features - best practices - issues & challenges
      * GLOW - see slides
         *  large condor pool on campus - large pooled grant
         *  incentive - will get a bunch of new machines, other incentives as well
         *  favorable from university's perspective - CHTC - center for high throughput computing targeting smaller users
         *  condor flocking used to glue them together
         *  unused cycles get used by groups with large, unlimited workflows 
         * some entry point for OSG
         * one central sys admin - have students in place around campus who are knowledgeable
         * members own the machines - so they have preemptive rights.  provides incentive for keeping their resources well managed.
         * monthly face-to-face meetings; some training for students
         * discussion about *buy-in scale*.  comparison UCLA, higher.  What makes sense for participation depends on the scale - whether opportunistic, or if its steady state  use of a good fraction of a rack then the other model applies (contributing a rack(s) to the pool)
         * 1/3 opportunistic; serial jobs and condor standard universe is popular
         * harder - large MPI - works best on dedicated owned resource due to requirements (eg. infiniband fabric).
         * question of schedulers to use
         * Connecting to OSG.  Getting GLOW stakeholder users running on OSG - exploring !GlideinWMS.  Note - similarity to Engage efforts.   Nebraska also experimenting with !GlideinWMS. 
         * Question of the *execution environment* inside GLOW.  Does it carry over to OSG?   Access to software.  (OSG-wide software cache?  AFS like, or Engage maintenance jobs).  
            *  Idea of software squid caches at sites synching from a central repository
            * What about vetting?  How high should the hurdle be?
         * AFS, NFSLite; individual groups manage their own storage.  No sharing of storage.   OSG workernode client provides client access.
         * FTE to engage users; engagement of PIs
         * Incentives: research groups do not have to pay for power.   CS provides more carrots - sysadmin, tech support, and more opportunistic machines
      * Purdue
         * see slides below
         * community cluster program / condo clustering 
         * cost effectiveness of procurement
         * PBS and Condor both used - PBS main, Condor the opportunistic; =condor_vacate= used in PBS prologue and eplilogue
         * Condor jobs are preemptable - they are the lowest tier.  Educating users as to best submit jobs - to avoid preemption
         * NFS on !BlueArc; 
         * Boot camp for Condor - management interface for deployment
         * Scoreboard
         * !CycleServer
         * Windows hurdle; porting cost; virtualization more attractive solution
         * discussion of distributed hadoop as filesystem for campus-wide aggregated storage; security issues and authentication infrastructure; yahoo interest; 
         * note purdue has centralized auth infra pw and uid, etc.
         * campus grid buy-in at the highest levels at Purdue
         * faculty buy-in
         * has grown to multi-campus grid

   * Lunch

   * Implementations, cont 
      * Chapel Hill 
         * Engagement is based at RENCI, based at UNC-Chapel Hill.   Difficulties getting traction for a number of reasons.
         * Director of research computing interested in large cluster purchases - campus grid less interesting.
         * Lack of commitment / engagement - need for grass roots effort?  could be successful, but the effort isn't there
         * At Duke the research computing under redesign - focus now being condo computing; getting users onto OSG as a result; this part has well. 
         * NCState - needs VCL involvement.  
         * One take away is recognition of the *effort* required to engage campus stakeholders (faculty, IT computing departments, etc)
      * U California 
         * see slides
         * each campus has its own portal
         * No connection yet with OSG clusters (eg. at UCSD, UCLA)
         * Grid appliance to each cluster which joins makes it easy; based on WS-Globus
         * Heavily web-based
         * Resource selection
         * Uses VNC sessions to run x applications over the network; runs on two compute nodes 
         * Job submission
         * Using grid desktop application - Flex FTK 3.5 from Adobe  (much better than gridsphere)
         * The portal has lots of options for pre-defined jobs; access to home directories on remote clusters
         * !MyProxy server key to authentication scheme
         * Q: what kinds of jobs are you targeting? eg 400K, 100K type jobs?  Yes, its possible.
         * Q: what percentage of jobs are pre-canned (eg. matlab) versus general user jobs
         * Q: application integration - eg. R - done by hand by the developers
     * Fermigrid
         * Slides coming 
         * Four things were needed: common grid services; stakeholder bilateral interop; deploy OSG interfaces; expose storage
         * uids/gid coordinated site-wide
         * site-wide NFS
         * lots of many stakeholders - persuaded by value added, and strong leadership
         * balancing act between the little guys, and the big dog
         * fermigrid allocations committee - discussion of utilization
         * every other week there is a grid users meeting - reach further than production coordinators
         * hidden clusters require access via the site-wide gateway
         * high availability services for voms, gums, saz
         * issue regarding opportunistic storage - how to handle requests such as "loan me 10 TB for a couple years"
         * Posix still preferred
         * I/O scalability issues with blue arc for case of large numbers of an application attempt to randomly access files simultaneously
         * virtualization has benefits and drawback
      * Nebraska
         * see slides
         * incubation phase, putting together ideas
         * 3 FTE dedicated to !GridNebraska
         * need for commoditized workflow, job submission
         * data management issues (eg. archival, respositories, tertiary, etc) maybe the most daunting
      * Clemson
         *  
         * 
         * 
      * NYSG (TBC)
         * 
         * 
         * 


   * Topical discussion
      * Deployment for campus-wide scheduling
      * Global storage systems at the CG
      * Seamless user environments
      * role and use of vm & cloud computing technologies; Magellan case
      * Challenges dealing with campus provincial issues (or how technical progress might inform that, or stimulating growth of a movement/community that provide clear examples of the benefits associated with CGs)

---++ Document Writing
   * Questions
      * Who is reading this?  Ans: OSG-EB
      * Why campus grid versus traditional campus cyberinfrastructure
      * What can it do if I want one?
      * Who needs to be on board, how to get hardware?
      * How is it shared?
      * Storage questions 
      * How to get departments together - engage
   * Homework assignments
      * Intro / exec summary - Rob
      * Opportunities - Dan
      * Difficulties on Street - John
      * Outreach and Engagement - Ben
      * Strategies for Resource Aggregation - Preston
      * Connecting Researchers to Resources - the User Experience: Brian, Prakashan
      * Leveraging Emerging Technologies - Doug  ( [[%ATTACHURL%/emerging-tech.doc][emerging-tech.doc]]: draft of emerging technology section)
         * Drivers and issues
         * VMs
         * Cloud 
      * Planning for Success - Keith
         * Sustainability
         * Operations - efficiency and availability
         * Critical services
      * How can OSG/OSG' help with Campus Grids?
         * What/should OSG be doing here?
         * Collecting best practices certainly
         * Identifying patterns
         * Building communities of expertise
         * Connections to campus resource sharing (eg. MOAB schedulers).. bridges
         * What capabilities/offerings does OSG itself bring?  Shared filesystems across multiple campuses as an example;   Sharing resources between campuses.


---++ January 20
   * 9 am
   * Review overnight findings from topical homework, document contributions
   * Lunch
   * Afternoon: *collating* of contributions into the CG technical summary document.
   * 4pm adjourn


---++ References
   * [[%ATTACHURL%/GridTrust.pdf][GridTrust.pdf]]: Grid Trust

   * [[%ATTACHURL%/ucgrid-fermilab-V1a.ppt][ucgrid-fermilab-V1a.ppt]]: UCGrid overview (ppt) - Prakashan Korambath

   * [[%ATTACHURL%/ucgrid-fermilab-V1a.pdf][ucgrid-fermilab-V1a.pdf]]: UCGrid overview (pdf) - Prakashan Korambath

   * [[%ATTACHURL%/Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf][Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf]]: Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf

   * [[%ATTACHURL%/ucgrid-fermilab-V1a.pdf][ucgrid-fermilab-V1a.pdf]]: UCGrid overview (pdf) - Prakashan Korambath



%META:FILEATTACHMENT{name="GridTrust.pdf" attachment="GridTrust.pdf" attr="" comment="Grid Trust" date="1263504878" path="GridTrust.pdf" size="1571669" stream="GridTrust.pdf" tmpFilename="/usr/tmp/CGItemp12798" user="RobGardner" version="1"}%
%META:FILEATTACHMENT{name="ucgrid-fermilab-V1a.ppt" attachment="ucgrid-fermilab-V1a.ppt" attr="" comment="UCGrid overview (ppt) - Prakashan Korambath" date="1263869337" path="ucgrid-fermilab-V1a.ppt" size="3383296" stream="ucgrid-fermilab-V1a.ppt" tmpFilename="/usr/tmp/CGItemp11931" user="DougOlson" version="1"}%
%META:FILEATTACHMENT{name="ucgrid-fermilab-V1a.pdf" attachment="ucgrid-fermilab-V1a.pdf" attr="" comment="UCGrid overview (pdf) - Prakashan Korambath" date="1263931144" path="ucgrid-fermilab-V1b.pdf" size="3347590" stream="ucgrid-fermilab-V1b.pdf" tmpFilename="/usr/tmp/CGItemp16572" user="DougOlson" version="2"}%
%META:FILEATTACHMENT{name="GridNebraska.pdf" attachment="GridNebraska.pdf" attr="" comment="" date="1263917132" path="GridNebraska.pdf" size="145052" stream="GridNebraska.pdf" tmpFilename="/usr/tmp/CGItemp17580" user="BrianBockelman" version="1"}%
%META:FILEATTACHMENT{name="Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf" attachment="Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf" attr="" comment="" date="1263924731" path="Purdue - OSG Campus Grid Workshop Jan 19 2010.pdf" size="5907823" stream="Purdue - OSG Campus Grid Workshop Jan 19 2010.pdf" tmpFilename="/usr/tmp/CGItemp24600" user="PrestonSmith" version="2"}%
%META:FILEATTACHMENT{name="CrimsonGrid.pdf" attachment="CrimsonGrid.pdf" attr="" comment="info gathered about Crimson Grid" date="1263917912" path="C:\Users\mcgee\Documents\CrimsonGrid.pdf" size="39805" stream="C:\Users\mcgee\Documents\CrimsonGrid.pdf" tmpFilename="/usr/tmp/CGItemp32465" user="JohnMcGee" version="1"}%
%META:FILEATTACHMENT{name="GLOWCampusGridsFNAL.pdf" attachment="GLOWCampusGridsFNAL.pdf" attr="" comment="" date="1263925005" path="GLOWCampusGridsFNAL.pdf" size="497050" stream="GLOWCampusGridsFNAL.pdf" tmpFilename="/usr/tmp/CGItemp16476" user="DanBradley" version="2"}%
%META:FILEATTACHMENT{name="emerging-tech.doc" attachment="emerging-tech.doc" attr="" comment="draft of emerging technology section" date="1263969754" path="emerging-tech.doc" size="24576" stream="emerging-tech.doc" tmpFilename="/usr/tmp/CGItemp12172" user="DougOlson" version="1"}%
