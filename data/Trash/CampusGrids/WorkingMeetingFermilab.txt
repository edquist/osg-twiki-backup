%META:TOPICINFO{author="RobGardner" date="1263920658" format="1.1" reprev="1.11" version="1.11"}%
%META:TOPICPARENT{name="CampusGridMeetings"}%
---+ <nop>%TOPIC%
%TOC%

---++ Introduction
   * Date: 1/19/10 to 1/20/10
   * Location: Fermilab
   * A working meeting to prepare a technical summary document describing and comparing current CG implementations, best practices and common patterns, as well as address technical issues moving forward in four topical areas identified last time (there may be more resulting from the workshop): global storage systems at the CG, seamless user environments, role and use of vm & cloud computing technologies, as well as challenges dealing with campus provincial issues (or how technical progress might inform that, or stimulating growth of a movement/community that provide clear examples of the benefits associated with CGs)
   * Previous planning meetings: MinutesDec15, MinutesJan5

---++ Workshop organization
   * The meeting will being at 10 am, January 19 and end at 4pm January 20
   * The deliverable of the meeting will be the CG technical document signed by all participants
   * The first morning will be providing a download and inventory of current implementations, noting best practices & challenges.   A comparison table will be created summarizing this discussion.  We expect to hear about GLOW, Purdue, Fermigrid, and Chapel Hill in particular.
   * The first afternoon will be driven by topic, namely the four mentioned above (at least).  Before the end of the first day people will be assigned topics for overnight homework/R&D and will prepare more in-depth summary notes and references for input into the document. 
   *  The morning of January 20 - each person will present their topical summary to the group for discussion.
   * Afternoon January 20: *collating* of contributions into the CG technical summary document.
   * Adjourn by 4pm, with a completed document submitted to the OSG EB.

---++ Workshop logistics
   * Tuesday, 19-Jan-2010 - Fermilab, Wilson Hall 3rd Floor North West Conference Room
   * Wednesday, 20-Jan-2010 - Fermilab, Wilson Hall 1st Floor North West Conference Room

---++ Audio conferencing instructions
   * Meeting ID: 226787
   * Meeting Password: none 
   * To attend via telephone dial 510-665-5437
   * To attend via web conference,
   * 1. Go to: http://abm.es.net/a/67326ee4bb3f812d1b89d6dd66c80020
   * 2. Sign in as a Guest or with your !Cisco MeetingPlace User ID, and click on Attend Meeting.
   * 3. Click on CONNECT and enter your phone and click OK. The system will now call you. 

---++ Video conferencing instructions
   * ESnet ad-hoc video Meeting ID: 88226787@gk1.es.net (if you use the gatekeeper)
   * Meeting password: none
   * to watch but not interact: http://mcu3.es.net
      * (Skip the "Sign-in name" field and the "PIN" field)
      * Enter the number 226787 into the Conference ID field
      * Select your preferred player ( !RealVideo or Quicktime) via the pull-down menu then click on the "Stream this conference" button to initiate the streaming output feed.
   * or GDS 001134988226787
   * or phone 1-510-883-7860 88226787#


---++ Attendees
   * Brian Bockelman (Nebraska)
   * Dan Bradley (Wisconsin)
   * Keith Chadwick (Fermilab)
   * Steve Gallo (Buffalo, TBC)
   * Sebastien Goasguen (Clemson)
   * Rob Gardner (U Chicago)
   * John !McGee (RENCI)
   * Doug Olson (LBNL)
   * Preston Smith (Purdue)
   * Sam Hoover (Clemson)
   * Ben Cotton (Purdue, bcotton@purdue.edu)
   * Prakashan Korambath (UCLA)  
   * Bill Strossman (UC Riverside)


---++ *January 19 - Notes*
   * 10 am CST - Introductions, overview, discussion
   *  Rob: Preliminary thoughts - what we can accomplish, etc.
   *  John: interest in how do you bring new campuses that have not yet been involved in grid computing - esp those that do not have existing knowledge base, history expertise in the required technolgies.  And competing interests and budgets. *How can commercial products be leveraged* with OSG software - eg. metaschedulers, open-source cloud computing. 
   *  Preston - at PU focusing on departments that don't have an CI already setup.   What are the small school or regional campus grids.
   *  Dan - concern about having the key people in place at the campus - people who can bridge the technical-political issues.   Needs to be a *local champion* who can convince them of the local benefit.  
   *  At Purdue --> Clemson it was Jim Bottom (CIO level)
   *  Steve (leads sw eng, grid comp) - imp to have a good relationship with CIO.  at UB - have HPC and computing IT. There is a good relationship between.  Have campus condor grid.  *Outreach.* Engaging Deans, getting support.  Providing storage and condor resources - defining roles for the community - proactive in the beginning.  
   *  Ben - faculty get what they want - get them.
   *  Keith - fermigrid is the fermilab campus grid. Laboratory CIO charge - consolidate diverse cyber silos.  (cdf, d0, cms t1, farm, etc etc).   Detailed knowledge needed to maintain.  Demand for centralized services. Decreasing budget constraints.  Construct out of pieces.  Central services - voms, gums, minimal site gateway (2005).   Demonstrating *interoperability*.  Services need to be *highly available* (!FermigridHA) - load share and redundancy.  Clustered filesystem (blue arc appliance).  About to release a !MyProxyHA, security.
   * Brian: the buy-in and idea materializing in Nebraska - it was always requiring buy-in, better to buy-in the go it alone and buy your own cluster, managed by grad students.  Led to old clusters around campus. vice chancellor of technology - understands economy of scale of pooling resources.   That resulted in grid initiative.  More than grid - integration tasks (eg. username/pw, training users).  *Education* will be an important part - eg. workflows and file management. 
   * Sam - big change came w/ Jim as CIO - required commitment of VP.  Unification became possible - IT working closely with faculty on research projects. 
   * Prakashan - coming from HPC background - campus resources very decentralized.  Underutilized resources - want to find a way to distribute these, across the campus.  Users coming from various backgrounds (eg. Windows) - teaching required, then they leave.   On-going teaching - led to a unified interface, web, independent of backend schedulers.  Led to development of UC  grid interface, two kinds of users, full users.  We take care of all the software install, etc.  Metascheduling across clusters.  Some applications are costly.  Campus authentication moving towards Shiboleth identification.  Workflow interfaces for very long running tasks (~ month) requiring little manual intervention.  6 campuses - resulting in UC Grid portal across system.  Includes Teragrid.  
      * 
      * 
      * 
      * 
      * 
      * 
      * 
      * 
      * 
      * 
      * 

   * Implementations - features - best practices - issues & challenges
      * GLOW
      * Purdue
      * Fermigrid
      * Clemson
      * Chapel Hill 
      * U California (remote)
      * NYSG (TBC)

   * Lunch

   * Topical discussion
      * Global storage systems at the CG
      * Seamless user environments
      * role and use of vm & cloud computing technologies; Magellan case
      * Challenges dealing with campus provincial issues (or how technical progress might inform that, or stimulating growth of a movement/community that provide clear examples of the benefits associated with CGs)
   * Homework assignments


---++ January 20
   * 9 am
   * Review overnight findings from topical homework, document contributions
   * Lunch
   * Afternoon: *collating* of contributions into the CG technical summary document.
   * 4pm adjourn


---++ References
   * [[%ATTACHURL%/GridTrust.pdf][GridTrust.pdf]]: Grid Trust

   * [[%ATTACHURL%/ucgrid-fermilab-V1a.ppt][ucgrid-fermilab-V1a.ppt]]: UCGrid overview (ppt) - Prakashan Korambath

   * [[%ATTACHURL%/ucgrid-fermilab-V1a.pdf][ucgrid-fermilab-V1a.pdf]]: UCGrid overview (pdf) - Prakashan Korambath

   * [[%ATTACHURL%/Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf][Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf]]: Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf

%META:FILEATTACHMENT{name="GridTrust.pdf" attachment="GridTrust.pdf" attr="" comment="Grid Trust" date="1263504878" path="GridTrust.pdf" size="1571669" stream="GridTrust.pdf" tmpFilename="/usr/tmp/CGItemp12798" user="RobGardner" version="1"}%
%META:FILEATTACHMENT{name="ucgrid-fermilab-V1a.ppt" attachment="ucgrid-fermilab-V1a.ppt" attr="" comment="UCGrid overview (ppt) - Prakashan Korambath" date="1263869337" path="ucgrid-fermilab-V1a.ppt" size="3383296" stream="ucgrid-fermilab-V1a.ppt" tmpFilename="/usr/tmp/CGItemp11931" user="DougOlson" version="1"}%
%META:FILEATTACHMENT{name="ucgrid-fermilab-V1a.pdf" attachment="ucgrid-fermilab-V1a.pdf" attr="" comment="UCGrid overview (pdf) - Prakashan Korambath" date="1263869499" path="ucgrid-fermilab-V1a.pdf" size="1772086" stream="ucgrid-fermilab-V1a.pdf" tmpFilename="/usr/tmp/CGItemp12078" user="DougOlson" version="1"}%
%META:FILEATTACHMENT{name="GridNebraska.pdf" attachment="GridNebraska.pdf" attr="" comment="" date="1263917132" path="GridNebraska.pdf" size="145052" stream="GridNebraska.pdf" tmpFilename="/usr/tmp/CGItemp17580" user="BrianBockelman" version="1"}%
%META:FILEATTACHMENT{name="Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf" attachment="Purdue_-_OSG_Campus_Grid_Workshop_Jan_19_2010.pdf" attr="" comment="" date="1263917628" path="Purdue - OSG Campus Grid Workshop Jan 19 2010.pdf" size="5792900" stream="Purdue - OSG Campus Grid Workshop Jan 19 2010.pdf" tmpFilename="/usr/tmp/CGItemp13413" user="PrestonSmith" version="1"}%
%META:FILEATTACHMENT{name="CrimsonGrid.pdf" attachment="CrimsonGrid.pdf" attr="" comment="info gathered about Crimson Grid" date="1263917912" path="C:\Users\mcgee\Documents\CrimsonGrid.pdf" size="39805" stream="C:\Users\mcgee\Documents\CrimsonGrid.pdf" tmpFilename="/usr/tmp/CGItemp32465" user="JohnMcGee" version="1"}%
