%META:TOPICINFO{author="BrooklinGore" date="1331751777" format="1.1" reprev="1.13" version="1.13"}%
%META:TOPICPARENT{name="WebHome"}%
---+Campus Infrastructure Workplan
Last update: 14-Mar-2012, BG+DF
This project is part of the OSG Production Area

---++ Overview

There are two main goals for this project:
   1 Integrate technology components into an OSG job submission capability that makes it easy for researchers to distribute and manage ensembles of jobs across multiple batch systems from their desktop, using their campus login credentials. The development name for this capability is BOSCO.
   1 Engage with and help researchers become more productive in their science by utilizing this capability.

[[https://twiki.grid.iu.edu/bin/view/CampusGrids/BoSCO][Bosco DOWNLOAD page]]

---++Researcher value proposition:

   * Enable researchers to dramatically extend computational capability to extend their science by
      1  harnessing resources on campus
      1 harnessing resources on collaborating campuses
      1 harnessing resources on the national cyber infrastructure.
   * Universal front-end makes it easy to manage multiple job submissions 
   * Enables jobs to be submitted independent of remote batch queue size
   * We will help a researcher take a submit file that currently submits to the PBS batch system and turn it into a Bosco submit file. Help will be in the form of documentation with sample files (e.g. maybe how to run R jobs or other common applications such as GROMACS) as well as hands on support when needed.

---++Project Team

| *Role* | *Who* | *%Effort Feb 14 - Mar 21*|
| Project Lead | Dan Fraser (OSG-Prod) | 20% |
| Project Mgr | Brooklin Gore (Morgridge) | 10% |
| External Support | Jaime Frey (Condor) | 20% |
| Contributor | Marco Mambelli (OSG-Prod) | 40% |
| External Support | Todd Tannenbaum (Condor) | 0% |
| Developer | Derek Weitzel (OSG-CI) | 30% |
| SW Integration and Usability Testing | Alain Roy (OSG-SW) | 10% |

---++Reporting

Brooklin to add items to the Production notes every Monday.

---++Approach and Timeline

Bosco will be developed in multiple phases as follows:

   * *Integration Part I (Bosco V0) March 2012* - Release to demonstrate baseline capability and for internal testing with control group
   * *Integration Part II (Bosco V1) June 2012* - Production Release that supports job submission to multiple back ends and agreed upon interfaces to the fabric of services. To be used as a replacement for existing Campus Infrastructures at VT, UFlorida. See below for additional details.
   * *Integration Part III (Bosco V2) TBD* - Additional functionality and ease of use enhancements and bug fixes for previous releases.

   * *Engagement (Mar - Sep) 2012* - Develop a plan to identify and engage researchers
      * Document -- What do we have to offer Campuses (Technology, Environments, App Experience, Know how)?
      * Transition -- Existing grids transition to BOSCO
      * Identify/contact -- One or two campus researchers to work with (identify quickly which researchers will not work)
         * Meetings: AHM, Condor Week, ...
         * Existing mailing lists
         * Campus Visits / presentations
         * One on one (e.g. Doug B.)
      * Work with researchers
         * Get their code running in an independent lab
         * Help with designing their scripts
         * Help them get running at scale
      * Track our experience with success / rejections on a wiki page
      * Register campus end points as they come online into OIM (some devel effort)
         * Register UW, VT, Engage, Glow, HCC, Purdue, UCSD, RENCI, XSEDE

---
---+ Integration Part I

---++V0 Goal:

Provide a management tool for researchers to manage large numbers of job submissions onto their local cluster. Design goal is to manage 1000 jobs against a batch system that can only support 100 jobs running at a time. Additionally,

   * Release the V0 version of Bosco by Mar 21, 2012.
   * Dan Fraser will provide updates at the area update meetings.
   * V0 support will be provided via email to the unmoderated bosco-discuss@opensciencegrid.org email list. 

---++ Bosco v0 prototype release

---+++Features

   * Simple installation and configuration via separate package and bosco_* command wrappers [Done 14-Mar, Jaime add final script from Marco and update docs]
   * The installation can be on a user’s desktop or on another machine that they log into. [DONE]
   * The package physically resides in the OSG VDT [MODIFIED, thus:]
      * Bosco is a Condor Contrib module, contributed by the OSG [Complete this when Condor 7.9 is released]
   * Package available as simple web-based download [Done 14-Mar, Jaime/Derek to change to static link]
   * Installs in user’s home directory [DONE]
   * Does not require root access for installation or configuration [DONE]
   * Submit node job queue size independent from remote batch queue size [EPIC FAIL]
      * This is a hard problem. For v0 we will require the user to specify the queue size with the bosco_cluster -add command [Derek, complete by 16-Mar]
   * Rebooting submit machine will not impact queued jobs (caveat: user must re-start Bosco manually) [DONE, not tested]
   * Regarding job status:
      * User can obtain some information about job positions in the batch queue (currently: I=in condor OR pbs queue, R=running on PBS) [DONE]
      * Add (condor_q -grid option to expose additional job state info from remote queue manager) [TJ (Condor) to complete by 26-Mar]
      * Add some periodic hold state reasons if there are problems w/the remote system (e.g. blahp not running, etc.) [TJ (Condor) to complete by 26-Mar]

---+++ Known limitations

   * Bosco package must be staged on submit node before installing
   * Submit node and cluster head node must be same HW/OS architecture and version.
   * No file I/O is provided between submit node and cluster head node – executable and data must be staged in and out manually
   * User must restart Bosco after submit node reboot
   * No OSG accounting support

---+++ User requirements

   * Submit-node: Any PBS and Condor supported Linux platform
   * Cluster head-node: Any PBS and Condor supported Linux platform
   * Condor version: 7.7.6+
   * PBS flavors: Torque and PBSPro

---++ Bosco v0 architecture overview

<img src="%ATTACHURLPATH%/bosco-v0-arch.png" alt="bosco-v0-arch.png" width='50%' height='50%' />    

---++ Bosco v0 sample user session

| *Step* | *Action* | *Arguments* | *Input* | *Output* |
| Download | Manually stage the bosco tar ball on the submit machine. ||||
| Install | =bosco_install= | None | None | Success/Fail |
|^| =source bosco_setenv.[csh,sh]= | None | None | None |
|^| =bosco_start= | None | None | Success/Fail |
|^| =bosco_cluster= | -add Hostname QSize | None | Success/Fail, entry in head node table |
|^| =bosco_cluster= | -list | Head-node table | List of –add’d head nodes and their status |
|^| =bosco_cluster= | -test | Submit file | Status of submitted jobs |
| Stage Data In | Manually transfer all executables and input data to batch system.  ||||
| Run | =condor-*= | Various | Various | Various |
| Stage Data Out | Manually transfer output data from batch system. ||||
| Optional Uninstall | >bosco_cluster | -remove Hostname | None | Success/Fail, head node table with Hostname removed, delete if empty |
|^| =bosco_stop= | None | None | Success/Failure |
|^| =bosco_uninstall= | None | None | Success/Failure |

---++ Bosco v0 workplan and schedule

| *Due* | *Owner* | *Status* | *Task* |
| 13-Feb | Gore/Fraser | %GREEN%Complete | Complete draft work plan for team comments |
|^| Frey/Weitzel | %GREEN%Complete | Complete functional versions of =bosco_*= commands |
|^| Roy/Weitzel | %GREEN%Complete | Develop test plan |
| 20-Feb | Gore/Fraser | %GREEN% Complete | Approve test plan |
|^| Fraser/Gore | %GREEN%Complete | Identify 2-3 usability and beta testers (Beta: Doug Benjamin (Atlas Argon), Suchandra (UChicago-OSG), Rob Gardner (UChicago), Brooklin Gore (UW); Usability: Joey Kahl (Condor Madison) |
| 23-Feb | Gore/Fraser | %GREEN%Complete | Workplan approved by OSG ET and reviewed by other area coordinators |
| 27-Feb | Mambelli/Fraser | In Progress | Web site up with Bosco overview, documentation and package download button |
|^| Fraser/Gore | %GREEN%Complete | Bosco v0 provided to beta users to validate install and functionality |
|^| Gore | %GREEN%Complete | Get workplan on OSG web for review/status tracking |
|^| Frey/Mambelli | %RED% *Behind* | Report on ability to track job state from cluster |
| 5-Mar | Entire Team | %RED% *Behind* | Review user feedback and identify development requirements |
|^| Roy | %RED% *Behind* | Begin usability testing |
| 12-Mar | Fraser/Gore | In Progress | Second round of beta testing - push error cases to evaluate quality of error messages |
| 19-Mar | Fraser | %RED% *FAIL* | Bosco v0 complete. We missed a few critical features that will take another week to implement |
| 21-Mar | Fraser | Not Started | Bosco presentation at OSG All Hands meeting |
| 22-Mar | Fraser | Not Started | Presentation of accomplishments, lessons learned and next steps to the ET and other area coordinators |
| 26-Mar | Fraser | In Progress | Bosco v0 complete (new date) |

---+ integration Part II
In progress

---++ Bosco v1production release

---+++Features (Still being defined)

   * All v0 features, plus
   * Support for multiple backend clusters (of the same type)
   * Support for additional cluster types (which ones TBD, candidates are Condor, SGE & LSF) [Nice to have]
   * Some accounting features (to understand if Bosco is being used successfully)
   * Report job state on cluster back to submit node (is my job making progress?)
   * Support for Macintosh as submit node (supports different OS on Submit node and Cluster)
   * File transfer support (files->cluster; results->submit 'automatic' via Condor) [MUST have]
   * Better error handling and clear messages that a scientist can understand and act on (initially, a 'traceroute' style tool that can find where problems are).
   * Be able to specify a cluster queue
   * Auto detect the cluster queue size so the user can submit an arbirtrary number of jobs w/o regard to cluster queue size
   * Auto start Bosco after submit node reboot
   * Add a bosco_cluster -status feature that gives user information on remote cluster health, etc.

---+++ Known limitations

   * Jobs will be lost if submit machine goes offline for any reason

---+++ User requirements

   * Submit-node: Mac OSX 10.7 (Lion) or Any PBS and Condor supported Linux platform
   * Cluster head-node: Any PBS, ___, or ___ and Condor supported Linux platform
   * Condor version: 7.7.6+
   * PBS flavors: Torque and PBSPro

---+++ Technical Implementation

   * steps in traceroute for bosco
      * job in schedd
      * job in gridmanager
      * ssh works
      * blahp starts
      * blahp accepts jobs
      * (input files staged)
      * job in remote queue
      * job enters running state in remote queue
      * job enters completed state in remote queue
      * (output files staged)
      * job completion reported by blahp
      * job completion reported by gridmanager
      * job leaves schedd successfully

---+ Engagement Part I

In Progress

---++ Bosco v1 sample user session

| *Step* | *Action* | *Arguments* | *Input* | *Output* |
|Download | Go to OSG/Bosco web page |||
|^|Click DOWNLOAD | N/A | N/A | File in home directory |
|The rest of the user session is the same as for v0 |||||

---++ Bosco v1 workplan and schedule

| *Due* | *Owner* | *Status* | *Task* |
| 12-Mar | Gore/Fraser | In Progress | Complete v1 draft work plan for team comments |
| 23-Mar | Frey/Tannenbaum | In Progress | Deliver file transfer design document |
| TBD | Frey | Not Started | Ensure we are using the latest release of the BLAHP and that all our patches from v0 are pushed upstream |

---
---+ Integration Part III

To be completed

---
---+ Parking Lot

---+++ These are items to be addressed above and are 'parked' here from ealier discussions

Open Questions:

Do we include 'The Factory' in v1. Does this change our overall approach of making it easy for a scientist to use? Does this imply a three tier architecture?

Finalize deliverables
   * Functionality
   * Target users (general and named) and use cases
   * Dan, potentials are: Belarmine

Test environment functional in Madison
   * Condor submit node
   * PBS environment

Documentation

Test plans
   * What are we testing in v0 vs v1
   * When/where
   * Usability, functional/regression, scalability
   * 1000 jobs -> 100 (PBS) slots
   * Understand latency (add’l overhead) introduced by Bosco. Can I run 20 minute jobs? 10 minute jobs?
   * Handles disconnect of submit host while jobs are running
   * Does not support changing IP address on submit node while jobs are running (v2 feature)
   * Easy to use by researcher
   * What happens if I submit to a cluster I haven’t bosco_cluster –add (etc.)
   * Do the submit daemons start up after a submit node reboot?

How do we distribute via the VDT (or OSG web page)

Determine if/when to support SGE and LSF

---++++ BOSCO v1 Design Requirements & Discussion (from Dan's 14-Feb Design Considerations Document)

BOSCO v0 utilizes: 
   * Master
   * Schedd
   * Grid-manager
   * Remote Gahp (SSH)

BOSCO v0 has several important characteristics:
   * It can be easily installed and operated from a researcher’s workstation (Linux for now but extendable in the next version). 
   * It connects directly (via SSH) to a PBS cluster and submits jobs that have been pre-staged on the cluster. 
   * There are no moving parts on the cluster so there is little or no BOSCO maintenance on the cluster
   * If the connection between the researcher’s workstation and the cluster is disconnected, 
   * Jobs already submitted to the cluster will finish normally
   * BOSCO submissions will resume once connectivity between the systems has resumed

BOSCO v0 also has some important limitations:
   * Users must login directly to the cluster to:
   * pre-stage their executables and data
   * manage the data output from their executables.
   * Because of this, v0 cannot be considered as an upgrade or replacement for existing campus grids.

In BOSCO v1 we are aiming to remove the limitations of v0 and also to allow jobs to be submitted to multiple clusters. 

BOSCO v1 requirements:
   * Enable users to manage job submissions to multiple clusters.
   * Design case ?? Enable a user to run 1000 jobs when only 10 jobs can be running on two separate cluster at one time. 
   * Users connect to the “submit host” via SSH to launch jobs
   * Executables and data must be available from the submit host
   * Job files and executables will be automatically staged from the “submit host” to the connected clusters and back.
   * Users will be able to readily determine (for debugging purposes) at what point a job submission fails and where the likely problem is.  
   * A user can easily install BOSCO on a machine that they have access to w/o requiring root. 
   * Instructions for installation and use should ideally fit on a single page.
   * The submit host and cluster OS can be any OS that Condor supports
   * The cluster type can be any that BLAH supports (e.g. PBS, LSF, SGE)

The proposed architecture we have been discussing introduces an embedded campus-pilot factory (ECF) into BOSCO. The ECF launches pilot jobs (containing startd’s) based on job pressure to the cluster worker nodes. These worker nodes run pilots containing startds that pull in user jobs from the BOSCO schedds and stage the output back to BOSCO.

BOSCO proposed v1 would utilize:
   * BOSCO v0 packages
   * Embedded Campus Pilot Factory
   * Collector (to manage jobs from the pilots)

Note: BOSCO v1 will need to have some mechanism to stage the executables required for the run the pilot jobs to the cluster.

Tradeoff considerations:
Pilot jobs assume a regular stream of communication to the scheduler. Breaking the communication could result in large numbers of job restarts and wasted cycles. The implication is that users should not install this on their local laptop or workstation, unless the workstation is consistently up for the duration of the project. 

Hence for v1, we are assuming that the user will probably not install BOSCO on her workstation, but will likely install it on an intermediate workstation. This is currently how we are deploying campus infrastructures at VirginiaTech, Nebraska, and FIU. 

For v1 with the ECF we assume:
   * BOSCO v1 runs on a submit host (that is probably not the user’s workstation)
   * The user connects to BOSCO from their laptop or workstation via SSH for executable and data management.
   * The submit host BOSCO can be set up either by the user or by a sysadmin for use by multiple users.
   * A job submission status capability is needed as part of v1 to help users debug their submissions and understand where and at what stage a problem is occurring. (This could be useful for Condor as part of this.)

One possibility for v2 would be to create a set of tools that would use ssh to stage files to/from the cluster. This would require the user to manually use these tools, and still may not be an effective replacement for the existing campus infrastructures, but this might allow BOSCO to remain on the users laptop or cluster.

Some Questions:
   * How much development would be needed?
   * Would this require moving parts to be installed on the cluster to support this?
   ** If so, how would that change the user requirements for maintaining the cluster?

---++++ Notes from 11/7/11 Discussion at FermiLab

* user-side install: need to add Condor bin to path
* ce-side install:
  + need to customize blahp config file
    could write script to do this
  + important to set up shared directories
    pbs staging commands often not supported
    pbs staging commands are failure-prone and hard to debug
  + assume home directory is shared for base setup of blahp-over-ssh

* ~/.condor
  + where in config file search order should this be?
    just after environment variable CONDOR_CONFIG
  + option for config to overlay system config?
* installing under ~/.condor by default
  + would just be local_dir
    is this a problem if home directory is shared?
    where to put LOCK_DIR?
* May need to help user configure ssh-key based login to cluster node
* Maybe have user store ssh key unencrypted
  + and have a seperate ssh key that only Condor uses?
* allow use of ssh agent as option?
* trivial obscuring of ssh passphrase
* preferred solution:
  generate ssh key for Condor's use, encrypted with hard-coded phrase
* add installation step with test job
* installation on cluster node
  should automate writing of blah config file (lrms file locations)

*Possible problems during job submission/execution*

* How do users already using pbs convert to condor model?
* in pbs, user writes script to run job
* should document how to set pbs queue (and make syntax easier)
* how does user determine status of their jobs
  condor_q says whether job is running
  useful to have additional information
    + how many jobs in pbs queue ahead of you
    + is job in pbs queue
    + are jobs running in pbs

* user needs easy way to test that system is working, with feedback
  about why things are failing
  + could have traceroute-type report of how far job got and what failed
  + focus on problems due to layers we're adding?
    i.e. don't worry about pbs cluster having no execute nodes
  + tool that anaylzes everything associated with a job and looks for
    potential problems

* Todd: we're hiding some information from user
  if job is idle, user can't see:
  + if other jobs are running
  + how many jobs are ahead of them in pbs queue
  + execute machines are in cluster
  we could gather some of this information and put it in job or grid ads
  extra information gathering could be only done for special smoke-test jobs

* steps in campus grid traceroute
  user          glidein-factory
  + schedd
                + job detection
                + glidein submission
                + job in schedd
                + job in gridmanager
                + (ssh blahp)
                + job in blahp
                + job in pbs
                + pbs run
                + condor starts
                + condor advertises
  + match
  + claim
  + activate
  + stage-in
  + run
  + stage-out
  + job marked completed and leaves queue

* common problem: jobs leave pbs queue without entering completed status
  and leaving status info in pbs logs
  blahp considers this a failure
  shouldn't need to
* common problem: blahp tries to use pbs staging commands, but these
  often aren't supported or enabled
  should make it easy to depend on a shared filesystem
  maybe even assume shared filesystem, with option to activate staging
* missing features on condor-g that would be useful
  + throttles that vary per resource
  + throttle job submission based on number of already submitted jobs
    that are idle



%META:FILEATTACHMENT{name="bosco-v0-arch.png" attachment="bosco-v0-arch.png" attr="" comment="Bosco v0 Architecture" date="1329934067" path="bosco-v0-arch.png" size="200048" stream="bosco-v0-arch.png" tmpFilename="/usr/tmp/CGItemp33885" user="BrooklinGore" version="2"}%
