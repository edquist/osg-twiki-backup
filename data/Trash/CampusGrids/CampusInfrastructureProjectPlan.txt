%META:TOPICINFO{author="BrooklinGore" date="1331582460" format="1.1" version="1.12"}%
%META:TOPICPARENT{name="WebHome"}%
---+Campus Infrastructure Workplan
Last update: 6-Mar-2012, BG+DF
This project is part of the OSG Production Area

---++ Overview

There are two main goals for this project:
   1 Integrate technology components into an OSG job submission capability that makes it easy for researchers to distribute and manage ensembles of jobs across multiple batch systems from their desktop, using their campus login credentials. The development name for this capability is BOSCO.
   1 Engage with and help researchers become more productive in their science by utilizing this capability.

[[https://twiki.grid.iu.edu/bin/view/CampusGrids/BoSCO][Bosco DOWNLOAD page]]

---++Researcher value proposition:

   * Enable researchers to dramatically extend computational capability to extend their science by
      1  harnessing resources on campus
      1 harnessing resources on collaborating campuses
      1 harnessing resources on the national cyber infrastructure.
   * Universal front-end makes it easy to manage multiple job submissions 
   * Enables jobs to be submitted independent of remote batch queue size
   * We will help a researcher take a submit file that currently submits to the PBS batch system and turn it into a Bosco submit file. Help will be in the form of documentation with sample files (e.g. maybe how to run R jobs or other common applications such as GROMACS) as well as hands on support when needed.

---++Project Team

| *Role* | *Who* | *%Effort Feb 14 - Mar 21*|
| Project Lead | Dan Fraser (OSG-Prod) | 20% |
| Project Mgr | Brooklin Gore (Morgridge) | 10% |
| External Support | Jaime Frey (Condor) | 20% |
| Contributor | Marco Mambelli (OSG-Prod) | 40% |
| External Support | Todd Tannenbaum (Condor) | 0% |
| Developer | Derek Weitzel (OSG-CI) | 30% |
| SW Integration and Usability Testing | Alain Roy (OSG-SW) | 10% |

---++Reporting

Brooklin to add items to the Production notes every Monday.

---++Approach and Timeline

Bosco will be developed in multiple phases as follows:

   * *Integration Part I (Bosco V0) March 2012* - Release to demonstrate baseline capability and for internal testing with control group
   * *Integration Part II (Bosco V1) June 2012* - Production Release that supports job submission to multiple back ends and agreed upon interfaces to the fabric of services. To be used as a replacement for existing Campus Infrastructures at VT, UFlorida. See below for additional details.
   * *Integration Part III (Bosco V2) TBD* - Additional functionality and ease of use enhancements and bug fixes for previous releases.

   * *Engagement (Mar - Sep) 2012* - Develop a plan to identify and engage researchers
      * Document -- What do we have to offer Campuses (Technology, Environments, App Experience, Know how)?
      * Transition -- Existing grids transition to BOSCO
      * Identify/contact -- One or two campus researchers to work with (identify quickly which researchers will not work)
         * Meetings: AHM, Condor Week, ...
         * Existing mailing lists
         * Campus Visits / presentations
         * One on one (e.g. Doug B.)
      * Work with researchers
         * Get their code running in an independent lab
         * Help with designing their scripts
         * Help them get running at scale
      * Track our experience with success / rejections on a wiki page
      * Register campus end points as they come online into OIM (some devel effort)
         * Register UW, VT, Engage, Glow, HCC, Purdue, UCSD, RENCI, XSEDE

---
---+ Integration Part I

---++V0 Goal:

Provide a management tool for researchers to manage large numbers of job submissions onto their local cluster. Design goal is to manage 1000 jobs against a batch system that can only support 100 jobs running at a time. Additionally,

   * Release the V0 version of Bosco by Mar 21, 2012.
   * Dan Fraser will provide updates at the area update meetings.
   * V0 support will be provided via email to the unmoderated bosco-discuss@opensciencegrid.org email list. 

---++ Bosco v0 prototype release

---+++Features

   * Simple installation and configuration via separate package and bosco_* command wrappers.
   * The installation can be on a user’s desktop or on another machine that they log into.
   * The package physically resides in the OSG VDT
   * Installs in user’s home directory
   * Does not require root access for installation or configuration
   * Submit node job queue size independent from remote batch queue size
   * Rebooting submit machine will not impact queued jobs
   * User can obtain some information about job positions in the batch queue
   * Package available as simple web-based download

---+++ Known limitations

   * Bosco package must be staged on submit node before installing
   * Submit node and cluster head node must be same HW/OS architecture and version.
   * No file I/O is provided between submit node and cluster head node – executable and data must be staged in and out manually
   * No OSG accounting support

---+++ User requirements

   * Submit-node: Any PBS and Condor supported Linux platform
   * Cluster head-node: Any PBS and Condor supported Linux platform
   * Condor version: 7.7.6+
   * PBS flavors: Torque and PBSPro

---++ Bosco v0 architecture overview

<img src="%ATTACHURLPATH%/bosco-v0-arch.png" alt="bosco-v0-arch.png" width='50%' height='50%' />    

---++ Bosco v0 sample user session

| *Step* | *Action* | *Arguments* | *Input* | *Output* |
| Download | Manually stage the bosco tar ball on the submit machine. ||||
| Install | =bosco_install= | None | None | Success/Fail |
|^| =source bosco_setenv.[csh,sh]= | None | None | None |
|^| =bosco_start= | None | None | Success/Fail |
|^| =bosco_cluster= | -add Hostname… | None | Success/Fail, entry in head node table |
|^| =bosco_cluster= | -list | Head-node table | List of –add’d head nodes and their status |
|^| =bosco_cluster= | -test [jobcnt] | Submit file | Status of submitted jobs |
| Stage Data In | Manually transfer all executables and input data to batch system.  ||||
| Run | =condor-*= | Various | Various | Various |
| Stage Data Out | Manually transfer output data from batch system. ||||
| Optional Uninstall | >bosco_cluster | -remove Hostname | None | Success/Fail, head node table with Hostname removed, delete if empty |
|^| =bosco_stop= | None | None | Success/Failure |
|^| =bosco_uninstall= | None | None | Success/Failure |

---++ Bosco v0 workplan and schedule

| *Due* | *Owner* | *Status* | *Task* |
| 13-Feb | Gore/Fraser | %GREEN%Complete | Complete draft work plan for team comments |
|^| Frey/Weitzel | %GREEN%Complete | Complete functional versions of =bosco_*= commands |
|^| Roy/Weitzel | %GREEN%Complete | Develop test plan |
| 20-Feb | Gore/Fraser | %GREEN% Complete | Approve test plan |
|^| Fraser/Gore | %GREEN%Complete | Identify 2-3 usability and beta testers (Beta: Doug Benjamin (Atlas Argon), Suchandra (UChicago-OSG), Rob Gardner (UChicago), Brooklin Gore (UW); Usability: Joey Kahl (Condor Madison) |
| 23-Feb | Gore/Fraser | %GREEN%Complete | Workplan approved by OSG ET and reviewed by other area coordinators |
| 27-Feb | Mambelli/Fraser | In Progress | Web site up with Bosco overview, documentation and package download button |
|^| Fraser/Gore | %GREEN%Complete | Bosco v0 provided to beta users to validate install and functionality |
|^| Gore | %GREEN%Complete | Get workplan on OSG web for review/status tracking |
|^| Frey/Mambelli | %RED% *Behind* | Report on ability to track job state from cluster |
| 5-Mar | Entire Team | %RED% *Behind* | Review user feedback and identify development requirements |
|^| Roy | %RED% *Behind* | Begin usability testing |
| 12-Mar | Fraser/Gore | Not Started | Second round of beta testing |
| 19-Mar | Fraser | Not Started | Bosco v0 complete |
| 21-Mar | Fraser | Not Started | Bosco presentation at OSG All Hands meeting |
| 22-Mar | Fraser | Not Started | Presentation of accomplishments, lessons learned and next steps to the ET and other area coordinators |

---+ integration Part II
In progress

---++ Bosco v1production release

---+++Features (Still being defined)

   * All v0 features, plus
   * Support for multiple backend clusters (which ones TBD, candidates SGE, LSF)
   * Some accounting features (to understand if Bosco is being used successfully)
   * Support for Macintosh as submit node
   * File transfer support (files->cluster; results->submit)
   * Need to enumerate others...yes, we need to nail down our requirements!!!

---+++ Known limitations

   * TBD

---+++ User requirements

   * Submit-node: Mac OSX 10.7 (Lion) or Any PBS and Condor supported Linux platform
   * Cluster head-node: Any PBS, ___, or ___ and Condor supported Linux platform
   * Condor version: 7.7.6+
   * PBS flavors: Torque and PBSPro

---+ Engagement Part I

In Progress

---++ Bosco v1 sample user session

| *Step* | *Action* | *Arguments* | *Input* | *Output* |
|Download | Go to OSG/Bosco web page |||
|^|Click DOWNLOAD | N/A | N/A | File in home directory |
|The rest of the user session is the same as for v0 |||||

---++ Bosco v1 workplan and schedule

| *Due* | *Owner* | *Status* | *Task* |
| 12-Mar | Gore/Fraser | In Progress | Complete v1 draft work plan for team comments |

---
---+ Integration Part III

To be completed

---
---+ Parking Lot

---+++ These are items to be addressed above and are 'parked' here from ealier discussions

Open Questions:

Do we include 'The Factory' in v1. Does this change our overall approach of making it easy for a scientist to use? Does this imply a three tier architecture?

Finalize deliverables
   * Functionality
   * Target users (general and named) and use cases
   * Dan, potentials are: Belarmine

Test environment functional in Madison
   * Condor submit node
   * PBS environment

Documentation

Test plans
   * What are we testing in v0 vs v1
   * When/where
   * Usability, functional/regression, scalability
   * 1000 jobs -> 100 (PBS) slots
   * Understand latency (add’l overhead) introduced by Bosco. Can I run 20 minute jobs? 10 minute jobs?
   * Handles disconnect of submit host while jobs are running
   * Does not support changing IP address on submit node while jobs are running (v2 feature)
   * Easy to use by researcher
   * What happens if I submit to a cluster I haven’t bosco_cluster –add (etc.)
   * Do the submit daemons start up after a submit node reboot?

How do we distribute via the VDT (or OSG web page)

Determine if/when to support SGE and LSF

---++++ BOSCO v1 Design Requirements & Discussion (from Dan's 14-Feb Design Considerations Document)

BOSCO v0 utilizes: 
   * Master
   * Schedd
   * Grid-manager
   * Remote Gahp (SSH)

BOSCO v0 has several important characteristics:
   * It can be easily installed and operated from a researcher’s workstation (Linux for now but extendable in the next version). 
   * It connects directly (via SSH) to a PBS cluster and submits jobs that have been pre-staged on the cluster. 
   * There are no moving parts on the cluster so there is little or no BOSCO maintenance on the cluster
   * If the connection between the researcher’s workstation and the cluster is disconnected, 
   * Jobs already submitted to the cluster will finish normally
   * BOSCO submissions will resume once connectivity between the systems has resumed

BOSCO v0 also has some important limitations:
   * Users must login directly to the cluster to:
   * pre-stage their executables and data
   * manage the data output from their executables.
   * Because of this, v0 cannot be considered as an upgrade or replacement for existing campus grids.

In BOSCO v1 we are aiming to remove the limitations of v0 and also to allow jobs to be submitted to multiple clusters. 

BOSCO v1 requirements:
   * Enable users to manage job submissions to multiple clusters.
   * Design case ?? Enable a user to run 1000 jobs when only 10 jobs can be running on two separate cluster at one time. 
   * Users connect to the “submit host” via SSH to launch jobs
   * Executables and data must be available from the submit host
   * Job files and executables will be automatically staged from the “submit host” to the connected clusters and back.
   * Users will be able to readily determine (for debugging purposes) at what point a job submission fails and where the likely problem is.  
   * A user can easily install BOSCO on a machine that they have access to w/o requiring root. 
   * Instructions for installation and use should ideally fit on a single page.
   * The submit host and cluster OS can be any OS that Condor supports
   * The cluster type can be any that BLAH supports (e.g. PBS, LSF, SGE)

The proposed architecture we have been discussing introduces an embedded campus-pilot factory (ECF) into BOSCO. The ECF launches pilot jobs (containing startd’s) based on job pressure to the cluster worker nodes. These worker nodes run pilots containing startds that pull in user jobs from the BOSCO schedds and stage the output back to BOSCO.

BOSCO proposed v1 would utilize:
   * BOSCO v0 packages
   * Embedded Campus Pilot Factory
   * Collector (to manage jobs from the pilots)

Note: BOSCO v1 will need to have some mechanism to stage the executables required for the run the pilot jobs to the cluster.

Tradeoff considerations:
Pilot jobs assume a regular stream of communication to the scheduler. Breaking the communication could result in large numbers of job restarts and wasted cycles. The implication is that users should not install this on their local laptop or workstation, unless the workstation is consistently up for the duration of the project. 

Hence for v1, we are assuming that the user will probably not install BOSCO on her workstation, but will likely install it on an intermediate workstation. This is currently how we are deploying campus infrastructures at VirginiaTech, Nebraska, and FIU. 

For v1 with the ECF we assume:
   * BOSCO v1 runs on a submit host (that is probably not the user’s workstation)
   * The user connects to BOSCO from their laptop or workstation via SSH for executable and data management.
   * The submit host BOSCO can be set up either by the user or by a sysadmin for use by multiple users.
   * A job submission status capability is needed as part of v1 to help users debug their submissions and understand where and at what stage a problem is occurring. (This could be useful for Condor as part of this.)

One possibility for v2 would be to create a set of tools that would use ssh to stage files to/from the cluster. This would require the user to manually use these tools, and still may not be an effective replacement for the existing campus infrastructures, but this might allow BOSCO to remain on the users laptop or cluster.

Some Questions:
   * How much development would be needed?
   * Would this require moving parts to be installed on the cluster to support this?
   ** If so, how would that change the user requirements for maintaining the cluster?



%META:FILEATTACHMENT{name="bosco-v0-arch.png" attachment="bosco-v0-arch.png" attr="" comment="Bosco v0 Architecture" date="1329934067" path="bosco-v0-arch.png" size="200048" stream="bosco-v0-arch.png" tmpFilename="/usr/tmp/CGItemp33885" user="BrooklinGore" version="2"}%
