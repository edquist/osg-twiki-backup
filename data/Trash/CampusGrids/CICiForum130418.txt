%META:TOPICINFO{author="MarcoMambelli" date="1366311521" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="BoscoInstall"}%
<!-- conventions used in this document
   * Local UCL_HOST = %URLPARAM{"INPUT_HOST" encode="quote" default="hostname"}%
   * Local UCL_USER = %URLPARAM{"INPUT_USER" encode="quote" default="user"}%
   * Local UCL_DOMAIN = %URLPARAM{"INPUT_DOMAIN" encode="quote" default="opensciencegrid.org"}%
   * Set TWISTY_OPTS_DETAILED = mode="div" showlink="Show Detailed Output" hidelink="Hide" showimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" hideimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" remember="on" start="hide" 
   * Set TOC2 =<div style="float:right; margin-right:-1.015em; padding:0.5em; background-color:white;">%TOC%<p class="twikiClear" /></div>
-->


---+!! Bosco + SkeletonKey for High Throughput R Applications

Here is an example using BOSCO and SkeletonKey with the OASIS software service to run distributed high-throughput R-applications on campus grid environments.
   * First we'll be installing BOSCO as shown in CampusGrids.BoscoAHM13
   * Then we'll install !SkeletonKey and use it to run R as seen in [[CampusGrids.Quickstart]] and [[CampusGrids.SoftwareAndDataAccess]].

---+ Getting Started

You will need login in your host, be able to share a =public_html= directory and have login access to a remote cluster.

You will need also access to  Web proxy (e.g. a squid proxy server). The closer this is to your cluster, the more efficient will be your jobs. Several campuses provide one (check with your network administrators). If you want to install one OSG provides a package and [[Documentation/Release3.InstallFrontierSquid][instructions]]. For this tutorial you can also use the OSG ITB proxy server, even if it will not be very efficient if you are far form Chicago!


This is a very abbreviated install document for Bosco.  For the full install document, view [[BoscoInstall][Bosco Installer]].  
And for more information on data transfer see SkeletonKey.

%NOTE% You will need to install Bosco and SkeletonKey  on a !RedHat (Or !CentOS or Scientific Linux) computer.  It must also not have HTCondor already running.

---+ Let's start with Bosco

---++ Download & Install Bosco
<literal>
 <a href="http://bosco.opensciencegrid.org/download/">
     <img src="https://raw.github.com/osg-bosco/bosco-download-images/master/images/download-orange.png" 
     alt="Bosco Download"
     style="border-width: 0;"/>
 </a>
</literal>

   
Visit the Bosco [[http://bosco.opensciencegrid.org/download/][download]] page.  Choose the Multi-Platform Installer.  After downloading the installer, from the terminal, untar it and run the installer as a regular user:
<pre class="screen">
%UCL_PROMPT% tar xzf boscoinstaller.tar.gz
%UCL_PROMPT% python boscoinstaller
</pre>


---++ Starting Bosco & adding your first cluster

First you will need to setup your environment to have Bosco installed:
<pre class="screen">
%UCL_PROMPT% source ~/bosco/bosco_setenv
</pre>

Start Bosco:
<pre class="screen">
%UCL_PROMPT% bosco_start
</pre>

Add your first cluster.  You will need your username and password from the sheet.
<pre class="screen">
%UCL_PROMPT% bosco_cluster --add demo%RED%XX%ENDCOLOR%@boscopbs.opensciencegrid.org pbs
</pre>


---++ Test the new Cluster

In order to confirm everything is working with the remote cluster, you may want to test the Bosco cluster.

<pre class="screen">
%UCL_PROMPT% bosco_cluster -t demo%RED%XX%ENDCOLOR%@boscopbs.opensciencegrid.org
</pre>

---+ Now setup !SkeletonKey

---++ Download and Install
!SkeletonKey uses a python script to install and set things up for the user.  The installation procedure is as outlined below:

   1. First download the !SkeletonKey installer script <pre class="screen">
%UCL_PROMPT% wget uc3-data.uchicago.edu/sk/install-skeletonkey.py
</pre>
   1. Pick a directory to install the CCTools and !SkeletonKey binaries in (e.g. =bin= in your home directory).  Ideally this directory should be in =$PATH=.  <pre class="screen">
%UCL_PROMPT% mkdir ~/bin
</pre>
   1. Pick a directory to export from Chirp (for now the tutorial will use /tmp)
   1. Run the installer, specifying the directory to install and the  directory to export from Chirp: <pre class="screen">
%UCL_PROMPT% python install-skeletonkey.py -b ~/bin -e /tmp
</pre>
   1. Add the directory specified in =-b= option (e.g. =~/bin=) to =$PATH=: <pre class="screen">
%UCL_PROMPT% PATH=$PATH:~/bin
</pre>
   1. Edit =~/.profile= and append the following line: <pre class="file">export PATH=$PATH:~/bin</pre>

---++ Setup and start Chirp

Add =chirp= in your path:<pre class="screen">
%UCL_PROMPT% cd bin/
%UCL_PROMPT% ln -s cctools-3.7.1-x86_64-redhat5/bin/chirp chirp
</pre>

Create your data directories: <pre class="screen">
%UCL_PROMPT% mkdir /tmp/%RED%your_user_name%ENDCOLOR%
%UCL_PROMPT% cd /tmp/%RED%your_user_name%ENDCOLOR%
%UCL_PROMPT% mkdir data output
%UCL_PROMPT% cd
</pre>

Start the chirp server: <pre class="screen">
%UCL_PROMPT% chirp_control start
</pre>

---++ Test !SkeletonKey

Here is a test that will just go through the mechanics of running skeleton key and generating a job wrapper.

---+++ Setting up binaries
In order for the job wrapper that !SkeletonKey provides to work correctly, you'll need to make the cctools binaries available on a webserver.  The !SkeletonKey installer created a file called =parrot.tar.gz= in the =~/bin= that you'll need to copy to your webserver and make it available over http: <pre class="screen">
%UCL_PROMPT% cp ~/bin/parrot.tar.gz %RED%~/public_html%ENDCOLOR%
%UCL_PROMPT% chmod 644 %RED%~/public_html/parrot.tar.gz%ENDCOLOR%
</pre>

---+++ Creating the job wrapper
You'll need to do the following on the machine where you installed !SkeletonKey
   1. Open a file called =sk_test.ini= and add the following lines: <pre class="file">
[Parrot]
location = %RED%http://your.host/~your_user/parrot.tar.gz%ENDCOLOR%

[Application]
script = /bin/hostname
</pre>
   1. In =sk_test.ini=, change the url =http://your.host/~your_name/parrot.tar.gz= to point to the url of the parrot tarball that you copied previously.
   1. Run !SkeletonKey on =sk_test.ini=: <pre class="screen">
%UCL_PROMPT% skeleton_key -c sk_test.ini
</pre>
    1. Finally, run the job wrapper to verify that it's working correctly <pre class="screen">
%UCL_PROMPT% sh ./job_script.sh
--2013-04-18 12:48:54--  http://uc3-test.uchicago.edu/~testu1/parrot.tar.gz
Resolving uc3-test.uchicago.edu... 128.135.158.156
Connecting to uc3-test.uchicago.edu|128.135.158.156|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10488915 (10M) [application/x-gzip]
Saving to: `parrot.tar.gz'

100%[=====================================================================================================>] 10,488,915  --.-K/s   in 0.02s

2013-04-18 12:48:54 (569 MB/s) - `parrot.tar.gz' saved [10488915/10488915]

uc3-test.uchicago.edu
</pre>

In the ini file used by !SkeletonKey, two sections are used.  The !SkeletonKey used the =location= setting in the =Parrot= section to determine where it can download Parrot binaries to use when running user applications.  In the =Application= section, the =script= setting indicates the command to run in the Parrot environment.

---+ Now submit the R jobs

OASIS  is the OSG Application Software Installation Service. 
For more information on OASIS see ReleaseDocumentation.OasisUpdateMethod and to install software in OASIS (you need to be a VO software manager) contact support@opensciencegrid.org

R has been installed in the OSG VO space and is available at the following paths:
   * RHEL5 64bit: =sw/R/rhel5/x86_64/current/=
   * RHEL6 64bit: =sw/R/rhel6/x86_64/current/=



---++ Combined data and software access example
The next example will be guide the user through creating a job that will read and write from a filesystem exported by Chirp using software that's available using CVMFS.  Before you start, please make sure that Chirp is installed and exporting a directory (this tutorial will assume that Chirp is exporting /tmp)

---+++ Creating the application tarball
Since we'll be running an application from a CVMFS repository, we'll create an application tarball to do some initial setup and then run the actual application

   1. Create a directory for the script <pre class="screen">
%UCL_PROMPT% mkdir /tmp/rjob_test
</pre>
   1. Create a shell script, =/tmp/rjob_test/myapp.sh= with the following lines: <pre class="screen">
ROOT_DIR=/cvmfs/uc3.uchicago.edu/sw
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ROOT_DIR/lib
$ROOT_DIR/bin/Rscript ./rjob_test/test.R ./rjob_test/data.grb $CHIRP_MOUNT/output/$1
echo "Finishing script at: "
echo `date`
</pre>
   1. Create a R script =/tmp/rjob_test/test.R= with the following lines: <pre class="file">
#!/usr/bin/Rscript --vanilla

library( raster)
args <- commandArgs(TRUE)
grbFile <- args[ 1]
scanHowMany <- args[ 2]
output <- args[3]
grb <- brick( grbFile)

for( n in 1:scanHowMany) {
r <- subset( grb, n)
cat( paste( names( r), cellStats( r, "sum"), sep= " "), "\n", file=output)
}
</pre>
   1. Next, make sure the =myapp.sh= script is executable and create a tarball: <pre class="screen">
%UCL_PROMPT% chmod 755 /tmp/rjob_test/myapp.sh
%UCL_PROMPT% cd /tmp
%UCL_PROMPT% tar cvzf rjob_test.tar.gz rjob_test
</pre>
    1. Then copy the tarball to your webserver <pre class="screen">
%UCL_PROMPT% cd /tmp
%UCL_PROMPT% cp rjob_test.tar.gz %RED%~/public_html/%ENDCOLOR%
%UCL_PROMPT% chmod 644 %RED%~/public_html/rjob_test.tar.gz %ENDCOLOR%
</pre>
    1. Finally, download the CVMFS repository key at =http://uc3-test.uchicago.edu/~testu1/oasis.opensciencegrid.org.pub= and make this available on your webserver: <pre clas=file>
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqQGYXTp9cRcMbGeDoijB
gKNTCEpIWB7XcqIHVXJjfxEkycQXMyZkB7O0CvV3UmmY2K7CQqTnd9ddcApn7BqQ
/7QGP0H1jfXLfqVdwnhyjIHxmV2x8GIHRHFA0wE+DadQwoi1G0k0SNxOVS5qbdeV
yiyKsoU4JSqy5l2tK3K/RJE4htSruPCrRCK3xcN5nBeZK5gZd+/ufPIG+hd78kjQ
Dy3YQXwmEPm7kAZwIsEbMa0PNkp85IDkdR1GpvRvDMCRmUaRHrQUPBwPIjs0akL+
qoTxJs9k6quV0g3Wd8z65s/k5mEZ+AnHHI0+0CL3y80wnuLSBYmw05YBtKyoa1Fb
FQIDAQAB
-----END PUBLIC KEY-----
</pre> For uc3 CVMFS the key is =http://uc3-data.uchicago.edu/uc3.key= :<pre calss=file>
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3tsg79ghhbxquF3m7oQ3
A7D77TfafuU2qK5SfW/HeERmBSfWdTNagygNhUjK1rROCmqekz3lnn25hma2Qodz
9W3oqbQHRdCT/MTPLpcTl/n12fCtjMDHPfclnc39gu6uPGkRU21DHCusPaznMtGL
hvwa3qSsi646UTaqKvD0dsUFVnUbVaG+XTi5jSQMHRTaGy1JCZBpVDMrMIgZzwDp
9TLy1/5VEejBtYBt2rpV09IieurmA2T4Wsa+7zPUazPx2g+xsMyQ3fCu1fP7oszx
JVbEnNXWhtuZ4R/1DrebXtojtrj6oc2bGlN92UDdthtC1/gE80Kc8tONfQt4P1Ea
KQIDAQAB
-----END PUBLIC KEY-----
</pre>

One thing to note here is that Parrot makes mounted CVMFS repositories available under =/cvmfs/repository_name= where repository_name is replaced by the name that the repository is published under.  

---+++ Creating a job wrapper
You'll need to do the following on the machine where you installed !SkeletonKey
   1. Open a file called =combined.ini= and add the following lines: <pre class="file">
[CVMFS]
repo1 = uc3.uchicago.edu
repo1_options = url=http://uc3-cvmfs.uchicago.edu/opt/uc3/,pubkey=http://repository_key_url,quota_limit=1000,proxies=squid-proxy:3128
repo1_key = http://repository_key_url
repo2 = oasis.opensciencegrid.org
repo2_options = url=http://oasis-replica.opensciencegrid.org/cvmfs/oasis/,pubkey=%RED%http://repository_key_url%ENDCOLOR%,quota_limit=1000,proxies=%RED%squid-proxy:3128%ENDCOLOR%
repo2_key = %RED%http://repository_key_url%ENDCOLOR%

[Directories]
export_base = /tmp/%RED%user%ENDCOLOR%
read = /, data
write = /, output

[Parrot]
location = http://your.host/~your_user/parrot.tar.gz

[Application]
location = http://your.host/~your_user/rjob_test.tar.gz
script = ./rjob_test/myapp.sh
</pre>
   1. In =rjob_test.ini=, change the url =http://your.host/~your_user/parrot.tar.gz= to point to the url of the parrot tarball that you copied previously.  
   1. Run SkeletonKey on =rjob_test.ini=: <pre class="screen">
%UCL_PROMPT% skeleton_key -c rjob_test.ini
</pre>
    1. Run the job wrapper to verify that it's working correctly <pre class="screen">
%UCL_PROMPT% sh ./job_script.sh test.output
</pre>

---+++ Using the job wrapper
---++++ Standalone
Once the job wrapper has been verified to work, it can be copied to another system and run: 
<pre class="screen">
%UCL_PROMPT% scp job_script %REDanother_host:~/%ENDCOLOR%
%UCL_PROMPT% ssh %RED%another_host%ENDCOLOR%
[user@another_host ~] sh ./job_script 
</pre>

---++++ Submitting to HTCondor 
The following part of the tutorial is optional and will cover using a generated job wrapper in a !HTCondor submit file.  
   1. On your !HTCondor submit node, create a file called sk.submit with the following contents <pre class="file">
universe = vanilla
notification=never
executable = ./job_script.sh
arguments = test.output.$(Process)
output = /tmp/sk/test_$(Cluster).$(Process).out
error = /tmp/sk/test_$(Cluster).$(Process).err
log = /tmp/sk/test.log
ShouldTransferFiles = YES
when_to_transfer_output = ON_EXIT
queue 5
</pre>
   1. Next, create =/tmp/sk= for the log and output files for condor <pre class="screen">
[user@condor-submit-node ~] mkdir /tmp/sk
</pre>
   1. Then copy the job wrapper to the !HTCondor submit node <pre class="screen">
%UCL_PROMPT% scp job_script.sh %RED%condor-submit-node%ENDCOLOR%:~/
</pre>
   1. Finally submit the job to !HTCondor and verify that the jobs ran successfully<pre class="screen">
%UCL_PROMPT% ssh %RED%condor-submit-node%ENDCOLOR%
[user@condor-submit-node ~] condor_submit sk.submit
</pre>

Something to note in the HTCondor submit file, is that we're passing the name of the output file that should be written using the =arguments= setting and then using the =$(Process)= variable to ensure that each queued job writes to a different file.  HTCondor will then pass the variable to the job_script.sh which then makes sure that it gets appended to the arguments passed to the =myapp.sh= script.








-- Main.MarcoMambelli - 18 Apr 2013
