%META:TOPICINFO{author="MarcoMambelli" date="1371148999" format="1.1" reprev="1.1" version="1.1"}%
<!-- conventions used in this document
   * Local UCL_HOST = %URLPARAM{"INPUT_HOST" encode="quote" default="hostname"}%
   * Local UCL_USER = %URLPARAM{"INPUT_USER" encode="quote" default="user"}%
   * Local UCL_DOMAIN = %URLPARAM{"INPUT_DOMAIN" encode="quote" default="opensciencegrid.org"}%
   * Set TWISTY_OPTS_DETAILED = mode="div" showlink="Show Detailed Output" hidelink="Hide" showimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleopen-small.gif" hideimgleft="/twiki/pub/TWiki/TWikiDocGraphics/toggleclose-small.gif" remember="on" start="hide" 
   * Set TOC2 =<div style="float:right; margin-right:-1.015em; padding:0.5em; background-color:white;">%TOC%<p class="twikiClear" /></div>
-->


---+!! Bosco + SkeletonKey for High Throughput R Applications

Here is an example using BOSCO and SkeletonKey with the OASIS software service to run distributed high-throughput R-applications on campus grid environments.
   * First we'll be installing BOSCO as shown in CampusGrids.BoscoQuickStart
   * Then we'll install !SkeletonKey and use it to run R as seen in [[CampusGrids.Quickstart]] and [[CampusGrids.SoftwareAndDataAccess]].

---+ Getting Started

You will need login in your host, be able to share a =public_html= directory and have login access to a remote cluster.

You will need also access to  Web proxy (e.g. a squid proxy server). The closer this is to your cluster, the more efficient will be your jobs. Several campuses provide one (check with your network administrators). If you want to install one OSG provides a package and [[Documentation/Release3.InstallFrontierSquid][instructions]]. For this tutorial you can also use the OSG ITB proxy server, even if it will not be very efficient if you are far form Chicago!


In this tutorial I will be using =bash= shell. If =echo $SHELL= returns something different from =/bin/bash= then run =/bin/bash= to start a Bash session.
This is a very abbreviated install document for Bosco.  For the full install document, view [[BoscoInstall][Bosco Installer]].  
And for more information on data transfer see SkeletonKey.

%NOTE% You will need to install Bosco and SkeletonKey  on a !RedHat (Or !CentOS or Scientific Linux) computer.  It must also not have HTCondor already running.

---+ Let's start with Bosco

---++ Download & Install Bosco
<literal>
 <a href="http://bosco.opensciencegrid.org/download/">
     <img src="https://raw.github.com/osg-bosco/bosco-download-images/master/images/download-orange.png" 
     alt="Bosco Download"
     style="border-width: 0;"/>
 </a>
</literal>

   
Visit the Bosco [[http://bosco.opensciencegrid.org/download/][download]] page.  Choose the Multi-Platform Installer.  After downloading the installer, from the terminal, untar it and run the installer as a regular user:
<pre class="screen">
%UCL_PROMPT% tar xvzf ./bosco_quickstart.tar.gz
%UCL_PROMPT% ./bosco_quickstart
</pre>


---++ Starting Bosco & adding your first cluster using the quick start

Supposing to have a ==boscopbs.opensciencegrid.org== PBS cluster I'm connecting it to BOSCO answering the questions:
   * boscopbs.opensciencegrid.org (FQDN of the cluster)
   * myuser (my user name)
   * pbs (queue manager in the cluster)

---+ Now setup !SkeletonKey

---++ Download and Install
!SkeletonKey uses a python script to install and set things up for the user.  The installation procedure is as outlined below:

   1. First download the !SkeletonKey installer script <pre class="screen">
%UCL_PROMPT% wget uc3-data.uchicago.edu/sk/install-skeletonkey.py
</pre>
   1. Pick a directory to install the CCTools and !SkeletonKey binaries in (e.g. =bin= in your home directory).  Ideally this directory should be in =$PATH=.  <pre class="screen">
%UCL_PROMPT% mkdir ~/bin
</pre>
   1. Pick a directory to export from Chirp (for now the tutorial will use /tmp/%RED%your_user%ENDCOLOR% where your_user is your username)
   1. Run the installer, specifying the directory to install and the  directory to export from Chirp: <pre class="screen">
%UCL_PROMPT% python install-skeletonkey.py -b ~/bin -e /tmp/%RED%your_user%ENDCOLOR%
</pre>
   1. Add the directory specified in =-b= option (e.g. =~/bin=) to =$PATH=: <pre class="screen">
%UCL_PROMPT% export PATH=$PATH:~/bin
</pre>
   1. Edit =~/.profile= and append the following line: <pre class="file">export PATH=$PATH:~/bin</pre>

---++ Setup and start Chirp

Add =chirp= in your path:<pre class="screen">
%UCL_PROMPT% cd bin/
%UCL_PROMPT% ln -s cctools-3.7.1-x86_64-redhat5/bin/chirp chirp
</pre>

Create your data directories (the root directory has to be the same that you used in the -e option during the SkeletonKey installation above): <pre class="screen">
%UCL_PROMPT% mkdir /tmp/%RED%your_user_name%ENDCOLOR%
%UCL_PROMPT% cd /tmp/%RED%your_user_name%ENDCOLOR%
%UCL_PROMPT% mkdir data output
%UCL_PROMPT% cd
</pre>

Start the chirp server: <pre class="screen">
%UCL_PROMPT% chirp_control start
</pre>

---++ Test !SkeletonKey

Here is a test that will just go through the mechanics of running skeleton key and generating a job wrapper.

---+++ Setting up binaries
In order for the job wrapper that !SkeletonKey provides to work correctly, you'll need to make the cctools binaries available on a webserver.  The !SkeletonKey installer created a file called =parrot.tar.gz= in the =~/bin= that you'll need to copy to your webserver and make it available over http: <pre class="screen">
%UCL_PROMPT% cp ~/bin/parrot.tar.gz %RED%~/public_html%ENDCOLOR%
%UCL_PROMPT% chmod 644 %RED%~/public_html/parrot.tar.gz%ENDCOLOR%
</pre>

---+++ Creating the job wrapper
You'll need to do the following on the machine where you installed !SkeletonKey
   1. Open a file called =sk_test.ini= and add the following lines: <pre class="file">
[Parrot]
location = %RED%http://your.host/~your_user/parrot.tar.gz%ENDCOLOR%

[Application]
script = /bin/hostname
</pre>
   1. In =sk_test.ini=, change the url =http://your.host/~your_name/parrot.tar.gz= to point to the url of the parrot tarball that you copied previously.
   1. Run !SkeletonKey on =sk_test.ini=: <pre class="screen">
%UCL_PROMPT% skeleton_key -c sk_test.ini
</pre>
    1. Finally, run the job wrapper to verify that it's working correctly <pre class="screen">
%UCL_PROMPT% sh ./job_script.sh
--2013-04-18 12:48:54--  http://uc3-test.uchicago.edu/~testu1/parrot.tar.gz
Resolving uc3-test.uchicago.edu... 128.135.158.156
Connecting to uc3-test.uchicago.edu|128.135.158.156|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 10488915 (10M) [application/x-gzip]
Saving to: `parrot.tar.gz'

100%[=====================================================================================================>] 10,488,915  --.-K/s   in 0.02s

2013-04-18 12:48:54 (569 MB/s) - `parrot.tar.gz' saved [10488915/10488915]

uc3-test.uchicago.edu
</pre>

In the ini file used by !SkeletonKey, two sections are used.  The !SkeletonKey used the =location= setting in the =Parrot= section to determine where it can download Parrot binaries to use when running user applications.  In the =Application= section, the =script= setting indicates the command to run in the Parrot environment.

---+ Now submit the R jobs

The next example will create a job that will read and write from a filesystem exported by Chirp using an application that's available using OASIS (or any other CVMFS repository).  The specific example runs a R script elaborating a raster image but you can easily change it.

OASIS  is the OSG Application Software Installation Service.
For more information on OASIS see ReleaseDocumentation.OasisUpdateMethod and to install software in OASIS (you need to be a VO software manager) contact support@opensciencegrid.org

R has been installed in the OSG VO space on OASIS and is available at the following paths:
   * RHEL5 64bit: =sw/R/rhel5/x86_64/current/=
   * RHEL6 64bit: =sw/R/rhel6/x86_64/current/=

%NOTE% Before you start, please make sure that Chirp is installed and exporting a directory (this tutorial will assume that Chirp is exporting =/tmp/%RED%your_user_name%ENDCOLOR%)

---++ Creating the application tarball
Since we'll be running an application from OASIS, we'll include in the application tarball a script to do some initial setup and then invoke the actual application

   1. Create a directory for the script <pre class="screen">
%UCL_PROMPT% mkdir /tmp/rjob_test
</pre>
   1. Create a R script =/tmp/rjob_test/test.R= with the following lines: <pre class="file">
#!/usr/bin/Rscript --vanilla

library( raster)
args <- commandArgs(TRUE)
grbFile <- args[1]
scanHowMany <- args[2]
output <- args[3]
grb <- brick( grbFile)

for( n in 1:scanHowMany) {
r <- subset( grb, n)
cat( paste( names( r), cellStats( r, "sum"), sep= " "), "\n", file=output)
}
</pre>
   1. Create a shell script , =/tmp/rjob_test/myapp.sh= setting up the environment and then running R (as visible above =test.R= requires 3 arguments: a raster file, =data.grb=, the number of iterations, =100=, and the output file): <pre class="screen">
ROOT_DIR=/cvmfs/uc3.uchicago.edu/sw
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ROOT_DIR/lib
$ROOT_DIR/bin/Rscript ./rjob_test/test.R ./rjob_test/data.grb 100 $CHIRP_MOUNT/output/$1
echo "Finishing script at: "
echo `date`
</pre>
   1. Next, make sure the =myapp.sh= script is executable and create a tarball: <pre class="screen">
%UCL_PROMPT% chmod 755 /tmp/rjob_test/myapp.sh
%UCL_PROMPT% cd /tmp
%UCL_PROMPT% tar cvzf rjob_test.tar.gz rjob_test
</pre>
    1. Then copy the tarball to your webserver <pre class="screen">
%UCL_PROMPT% cd /tmp
%UCL_PROMPT% cp rjob_test.tar.gz %RED%~/public_html/%ENDCOLOR%
%UCL_PROMPT% chmod 644 %RED%~/public_html/rjob_test.tar.gz %ENDCOLOR%
</pre>
    1. Finally, copy or download the CVMFS repository key and make this available on your Web server. If it is already available on a public server that you trust, you can use directly that URL
      * For OASIS the key is at =http://uc3-test.uchicago.edu/~testu1/oasis.opensciencegrid.org.pub=: <pre clas=file>
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAqQGYXTp9cRcMbGeDoijB
gKNTCEpIWB7XcqIHVXJjfxEkycQXMyZkB7O0CvV3UmmY2K7CQqTnd9ddcApn7BqQ
/7QGP0H1jfXLfqVdwnhyjIHxmV2x8GIHRHFA0wE+DadQwoi1G0k0SNxOVS5qbdeV
yiyKsoU4JSqy5l2tK3K/RJE4htSruPCrRCK3xcN5nBeZK5gZd+/ufPIG+hd78kjQ
Dy3YQXwmEPm7kAZwIsEbMa0PNkp85IDkdR1GpvRvDMCRmUaRHrQUPBwPIjs0akL+
qoTxJs9k6quV0g3Wd8z65s/k5mEZ+AnHHI0+0CL3y80wnuLSBYmw05YBtKyoa1Fb
FQIDAQAB
-----END PUBLIC KEY-----
</pre> 
      * For UC3's CVMFS the key is =http://uc3-data.uchicago.edu/uc3.key=:<pre calss=file>
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3tsg79ghhbxquF3m7oQ3
A7D77TfafuU2qK5SfW/HeERmBSfWdTNagygNhUjK1rROCmqekz3lnn25hma2Qodz
9W3oqbQHRdCT/MTPLpcTl/n12fCtjMDHPfclnc39gu6uPGkRU21DHCusPaznMtGL
hvwa3qSsi646UTaqKvD0dsUFVnUbVaG+XTi5jSQMHRTaGy1JCZBpVDMrMIgZzwDp
9TLy1/5VEejBtYBt2rpV09IieurmA2T4Wsa+7zPUazPx2g+xsMyQ3fCu1fP7oszx
JVbEnNXWhtuZ4R/1DrebXtojtrj6oc2bGlN92UDdthtC1/gE80Kc8tONfQt4P1Ea
KQIDAQAB
-----END PUBLIC KEY-----
</pre>

One thing to note here is that Parrot makes mounted CVMFS repositories, like OASIS, available under =/cvmfs/repository_name= where repository_name is replaced by the name that the repository is published under (see the configuration file =rjob_test.ini= below).  

---+++ Creating a job wrapper
You'll need to do the following on the machine where you installed !SkeletonKey
   1. Open a file called =rjob_test.ini= and add the following lines: <pre class="file">
[CVMFS]
repo1 = uc3.uchicago.edu
repo1_options = url=http://uc3-cvmfs.uchicago.edu/opt/uc3/,pubkey=%RED%http://repository_key_url%ENDCOLOR%,quota_limit=1000,proxies=%RED%squid-proxy:3128%ENDCOLOR%
repo1_key = %RED%http://repository_key_url%ENDCOLOR%
repo2 = oasis.opensciencegrid.org
repo2_options = url=http://oasis-replica.opensciencegrid.org/cvmfs/oasis/,pubkey=%RED%http://repository_key_url%ENDCOLOR%,quota_limit=1000,proxies=%RED%squid-proxy:3128%ENDCOLOR%
repo2_key = %RED%http://repository_key_url%ENDCOLOR%

[Directories]
export_base = /tmp/%RED%your_user%ENDCOLOR%
read = /, data
write = /, output

[Parrot]
location = %RED%http://your.host/~your_user%ENDCOLOR%/parrot.tar.gz

[Application]
location = %RED%http://your.host/~your_user%ENDCOLOR%/rjob_test.tar.gz
script = ./rjob_test/myapp.sh
</pre>
   1. In =rjob_test.ini=, change the url =http://your.host/~your_user/parrot.tar.gz= to point to the url of the parrot tarball that you copied previously. And set correctly the value of the exported local directory (/tmp/your_user) and the CVMFS servers (repository keys and proxy) 
   1. Run SkeletonKey on =rjob_test.ini=: <pre class="screen">
%UCL_PROMPT% skeleton_key -c rjob_test.ini
</pre>
    1. Run the job wrapper locally to verify that it's working correctly <pre class="screen">
%UCL_PROMPT% sh ./job_script.sh test.output
</pre>

---++ Submitting the job wrapper via Bosco

Once the job wrapper has been verified to work, multiple instances can be run via Bosco. In this example we'll run 5 instances (=queue=5= in the submit file). 

<!--
<pre class="screen">
%UCL_PROMPT% scp job_script %REDanother_host:~/%ENDCOLOR%
%UCL_PROMPT% ssh %RED%another_host%ENDCOLOR%
[user@another_host ~] sh ./job_script 
</pre>
-->

Create =/tmp/bosco_logs = for the log and output files for Bosco <pre class="screen">
%UCL_PROMPT% mkdir /tmp/bosco_logs
</pre>

Create Bosco (!HTCondor) submit file called =rjob_test.submit= with the following contents <pre class="file">
universe = grid
grid_resource = batch %RED%pbs%ENDCOLOR% %RED%demoXX@boscopbs.opensciencegrid.org%ENDCOLOR%
notification=never
executable = ./job_script.sh
arguments = test.output.$(Process)
output = /tmp/bosco_logs/rjtest_$(Cluster).$(Process).out
error = /tmp/bosco_logs/rjtest_$(Cluster).$(Process).err
log = /tmp/bosco_logs/rjtest.log
ShouldTransferFiles = YES
when_to_transfer_output = ON_EXIT
queue 5
</pre>
Make sure to specify the grid_resource line as it was suggested when you added the cluster with =bosco_cluster --add=. You can use also vanilla jobs but there may be some more network requirement about the Bosco submit host. See [[CampusGrids.BoSCO]] for more information.

<!--   
Then copy the job wrapper to the !HTCondor submit node <pre class="screen">
%UCL_PROMPT% scp job_script.sh %RED%condor-submit-node%ENDCOLOR%:~/
</pre>
-->

Finally submit the job to Bosco and verify that the jobs ran successfully<pre class="screen">
%UCL_PROMPT% condor_submit rjob_test.submit
</pre>

Something to note in the HTCondor submit file, is that we're passing the name of the output file that should be written using the =arguments= setting and the file name we are using includes the =$(Process)= variable to ensure that each queued job writes to a different file.  HTCondor will pass the variable to the job_script.sh which then makes sure that it gets appended to the arguments passed to the =myapp.sh= script.








-- Main.MarcoMambelli - 18 Apr 2013
