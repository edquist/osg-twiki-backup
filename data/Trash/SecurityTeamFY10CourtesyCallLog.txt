%META:TOPICINFO{author="JenyTeheran" date="1500921759" format="1.1" reprev="1.19" version="1.19"}%
%META:TOPICPARENT{name="Trash.SecurityTeamFY10SecuritySupport"}%
<!--
   * Set ALLOWTOPICVIEW = Main.SecurityTeamGroup
   * Set ALLOWTOPICCHANGE = Main.SecurityTeamGroup
-->
%TOC%
---+ Courtesy Call Log

This is a log of courtesy calls for the Trash.SecurityTeamFY10SecuritySupport program.

---++ Jeff !DeReus (GROW VO)

Email	jeffrey-dereus@uiowa.edu

Phone	1-319-335-6079

Call scheduled 2pm Mon Aug 17 2009.

http://www.its.uiowa.edu/research/grow

https://grow-voms.its.uiowa.edu:8443/voms/GROW

GROW applications are coming from several disciplines including Atmospheric Science, Bioinformatics, Computational Science, Environmental Science, Geographic Information Science, High Energy and Particle Physics, Management Science, Medical Imaging, and Statistical Computing. GROW will provide computing resources to OSG as well as be interested in using both computing and data resources on OSG.

Community:	Grid computing/application communities within the state of Iowa and their affiliated stakeholders

Jeff attended OSG AHM earlier this year. Didn't come to site admins workshop due to limited funding.
Using same name as old GROW VO (based on OSG recommendation) but new install.
User community not appearing. Discussing with Abhishek.
Bringing up a new cluster.

Jeff is responsible for OSG system admin and other systems.
Bringing the VO up was easy.
Some OSG 1.2 upgrade documentation troubles, though. 
Got certificates OK. Became RA agent.

VO membership will be based on Iowa faculty sponsorship.
Will run VO jobs on campus first.
New cluster will mainly serve GROW VO members. Backfill with campus Condor. Willing to serve other OSG VOs while waiting for GROW VO membership to appear.

No security concerns.

---++ Scott Beardsley (UCD_T3: UC Davis T3 CMS SE and CE nodes)

Email	sbeardsley@ucdavis.edu

Phone	+1 530 752 0547

Call scheduled 1:30pm Mon Aug 17 2009.

Bill Broadley <bill@ucdavis.edu> will also join.

Supported VOs	cms mis ops osg

cms.tier3.ucdavis.edu:2119

Scott and Bill are the cluster team at UC-Davis. They're officially in the Geology department and manage 10-15 clusters across campus departments supporting computational science and engineering. Brought CMS T3 cluster up in collaboration with the Physics department. They just recently got the cluster hardware. Other clusters in other departments are not in OSG and have separate accounting.

Scott was at the recent site admins workshop in Indy. His first OSG meeting. He joins the T3 calls. Bill is less focused on grid aspects but is knowledgeable about cluster security.

Scott asked about FNAL KCA using MD5. I told him about new FNAL KCA coming online with IGTF approval using SHA-1.

Concerns:
   * CA bundle signing. move to https.
   * signing of VDT/OSG software. They use signed RPM/Debian packages on their systems.
   * Brought the CMS cluster up using SL5. ISO on HTTP site. No checksum. Can we raise this as a SL security concern? Could OSG Security Team provide checksums for SL distributions? Could put MD5SUM or SHA1SUMS of ISO directory on https server. Provide https page for verifying packages / CAs.
   * They're using DLV: publish dnssec keys with ISC. Could OSG use it?
   * Where to find a summary of different services and ports required for OSG? Squid needs a monitoring port open. Can we provide firewall rules?
   * They are using GUMS. Do they still need a grid-mapfile? CEMon requires it?

---++ Patrick !McGuigan (DOSAR VO and UTA_DPCC/UTA_SWT2/SWT2_CPB)

mcguigan@uta.edu (UTexas Arlington)

Phone	+1-817-272-1051

secondary contact: Mark Sosebee <sosebee@uta.edu> (not in OIM?)

Distributed Organization for Scientific and Academic Research

A community of physics researchers interested in furthering the development of grid based analyses.

Call 2:00pm Tue Aug 25 2009.

Need to update OIM info. He's receiving reminder emails.

We met at OSG AHM in livingston.
Patrick involved since before it was OSG.
Started with Grid2003. Computing Site.
Focus on ATLAS and HEP.
2 clusters part of ATLAS T2. UTA_WT2. UTA_DPCC.
First site started in 2003.
Mark taking more active role with clusters.
Patrick administers DOSAR VOMS server.
Talk to Horst (OU) about DOSAR.
Preparing to move VOMS server to Lousiana Tech.

Other DOSAR contacts are
Dick Greenwood at LTU,
Jayu at UTA,
Joel Snow at Langston.

Running an older version of VOMS.

DOSAR membership is based on recommendation from someone in the group.
Email to Patrick or Horst. People in DOSAR know each other.

UTA security officer: Shaun Lam.

CA/CRL updates. Want one site installation. Push that out to machines.
RSV complains about CA/CRLs not up-to-date.
Is this fixed in OSG 1.2? Should be...

At OSG 1.0 now. Moving to 1.2 within a month.
Using GUMS 1.2.16. Last upgrade changed configuration syntax.
Will update GUMS within a month.

Use PBS/Torque.

Storage Element: Bestman/Gateway because using xrootd.
ATLAS supports: dcache, xrootd, lustre/POSIX

DOSAR used Condor-G in the past. Ask Horst for details.

Feedback to security team:
   * Concern about revoked certificates. Making sure we're handing VOMS updates. Making sure GUMS is polling VOMS. Maybe more monitoring/verification?
   * Security notifications are helpful.

---++ David Lesny (!IllinoisHEP Trash/Tier3gs)

Email:	ddl@illinois.edu

Phone:	1-217-333-4972

In person meeting 2:00pm Mon Aug 24 2009 (461 Loomis on UIUC campus).

Mark Neubauer is the PI.

T3gs = T3 + grid services (which makes them an "unofficial" Tier2)

Grid services include CE, SE, Local File Catalog, DQ2. In other words, Panda jobs run there.
They allow ATLAS production jobs from Panda production queue and other jobs from the Panda analysis queue.
They may disable the Panda analysis queue in the future and just support ATLAS production jobs and local jobs.

Dave is the primary sysadmin for the systems and handles security issues.
He's assisted by Larry Nelson.
Dave attended an OSG storage workshop at Fermi and the admin workshop in Indy this month.
They're running the latest GUMS (v1.3.1.6).
He updated his contact info in OIM today. Registered their GUMS server.

For security issues, Dave would report them to the Physics department IT administrator, who would escalate to campus IT security.

He has IPtables configured to allow SSH only locally.
Was interested in GUMS banning. I pointed him to TWiki page about it.
He finds VDT pacman installs work well. Can easily manage multiple installed versions. VDT dCache install worked well for him.

He can always use better monitoring tools.
He watches RSV, Panda, and ATLAS web dashboards.
Larry watches messages at a central syslog host.
They're interested in syslog-ng.

---++ Tufts (Atlas T3)

<verbatim>
OSG security call with Tufts.
Aug 27 2009 2pm CT

Attending:
  Jim Basney (OSG Security)
  Peter Doherty (SBGrid site admin)
  John Hover (BNL - ATLAS)
  Torre Wenaus (BNL - Panda)
  Durwood Marshall (Tufts)
  Lionel Zupan (Tufts)
  Vik Solem (Tufts)
  Paul Nash (Tufts)


Summary:

Tufts is a long-time ATLAS participant but is new to OSG.  They are
joining OSG as a Tier3, in collaboration with the Boston University
and Harvard Tier2 sites. There was a misconception that Tufts needed
to grant worldwide access to their systems to participate, but we
discussed that they can configure their systems to serve local Tufts
researchers only without accepting outside jobs or unrestricted
external network connections.

We discussed the old OpenSSL version in the Globus Toolkit. OSG sites
should install Globus from VDT (http://vdt.cs.wisc.edu/), which links
Globus against the system's OpenSSL libraries. This is functionality
provided in the latest Globus releases. The OSG security and VDT teams
actively participate in the Globus Security Committee.

We also discussed VO software administrators and VO-specific software
areas. In ATLAS, Xin maintains the ATLAS analysis software in the
ATLAS software area on ATLAS sites. Tufts can control this access. In
any case, no outside OSG or VO personnel can modify a site's system
software or VDT installation. VO software administrators only have
access to their specific VO software directory. OSG strongly supports
the principal of local site control.

We discussed the best avenues for Tufts to receive support. OSG
mailing lists provide a forum for site administrators to share their
expertise. Rik Yoshida is the ATLAS T3 coordinator.


Jim promised to pass along the following information:

Compute Element Firewalls configuration:
https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ComputeElementFirewalls

OSG mailing lists:
http://www.opensciencegrid.org/Consortium_Mailing_Lists

OSG security:
https://twiki.grid.iu.edu/twiki/bin/view/Security/WebHome

Condor vulnerability assessment:
http://pages.cs.wisc.edu/~kupsch/vuln_assessment/


Minutes:
Peter has talked with Lionel and Durwood in the past.
SBGrid is doing outreach work in Boston area. Got in touch with Tufts.

Tufts ATLAS Tier3 for long time. Joining OSG now.
Boston University and Harvard act as Tier2.
Working to reproduce BU/Harvard setup at Tufts.

John: Typically OSG gatekeepers allow access from all IP addresses.

Lionel - Associate Dir of Research Technology (Tufts)

Tufts received a recommendation to allow access from all Class B IP
addresses. 524,000 IP addresses
We (Jim/John/Torre/Peter) think this is not a requirement.

Tufts is installing OSG/ATLAS software stack on shared systems.
Valuable equipment inside Tufts border.
Cluster. Storage system.
Center for scientific visualization.
Central/Shared resources.

Concern: An OSG/ATLAS security issue could take down the shared Tufts
resources.

As a T3, Tufts will support local Tufts usage.
Could consider opening up in the future.
Small group at Tufts can make submissions to gatekeeper.
Group was expanded to include Saul from BU.
BU and Harvard say restricting at IP level is not possible?
If user is at a conference, their IP would be blocked?
We think IP restricts are possible.
Perhaps Tufts researchers can use a VPN when off campus.

Jim: Follow-up with install documentation.

Connections open:
Job submissions. Using Panda?
Data from BU: remotely mount via GPFS or upload/download.
10 Gbps connection to BU.

Panda can allow a restricted number of sites.
Or you can run your own submitter with pilot jobs.

Any restrictions on outbound access? No.

Where can our system administrators find support?
OSG mailing lists. Email security team.
Rik Yoshida is ATLAS T3 coordinator.

Security reviews of software?
Formal review of Panda.
ATLAS analysis software stack? Has EGEE done a review? Not that we know.
Paul noticed issues of old SSL libraries.
Globus had old OpenSSL version.
VDT install uses system OpenSSL.
We work with Globus Security Committee.

Email address for VDT updates?

Opening up ports? Should just open up port range.

Local Panda pilot submissions will not require inbound access.

Interested in opening collaboration, at Bioinformatics level.
Talking with Ian Stokes-Rees.

Can an outside OSG administrator upgrade or change system software?
That's not the case.
VO software administrator can have rights to install VO software.
ATLAS software can be installed this way.
Are the Pilots handing the ATLAS installation?
Xin does the install using Pilots.
Tufts could do this through a local submission.
Tufts would be in charge of software upgrades.
VOs can have VO-specific software area.
Panda is outside VO software area.
Tufts would upgrade their own scheduler.
Will talk with Rik Yoshida.
</verbatim>

---++ Pablo Yepes (Rice)

Email	yepes@rice.edu

Phone	1-713-348-5943

Call scheduled 2pm Fri Nov 6 2009.

osg-gate.rice.edu:2119

Pablo is the primary individual who is coordinating all of the OSG activities at Rice.  He mentioned that Rice was a member of the OSG a number of years ago and they dropped out.  Within the last year they wanted to provide access to the CMS data for students and researchers so they rejoined OSG as a tier3 site.

Covered the responsibilities of the OSG security team.

They have a 4 node cluster that is currently set up with OSG.  They do allow non-local users to submit jobs, but doesn't think he has seen that may outside users because their site is so small.  They are using a gridmap file because it was easier to set up.  He was not sure of the VDT version that they were using, but it was installed around the March 2009 timeframe and has not been updated since.  They mainly support local users doing CMS work.  He was not sure with many of the VO questions we had, maybe because it's a tier3 site?!?

Pablo did not really have any security concerns.  He commented that the more transparent the OSG security team could be with sites the better.  I think the meaning of that is whatever we could share is spelled out clearly so he could understand and take action.  He mentioned the easier we could make instructions (such as kernel upgrades) the better.  We informed him that we try to work with individual sites, but it's difficult since there are so many, and they are the experts at their site.

They are using SL for their cluster.  Computers in their department are generally not given external access.  However, they were able to ask for access and were given a special subnet for the cluster.  He said that local access for users is with a username/password over SSH.  The authentication service is run by the university (he was not sure what it was).  They do have a SE at their site and they are using bestman along with a large disk connected via NFS.

---++ Alan Sill (TTU-ANTAEUS)

Email	Alan.Sill@ttu.edu

Phone	1-806-742-4350

Call scheduled 3pm Thu Dec 3 2009.

antaeus.hpcc.ttu.edu:2119

Alan has been involved in the OSG since the beginning.  His involvement is primarily for enabling research for the ttu.edu students and professors and they provide a resource (anteus) as well.  He covered what his responsibilities are and the resources they provide.

<LI>Alan's main job is interface between scientists to get them what they need to do their research.
<LI>He takes shifts in the sites CMS monitoring
<LI>Largest machine is #175 on the top 500 list (3360 cores).
<LI>220 cores for OSG

TTU decided to build a larger Tier 3 site for their own local users because they did not want to have to provide cycles for other researchers not on their campus.  Alan is also familiar with the TeraGrid and is the TG "champion" on his campus.

TTU's primary VO is CMS and they are using GUMS and PRIMA.  Alan mentioned that they were running a VOMS server the the TIGRE project which they never made into an OSG VO.  He also runs a PE GRID (for petroleum research) and runs a VOMS server for that as well.  It was interesting that he was using the VDT software for non-OSG grid environments.  He said that with a few tweaks it works great, and he doesn't have to support any grid software himself.

Since Alan has been involved with the OSG for a long time he was able to participate in some of the early security working groups.  He said that he was involved in a joint security working group run by Bob Cowles.  There was also a security assessment group within EGEE that Alan was involved in, but he doesn't know if that still exists (may be the EGEE vulnerability assessment group?).  Alan also mentioned an early security document they called the "Taiwan Accord" by Alan, Bob, and another individual he didn't remember.  In there he said that there was a provision to drop an OSG site.

We discussed the recent kernel update announcements that were sent out.  He did not feel that they were very beneficial since he was already getting all of the information via other sources.  He wanted to see more specific information in the announcements that would address exactly how the problems affect the OSG sites.  He was strongly encouraging getting involved in the T3 support groups and getting a better idea of what the sites really need.  He felt that there is not an overall T3 support structure (like there may be for T2 sites).  They seem to break down by project types (ie. CMS, ATLAS).  He felt that we should exploit the VO structure more for communication purposes.  And the VO may be the best primary tool to working with individual member sites in getting them patched, etc..

Alan also thinks that there is a higher payoff working with the smaller T3 sites, and would like to see many more T3 sites getting involved in the OSG.  He had asked the question of how does cyberinfrastructure get to the rest of the educational environments (around 6400) that it's not currently reaching?  Alan also feels that the OSG is not really a grid because of the current structure.  Sites are not getting any money for their involvement, so a very loose knit group and not much local security support at most sites.

Alan said that there is a pretty extensive security team on the TTU campus.  They have pretty much left him to do his grid stuff and he keeps in contact with the campus security staff.  Other security issues that we discussed were the group accounts that VO's use rather than individual accounts.  He doesn't remember when VO's starting doing that, but he thinks that is a VO decision (since some VO's don't do this).  He was wondering if some VO's were not aware of the group accounts (and maybe we should notify them and encourage individual accounts).  Alan seems to think that there is a WLCG vulnerability security list and that we should have an equivalent in OSG (maybe it's the EGEE group he was referring to earlier?).

When I asked Alan if there were any specific ways to improve security he came up with the following list.

Security announcements
<LI>They are not pre-digested enough (more pre-processing of info is needed)
<LI>Have to log in to GOC just to get list of external links (that he likely has received elsewhere).
<LI>Know your audience, who is going to be hitting the site
<LI>Make it worth their while (nobody will follow the links if they are not)
<LI>How does the security vulnerability unfold within the OSG environment?
<LI>How can it be used to get root on an OSG CE?
<LI>Realizes that some sites need more predigested information than others.

Need to work with sub-collection of VO's.

Bigger picture thinking:
<LI>How am I going to get it to work in a setting where it's never been done before?
<LI>How would things work if I scaled it by a factor of 10 (or more), would it work?

---++ Malina Kirn (umd-cms)

Email	m.a.kirn@gmail.com

Phone	1-301-405-5970

Call scheduled 3pm Fri Jan 22 2010.

hepcms-0.umd.edu:2119

Malina has been involved with OSG for 4 years doing research and running jobs, and as an admin for about 2 years.  She is currently the only admin at the site, but does have someone to back her up if she is out sick or on vacation.  Malina is also the security person at umd-cms.

There have been some security announcements that she was not sure what to do with.  One of the annoncements had to do with looking for rootkits.  She did mention that an email came out later that had more information in it with the location of the rootkits.  She said that it would help to spell out exactly how to fix things if possible.

umd-cms info
<UL>
<LI>Site executive is a professor with a grant to do OSG work
<LI>There are around 30 local users total
<LI>Smaller number of active users (around 10)
<LI>They do service jobs from other CMS sites (allow all sites within CMS VO)
<LI>64 CE batch slots
<LI>2 grid management nodes
<LI>They have received stimulus funds to double batch slots
<LI>May add 1 more management node
<LI>SE will be 40-50 TB after the upgrade
<LI>Run bestman gateway (NFS mount)
<LI>Main research goals<BR>
ion analysis, lepto quark, hadron exotica physics
<LI>Jobs subminssions via crab<BR>
condorg or glite<BR>
recently use glide?
</UL>

umd-cms security info
<UL>
<LI>Use deny hosts (for ssh brute force attacks)
<LI>Use generic rootkit detector (chrootkit)
<LI>Don't have much in IP tables at all
<LI>Using a NAT<BR>
       Worker nodes do not have external IP addresses
<LI>Moved to SL5 (one month ago) pretty much required in CMS<BR>
yum updates has problems with some packages (ie. rocks)<BR>
SL4 yum updates worked well<BR>
Has to do manually and does not know how to handle that
<LI>Head node and grid node are verifying packages
<LI>She is on the enterprise watch list (does get alerts)
</UL>

Are there any sites in CMS who would be in the same position that could help with her SL5 issues?<BR>
 - Michael Thomas may be able to help (she may ask him)

Not sure about web portals

Security concerns<BR>
- updating yum packages (mentioned above)<BR>
- Would like to start using a software firewall<BR>
       Is going to take alot of time that she does not have<BR>
       Takes time away from other work that she has

Can we add to T3 security page<BR>
- How to configure a PAM module to check for weak passwords when they are set/changed.

She does appreciate our alerts<BR>
- But how much do we rely on them to update<BR>
- She is erring on the side of a working system rather than a package breaking the software stack

gcc vulnerability RHSA-2010:0039-01

One thing that comes up alot with updates is reboots<BR>
Are there stardard guidelines for doing rolling updates?<BR>
How to remove a node from the condor pool, finish the jobs that are running and then allow for an update, and then placed back in the pool.<BR>
If a CE is rebooted will jobs stop running on worker nodes and does it lose job states?<BR>
Would like to see documentation on this item<BR>

Wants to upgrade to GUMS.

%META:TOPICMOVED{by="JenyTeheran" date="1500921759" from="SecurityTeam.FY10CourtesyCallLog" to="Trash.SecurityTeamFY10CourtesyCallLog"}%
