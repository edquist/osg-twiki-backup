%META:TOPICINFO{author="IwonaSakrejda" date="1285362120" format="1.1" version="1.37"}%
%META:TOPICPARENT{name="WorkerNodeClient"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="3"}%


---++ Description
Glexec is commonly used for what are referred to as "pilot" or "glidein" jobs.  Normally  users submit their jobs directly to a remote site (or compute element gatekeeper).   The user job is authenticated/authorized to run at that site based on the user's proxy credentials and run under the local unix account assigned.

In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue).   The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources.  It will then submit what is called a pilot/glidein job to a remote site.  This pilot job gets authenticated/authorized to run on a worker node in that site's cluster.  It will then "pull" down user jobs from the centralized queue and execute them.  Both the pilot and the user job are run under the pilot job's proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user's proxy credentials and, thus, the user's jobs do not run using it's own local unix account.

Glexec is a security tool that can be used to resolve this problem.   It is meant to be used by VOs that run these pilot-based jobs.  It  has a number of authentication plugins and can be used both by EGEE LCAS/LCMAPS and by OSG with GUMS and SAZ.   

The pilot job will "pull" user jobs down from the central queue and invoke glexec which will then 
   1 authenticate the user job's proxy, 
   1 perform an authorization callout (GUMS/SAZ in the case of OSG) similar to that done by the gatekeeper,
   1 and then run the user job under the local account assigned by the authorization service (for OSG, this is GUMS) for that user.

In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node.  The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.

Many worker node clusters use shared file systems like NFS for much of their software and user home accounts.  Since glexec is an suid program, it must be installed on every single worker node individually.  Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.

For more information regarding pilot-based systems and glexec:
   1 <a href="http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.html">glideinWMS - The glidein based WMS</a>
   1 <a href="http://iopscience.iop.org/1742-6596/119/5/052029/pdf/jpconf8_119_052029.pdf">Addressing the pilot security problem with gLexec (pdf)</a>

---++ Installation
This section describes any prerequisite software/considerations that must be taken into account before the gLexec software is performed.  It should be reviewed completely before starting the installation process.

The installation process described here is for a _pacman_ installation from the VDT.

An alternative, RPM distribution, is under development/test and is available here: http://vdt.cs.wisc.edu/glexec-rpm/ . Keep in mind that this RPM distribution is under development and evolving.  When released for production this section will contain both installation methods.

---+++ gLexec must be installed as root
The =glexec= program is an =suid= program and thus must be installed on every single worker node individually. 
It cannot and must not be exported over NFS and it is probably not a good idea to use any other shared filesystem either.  

---+++ Same location on all worker nodes
It is recommended that the gLexec package be installed within the OSG Worker Node Client area because most of its dependencies are the same 
as those within the worker node client.   It must be installed in the *same*  location on all worker nodes managed by the same Compute Element.
The reason for this is that the location of the gLexec executable is defined on the CE. Details are described in the [[GlexecInstall#Configuring_the_CE_node][Configuring the CE node section]] of this document.

---+++ Create local user "glexec"
You must create a <b>"glexec" local user</b> before you attempt the installation. 
gLexec uses it for privilege separation and the installation will fail without it.

---+++ Create group ids for tracking purposes
Starting in OSG 1.0.0, gLexec also requires a range of <b>group ids</b> for tracking purposes. 
   * Define at least 4 group ids per batch slot per worker node. 
   * They must be consecutive and in any range (default range is 65000-65049, configured in post-install section below).   
   * The same group ids can be used on every worker node.

A good way to handle this is to multiply the number of batch slots on the largest worker node by 6
and then share the group ids between all the worker nodes.

For example to create group ids matching the default range (for 8 batch slots with a little extra):
<pre class="screen">
# groupadd -g 65000 glexec00
# groupadd -g 65001 glexec01
# groupadd -g 65002 glexec02
     :
     :
# groupadd -g 65049 glexec49
# useradd -c "gLexec user" -s /sbin/nologin glexec
</pre>

These group ids will be added to a gLexec configuration file during the post-installation configuration process.

---+++ Install CA Certificates
Remember to install the CA Certificates as for every VDT install. A simple example for a self contained installation is in CaCertificatesUpdatesConfig and CaCertificatesInstall is a more complete reference about installation and maintenance of CA Certificates and CRLs.

---+++ Configure each node with host cert or proxy
Each worker node which is executing =glexec= needs to have either a host certificate or a host proxy distributed from the gatekeeper.

If you don't have host certificates for the worker nodes, there is a host proxy distribution script located at  =http://fermigrid.fnal.gov/files/glexec/host_dist_latest.tgz= which can be installed on your gatekeeper and will automatically create a proxy from your gatekeeper's host certificate and push it out to =/etc/grid-security/hostproxy.pem= and =/etc/grid-security/hostproxykey.pem=.

---+++ Installing the gLexec package
<b>gLexec must be installed as root</b>.

It is recommended that the gLexec package be installed within the OSG Worker Node Client area because most of its dependencies are the same 
as those within the worker node client.   It must be installed in the *same*  location on all worker nodes managed by the same Compute Element.
The reason for this is that the location of the gLexec executable is defined on the CE. Details are described in the  [[GlexecInstall#Configuring_the_CE_node][Configuring the CE node section]] of this document.

To install:
<pre class="screen">
# pacman -get %CACHE%:Glexec
# source ./setup.sh
# vdt-post-install
</pre>

%NOTE% The =setup.sh= file is not changed to put the =glexec= binary into your path.

After the =pacman= installation is complete, perform these checks:
   1 The install will create config files under =/etc/glexec= if the directory does not already exist.
   1 The install should have created the =/var/log/glexec= directory. If it does not exist, create it with 555 permissions with root owner/group.
      * dr-xr-xr-x 2 root root 4096 Jun 29 03:32 /var/log/glexec
   1 Verify that the  =VDT_LOCATION/glexec-osg/sbin/glexec= binary has =suid= permissions. If it does not, check the =vdt_install.log= and the =post-install/README= file to see it sheds light on a problem.
      * -rwsr-sr-x 1 root root 157394 Jun 29 03:32 glexec. 




---++Post-install Configuration of gLexec
The following steps need to be executed manually after the =gLexec= installation is complete.

---+++Modify /etc/glexec/contrib/gums_interface/getmapping.cfg

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg= to point to the correct location of your GUMS server.  

The =gums_url= variable is the only variable you should have to change unless you want to use the XACML2 interface; see the "Using the new XACML2 GUMS interface" section below. 

If you're using host proxies instead of host certificates, as described above, you will also need to change =host_x509_file= and =host_key_file= .

<pre class="file">
# gums_interface/getmapping config file
#
# this is a shell script that gets sourced
#

vdt_location=/opt/vdt
log_dir=/var/log/glexec

########################
# GUMS config parameters
########################
<b>
# Use this if you are using host proxies
#host_x509_file="/etc/grid-security/hostproxy.pem"
#host_key_file="/etc/grid-security/hostproxykey.pem"

# Use this if you are using host certs
host_x509_file="/etc/grid-security/hostcert.pem"
host_key_file="/etc/grid-security/hostkey.pem"
</b>
# Use this if you have a host cert mapfile
#host_mapfile=/etc/glexec/contrib/gums_interface/host_mapfile
<b>
##################################
# Mapping program
map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"
# Point it to your GUMS machine
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"
</b>
#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface
#map_exe="/opt/vdt/prima/bin/prima_scas_map_args"
# Point it to your GUMS machine
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"


######################
# Optional SAZ command
# Leave it blank if you don't use SAZ
sazcmd_location=
sazcfg=

# if set to 1, will map even if SAZ fails
saz_warn_only=0

####################
# monitoring program
# leave empty if none should be used
# if set, it will accept arguments
#   mon_pid log_dir job_id target_uid user_DN [vo issuer [user_FQAN]*]
monitor_exe=/opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor

# Should monitoring errors be nonfatal?
continue_on_monitor_error=0

</pre>

---+++ Using the new XACML2 GUMS interface 
Starting with this version of VDT, GUMS and PRIMA can use two different ways to communicate with each other  
   * the old PRIMA legacy protocol, based on SAML v1
   * the new XACML2 protocol defined by the OSG-EGEE interoperability group

The default installation (describe above) will, for backward compatibility reasons, use the old PRIMA legacy protocol.
However, if you installed GUMS v1.3.0 or later, you can switch to the new interface.

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg=, 
commenting out the default =map_exe= and =gums_url=, then
uncomment and configure the alternative one.

<pre class="file">
##################################
# Mapping program<b>
#map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"</b>
# Point it to your GUMS machine<b>
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"
</b>
#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface<b>
map_exe="/opt/vdt/prima/bin/prima_scas_map_args"</b>
# Point it to your GUMS machine<b>
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"
</b>
</pre>

---+++Modify /etc/glexec/tracking_groups.cfg

Modify the file =/etc/glexec/tracking_groups.cfg= with the correct tracking group id range that you created in the pre-installation section.
<pre class="file">
# This file defines the range of GIDs
# used for tracking purposes
#<b>
# Make sure this range of GIDs is not used for any other purpose
#</b>
min_gid=65000
max_gid=65049
</pre>

---++ Testing gLexec locally on the worker node

Now, __as a non-privileged user (not root)__ , do the following (where "yourvo" is your vo, etc.):
<pre class="screen">
>  cd $VDT_LOCATION
>  source setup.sh
>  voms-proxy-init -voms yourvo:/yourvo -file /tmp/x509_xyz

> export GLEXEC_CLIENT_CERT=/tmp/x509_xyz
> $VDT_LOCATION/glexec-osg/sbin/glexec /usr/bin/id
It appears that the value of pthread_mutex_init is 10603520
uid=13160(fnalgrid) gid=9767(fnalgrid)
</pre>
If gLexec is successful, it will print out the uid and gid that your proxy would normally be mapped to by your gums server.

.

---++ gLexec log files
There are four gLexec log files that are made on each worker node and they have  four different time stamp formats.  They all live, by default, in =/var/log/glexec= directory:

---+++ glexec_log
This is the main log of each glexec. It logs in GMT
<pre class="file">
glexec[14507]: 20080109T173203Z glexec pid: 14507
glexec[14507]: 20080109T173203Z Reading glexec configuration file failed, continue with default values.
glexec[14507]: 20080109T173203Z uid: real/eff cdffgrid(3377)/root(0), gid: real/eff cdf(3200)/root(0)
</pre>

---+++ gums_interface.log
This logs the gums mapping process. It logs in local time.
<pre class="file">
glegumsi[23138#23137] 2008-12-02T13:58:17Z Caller account (500,500)
glegumsi[23138#23137] 2008-12-02T13:58:17Z Base subject  '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872/CN=proxy'
glegumsi[23138#23137] 2008-12-02T13:58:17Z Found subject '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Primary   FQAN '/cms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Secondary FQAN '/cms/uscms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:18Z GUMS mapped to user 'uscms466'
glegumsi[23138#23137] 2008-12-02T13:58:18Z Mapped as (504,504)
glegumsi[23138#23137] 2008-12-02T13:58:18Z Launching monitor /opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor
glegumsi[23138#23137] 2008-12-02T13:58:18Z Using tracking GID 65002
</pre>

---+++ lcas_lcmaps.log
The log of the lcas and lcmap portions of gLexec.
It  logs with a %RED%zero-indexed month%ENDCOLOR%  (January=0, February-1, etc.).  Time is local time.
<pre class="file">
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 5: 2008-00-09.11:32:03-14507-glexec_get_accounts :       lcmaps_plugin_gu
ms-plugin_run(): Adding POOL_INDEX :  (void)
LCMAPS 0: 2008-00-09.11:32:03-14507-glexec_get_accounts :       lcmaps_plugin_gu
ms-plugin_run(): gums plugin succeeded
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_dummy_good.mod
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_dummy_good.mod
LCMAPS 0: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-lcmaps_run_
with_pem_and_return_account(): succeeded
LCMAPS 7: 2008-00-09.11:32:03-14507-glexec_get_accounts : Termination LCMAPS
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-lcmaps_term
(): terminating
</pre>

---+++ glexec_monitor.log
The gLexec utility also has a process called glexec_monitor which runs as root through the life of the glexec'd process.
It logs in  GMT format
<pre class="file">
glemon[23093#22904]: 2008-12-02T19:57:06Z Tracking gid: 65001
glemon[23093#22904]: 2008-12-02T19:57:06Z Monitor pid: 23095
glemon[23093#22904]: 2008-12-02T19:57:07Z Parents: 137980749#22904,137980747#22903,137477937#14901,137477234#14871,3906#3203
glemon[23093#22904]: 2008-12-02T19:57:07Z System boot time: 1226868018
glemon[23327#23137]: 2008-12-02T19:58:18Z Started, should use uid 504
glemon[23327#23137]: 2008-12-02T19:58:18Z Used DN: "/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used VO: "cms" Issuer: "/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used FQAN: "/cms/Role=NULL/Capability=NULL"
glemon[23327#23137]: 2008-12-02T19:58:18Z Tracking gid: 65002
glemon[23327#23137]: 2008-12-02T19:58:18Z Monitor pid: 23329
glemon[23327#23137]: 2008-12-02T19:58:19Z Parents: 137987916#23137,137987916#23136,137477937#14901,137477234#14871,3906#3203
glemon[23327#23137]: 2008-12-02T19:58:19Z System boot time: 1226868018
glemon[23093#22904]: 2008-12-02T19:59:07Z Terminated, CPU user 0 system 0
glemon[23093#22904]: 2008-12-02T19:59:08Z Killing procd 23096
glemon[23093#22904]: 2008-12-02T19:59:08Z All childs terminated
</pre>

---++  Configuring the CE node to use gLexec
---+++ config.ini changes
After all worker nodes have been configured for =glexec= , the CE node =$VDT_LOCATION/osg/etc/config.ini= file needs to be modified to tell the
location of the glexec executable.  As mentioned previously, this is why the =glexec= executable __must__ be installed in the same location on all worker nodes.
<pre class="file">
[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = YOUR_WN_VDT_LOCATION/sbin/glexec
</pre>

To effect the change, you need to run this on the CE node:
<pre class="screen">
# source YOUR_CE_VDT_LOCATION/setup.sh
# configure-osg -c
</pre>

This will populate the =OSG_GLEXEC_LOCATION= variable in =$VDT_LOCATION/osg/etc/osg-job-environment.conf= file which contains variables
available to all grid jobs.  This is how pilot jobs invoke =glexec= ..

---+++ Testing the CE configuration
To test this, create a script like this to run on a worker node via your batch system:
<pre class="file">
#!/bin/bash
echo "OSG_GLEXEC_LOCATION=$OSG_GLEXEC_LOCATION"
export GLEXEC_CLIENT_CERT=$X509_USER_PROXY
"$OSG_GLEXEC_LOCATION" /usr/bin/id
echo glexec result: $?
</pre>

Submit the job via your batch system. The result should be something like this:
<pre class="screen">
OSG_GLEXEC_LOCATION=/opt/osg-wn-client/glexec-osg/sbin/glexec
uid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)
glexec result: 0
It appears that the value of pthread_mutex_init is -43132256
</pre>

%BR%

---++ *Comments*
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          =  DaveDykstra

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = SuchandraThapa
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = IwonaSakrejda
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->