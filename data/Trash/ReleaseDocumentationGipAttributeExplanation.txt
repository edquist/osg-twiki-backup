%META:TOPICINFO{author="KyleGross" date="1329339610" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="GenericInformationProvider"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%

---++ Explanation of GIP / Glue Attributes

What follows is a large amount of [[http://glueschema.forge.cnaf.infn.it/Spec/V13][Glue attributes]] which are added by the GIP with some notes on what the attributes mean.

%NOTE% Not all the values here have been properly configured; yet, they are just provided as an example.

---+++ !GlueSite

*OSG unique site name* <br /> =GlueSiteName: CIT_CMS_OSG=

*Human readable description of the site* <br /> =GlueSiteDescription: Caltech CMS OSG Site=

*Site email contacts*
<verbatim>
GlueSiteUserSupportContact: mailto: my@email.com
GlueSiteSysAdminContact: mailto: my@email.com
GlueSiteSecurityContact: mailto: my@email.com
</verbatim>

*Geographic location of the site*
<verbatim>
GlueSiteLocation: Pasadena, US 
GlueSiteLatitude: 34
GlueSiteLongitude: 118
</verbatim>

*Web site describing the site, if any*
<verbatim>
GlueSiteWeb: http://www.opensciencegrid.org
</verbatim>

---+++ !GlueCEUniqueID

GlueCEUniqueID defines a gatekeeper combined with a jobmanager and queue. The format for the GlueCEUniqueID is hostname:port/jobmanager-batch-queue .

*Please note that 'jobmanager-condor-atlas' does not exist as a submittable jobmanager. 'jobmanager-condor' is the actual jobmanager that exists while 'atlas' is the queue that the job should be submitted to*

<verbatim>
dn: GlueCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid
GlueCEHostingCluster: rsgrid3.its.uiowa.edu
GlueCEName: atlas
GlueCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCEInfoGatekeeperPort: 2119
GlueCEInfoHostName: rsgrid3.its.uiowa.edu
GlueCEInfoLRMSType: condor
GlueCEInfoLRMSVersion: 6.7.7
</verbatim>

*How many total CPU's (busy or available) are accessible on this queue via Grid interfaces to all VO. This parameter can be customized by site administrators.*
<verbatim>
GlueCEInfoTotalCPUs: 10
GlueCEInfoJobManager: condor
GlueCEInfoContactString: my-ce.my.domain:2119/jobmanager-condor-atlas
</verbatim>

*How many total job slots (busy or available) are accessible by policy via Grid interfaces for a certain VO. For PBS, VO information is not available: the parameter is the total number of job slots for any VO i.e. !GlueCEInfoTotalCPUs*
<verbatim>
GlueCEPolicyAssignedJobSlots: 0
</verbatim>

*How many CPU's are available right now via Grid interfaces to all VO*
<verbatim>
GlueCEStateFreeCPUs: 10
</verbatim>

*How many job slots are available right now via Grid interfaces for a certain VO. For PBS, VO information is not available: the parameter is min( maximum_number_of_queuable_jobs - total_jobs_in_queue , free_cpus_for_the_queue ). In general, maximum_number_of_queuable_jobs may be different from free_cpus_for_the_queue by administrative policy.*
<verbatim>
GlueCEStateFreeJobSlots: 0
</verbatim>

*RunningJobs + WaitingJobs*
<verbatim>
GlueCEStateTotalJobs: 0
</verbatim>

*How many jobs are waiting in the queue*
<verbatim>
GlueCEStateWaitingJobs: 0
GlueCEStateWorstResponseTime: 0
</verbatim>

*How long a VO can expect to wait before being submitted to a machine*
<verbatim>
GlueCEStateEstimatedResponseTime: 0
</verbatim>

*How long a job may remain in the queue before being removed (CPU time)*
<verbatim>
GlueCEPolicyMaxCPUTime: 0
GlueCEPolicyMaxRunningJobs: 0
GlueCEPolicyMaxTotalJobs: 0
</verbatim>

*How long a job may remain in the queue before being removed (Wall time)*
<verbatim>
GlueCEPolicyMaxWallClockTime: 0
GlueCEPolicyPriority: 1
</verbatim>

*Which VO may run jobs on this queue*
<verbatim>
GlueCEAccessControlBaseRule: VO:atlas
GlueForeignKey: GlueClusterUniqueID=rsgrid3.its.uiowa.edu
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
</verbatim>

---+++ !GlueSubClusterUniqueID

GlueSubClusterUniqueID provides information about what grid software version is installed on the cluster as well as general information about a cluster. It also can hold VO published information about what software has been installed.

<verbatim>
dn: GlueSubClusterUniqueID=rsgrid3.its.uiowa.edu, GlueClusterUniqueID=rsgrid3.its.uiowa.edu, mds-vo-name=local,o=grid
GlueChunkKey: GlueClusterUniqueID=rsgrid3.its.uiowa.edu
</verbatim>

*GlueHostApplicationSoftwareRunTimeEnvironment lists all the software installed on the cluster*
<verbatim>
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_1_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_1_1
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_2_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_3_0
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_3_1
GlueHostApplicationSoftwareRunTimeEnvironment: LCG-2_4_0
GlueHostApplicationSoftwareRunTimeEnvironment: R-GMA
GlueHostArchitectureSMPSize: 2
</verbatim>

*The SI and SF benchmarks have been used to make sure that a job will finish in a cluster before being removed*
<verbatim>
GlueHostBenchmarkSF00: 0
GlueHostBenchmarkSI00: 381
GlueHostMainMemoryRAMSize: 513
GlueHostMainMemoryVirtualSize: 1025
</verbatim>

*Method of publishing information about whether or not the cluster nodes have inbound / outbound access*
<verbatim>
GlueHostNetworkAdapterInboundIP: FALSE
GlueHostNetworkAdapterOutboundIP: TRUE
GlueHostOperatingSystemName: Redhat
GlueHostOperatingSystemRelease: 7.3
GlueHostOperatingSystemVersion: 3
GlueHostProcessorClockSpeed: 1001
GlueHostProcessorModel: PIII
GlueHostProcessorVendor: intel
GlueSubClusterName: rsgrid3.its.uiowa.edu
GlueSubClusterUniqueID: rsgrid3.its.uiowa.edu
GlueSubClusterPhysicalCPUs: 0
GlueSubClusterLogicalCPUs: 0
GlueSubClusterTmpDir: /tmp
GlueSubClusterWNTmpDir: /tmp
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
</verbatim>

---+++ !GlueCESEBindGroupCEUniqueID

Defines which storage element is considered close to this Computing Element. This is done on a per VO basis. For example, these attributes can define the default location for Atlas data to go would be to rsgrid3.its.uiowa.edu and into the directory =/storage/atlas=. Another convention on the LCG is that the mount point has a folder in it for each VO that is accepted.

<verbatim>
dn: GlueCESEBindGroupCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid, GlueCESEBindGroupCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCESEBindGroupSEUniqueID: rsgrid3.its.uiowa.edu

dn: GlueCESEBindSEUniqueID=rsgrid3.its.uiowa.edu, GlueCESEBindGroupCEUniqueID=rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas, mds-vo-name=local,o=grid
GlueCESEBindSEUniqueID: rsgrid3.its.uiowa.edu
GlueCESEBindCEAccesspoint: /storage
GlueCESEBindCEUniqueID: rsgrid3.its.uiowa.edu:2119/jobmanager-condor-atlas
GlueCESEBindMountInfo: none
GlueCESEBindWeight: 0
</verbatim>

---+++ !GlueSEUniqueID

Defines an access point of a Storage Element.

<verbatim>
dn: GlueSEUniqueID=rsgrid3.its.uiowa.edu,mds-vo-name=local,o=grid
GlueSEUniqueID: rsgrid3.its.uiowa.edu
GlueSEName: my-site-name:disk
GlueSEPort: 8443
GlueSESizeTotal: 0
GlueSESizeFree: 0
GlueSEArchitecture: disk
GlueInformationServiceURL: ldap://rsgrid3.its.uiowa.edu:2135/mds-vo-name=local,o=grid
</verbatim>

---+++ !GlueSEAccessProtocolLocalID

Defines the protocols that may be used to access this Storage Element. The example below defines gsiftp as the protocol.

<verbatim>
dn: GlueSEAccessProtocolLocalID=gsiftp, GlueSEUniqueID=rsgrid3.its.uiowa.edu,Mds-Vo-name=local,o=grid
GlueSEAccessProtocolLocalID: gsiftp
GlueSEAccessProtocolType: gsiftp
GlueSEAccessProtocolVersion: 1.0.0
GlueSEAccessProtocolPort: 2811
GlueSEAccessProtocolSupportedSecurity: GSI
GlueChunkKey: GlueSEUniqueID=rsgrid3.its.uiowa.edu
</verbatim>

%STOPINCLUDE% 

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BurtHolzman
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


############################################################################################################
-->