%META:TOPICINFO{author="MarcoMambelli" date="1259106010" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+ !! Components of a Tier 3 Cluster
%TOC%

A cluster is a set of computers connected by a network, nodes, that work together providing different functionalities.
The following sections will describe different elements that you may find in a Tier 3 (T3): interactive nodes, computers allowing the users to log in and run their applications, a batch queue, allows user to schedule jobs that will run on one or more of the batch nodes, storage nodes, computers allowing to store efficiently big amounts of data. 
In order to work together interactive nodes and batch queue need some nodes that are functional to the cluster, like the NFS server and the queue headnode. Other nodes, also called cluster headnodes or gatekeepers, provide services outside the cluster to the Grid.
The last section introduces briefly the network that is better described in ClusterNetwork.

---++ Interactive nodes of the Cluster
The user of a T3 will conduct all activities through the interactive nodes or connecting from outside. 
On the interactive nodes users will:
   * Have a home area for personal and important files. Generally this is small (100's of GB), backed up and accessible from all the interactive nodes. 
   * Be able to run their application (for coding, for their science) interactively.
   * Have access to a big (~1/2 TB) and fast scratch space.
   * Be able to download small data sets from the Grid.
   * Be able to submit jobs to the Grid, and retrieve output.
In addition, he will be able to:
   * Submit jobs to the local T3 batch queue and retrieve the results.
From the system point of view, a cluster with interactive nodes will have also a NFS server (node without user login) with user areas and shared services installed.

Note that a T3 with interactive nodes without any batch capability is still very usable. 

---++ Batch nodes of the Cluster
A lot of scientific computations are time consuming also on modern CPUs. Luckily several problems are also embarrassingly parallel problems: this means that the input data can be split on unit boundaries obtaining independent jobs. A simple way to speed them up is to run many jobs in parallel. The T3 design presented here will achieve this by constructing a batch queue with many nodes to which a user may submit a job. 
The _Local Resource Manager_ (LRM, or _queue manager_) is a software (i.e. Condor, LSF, PBS, SGE, ...) that runs on a node, the headnode of the queue, and allows to schedule jobs on all the nodes in the queue, the worker nodes, without the users accessing them directly. 
There are also frameworks which automatically split complex jobs (workflow) in several simple jobs and submit them to many nodes, combining the outputs at the end.

The storage section below describes how jobs execution can be optimized by combining job scheduling with data placement.

A Compute Element, CE, provides a Grid front-end for the batch queue, allowing to submit to the batch queue jobs coming from the Grid

---++ Data Storage nodes of the Cluster
Scientific computing often requires to store huge amount of data. The storage described here is different form the NFS storage that can be used to share home directories or software installations but is not enough performant for big scientific data, specially if the T3 has many worker nodes.
There are several commercial solutions. In these documents we are describing a solution based on distributed file systems. Distributed File Systems (DFS), like [[][Xrootd]], are obtained allowing several nodes, _storage nodes_, to share their own disk in a single file space and to host part of the data.

Batch nodes may share their local disk becoming also storage nodes of a DFS. 
In this case, optimal scheduling will try to schedule i/o intensive jobs so that they will operate on data stored on the local disk.

Some sites may prefer separate nodes for storages like NAS boxes and use those for a DFS. Some sites may prefer commercial storage solutions like GPFS or Lustre. Sometimes campuses provide storage solutions and the T3 could simply use that.

A Storage Element, SE, provides a Grid front-end for a storage space in the T3. This can be the local disk of the computer hosting the SE or the DFS or any other storage available in the T3. The SE allows services and users on the Grid to access the files in that storage, according to the desired protocols and permissions.
Some clusters, e.g. ATLAS and CMD Tier 1s and Tier 2s, prefer to provide a Grid view to the system where all data resides, some, e.g. US-ATLAS Tier 3s, prefer to expose to the Grid only a smaller portion of it, e.g. the local data disk on the SE node. The latter allows more flexibility on the management of the internal storage but forces a two step transfer for all the data coming from or going to the Grid.
When data is exposed to the grid it is usually recorded in a catalog to know which SE make it available, normally VOs do that, therefore it can not be moved or deleted locally because that would cause inconsistencies.

---++Network Configuration
All nodes of a T3 cluster need to be connected on a network, intranet. This may be the public Internet or a "private" intranet, separated and more protected. Different configurations are possible depending on the location of your computers and on constraints from the machine or the network. 
We recommend that as much of the T3 cluster as possible live only in a "private" network mainly for security reasons. Also the relatively large number of batch nodes may become an issue in address assignment.
Most of the nodes anyway may need to be able to have outbound connectivity, reach the public Internet, e.g. to access databases or download files. And a few of the nodes will need also inbound connectivity, e.g. to allow to download files from the T3 or to allow direct user login. If all the cluster is connected on a private intranet, these nodes need to be dual homed (have two network interfaces [NIC]) to be also on a "public" network, extranet.

The Compute Element needs to be visible in order to receive jobs from the Grid. 
The Storage Element needs to be visible in order to exchange data with the Grid.
It is convenient for interactive nodes to be "public" so that users may log in directly to them.
A Web proxy should also be on the "public" network for performance reasons.

By extranet or "public" network, we mean the network that is not only local to the T3 cluster. Sometimes this can be the truly public Internet; sometimes, this might be a campus or departmental network which is indirectly connected to the Internet. If the network to which you plan to connect your T3 cluster is of the latter kind, you will need a gateway (bastion) node to log into your interactive nodes and you will have to ask your campus/departmental network administrator to make your Storage Element visible to the Internet. Look in the [[][section on Storage Element]] for further details.

ClusterNetwork describes in more detail possible network configurations.


-- Main.MarcoMambelli - 23 Nov 2009

%META:FILEATTACHMENT{name="network1.png" attachment="network1.png" attr="" comment="natted network" date="1258991411" path="network1.png" size="24060" stream="network1.png" tmpFilename="/usr/tmp/CGItemp16044" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network3.png" attachment="network3.png" attr="" comment="open network" date="1258991448" path="network3.png" size="35616" stream="network3.png" tmpFilename="/usr/tmp/CGItemp16209" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network-connection.png" attachment="network-connection.png" attr="" comment="Fig. 3 - Network connection to the outside" date="1258991597" path="network-connection.png" size="16505" stream="network-connection.png" tmpFilename="/usr/tmp/CGItemp16227" user="MarcoMambelli" version="1"}%
