%META:TOPICINFO{author="JamesWeichel" date="1259795603" format="1.1" reprev="1.5" version="1.5"}%
%META:TOPICPARENT{name="WebHome"}%
---+ !! Components of a Tier 3 Cluster
%TOC%

A cluster is a set of computers (nodes) connected by a network that work together providing different functions.
The following sections will describe different elements that you may find in a Tier 3 (T3): a batch queue - allowing a user to schedule jobs that will run on one or more of the batch nodes, storage nodes - computers allowing to efficiently store efficiently large amounts of data,  interactive nodes - computers allowing the users to log in and run their applications.
In order to work together, interactive nodes and the batch queue need some nodes that are functional within the cluster, like the NFS server and the queue headnode. Other nodes, called cluster headnodes or gatekeepers, provide services outside the cluster to the Grid.
The last section introduces briefly the network that is better described in ClusterNetwork.

---++ Batch nodes of the Cluster
Many scientific computations require long run times, even on modern CPUs. Luckily several problems are also embarrassingly parallel problems: this means that the input data can be split on boundaries that can be processed by independent jobs. A simple way to speed them up is to run many jobs in parallel. The T3 design presented here will achieve this by constructing a batch queue with many nodes to which a user may submit a job. 
The _Local Resource Manager_ (LRM, or _queue manager_) is a software (i.e. Condor, LSF, PBS, SGE, ...) that runs on a node, the headnode of the queue, and schedules jobs on all the worker nodes in the queue, without the users accessing them directly. 
There are also frameworks which automatically split complex jobs (work flows) into several simple jobs and submits them to many nodes, combining the outputs at the end.
A Compute Element, CE, provides a Grid front-end for the batch queue, allowing to submit to the batch queue jobs coming from the Grid

---++ Interactive Nodes of the Cluster
The user of a T3 may also conduct activities through the interactive nodes which can be accessed from outside the cluster. 
On the interactive nodes users will:
   * Have a home area for personal and important files. Generally this is small (100's of GB), backed up and accessible from all the interactive nodes. 
   * Be able to run their application (for coding, for their science) interactively.
   * Have access to a large (~1/2 TB) and fast scratch space.
   * Be able to download small data sets from the Grid.
   * Be able to submit jobs to the Grid, and retrieve output.
In addition, the user will be able to:
   * Submit jobs to the local T3 batch queue and retrieve the results.
From the system point of view, a cluster with interactive nodes will have also a NFS server (node without user login) with user areas and shared services installed.

The ratio of interactive nodes to worker nodes is an important consideration when setting up a cluster. In general, the most efficient use of resources is to submit the vast majority of the workloads into the batch queues. This is the setup for the vast majority of CMS and Atlas T2 and T3 sites.  Shared interactive nodes are more difficult to administer, since there are fewer pre-defined constraints on the nodes, and these can easily take up a significant portion of a system administrators time with unpredictable users. Still, interactive nodes are often useful for scientists and may be required for example for those who need to login to submit batch jobs or to configure and debug short running jobs in an interactive environment prior to submitting longer running jobs to the batch queue system. In general, administrators should determine the needs of their users and configure as few shared interactive nodes as possible.

Note that a T3 with interactive nodes without any batch capability is still usable. 


The storage section below describes how jobs execution can be optimized by combining job scheduling with data placement.

---++ Data Storage nodes of the Cluster
Scientific computing often requires to storage of a huge amount of data. The storage described here is different from the NFS storage that can be used to share home directories or software installations because NFS does not provide the performance required for large amounts of scientific data.  This is especially true if the T3 has many worker nodes.
There are several commercial solutions. In these documents we are describing a solution based on distributed file systems. Distributed File Systems (DFS), like [[][Xrootd]],  allow several nodes, _storage nodes_, to share their own disks containing part of the data that is accessible as if it were a single file space.

Batch nodes may share their local disk becoming also storage nodes of a DFS. 
In this case, optimal scheduling will try to schedule i/o intensive jobs so that they will operate on data stored on the local disk.

Some sites may prefer separate nodes for storage like NAS boxes and use those for a DFS. Some sites may prefer commercial storage solutions like GPFS or Lustre. Sometimes campuses provide storage solutions and the T3 could simply use those.

A Storage Element, SE, provides a Grid front-end for a storage space in the T3. This can be the local disk of the computer hosting the SE or the DFS or any other storage available in the T3. The SE allows services and users on the Grid to access the files in that storage system, according to the desired protocols and permissions.
Some clusters, e.g. ATLAS and CMS Tier 1s and Tier 2s, prefer to provide a Grid view to the system where all data resides.  Others, e.g. US-ATLAS Tier 3s, prefer to expose to the Grid only a smaller portion of the storage, e.g. the local data disk on the SE node. The latter allows more flexibility on the management of the internal storage but forces a two step transfer for all the data coming from or going to the Grid.
When data is exposed to the grid it is usually recorded in a catalog that identifies which SE the data is on.  Normally VOs do that, therefore it can not be moved or deleted locally because that would cause inconsistencies.

---++Network Configuration
All nodes of a T3 cluster need to be connected on a network, intranet. This may be the public Internet or a "private" intranet, separated and more protected. Different configurations are possible depending on the location of your computers and on constraints from the machine or the network. 
We recommend that as much of the T3 cluster as possible live only in a "private" network, mainly for security reasons.  Also the relatively large number of batch nodes may become an issue in address assignment.
Most of the nodes may need to have outbound connectivity, i.e. reach the public Internet, to access databases or download files. And a few of the nodes will need also inbound connectivity, e.g. to allow to download files from the T3 or to allow direct user login. If all the cluster is connected on a private intranet, these nodes need to be dual homed (have two network interfaces [NIC]) to be also on a "public" network, extranet.

The Compute Element needs to be visible in order to receive jobs from the Grid. 
The Storage Element needs to be visible in order to exchange data with the Grid.
It is convenient for interactive nodes to be "public" so that users may log in directly to them.
A Web proxy should also be on the "public" network for performance reasons.

By extranet or "public" network, we mean the network that is not only local to the T3 cluster. Sometimes this can be the truly public Internet; sometimes, this might be a campus or departmental network which is indirectly connected to the Internet. If the network to which you plan to connect your T3 cluster is of the latter kind, you will need a gateway (bastion) node to log into your interactive nodes and you will have to ask your campus/departmental network administrator to make your Storage Element visible to the Internet. Look in the [[][section on Storage Element]] for further details.

ClusterNetwork describes in more detail possible network configurations.

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 25 Nov 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

%META:FILEATTACHMENT{name="network1.png" attachment="network1.png" attr="" comment="natted network" date="1258991411" path="network1.png" size="24060" stream="network1.png" tmpFilename="/usr/tmp/CGItemp16044" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network3.png" attachment="network3.png" attr="" comment="open network" date="1258991448" path="network3.png" size="35616" stream="network3.png" tmpFilename="/usr/tmp/CGItemp16209" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network-connection.png" attachment="network-connection.png" attr="" comment="Fig. 3 - Network connection to the outside" date="1258991597" path="network-connection.png" size="16505" stream="network-connection.png" tmpFilename="/usr/tmp/CGItemp16227" user="MarcoMambelli" version="1"}%
