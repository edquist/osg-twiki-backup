%META:TOPICINFO{author="KyleGross" date="1328026268" format="1.1" version="1.10"}%
%META:TOPICPARENT{name="WebHome"}%
---+ !! Components of a Tier 3 Cluster
%DOC_STATUS_TABLE%
%TOC%

A cluster is a set of computers (nodes) connected by a network that work together providing different functions.
The following sections will describe different elements that you may find in a Tier 3 (T3): a batch queue - allowing a user to schedule jobs that will run on one or more of the batch nodes, storage nodes - computers allowing to efficiently store efficiently large amounts of data,  interactive nodes - computers allowing the users to log in and run their applications.
In order to work together, interactive nodes and the batch queue need some nodes that are functional within the cluster, like the NFS server and the queue headnode. Other nodes, called cluster headnodes or gatekeepers, provide services outside the cluster to the Grid.
The last section introduces briefly the network that is better described in ClusterNetwork.

---++ Batch nodes of the Cluster
Many scientific computations require long run times, even on modern CPUs. Luckily several problems are also pleasantly parallel problems: this means that the input data can be split on boundaries that can be processed by independent jobs. A simple way to speed them up is to run many jobs in parallel. The Tier 3 design presented here will achieve this by constructing a batch queue with many nodes to which a user may submit a job. 
The _Local Resource Manager_ (LRM, or _queue manager_) is software (i.e. Condor, LSF, PBS, SGE, ...) that runs across your cluster and schedules jobs on all the worker nodes in the queue, without the users accessing them directly. 
There are also frameworks which automatically split complex jobs (work flows) into several simple jobs and submits them to many nodes, combining the outputs at the end.
A Compute Element, CE, provides a Grid front-end for the batch queue, allowing to submit to the batch queue jobs coming from the Grid

---++ Interactive Nodes of the Cluster
The user of a T3 may also conduct activities through the interactive nodes which can be accessed from outside the cluster. 
On the interactive nodes users will:
   * Have a home area for personal and important files. Generally this is small (100's of GB), backed up and accessible from all the interactive nodes. 
   * Be able to run their application (for coding, for their science) interactively.
   * Have access to a large (~1/2 TB) and fast scratch space.
   * Be able to download small data sets from the Grid.
   * Be able to submit jobs to the Grid, and retrieve output.
In addition, the user will be able to:
   * Submit jobs to the local T3 batch queue and retrieve the results.
Interactive nodes may use NFS to share the home area (item one above).

You should normally have one or two interactive nodes at a Tier 3 site. Each interactive node that you add to the system is extra effort to administer. 

Note that a Tier 3 with interactive nodes without any batch capability is still usable. 


The storage section below describes how jobs execution can be optimized by combining job scheduling with data placement.

---++ Data Storage nodes of the Cluster
Scientific computing often requires to storage of a huge amount of data. The storage described here is different from the NFS storage that can be used to share home directories or software installations because NFS does not provide the performance required for large amounts of scientific data.  This is especially true if the Tier 3 has many worker nodes.
In these documents we are describing a solution based on Xrootd, which is distributed file systems. You are welcome to choose another solution, including a commercial solution, but we do not describe any other solution in this documentation.
Distributed File Systems (DFS), like Xrootd,  allow several nodes, _storage nodes_, to share their own disks containing part of the data that is accessible as if it were a single file space.

Batch nodes may countribute their local disk  to a DFS and therefore be also storage nodes. 
In this case, optimal scheduling will try to schedule I/O intensive jobs so that they will operate on data stored on the local disk.

Some sites may prefer separate nodes for storage like NAS boxes and use those for a DFS. Some sites may prefer commercial storage solutions like GPFS or Lustre. Sometimes campuses provide storage solutions and the T3 could simply use those.

A Storage Element, SE, provides a Grid front-end for a storage space in the T3. This can be the local disk of the computer hosting the SE or the DFS or any other storage available in the T3. The SE allows services and users on the Grid to access the files in that storage system, according to the desired protocols and permissions.
Some clusters, e.g. ATLAS and CMS Tier 1s and Tier 2s, prefer to provide a Grid view to the system where all data resides.  Others, e.g. US-ATLAS Tier 3s, prefer to expose to the Grid only a smaller portion of the storage, e.g. the local data disk on the SE node. The latter allows more flexibility on the management of the internal storage but forces a two step transfer for all the data coming from or going to the Grid.
When data is exposed to the grid it is usually recorded in a catalog that identifies which SE the data is on.  Normally VOs do that, therefore it can not be moved or deleted locally because that would cause inconsistencies.

---++Network Configuration
All nodes of a Tier 3 cluster need to be connected on a network. This may be the public Internet or a "private" intranet, separated and more protected. Different configurations are possible depending on the location of your computers and on constraints from the machine or the network. 
We recommend that as much of the Tier 3 cluster as possible live only in a "private" network, mainly for security reasons.  Also you may have not enough IPv4 address to put your all cluster in the Internet.
Most of the nodes may need to have outbound connectivity, i.e. reach the public Internet, to access databases or download files. And a few of the nodes will need also inbound connectivity, e.g. to allow to download files from the Tier 3 or to allow direct user login. If all the cluster is connected on a private intranet, the few nodes mentioned above need to be dual homed (have two network interfaces [NIC]) to be also on a "public" network, extranet.

The Compute Element needs to be visible in order to receive jobs from the Grid. 
The Storage Element needs to be visible in order to exchange data with the Grid.
It is convenient for interactive nodes to be "public" so that users may log in directly to them.
A Web proxy should also be on the "public" network for performance reasons.

By extranet or "public" network, we mean the network that is not only local to the Tier 3 cluster. Sometimes this can be the truly public Internet; sometimes, this might be a campus or departmental network which is indirectly connected to the Internet. If the network to which you plan to connect your Tier 3 cluster is of the latter kind, you will need a gateway (bastion) node to log into your interactive nodes and you will have to ask your campus/departmental network administrator to make your Storage Element visible to the Internet. Look in the [[https://twiki.grid.iu.edu/bin/view/Tier3/WebHome#Installing_a_Storage_Element_SE][section on Storage Element]] for further details.

ClusterNetwork describes in more detail possible network configurations.

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 25 Nov 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Trash/Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Planning
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during DOC workshop
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DocWorkshop
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->

%META:FILEATTACHMENT{name="network1.png" attachment="network1.png" attr="" comment="natted network" date="1258991411" path="network1.png" size="24060" stream="network1.png" tmpFilename="/usr/tmp/CGItemp16044" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network3.png" attachment="network3.png" attr="" comment="open network" date="1258991448" path="network3.png" size="35616" stream="network3.png" tmpFilename="/usr/tmp/CGItemp16209" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="network-connection.png" attachment="network-connection.png" attr="" comment="Fig. 3 - Network connection to the outside" date="1258991597" path="network-connection.png" size="16505" stream="network-connection.png" tmpFilename="/usr/tmp/CGItemp16227" user="MarcoMambelli" version="1"}%
