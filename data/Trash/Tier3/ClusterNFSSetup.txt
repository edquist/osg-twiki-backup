%META:TOPICINFO{author="ElizabethChism" date="1466010159" format="1.1" version="1.14"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC%

---++ Introduction
%DISCLAIMER% If you are using Rocks, commonly used by CMS, or you install using Kickstart files like the one provided by ATLAS, then you don't need any of this. Both of them will take care of the NFS configuration for you

   * We'll setup a shared file system using NFSv4 as discussed in "Customization of the base system" (first option).
   * The NFS server hostname is =gc1-nfs=, all other hosts import the home directories and some service directories
   * NFS directories are mounted on =/nfs= and linked to the required destination to make the configuration more explicit

---+++!! Engineering Considerations
Here some information that will help your planning and clarify the context

---++++!! Directories exported
When dealing with NFS, or other shared file systems, keep in mind these distinctions:
   1 actual directory on the NFS server 
   2 directory exported by the NFS server - if the path used by the NFS daemon  
   3 mount point on the NFS client (the server can mount it as well)
   4 final desired path on the NFS client (can be used on the server as well)
1 and 2 can be the same but the use of links allows for more flexibility. The same is valid for 3 and 4. In the example below these will be 4 different paths.

NFSv4 requires a single directory tree. All the directories exported are grouped in a pseudo-filesystem under =/export=
   * =home= - Home directories for Grid users; these are often shared group accounts with a name similar to the VO's name; for example *osg*, *osgedu*, *uscms*, *usatlas*, ...).  Local accounts for the *condor* user, and any other local user.
   * =condor= - Condor installation and configuration directories. It may have a subdirectory for each Condor version installed. This will allow for an easy upgrade procedure for Condor. See ClusterCondorInstall for more information. 
   * =osg= - Directories used by OSG, e.g. =OSG_GRID=, =OSG_APP= and =OSG_DATA=. Their functions is explained in the OSG documentation, for example ReleaseDocumentation.LocalStorageConfiguration.
   * =certificates= - CA certificate directory, shared across the cluster, updated by a process running on the CE node (*gc1-ce*) as part of the Grid installation.

The directories are supposed to exist already somewhere in the filesystem of the NFS server and are linked (with a bid mount) in the =/export= pseudo-filesystem

---++++!! Network configuration of NFS
The NFS server is =gc1-nfs= and we decide to export the NFS shares to all the intranet, all the local network. Consistently with ClusterNetworkSetup this is =192.168.192.0= with NETMASK 255.255.192.0 (18 fixed bits).
If you assume that the host is connected to the intranet with its first network interface, NIC (=eth0=), and that your NFS domain is all the intranet, then it is possible to evaluate automatically the NFS domain:<pre class="screen">ifconfig eth0 | grep "inet addr" | sed 's|:| |g' | awk '{print $3" "$7}' | xargs ipcalc -n | cut -b 9-</pre>

Given the NIC attached to the intranet:<pre class="screen">
# intranet network interface
EXTIF="eth0"
</pre>
You can use:<pre class="screen">
EXTIP="`ifconfig $EXTIF | awk  /$EXTIF/'{next}//{split($0,a,":");split(a[2],a," ");print a[1];exit}'`"
EXTNETMASK="`ifconfig $EXTIF | awk  /$EXTIF/'{next}//{split($0,a,":");split(a[4],a," ");print a[1];exit}'`"
EXTPREFIX=`ipcalc -p $EXTIP $EXTNETMASK | awk --field-separator "=" '{print $2;exit}'`
network_domain=$EXTIP"/"$EXTNETMASK
</pre>
To evaluate automatically IP, NETMASK and IP/net string to use in the following steps of the configuration.
The commands above are generic and should work on any Linux distribution, no matter the locale.

Note that you don't have to export all the directories to all the hosts on the intranet. You can export some directories only to one host or a different subnet. Feel free to customize the examples.


---+++!!Help!
If a problem occurs during installation or verification of the service, see [[#DebugInfo][Debugging Information]].
%BR%
If you cannot resolve the problem, the best way to get help is by following Tier3Help suggestions.

---++ Checklist
Before configuring the NFS server you must:
   1. Have a working host (see ClusterOSInstall and ClusterNetworkSetup)
   1. Have the directory you want to export (you may add content later but the directory must exist). This document will suppose the directories: <pre class="screen">
/data/home
/data/condor
/data/osg
/data/certificates
</pre> according to the layout described in ClusterFileSystem.

---++ Installation Procedure
This procedure includes a part that must be performed on the NSF server and a part that must be performed on each client.

---+++ Quick install using the configuration script
The attached script [[%ATTACHURL%/osgtier3-configure-nfsv4.sh][osgtier3-configure-nfsv4.sh]] will run for you the steps in the next sections of the installation procedure. If you prefer to configure NFS by hand continue with the next sub-section "Configuration of the NFS server".

Download the script.

Then setup the NFSv4 server by running as root:  <pre class="screen">./osgtier3-configure-nfsv4.sh -d /data 
</pre>Where =/data= is the path of the directories you want to export (see the Checklist above). You can add the option:
 * =-n 192.168.192.0/255.255.192.0= setting your network/netmask. Use this only if the script is not guessing them correctly. 


Finally setup each client by running as root:  <pre class="screen">./osgtier3-configure-nfsv4.sh -s gc1-nfs 
</pre>Where =gc1-nfs= is the name or the IP of the NFS server. 

This did all the steps described in the installation procedure.

---+++ Configuration of the NFS server
The NFS server is called =gc1-nfs=.  Again here we are choosing some options that imply conventions and/or policy of the local site, and so the following is meant to be suggestive.  For example the =no_root_squash= option allows the root account on another machine in the cluster to modify files exported by the NFS server. This may not be desirable since it is a little less protective of the files on the NFS server.
   * All exported directories are grouped in =/exports=, NFS root directory
   * Create the directories under =/exports=: <pre class="screen">mkdir /exports
mkdir /exports/home   
mkdir /exports/condor 
mkdir /exports/osg   
mkdir /exports/certificates 
</pre>
   * prepare the destination directories with a path similar to the one on the clients. You will need them if you create users or make the shared installations on the NFS server.<pre class="screen">
mkdir /nfs
mv /home /home.save
ln -s /nfs/home /home
mkdir /share
ln -s /nfs/osg /share/osg
mkdir /etc/grid-security
ln -s /nfs/certificates /etc/grid-security/certificates
</pre>
   * link the exported directories to the real directories on the server by using _bind mounts_
      * Supposing that the real directories you want to export are:<pre class="screen">
/data/home
/data/condor
/data/osg
/data/certificates
</pre>
      * then add the following lines to =/etc/fstab=:<pre class="file">
/data/home              /exports/home           none    bind            0 0
/data/condor            /exports/condor         none    bind            0 0
/data/osg               /exports/osg            none    bind            0 0
/data/certificates      /exports/certificates   none    bind            0 0
/exports               /nfs            none    bind            0 0
</pre>
   * Directories are exported separately which allows mounting with different permissions.  For example, may want to mount the =condor= directory as read only from the compute nodes.
   * The configuration is in =/etc/exports=.  If the file does not exist, create it. Add lines similar to the ones in the example file: one line for each directory in =/exports=. Each line starts with the directory name, followed by the IP address of the machine  and the subnet mask (=/24= should work), followed by the parameters =(async,no_root_squash,rw)=
   * Here an example of =/etc/exports= (my IP is =192.168.192.50=, subnet =/18=, consistent with ClusterNetworkSetup):<pre class="file">cat /etc/exports
/exports    192.168.192.50/18(fsid=0,no_subtree_check,async,r) 
/exports/home   192.168.192.50/18(async,no_root_squash,rw,mp,nohide,no_subtree_check)
/exports/condor 192.168.192.50/18(async,no_root_squash,rw,mp,nohide,no_subtree_check)
/exports/osg    192.168.192.50/18(async,no_root_squash,rw,mp,nohide,no_subtree_check)
/exports/certificates   192.168.192.50/18(async,no_root_squash,rw,mp,nohide,no_subtree_check)</pre>
<!-- 
insecure - allows ports > 1024 (IPPORT_RESERVED)
rw - ro is default
async - avoid (min performance increase not worth data loss) - recheck for NFS server on VMs?
nohide - do not hide mounted directories
no_subtree_check - do not check that file belongs to proper tree (only fs)
refer - possible to refer some other server
mp mountpoint - mountpoint secified of from file
Other example, options used in ATLAS - compare options
<pre>/exports 192.168.60.50/18(fsid=0,insecure,no_subtree_check)
/exports/home   192.168.60.50/18(rw,nohide,insecure,no_subtree_check,root_squash)
/exports/condor 192.168.60.50/18(rw,nohide,insecure,no_subtree_check,root_squash)
/exports/osg    192.168.60.50/18(rw,nohide,insecure,no_subtree_check,root_squash)
/exports/certificates   192.168.60.50/18(rw,nohide,insecure,no_subtree_check,root_squash)
</pre>
-->
   * =/exports= is the root of the NFS file system, identified by =fsid=0=, everything else you'd like to export has to be accessible under it. To export directories from outside a _bind mount_ is necessary, add it to =/etc/fstab=.
   * If you run in a VM environment (on virtual machines) add also the =async= option to improve performance. It is not important for real host and can cause data loss if the server crashes. 
   * NFS has to be restarted in order for any change to take effect.  This is done by =/etc/init.d/nfs restart=.
   * To enable NFS at boot-time, and to start it up: <pre class="screen">
chkconfig --level 345 nfs on
/etc/init.d/nfs start
</pre>
<!--  portmap is not necessary for NFSv4
   * Also check that the =portmap= service is running at levels 3,4,5:<pre class="screen">chkconfig --list portmap
portmap        	0: off 1: off 2:off 3: on 4: on 5: on 6: off 
</pre>
-->
   * restart the services:<pre class="screen">
/sbin/service rpcidmapd restart
/sbin/service portmap restart
/sbin/service nfs restart
</pre>
   * Another important file is =/etc/idmapd.conf=. You can use it to define the domain and how the users are mapped. Use =man idmapd= and =man idmapd.conf= to find out more if you want to use if.
   * Now you can test if NFS is working locally by following [[#LocalTest][Validation on the server]]
   * Note how in the mount directory in the command uses only  the relative path starting from the NFS root.
   * On the NFS server (*gc1-nfs*) the exported directories can be mounted but normally are linked to their destination (the path used on the clients).  This is usually a soft link. Sometimes a bind mount is used.

---+++ Configuration on client machines: /etc/fstab
   * Almost all other hosts import directories from the NFS server.  This is controlled editing =/etc/fstab= adding the lines (the NFS server is =gs1-nfs= and it is configured as above):<pre class="file">
# cluster mounts
gc1-nfs:/condor         /nfs/condor     nfs4     bg,intr,noatime
gc1-nfs:/home           /nfs/home       nfs4     bg,intr,noatime
gc1-nfs:/osg            /nfs/osg        nfs4     bg,intr,noatime
gc1-nfs:/certificates   /nfs/certificates nfs4   bg,intr,noatime
</pre>
   * Individual client hosts can be customized (e.g. worker nodes import the =condor= directory as read-only, SE does not need tne =condor= directory, ...).  
   * As seen in the file above, all directories are NFS-mounted in =/nfs=, for clarity. Mount points are created.  All directories are then linked to their final path.
   * Running the commands listed below will create the mountpoints for the nfs mounts and symlinks between the mount points and the location on the file system where the files should be<pre class="screen">
mkdir /nfs
mkdir /nfs/condor
mkdir /nfs/home
mkdir /nfs/osg
mkdir /nfs/certificates
mv /home /home.save
ln -s /nfs/home /home
mkdir /share
ln -s /nfs/osg /share/osg
mkdir /etc/grid-security
ln -s /nfs/certificates /etc/grid-security/certificates
</pre>
   * And if you use a shared Condor installation, add also<pre>
ln -s /nfs/condor/condor /opt/condor
</pre>
   * A mount command will need to be run for each of the mount points <pre class="screen">
mount /nfs/condor
mount /nfs/home
mount /nfs/osg
mount /nfs/certificates
</pre>
   * or you can use =mount -a -tnfs4= to use all the entries in =/etc/fstab=

---++ Service Configuration/Startup/Shutdown
To change the configuration, e.g. to add an exported directory, you can edit the file =/etc/exports=.

The server for NFSv4 is =nfsd=, the same as NFSv3.
To start/stop/restart NFS you can issue <pre>service nfs start/stop/restart</pre> 
Alternatively you can invoke directly <pre>/etc/init.d/nfs start/stop/restart</pre>

---++ Validation of Service Operation
Commands to be used to verify the service is working correctly.  

#LocalTest
---+++ Validation on the server
To test if NFS is working locally you can do the following:<pre class="screen">
mkdir /tmp/test
mount -t nfs4 gc1-nfs:/condor /tmp/test
touch /exports/condor/testfile
ls -l /tmp/test/
# cleanup
umount /tmp/test
rmdir /tmp/test
</pre>
Note how in the mount directory in the command uses only  the relative path starting from the NFS root.

#ClientTest
---+++ Validation on a client

#DebugInfo
---++ Debugging Information
---+++!!File Locations
Log files for the NFS daemon are in the system log, usually =/var/log/messages=

---+++!!Debugging Procedure
To start troubleshooting be aware of:
   * the IP address and NETMASK of each host involved in the problem (e.g. server and client)
   * the content of =/etc/exports= on the server
   * the content of =/etc/fstab= on each host involved (the entries about NFS exports/imports)

When mounting an exported directory on the server fails:
   * if you see a permission error, double check your network configuration and compare it with the setting in =/etc/exports=.
   * if you see the directory in the pseudo file system (=/exports=) instead of the mounted one (bind mount), check that the directories are in =/etc/fstab=, are mounted correctly and that you use the =nohide= option.

When mounting an exported directory on the client fails:
   * control that you can mount it on the server
   * control if you can mount it as NFSv3
   * if you have an error like =mount.nfs4: Operation not permitted= the host/netmask in =/etc/exports= on the server may not include your IP. Remember that all the parents including the root of the pseudo file system should allow you to mount them.
   * if you see =reason given by server: No such file or directory=, check the path used against the one exported. Remember that the root of the pseudo file system should be omitted.

To see the directories exported by one server (NFSv3 mounts) use:<pre class="screen">showmount -e </pre>
Remember that the names in !NFSv4 do not include the root of the pseudo file system (usually first entry returned).

<!--
---+++!!Caveats/Known Issues
None
-->

---+++!!References
References to additional information.  This document should be as concise as possible, but the reader can use these links if he/she wants more background or information.
   * A good writing about NFv4 and FC or RedHat Linux: http://www.vanemery.com/Linux/NFSv4/NFSv4-no-rpcsec.html

---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- OldClusterNFSSetup provides an example of NFSv3 configuration. -->

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %NO%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during DOC workshop
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DocWorkshop
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->

%META:FILEATTACHMENT{name="osgtier3-configure-nfsv4.sh" attachment="osgtier3-configure-nfsv4.sh" attr="" comment="Script to configure NFSv4 server/client" date="1275591385" path="osgtier3-configure-nfsv4.sh" size="9911" stream="osgtier3-configure-nfsv4.sh" tmpFilename="/usr/tmp/CGItemp2185" user="MarcoMambelli" version="1"}%
