%META:TOPICINFO{author="MarcoMambelli" date="1263244703" format="1.1" reprev="1.1" version="1.1"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>Installation of a Tier 3 with a shared File System*
%TOC%

---++ Introduction
This cluster includes few nodes in a batch queue managed with Condor, one or more interactive nodes, a Conpute Element and a Storage Element. It relies on a shared file system provided by a NFS server. All nodes have Scientific Linux 5.4. Here is a map of the nodes
   * =gc1-hn= (=gc1-ce=) =192.168.192.51= the head-node for the queue and also the Compute Element 
   * =gc1-nfs=  =192.168.192.51= the NFSv4 server
   * =gc1-se= =192.168.192.52= the Storage Element 
   * =gc1-ui001= =192.168.192.61= a user interface machine, can also be referred to as Interactive Node 1
   * =gc1-wn001= =192.168.192.100= Worker Node 0
   * =gc1-wn002= =192.168.192.101= Worker Node 1 ...
   * =gc1-wnNNN= =192.168.192.&lt;N+100>= Worker Node N (N<154)
<!--
   * =gc1-xrdr.yourdomain.org= the xrootd redirector 
   * =gc1-gums.yourdomain.org=  the GUMS server 
-->

This installation has been tested on a virtual cluster with 3 worker nodes and one user interface. 
There is no difference (in the installation described) if real nodes are used instead of the virtual ones.
Adding nodes is easy and initially requires no changes. The numbering of the hosts will have to be corrected if you have more than 154 worker nodes or more than 39 interactive nodes.

---++ Cluster planning
   * 
---++ Basic node setup

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 11 Jan 20109 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%
