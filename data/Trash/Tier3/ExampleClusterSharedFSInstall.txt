%META:TOPICINFO{author="RobertEngel" date="1287692243" format="1.1" reprev="1.7" version="1.7"}%
%META:TOPICPARENT{name="WebHome"}%
%DOC_STATUS_TABLE%

---+!! *<nop>Installation of a Tier 3 with a shared File System*
%TOC%

---++ Introduction
This cluster includes few nodes in a batch queue managed with Condor, one or more interactive nodes, a Conpute Element and a Storage Element. It relies on a shared file system provided by a NFS server. All nodes have Scientific Linux 5.4. Here is a map of the nodes
   * =gc1-nfs=  =192.168.192.50= the NFSv4 server
   * =gc1-hn= (=gc1-ce=) =192.168.192.51= the head-node for the queue and also the Compute Element 
   * =gc1-se= =192.168.192.52= the Storage Element 
   * =gc1-ui001= =192.168.192.61= a user interface machine, can also be referred to as Interactive Node 1
   * =gc1-wn001= =192.168.192.100= Worker Node 0
   * =gc1-wn002= =192.168.192.101= Worker Node 1 ...
   * =gc1-wnNNN= =192.168.192.&lt;N+100>= Worker Node N (N<154)
<!--
   * =gc1-xrdr.yourdomain.org= the xrootd redirector 
   * =gc1-gums.yourdomain.org=  the GUMS server 
-->

This installation has been tested on a virtual cluster with 3 worker nodes, one user interface, one Compute Element and one Storage Element with !BeStMan gateway over the local file system. 
There is no difference (in the installation described) if real nodes are used instead of the virtual ones.
Adding nodes is easy and initially requires no changes. The numbering of the hosts will have to be corrected if you have more than 154 worker nodes or more than 39 interactive nodes.

---++ Cluster planning
Generally the first step is the planning of a Tier 3. This is done following the local requirements and eventual suggestions from the VO.
The [[Tier3.WebHome][Tier 3 page]] provides several resources to help the planning:
   * [[Tier3.WebHome#Network_configuration][Information on possible network configurations]]
   * Each [[Tier3.WebHome#Component_Installation_and_Setup][component documented]] presents how it is used and why you may like to install it.
   * [[Tier3.WebHome#Security][An overview of security implications]]
   * [[Tier3.WebHome#Site_planning][A planning guide]]
Other choices include the OS to install and some basic configuration.

Once you decide the network topology and what to install on your computers, then some basic naming choices are required:
   * choose (or determine, if imposed by the setup) the subnet for the Tier 3.
   * choose the name of the nodes of the Tier 3 and the domain (if not determined for you).

In this example planning and naming choices have been described in the Introduction above.

---++ Basic node setup
This part is common to each node.
We recommend to start installing the NFS server first.

You start with installing the OS, Scientific Linux 5.4. Some suggestion are provided in  ClusterOSInstall. In our installation we used a standard SL 5.4, using the DVD images available for download. This meant that no cluster management system or kickstart file were used, so all the manual configurations in the documents linked in this section have been tested.

The next step is the network configuration. For this example we choose a static network configuration and some suggestion are provided in ClusterNetworkSetup.

The configuration of ssh (to allow easy login to the nodes) is described in ClusterSSHSetup.

This step involves different operations on the NFS server and in order to test the other nodes the server must be setup first.
The Tier 3 described in this example relies on a common file system shared using NFS. The structure of the file system is described in ClusterFileSystem and ClusterNFSSetup provides instruction to configure it, both on the NFS server and on the clients (all the other nodes in the Tier 3).

Once you have a working NFS you can proceed to setting up the accounts as described in ClusterAccountsSetup.

---++ Install the shared software
Some software packages are installed on a shared directory and exported and used on the whole Tier 3. We suggest to perform the installation on the NFS server since it will be performed on the local disk instead of using the network and is be more reliable.
These packages include:
   * [[Tier3.WebHome#Setting_up_the_Pacman_Package_Ma][Pacman]], the OSG package management
   * the [[Tier3.WebHome#Worker_node_client][Workes Node client]], a set of software packages expected by Grid jobs submitted to any OSG Compute Element. Alternatively this can be installed on each batch node and the Compute Element. 
   * the [[Tier3.WebHome#User_client_Interactive_nodes][OSG Client]], a set of software packages used by interactive users to interact with the Tier 3 or the Grid. Alternatively this can be installed on each interactive node.
   * the [[Tier3.WebHome#Phase_2_Quick_guide_for_setting][Condor]] queue manager since we decided to have a installation shared across all the nodes as described in CondorSharedInstall. 

---++ Customization on each node
After the step described above in the basic node setup, each node requires some actions to complete its installation and configuration.

---+++ NFS server (=gc1-nfs=)
In the previous step the node running the NFS server had a different procedure during the NFS setup.
Furthermore we recommend to perform the installation of the shared software on it.

---+++ Head Node or Compute Element (=gc1-hn=)
In this configuration the same node acts both as cluster head node and Compute Element.
=gc1-ce= is another alias for =gc1-hn=.

Complete the condor configuration as described in the [[CondorSharedInstall#Management_node_installation][headnode section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the references to =gc1-hn= as head node (CONDOR_HOST) are there

Next you can [[Tier3.WebHome#Installing_a_Compute_Element][install the OSG CE]], making sure to specify to use the installed Condor.

---+++ User interactive nodes
Complete the condor configuration as described in the [[Tier3.CondorSharedInstall#Setting_up_the_other_nodes][other node section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the configuration file with the hostname (=condor_config.gc1-ui001=) points to the user interactive node configuration
Make sure that you can access the shared OSG Client or install it locally.

---+++ Worker Nodes
Complete the condor configuration as described in the [[Tier3.CondorSharedInstall#Setting_up_the_other_nodes][other node section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the configuration file with the hostname (=condor_config.gc1-ui001=) points to the user interactive node configuration
Make sure that you can access the shared Workernode Client or install it locally.

---+++ Storage Element
The [[Tier3.WebHome#Installing_a_Storage_Element_SE][section about SEs]] presents several alternative Storage Elements. In this example we'll install !BeStMan following [[Tier3.BeStManGateway][the BeStManGateway installation guide]].

---++ Service Configuration/Startup/Shutdown
The services on the Compute Element and the Storage Element can be turned on/off using VDT:
<pre class="screen">source $VDT_LOCATION/setup.sh
vdt-control --on/--off</pre>

Condor can be turned on/off using the init script:
<pre class="screen">/etc/init.d/condor start/stop</pre>

---++ Validation of Service Operation
Tests include the tests for all the component, e.g.:
   * batch job submission to test Condor
   * Grid authentication to test the CE
   * Grid submission to the condor-jobmanager to test both the CE and Condor
   * File transfer to test the SE

---++ Debugging Information
Check individual components for debugging information.
There will be system logs in =/var/log=, log files in the VDT installation and local log files resulting from the operation (job submission, file transfer)

---++++++!!Basic Tests:
---++++++!!File Locations
---++++++!!Debugging Procedure
---++++++!!Caveates/Known Issues
---++++++!!Screendump of Install


---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Knowledge
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = BrianBockelman
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
 
-->
