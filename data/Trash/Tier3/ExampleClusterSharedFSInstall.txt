%META:TOPICINFO{author="MarcoMambelli" date="1263328699" format="1.1" reprev="1.2" version="1.2"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>Installation of a Tier 3 with a shared File System*
%TOC%

---++ Introduction
This cluster includes few nodes in a batch queue managed with Condor, one or more interactive nodes, a Conpute Element and a Storage Element. It relies on a shared file system provided by a NFS server. All nodes have Scientific Linux 5.4. Here is a map of the nodes
   * =gc1-hn= (=gc1-ce=) =192.168.192.51= the head-node for the queue and also the Compute Element 
   * =gc1-nfs=  =192.168.192.51= the NFSv4 server
   * =gc1-se= =192.168.192.52= the Storage Element 
   * =gc1-ui001= =192.168.192.61= a user interface machine, can also be referred to as Interactive Node 1
   * =gc1-wn001= =192.168.192.100= Worker Node 0
   * =gc1-wn002= =192.168.192.101= Worker Node 1 ...
   * =gc1-wnNNN= =192.168.192.&lt;N+100>= Worker Node N (N<154)
<!--
   * =gc1-xrdr.yourdomain.org= the xrootd redirector 
   * =gc1-gums.yourdomain.org=  the GUMS server 
-->

This installation has been tested on a virtual cluster with 3 worker nodes and one user interface. 
There is no difference (in the installation described) if real nodes are used instead of the virtual ones.
Adding nodes is easy and initially requires no changes. The numbering of the hosts will have to be corrected if you have more than 154 worker nodes or more than 39 interactive nodes.

---++ Cluster planning
Generally the first step is the planning of a Tier 3. This is done following the local requirements and eventual suggestions from the VO.
The [[Tier3.WebHome][Tier 3 page]] provides several resources to help the planning:
   * [[Tier3.WebHome#Network_configuration][Information on possible network configurations]]
   * Each [[Tier3.WebHome#Component_Installation_and_Setup][component documented]] presents how it is used and why you may like to install it.
   * [[Tier3.WebHome#Security][An overview of security implications]]
   * [[Tier3.WebHome#Site_planning][A planning guide]]
Other choices include the OS to install and some basic configuration.

Once you decide the network topology and what to install on your computers, then some basic naming choices are required:
   * choose (or determine, if imposed by the setup) the subnet for the Tier 3.
   * choose the name of the nodes of the Tier 3 and the domain (if not determined for you).

In this example planning and naming choices have been described in the Introduction above.

---++ Basic node setup
This part is common to each node.
We recommend to start installing the NFS server first.

You start with installing the OS, Scientific Linux 5.4. Some suggestion are provided in  ClusterOSInstall.

The next step is the network configuration. For this example we choose a static network configuration and some suggestion are provided in ClusterNetworkSetup.

The configuration of ssh (to allow easy login to the nodes) is described in ClusterSSHSetup.

This step involves different operations on the NFS server and in order to test the other nodes the server must be setup first.
The Tier 3 described in this example relies on a common file system shared using NFS. The structure of the file system is described in ClusterFileSystem and ClusterNFSSetup provides instruction to configure it, both on the NFS server and on the clients (all the other nodes in the Tier 3).

Once you have a working NFS you can proceed to setting up the accounts as described in ClusterAccountsSetup.

---++ Install the shared software
Some software packages are installed on a shared directory and exported and used on the whole Tier 3. We suggest to perform the installation on the NFS server since it will be performed on the local disk instead of using the network and is be more reliable.
These packages include:
   * [[Tier3.WebHome#Setting_up_the_Pacman_Package_Ma][Pacman]], the OSG package management
   * the [[Tier3.WebHome#Worker_node_client][Workes Node client]], a set of software packages expected by Grid jobs submitted to any OSG Compute Element. Alternatively this can be installed on each batch node and the Compute Element. 
   * the [[Tier3.WebHome#User_client_Interactive_nodes][OSG Client]], a set of software packages used by interactive users to interact with the Tier 3 or the Grid. Alternatively this can be installed on each interactive node.
   * the [[Tier3.WebHome#Quick_Guide_for_setting_up_a_Con][Condor]] queue manager since we decided to have a installation shared across all the nodes as described in CondorSharedInstall. 

---++ Customization on each node
After the step described above in the basic node setup, each node requires some actions to complete its installation and configuration.

---+++ NFS server (=gc1-nfs=)
In the previous step the node running the NFS server had a different procedure during the NFS setup.
Furthermore we recommend to perform the installation of the shared software on it.

---+++ Head Node or Compute Element (=gc1-hn=)
In this configuration the same node acts both as cluster head node and Compute Element.
=gc1-ce= is another alias for =gc1-hn=.

Complete the condor configuration as described in the [[CondorSharedInstall#Management_node_installation][headnode section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the references to =gc1-hn= as head node (CONDOR_HOST) are there

Next you can [[Tier3.WebHome#Installing_a_Compute_Element][install the OSG CE]], making sure to specify to use the installed Condor.

---+++ User interactive nodes
Complete the condor configuration as described in the [[Tier3.CondorSharedInstall#Setting_up_the_other_nodes][other node section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the configuration file with the hostname (=condor_config.gc1-ui001=) points to the user interactive node configuration
Make sure that you can access the shared OSG Client or install it locally.

---+++ Worker Nodes
Complete the condor configuration as described in the [[Tier3.CondorSharedInstall#Setting_up_the_other_nodes][other node section of the Condor installation document]], especially:
   * create the local directories
   * make sure that the configuration file with the hostname (=condor_config.gc1-ui001=) points to the user interactive node configuration
Make sure that you can access the shared Workernode Client or install it locally.

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 11 Jan 20109 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%
