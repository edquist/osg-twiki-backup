%META:TOPICINFO{author="MarcoMambelli" date="1259129605" format="1.1" version="1.11"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! Tier 3 Documentation WIP 
%TOC% 

---++ Dev notes
*ToDo* part and *Links* are only for the Dev version

---+++ Site planning
*ToDo:* Prepare a shorter version of the Site Planning, as first instance rely on the one from OSG Release documentation.

The [[ReleaseDocumentation.SitePlanning][OSG SitePlanning]] document helps understanding OSG components and presents information on possible configurations.  

---+++ Cluster description
The hosts are organized in a cluster: structure with machines with different functions and a network infrastructure.

We suppose that all machine are networked together:
   * the intranet is the network connecting all the hosts
   * the extranet, outside network, is the network where user hosts and clients accessing the cluster reside, this can be the public internet or a private network
   * at least some machines, headnodes, are directly visible from the outside network
   * intranet and outside network may be the same (no need to have two networks)

The hosts have different functions:
   * head node - node with services accessible from the outside netwok
   * worker nodes - series of machines on the intranet, all identical (except the name), used to run jobs
   * servers - servers with specific functions useful for the cluster
Distinct functions may be on the same host.

A Local Resource Manager allows to contact a head node to schedule jobs on the worker nodes.

A shared file system is a disk space shared among the hosts of the cluster, e.g. NFS, xrootd.

Clusters come in different sizes. We'll refer to a small cluster for a cluster with less than 50 job slots.

Clusters can provide to the Grid and/or access several OSG services:
   * Compute element (CE)
   * Storage element (SE)
      * Classic SE - gsiftp
      * SRM
   * GUMS

---+++ Getting started
Hosts setup

---++++ Base systems
Hosts are supposed to have an operating system.
The reference OS used in this document is Scientific Linux 5.4, a linux distribution based on RHEL 5.4.
Anyway these instructions and examples should work for any RHEL based OS (e.g. RHEL, SL variants, !CentOS, ...) with little or no variation.
The OS installation is not covered by this document. ClusterOSInstall contains pointers to some documentation for your convenience.

---++To: ClusterOSInstall
Here there are links to documents about OS Installation.

[[https://www.scientificlinux.org/news/sl54][Here]] you can find more information about SL and [[https://www.scientificlinux.org/download][download]] the distribution.

There are several methods to install the OS:
   * Starting from installation media
   * Using a kickstart file
   * Using a cluster management system
   * ...

Here are some contributed examples:
   * By the US-ATLAS Tier 3 effort:
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/UsbKeysetup
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/BNLKickStart
   * By the Mariland Tier 3 of CMS:
      * http://hep-t3.physics.umd.edu/HowToForAdmins.html#rocks                                                                                                                                                                                    
   * By Rocks:
      * http://www.rocksclusters.org/roll-documentation/base/5.2/

------end-To

*Links*:
   * virtualized http://twiki.mwt2.org/bin/view/Admins/VMwareServerLaptop

---++++ Customization of the base system
To ease the following installation steps and to have a common base here are established some conventions and customizations of the base OS, like user accounts, a shared file system and others.

File system structure:
   * =/exports= - root of the NFSv4 pseudo filesystem (directory tree exported by NFS)
   * =/nfs= - collects the directories mounted from NFS
   * =/home= - home directories (shared)
   * =/opt= - grid software
   * =/scratch= - local scratch area
   * =/storage= - data disk used for distributed file system (xrootd)

More in detail the structure is:<pre>
/exports 
/exports/certificates
/exports/condor
/exports/condor/condor-etc
/exports/condor/condor-7.4
/exports/condor/condor
/exports/home
/exports/osg
/exports/
/nfs/ 
/nfs/condor/
/nfs/condor/condor-etc
/nfs/condor/condor-7.4
/nfs/condor/condor
/nfs/home
/nfs/osg
/nfs/osg/app
/nfs/osg/app/pacman-3.29
/nfs/osg/app/pacman
/nfs/osg/data
/home 
/opt/condor 
/opt/condor-7.4         # if no NFS
/opt/pacman
/opt/pacman-3.29     # if no NFS
/storage 
/storage/xrootd 
/scratch 
/scratch/condor 
</pre>

[NO NFS] green tags

A first option, suitable for small Tier 3s makes use of shared directories to ease administration. ClusterNFSSetup describes how to setup the server and the clients for NFSv4

A second option performs the installation without the use of shared disk space.
If you decide not to have any NFS file system you will be limited in the possible options (e.g. no shared Condor installation) and some of the administrative tasks will be more difficult.


*Links*:
   * ReleaseDocumentation.ClusterGettingStarted
   * ReleaseDocumentation.ClusterNetworkConfiguration
   * Setup SSH public key infrastructure: ReleaseDocumentation.ClusterConfigureSSH
   * Setup an NFS server: ReleaseDocumentation.ClusterNFSServer
   * Creating user accounts: ReleaseDocumentation.ClusterCreateAccounts
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/Setupnfsv4Servermachine
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/KickstartFilenfsv4Servermachine
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/GetHostCertificate
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/UseraccountManagement
   * http://wiki.archlinux.org/index.php/NFSv4
   * http://dag.wieers.com/blog/tunneling-nfs4-over-ssh
   * http://nfsv4.bullopensource.org/doc/NFS3_NFS4_migration.pdf
   * http://www.ietf.org/rfc/rfc3010.txt
   * Filesystem Hierarchy Standard:
      * http://www.pathname.com/fhs/
   * http://www.pathname.com/fhs/pub/fhs-2.3.html

---+++ Cluster management system
*ToDo* add cluster management system. Later, not now

Alternatives:
   * rocks
   * puppet

---+++ Distributed file system 
xrootd is a shared file system distributed across several hosts, each one serving part of it. It is optimized to store data files, not for programs or system files.
It is used at its best if the jobs are running on the host serving their input files.

You may want to install xrootd if you have big data files and would like an uniform file system space using the disk space available on several nodes (dedicated data servers or the worker nodes).

XrootdInstall describes the installation of the xrootd file system

*ToDo* fix name spelling: find and coreect it everywhere
Should it be installed in /opt/osg or in /opt/xrootd? 
If multiple services are installed on the same host (CE, SE, xrootd, ...) should they share the same installation diretory or have different ones?


*Links*:
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/XrootdStandAlone
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/GridFTPXrootd
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/XrootdGridftpports

---+++ Quick Guide for setting up a Condor Cluster
Condor 
Each cluster needs a Local Resource Manager (LRM, also called _queue manager_ or _scheduler_) to schedule the jobs on the worker nodes. Common LRMs are Condor, LSF, PBS, SGE, if you have one installed or a preferred one you can use that one. 
If not CondorInstall describes how to install and configure Condor 7.4, the latest production version of Condor.
Here presented are two alternatives to install Condor:
   * CondorSharedInstall - is an easy way to add Condor to small clusters using a shared file system 
      * it is easy to add new nodes
      * configuration changes and updates are also very quick
      * cons: shared installation may slow down several nodes using it
   * CondorRPMInstall - Condor is installed on each node using the RPM packages of Condor
      * no contention for shared files
      * only option if no shared file system
      * cons: some more work to add nodes 
      * cons: updates need to be installed on all nodes 

The base configuration of condor can be modified to add some extra features:
   * CondorServiceJobSlotSetup describe how to add a service jobs slot to each node. This will allow to install application software or to run monitoring jobs on the worker nodes without modify the scheduling of the regular jobs
   * CondorHawkeyeSetup describes how to setup Condor's Hawkeye and shows an example using Hawkeye to conditionally schedule  or suspend jobs depending on the activity on the worker node

*ToDo* add Quill part and file staging part
   * CondorQuillSetup describes how to install and use Quill, a database storing information about the jobs scheduled using Condor 
Interesting notes about file staging

*Links*:
   * Install Condor job management system: ReleaseDocumentation.ClusterCondorInstall
   * http://hep-t3.physics.umd.edu/HowToForAdmins.html#rocks
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/CondorInstallation
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/CondorHawkeye
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/ThroughputCleanup

---+++ Cluster monitoring
*ToDo* add part about monitoring, specially Ganglia

Ganglia -notes. In theory distributed as RPM. The x86_64 is dated 4/2006 (Orville), the most recent RPM is 9/2008 (Wien)

*Links*:
   * http://ganglia.sourceforge.net/

---+++ User client
The user client machine is a host that allows users to login and provides them with software to access Grid services and specific VO software. This document covers only the installation of Grid clients, leaving to the VOs the documentation of their software.

SiteCoordination.GridClientTutorial shows the installation of the Grid client software.
Note that users will need to [[ReleaseDocumentation.GridCertificateRequestTutorial][request a certificate]] and [[ReleaseDocumentation.RequestVOMembershipTutorial][register in a VO]] before being able to use the Grid.

Examples from VOs:
   * ATLAS VO: https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SoftwareInstallation
 
Links:
   * SiteCoordination.GridClientTutorial
   * ReleaseDocumentation.GridCertificateRequestTutorial
   * ReleaseDocumentation.RequestVOMembershipTutorial
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SoftwareInstallation

---+++ Worker node client
The worker node client is used to allow non interactive jobs (the ones submitted using the Grid or Condor) to access the Grid.
We are installing the worker node client in =/opt/wn=, linked to =/nfs/osg/wn= using the instruction in the OSG release documents: ReleaseDocumentation.WorkerNodeClient

In short, on the node chosen for the installation, e.g. =gc1-ce=:<pre>
source /nfs/osg/app/pacman/setup.sh
mkdir /nfs/osg/wn
ln -s /nfs/osg/wn /opt/osg
cd /opt/wn
pacman -get http://software.grid.iu.edu/osg-1.2:wn-client
ln -s /etc/grid-security/certificates /opt/wn/globus/TRUSTED_CA
</pre>

On all other nodes that may run jobs, e.g. the worker nodes:<pre>
ln -s /nfs/osg/wn /opt/osg
</pre>

Alternatively, if there is no shared file system or if there is a load concern, the worker node client can be installed locally, in =/opt/wn= on all the worker nodes.

Links:
   * https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/WorkerNodeClient

---+++ Grid users authentication
There are 3 options:
   * use a gridmap file
   * rely on an external GUMS server
   * install a GUMS server

Links:
   * ReleaseDocumentation.GUMSHandsOn
   * ReleaseDocumentation.InstallConfigureAndManageGUMS
   * ReleaseDocumentation.GridColombiaInstallGUMS

---+++ Installing a CE
Installing the OSG CE

Links:
   * ReleaseDocumentation.InstallCETutorial

---+++ Installing a SE
Installing GridFTP

Installing BeStMan SRM Gateway

Links:
   * ReleaseDocumentation.BestmanGateway
   * ReleaseDocumentation.InstallSETutorial
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SetupSE
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/ThroughputCleanup

---+++ Installing RSV
 
---+++ Squid Web proxy

---+++ Network performance

Links:
   * ReleaseDocumentation.NetworkPerformanceToolkit
   * ReleaseDocumentation.NetworkPerformanceTutorial

---+ Doc reprise
The documentation includes:
   * a description of the Tier 3 cluster and its main components
   * modular description of components that can be installed separately 
   * the installation of the nodes 
Components are hardware and software that provide a functionality, e.g. the queue manager, a Compute Element, Xrootd, ...
Nodes are computers belonging to the cluster.
Components can be placed on same or different nodes, depending on the number of nodes that are available.

---++Description
---+++What is a Tier 3?
---+++Components of a Tier 3 cluster
---+++Let's set some names
To simplify documentation and examples we give a name to the cluster: =gc1= (Grid Cluster 1). Each host in the cluster, node, will have a unique fully qualified domain name (FQDN).  The FQDN includes a local hostname and a parent domain name (here "yourdomain.org"). 
The following is a list of roles (abstract functionalities) that nodes may have within the cluster. We use the roles to define the hostnames. This means that the host that is your compute element (if you have a CE) has an FQDN =gc1-ce.yourdomain.org=.
There is no one-to-one correspondence between these names and the nodes. If you have no node covering a role, then there will be no node with that name. If a node is covering two or more roles, then all those names will refer to the same node and can be _cnames_, e.g. the Storage Element and the Xrootd redirector may be the same host, making gc1-se and gc1-xrdr two names of the same node. 
The list is:
   * =gc1-nfs.yourdomain.org= a NFSv4 server 
   * =gc1-ce.yourdomain.org= a Compute Element
   * =gc1-hn.yourdomain.org= a cluster head-node
   * =gc1-se.yourdomain.org= a Storage Element 
   * =gc1-xrdr.yourdomain.org= xrootd redirector
   * =gc1-ui.yourdomain.org=  your user interface machine
   * =gc1-c001.yourdomain.org= compute node 1
   * =gc1-c002.yourdomain.org= compute node 2
   * =gc1-c003.yourdomain.org= compute node 3
   * =gc1-gums.yourdomain.org= a GUMS server

In the documents the machines will sometimes be referred to by their local hostname, e.g. =gc1-ce= instead of =gc1-ce.yourdomain.org=

---+++Network configuration
Your campus or your department will provide you a network connection that in these documents we refer as _extranet_, i.e. the network that connects also to the world outside of your Tier 3 cluster.

[[ClusterNetwork][Read more]] about the different topologies and the default configuration, including subnet IP address, for the network connecting the nodes of a Tier 3 used in the examples.

---++Site Planning
---++Components
Current part of the documentation

   * Fig. 1- Tier 3 cluster components and dependencies: <br />
     <img src="%ATTACHURLPATH%/gc-dependencies-wb-sm.png" alt="gc-dependencies-wb-sm.png" width='387' height='314' />    

---+++Preliminaries
---++++SSH Configuration
To make the cluster secure we'll use a host-based SSH key infrastructure. This section will introduce keys, agents, how to configure them and how to use them to ease in a safe way the administration of the T3.

ClusterSSHSetup describes an example of SSH configuration and use. That is a suggested setup, make sure to follow the policies and recommendation in place at your university or laboratory. 

---++++
---++Nodes 
Let's set some names (here?)
Current placement
NFS
HN CE
SE
WN1, WN2
NAT, Squid

---+++Clusters



---++


-- Main.MarcoMambelli - 12 Nov 2009