%META:TOPICINFO{author="KyleGross" date="1329928282" format="1.1" version="1.26"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC%

<!-- Local variables
   * Set CONDORREL = 7.4.2
   * Set AS_OF_DATE = May 20, 2010
-->

---++ Introduction
%NOTE% We recommend to use the [[CondorRPMInstall][RPM distribution of Condor]]. Installation is more standard and automatic. Reasons to use this installation based on the TAR distribution may include: your platform is not supported by the current RPM release, or if you desire multiple version of Condor installed and selectable at the same time.
%DISCLAIMER%  If you are using Rocks, commonly used by CMS, or you install using Kickstart files like the one provided by ATLAS, then you may not need any of this. The may install and setup Condor for you. Check your VO documentation first. 

We will use the latest stable release of Condor. As of %AS_OF_DATE%, this is %CONDORREL%.  Our approach will be to install Condor in  =/opt/condor= on the management node (*gc1-ce*) and share it with the other nodes in the cluster.  We will also prepare the host-specific configuration files for all nodes participating in the Condor pool and store them in a standard location =/nfs/condor/condor-etc=.  This will make it easy to make cluster-wide configuration changes.

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.4/3_2Installation.html

---+++ A note about the directory structure
This Condor installation was structured to facilitate upgrades to Condor with minimal effort.  The Condor release directory on each host is =/opt/condor= which is a soft link to the NFS exported =/nfs/condor/condor= which in turn is a link to the actual Condor installation directory which includes the version number: =/nfs/condor/condor-%CONDORREL%=.  This allows to have multiple releases installed in subdirectories of =/nfs/condor/=. Some configuration files are kept outside in the =/nfs/condor/condor-etc= directory. The motivation for this choice will hopefully become more clear in the [[#Upgrades][Upgrades]] section below.

---+++ A note about Condor configuration files
Condor is highly configurable. All parameters have default values (described in the [[http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.htm][Condor manual]]) and can be overridden by the content of a chain of configuration files (=condor_config=). Each of this files may contain the entry =LOCAL_CONFIG_FILE= that can point to the following file in the chain. This allows the specification of multiple files as the local configuration file, each one processed in the order given (with parameters set in later files overriding values from previous files). In our configuration we'll use three levels of configuration:
   * condor_config - provided by the Condor release and left mostly unmodified, this is in the release directory
   * condor_config.cluster - modification common to all the machines in the cluster, this is in a shared configuration directory
   * condor_config.$(HOSTNAME) - files specific for one node (or one type of nodes, e.g. all the worker nodes), these are in the shared configuration directory as well
The [[http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.htm][Condor manual]] provides a complete guide on Condor configuration.

---++ Management node installation
Install Condor on the machine that will be the central manager.  In this tutorial use *gc1-ce*, the same host as the OSG compute element.  

If the =/nfs/condor= directory is exported with root squash from a NFS server that is not also the central manager you'll have to adapt the following instructions. You have to edit the configuration files in =/nfs/condor/condor-etc/= from the NFS server. Furthermore you have two options for the installation:
   1. you disable root squash for the installation (and any time you perform Condor upgrades).
   2. you perform installation (and upgrades) on the NFS server, then you remove the local Condor directories from it and you create them on the central manager (the same way that you do on the other nodes).

   * Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl][download page]] and choose the Condor version (e.g. %CONDORREL%, current production version).
   * Choose the correct distribution for your platform (the one below condor %CONDORREL% for RHEL 5 dynamically linked ):
      *  =condor-%CONDORREL%-linux-x86-rhel5-dynamic.tar.gz=  (for  x86 OS)
      *  =condor-%CONDORREL%-linux-x86_64-rhel5-dynamic.tar.gz=  (for x86_64 OS)
   * Save the downloaded file to your home directory.
   * Create the machine-specific directories that Condor uses: <pre>
mkdir /scratch
mkdir /scratch/condor
</pre>

   * Make sure the *condor* user exists with a shared home directory (=/home/condor=) (See [[#No_shared_user_directories][the instructions below]] if this is not possible).

   * Create the shared directory for configuration files:<pre class="screen">
mkdir /nfs/condor/condor-etc
</pre>

   * Prepare the target installation directory: <pre class="screen">
cd /nfs/condor
mkdir condor-%CONDORREL%
ln -s condor-%CONDORREL% condor
</pre>

   * Make sure  that =/opt/condor= is a soft link to =/nfs/condor/condor=.<pre class="screen">
ln -s /nfs/condor/condor /opt/condor
</pre>

   * Create a temporary directory and extract the source from the downloaded file in your home directory<pre class="screen">
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xvzf ~/condor-%CONDORREL%-linux-x86-rhel5-dynamic.tar.gz 
</pre>

   * Run the Condor installation script:<pre class="screen">
cd condor-%CONDORREL%
./condor_install --prefix=/opt/condor --local-dir=/scratch/condor --type=manager</pre>   
   * The installation script will place the machine-specific configuration file in the the directory specified by =local-dir=.  We want to manage these files in the shared area, so: <pre class="screen">
mv /scratch/condor/condor_config.local /opt/condor/etc/
</pre>
   * Open the global Condor configuration file =/opt/condor/etc/condor_config= in an editor.
      * Change the value of this variable (find it in the file):<pre class="file">
LOCAL_CONFIG_FILE = /opt/condor/etc/condor_config.local
</pre>
   * Open the local Condor configuration file =/opt/condor/etc/condor_config.local= in an editor.
      * On top add this line:<pre class="file">
LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.cluster
</pre>
   * Optionally you can copy the important values (RELEASE_DIR, MAIL, CONDOR_IDS, LOCK, JAVA, JAVA_MAXHEAP_ARGUMENT) from =condor_config.local= into =condor_config.cluster= and have the global Condor configuration file point directly to the second one. Inspect the file to make sure that no important setting is skipped.
   * Edit the cluster Condor configuration file =/nfs/condor/condor-etc/condor_config.cluster= with the following content:
      * Copy the following content (attached in [[][condor_config.cluster]]) changing the values to suite your cluster (yourdomain.org, gc1-hn.yourdomain.org). You may find some suggestion in the local configuration file =/scratch/condor/condor_config.local=:<pre class="file">
## Condor configuration for OSG T3
## For more detial please see
## http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.$(HOSTNAME)
LOCAL_DIR = /scratch/condor
# The following should be your T3 domain
UID_DOMAIN = %RED%yourdomain.org%ENDCOLOR%
# Human readable name for your Condor pool
COLLECTOR_NAME = "Tier 3 Condor at $(UID_DOMAIN)"
# A shared file system (NFS), e.g. job dir, is assumed if the name is the same
FILESYSTEM_DOMAIN = $(UID_DOMAIN)
ALLOW_WRITE = *.$(UID_DOMAIN)
CONDOR_ADMIN = root@$(FULL_HOSTNAME)
# The following should be the full name of the head node
CONDOR_HOST = %RED%gc1-hn.yourdomain.org%ENDCOLOR%
# Port range should be opened in the firewall (can be different on different machines)
# This 9000-9999 is coherent with the iptables configuration in the T3 documentation 
IN_HIGHPORT = 9999
IN_LOWPORT = 9000
# This is to enforce password authentication
SEC_DAEMON_AUTHENTICATION = required
SEC_DAEMON_AUTHENTICATION_METHODS = password
SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi,kerberos
SEC_PASSWORD_FILE = /scratch/condor/condor_credential
ALLOW_DAEMON = condor_pool@*
##  Sets how often the condor_negotiator starts a negotiation cycle 
##  for negotiator and schedd). 
#  It is defined in seconds and defaults to 60 (1 minute), default is 300. 
NEGOTIATOR_INTERVAL = 20
##  Scheduling parameters for the startd
TRUST_UID_DOMAIN = TRUE
# start as available and do not suspend, preempt or kill
START = TRUE
SUSPEND = FALSE
PREEMPT = FALSE
KILL = FALSE
</pre>
      * Make sure that you have the following important line in the file <pre>CONDOR_HOST = gc1-hn</pre> (in red in the example above). 
         * *Note:* =CONDOR_HOST= can be set with or without the domain name: =gc1-hn= or =gc1-hn.yourdomain.org=
         * *Note:* =CONDOR_HOST= is the node running the Condor collector and negotiator. If you are running them on separate nodes, define both =NEGOTIATOR_HOST= and =COLLECTOR_HOST= and set each with the correct host name.
   * Link =condor_config= in the *condor* user's home directory to the location of the global configuration file (this allows condor to find the file without requiring environment variables):<pre class="screen">
ln -s /opt/condor/etc/condor_config ~condor/condor_config
</pre>
   *  Create the files with the host configuration specific for the nodes using the following content. We will create 3 base configuration files: one for the headnode, one for worker nodes, one for the interactive nodes (user interface). (specific for the headnode) copying the following line:
      * For the headnode, =/nfs/condor/condor-etc/condor_config.headnode=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, COLLECTOR, NEGOTIATOR
</pre>
      * For the worker nodes, =/nfs/condor/condor-etc/condor_config.worker=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, STARTD
</pre>
      * For the interactive nodes, =/nfs/condor/condor-etc/condor_config.interactive=:<pre class="file">
## OSG T3 host configuration
## For more info: http://www.cs.wisc.edu/condor/manual/v7.4/3_3Configuration.html
# List of daemons on the node (headnode requires collector and negotiator, 
# schedd required to submit jobs, startd to run jobs)
DAEMON_LIST = MASTER, SCHEDD
</pre>
   * Then for each node create a link pointing to the template, e.g.:<pre class="screen">
cd /nfs/condor/condor-etc/
ln -s condor_config.headnode condor_config.gc1-hn
ln -s condor_config.interactive condor_config.gc1-ui001
ln -s condor_config.worker condor_config.gc1-wn001
ln -s condor_config.worker condor_config.gc1-wn002
ln -s condor_config.worker condor_config.gc1-wn003
</pre> Each node must have its own =condor_config.${HOST}= file. If some nodes require a special configuration you can copy the template (e.g. =condor_config.worker=) and customize it.
   * If you are not installing Condor on the headnode (e.g. on the NFS server), stop here, login on the headnode end follow the instructions in the next section. On the headnode, setup Condor and set the password that will be used by the Condor system (at the prompt enter the same password for all nodes):<pre class="screen">
source /opt/condor/condor.sh
condor_store_cred -c add
</pre>
   * Start up the Condor master and check for running processes <pre class="screen">
/opt/condor/sbin/condor_master
ps -ef |grep condor
condor    4404     1  0 06:42 ?        00:00:00 /opt/condor/sbin/condor_master
condor    4405  4404  0 06:42 ?        00:00:00 condor_collector -f
condor    4406  4404  1 06:43 ?        00:00:00 condor_negotiator -f
root      4410  4348  0 06:43 pts/2    00:00:00 grep condor
</pre>
   * To shutdown <pre>/opt/condor/sbin/condor_off -master
</pre>
   * Enable automatic startup at boot by setting the correct path of the =condor_master= executable (=MASTER=/opt/condor/sbin/condor_master=) in the boot file =/opt/condor/etc/examples/condor.boot=. Then:<pre class="screen">
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor on
</pre>
   * Add Condor commands to the user environment (if you like to have condor commands in the path): <pre class="screen">
cp /opt/condor/condor.sh /opt/condor/condor.csh /etc/profile.d/
</pre> 

---++ Setting up the other nodes 
This should be executed on all other nodes where Condor should be running (worker nodes, submit host)
   * Link the installation directory and create the local Condor directories in =/scratch/condor= (making sure that =execute= directory has the right permissions =a+rwx +t=) <pre class="screen">
ln -s /nfs/condor/condor /opt/condor
mkdir /scratch
mkdir /scratch/condor
mkdir /scratch/condor/execute
mkdir /scratch/condor/log
mkdir /scratch/condor/spool
chown condor /scratch/condor/*
chmod a+rwx /scratch/condor/execute
chmod +t /scratch/condor/execute
</pre>
   * At the end you should see something like: <pre class="screen">
ls -l /scratch/condor/
total 12
drwxrwxrwt 2 condor root 4096 Oct 15 04:07 execute
drwxr-xr-x 2 condor root 4096 Oct 15 04:15 log
drwxr-xr-x 2 condor root 4096 Oct 15 04:02 spool
</pre>
   * Setup Condor and set the password that will be used by the Condor system (at the prompt enter the same password for all nodes):<pre class="screen">
source /opt/condor/condor.sh
condor_store_cred -c add
</pre>
   * Enable automatic startup:<pre class="screen">
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor on
</pre>
   * Add Condor commands to the user environment (if you like to have condor commands in the path): <pre class="screen">
cp /opt/condor/condor.sh /opt/condor/condor.csh /etc/profile.d/
</pre>
   * Start Condor and check the processes:<pre class="screen">/etc/init.d/condor start
# should reply: Starting up Condor
ps -ef |grep condor
# you should see condor_master and the desired daemons (depending on the host)
</pre>

---++ Upgrades
As explained in the section about the directory structure, this Condor installation was structured to facilitate upgrades with minimal effort.  The Condor directory on each host is =/opt/condor=. This is a link to the exported =/nfs/condor/condor= that is a link to the real Condor installation directory that has also the version number in the name (e.g. =/nfs/condor/condor-%CONDORREL%=).

Only one version of Condor will be used at the time but linking =/nfs/condor/condor= to a new directory, e.g. =/nfs/condor/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if desired simply changing one link (e.g. if the new installation is not working properly). 

---++ Setup
Executing <pre>source /opt/condor/condor.sh</pre> will add the condor commands to your path and set the =CONDOR_LOCATION= variable.
If you copied the =condor.sh= and =condor.csh= files in =/etc/profile.d/= as described above, Condor will be in the environment of every user automatically, no need to source any file.


---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status= and submit test jobs with =condor_submit=. 
Check CondorTest for more.

---++ Special needs
The following sections present instructions or suggestion for uncommon configurations

---+++ Changes to the Firewall (IPtables)
If you are using a Firewall (e.g. iptables) on all nodes you need to open the ports used by Condor:
   * Edit the =/etc/sysconfig/iptables= file to add these lines ahead of the reject line:<pre class="file">
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p tcp -m tcp --dport 9000:10000 -j ACCEPT  
-A RH-Firewall-1-INPUT  -s &lt;network_address> -m state --state ESTABLISHED,NEW -p udp -m udp --dport 9000:10000 -j ACCEPT 
</pre> where the _network_address_ is the address of the intranet of the T3 cluster e.g. 192.168.192.0/18. (Or the extranet if your T3 does not have a separate intranet). You can omit the =-s= option if you have nodes of your Condor cluster (startd, schedd, ...) outside of that network.
   * Restart the firewall:<pre class="screen">
/etc/init.d/iptables restart
</pre>

---+++ Swap configuration on the Condor submit host
If you have no swap space on the submit host all your job submissions will fail (remain Idle) and in =/scratch/condor/log/SchedLog= you will see errors like "Swap space estimate reached! No more jobs can be run!". 

You can check your swap memory with =cat /proc/meminfo |grep Swap= and you will see something like:<pre> 
SwapCached:          0 kB
SwapTotal:     2097144 kB
SwapFree:      2097080 kB
</pre>
If the numbers above are all 0, you have no swap space.

The recommendation is to enable a swap space on your host. Here some information to [[http://www.cyberciti.biz/faq/linux-add-a-swap-file-howto/][add a swap file]] or [[http://www.xenotime.net/linux/doc/swap-mini-howto.txt][a swap-howto, see section 5]]. If you cannot, here is a workaround in Condor to avoid to use swap space. It will work for small test clusters.

In the file =condor_config.gc1-ui= file add the line: <pre class="file">
RESERVED_SWAP = 0
</pre>

---+++ Multiple Condor version at the same time
The  [[#Upgrades][Upgrades]] section above shows an easy way to deal with upgrades but there may be the need to run multiple version of Condor at the same time, e.g. if the cluster has different OS that require a different binary version of Condor. If your cluster is heterogeneous (nodes are different) the [[CondorRPMInstall][RPM installation]] may be easier. 

To have multiple versions of Condor in a shared installation:
   * download and install all the versions in subdirectories of =/nfs/condor/=<pre class="screen">
mkdir /nfs/condor/condor-versionX
mkdir /tmp/install
tar xvzf <condor-versionX.tar.gz>
cd condor-versionX
./condor_install --prefix=/nfs/condor/condor-versionX --local-dir=/tmp/scratch-versionX --type=manager</pre>   
</pre>
   * on each node link the correct version to =/opt/condor=:<pre class="screen">
ln -s /nfs/condor/condor-versionX /opt/condor
</pre> 
   * on each node fix the file =/opt/condor/etc/condor_config= as described above
   * other instruction are similar:
      * prepare the same way the shared configuration files
      * create the local condor directories
      * fix the automatic startup

---+++ No shared user directories
Condor is assuming that the directories used to submit jobs (e.g. the home directories) are shared among all the nodes that share the same string for =FILESYSTEM_DOMAIN =. If this is not the case for you, you have to:
   * change the =FILESYSTEM_DOMAIN= in the cluster or host configuration file so that it is different for all the nodes, e.g. =FILESYSTEM_DOMAIN == $(HOSTNAME).yourdomain.org=.
   *  when you submit a job, tell Condor to transfer the files: the executable and the input and output files. If files cannot be moved, the job will not go to a node with a =FILESYSTEM_DOMAIN= different from the submit host.

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 17 Nov 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- Page Info
There have been proposal to remove this page because RPM is the recommended way to install Condor.
Anyway this (tarfile) installation is the only way to install multiple version at the same time.
- For testing
- In an heterogeneous cluster (each node would have its own version)
-->

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %NO%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   reviewed during DOC workshop
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DocWorkshop
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->


%META:FILEATTACHMENT{name="use-processor.pl.txt" attachment="use-processor.pl.txt" attr="" comment="perl script" date="1258482893" path="use-processor.pl.txt" size="508" stream="use-processor.pl.txt" tmpFilename="/usr/tmp/CGItemp16856" user="MarcoMambelli" version="1"}%
