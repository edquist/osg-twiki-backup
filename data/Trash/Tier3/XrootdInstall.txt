%META:TOPICINFO{author="MarcoMambelli" date="1271974200" format="1.1" reprev="1.32" version="1.32"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop> Xrootd Installation*
%TOC%

---++ Introduction
Xrootd is a high performance network storage system widely used in high energy physics experiments such as ATLAS and ALICE. The underlying Xroot data transfer protocol provides highly efficient access to ROOT based data files.  This page provides instructions for creating a simple Xrootd storage system consisting of:
   * One redirector node
   * One or more data server node(s)


---++ Getting Started

---+++ Identify hosts and roles
   * Identify the redirector node
   * Identify the data server(s)

---+++ Check firewall rules
It is important to make sure your firewall (if any) isn't blocking the ports that Xrootd uses for communication.  Otherwise, it becomes difficult to diagnose errors and failures may occur.  Xrootd uses the following ports:

| *Host* | *Port Number* | *Protocol* |
|!Xrootd data server| random ports selected by the OS |tcp|
|!Xrootd redirector|1094|tcp|
||2094|tcp|
||1213|tcp|

Check the firewall rules with =/etc/init.d/iptables status=.   See below for more information on firewall configuration settings for Xrootd.

---+++ Create the xrootd user account
You need a non privileged Xrootd user account on the redirector and all data servers, and that user *must have a login shell*.    If you manage your accounts by hand (rather than using a service such as LDAP) you would, on your management node:
<pre class="screen">
/usr/sbin/groupadd xrootd 
/usr/sbin/useradd --gid xrootd xrootd
</pre>
and copy =/etc/passwd, /etc/shadow, /etc/group,= and =/etc/gshadow= to each of the xrootd nodes.

---+++ Install Pacman
Pacman is a package management program used to install OSG software.
ReleaseDocumentation.PacmanInstall describes how to install Pacman.
You may select  =/opt/pacman= as the installation directory.<br>
%TWISTY{
mode="div"
showlink="Show an example of how to install Pacman..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="screen">
cd /opt
wget http://atlas.bu.edu/~youssef/pacman/sample_cache/tarballs/pacman-3.29.tar.gz
tar --no-same-owner -xzvf pacman-3.29.tar.gz
cd pacman-3.29
source setup.sh
cd ..
ln -s pacman-3.29 pacman
</pre>
%ENDTWISTY%
Once Pacman is installed do: =source /opt/pacman/setup.sh=.  You are ready to install Xrootd.

---++ Installing and configuring the redirector
Two areas need to be setup: _storage_path_ and _storage_cache_. You may use  separate disk partitions for these but for the redirector we recommend simply creating two directories:
   * /storage/cache is a service directory for the redirector. 
   * /storage/path is a directory containing internal Xrootd references to files in the system.  Xrootd clients access files using _storage_path_. 
<pre class="screen">
mkdir -p /storage/path
mkdir -p /storage/cache
</pre>

Next create and change to an installation directory. <pre class="screen">
mkdir -p /opt/xrootd/
cd /opt/xrootd/
</pre>

Install the !Xrootd package from the %CACHE% cache.  Pacman will ask whether you want to trust the cache (answer =yall=).
<pre class="screen">
pacman -get %CACHE%:Xrootd
</pre>

Update the environment and run the post installation script:
<pre class="screen">
source setup.sh
vdt-post-install
</pre>

You can verify that the version installed is the version you expected by invoking =vdt-version= : <pre class="screen">
vdt-version
</pre>

This completes the installation part of the Xrootd redirector. The next step is to configure the Xrootd redirector.
   * $VDT_LOCATION is the installation directory of the Xrootd redirector (=/opt/xrootd=)
   * =xrootd= is the non-privileged user that runs the Xrootd redirector service 
   * =/storage/cache= and =/storage/path= are the directories discussed above
   * =--public-cache-size 1= is a configuration option necessary for historical reasons (it will be removed from future releases).

<pre class="screen">
cd /opt/xrootd
source setup.sh
$VDT_LOCATION/vdt/setup/configure_xrootd  --server y  --user xrootd  --this-is-xrdr --xrdr-storage-path /storage/path   --xrdr-storage-cache /storage/cache --public-cache-size 1
</pre>

Now =vdt-control -list= shows the Xrootd service is installed:
<pre class="screen">
vdt-control --list
Service                 | Type   | Desired State
------------------------+--------+--------------
xrootd                  | init   | enable
</pre>

Startup the redirector:
<pre class="screen">
vdt-control --on
</pre>

Here is an example of running Xrootd redirector services (=xrootd= is the Xrootd user):
<pre  class="screen">
ps -fu xrootd
UID        PID  PPID  C STIME TTY          TIME CMD
xrootd    2852     1  0 Apr20 ?        00:00:00 /opt/xrootd/xrootd/bin//xrootd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/etc/xrootd.cfg
xrootd    2913     1  0 Apr20 ?        00:00:00 /opt/xrootd/xrootd/bin//xrootd -n xrootd_cnd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/e
xrootd    2985     1  0 Apr20 ?        00:00:00 /opt/xrootd/xrootd/bin//cmsd -l /opt/xrootd/xrootd/var/logs/cmslog -c /opt/xrootd/xrootd/etc/xrootd.cfg
<!--
ps ax | grep xrootd
13479 pts/2    Sl     0:00 /opt/xrootd/xrootd/bin//xrootd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/etc/xrootd.cfg
13537 pts/2    Sl     0:00 /opt/xrootd/xrootd/bin//xrootd -n xrootd_cnd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/etc/xrootd_cnd.cfg
13601 pts/2    Sl     0:00 /opt/xrootd/xrootd/bin//cmsd -l /opt/xrootd/xrootd/var/logs/cmslog -c /opt/xrootd/xrootd/etc/xrootd.cfg
-->
</pre>

---++ Installing and configuring the data server(s)
These instruction must be executed on each of the data servers.

Two areas need to be setup: _storage_path_ and _storage_cache_.  In the simplest case two directories will suffice,  e.g =/storage/path= and =/storage/cache=.  For larger data servers we recommend creating separate disk partitions (see below).  Note on Xrootd data servers: 
   * =/storage/path= is a directory containing symbolic links to files in _storage_cache_.  
   * =/storage/cache= contains actual data files

Next create and change to an installation directory:
<pre class="screen">
mkdir -p /opt/xrootd/
cd /opt/xrootd/
</pre>

Install the !Xrootd package from the %CACHE% cache.  Pacman will ask whether you want to trust the cache (answer =yall=).
<pre class="screen">
pacman -get %CACHE%:Xrootd
</pre>

Update the environment and run the post installation script:
<pre class="screen">
source setup.sh
vdt-post-install
</pre>

Verify the version installed is the one you expected by invoking =vdt-version= (=man vdt-version= to get more info). 
<pre class="screen">
vdt-version
</pre>

This completes the installation of an Xrootd data server. The next step is to configure it.  The command below uses the defaults chosen in these instructions which you may need to change to match your specific choices:
   * $VDT_LOCATION is a directory of Xrootd installation (=/opt/xrootd=)
   * xrootd is the non-privileged user that runs the Xrootd data service 
   * FQDN of the redirector host, e.g.  =redirector.yourdomain.org= 
   * =/storage/cache= and =/storage/path= are the directories created above
   * =--public-cache-size 1= is an option needed for historical reasons. It will be removed from future versions.

<pre class="screen">
cd /opt/xrootd
source setup.sh
$VDT_LOCATION/vdt/setup/configure_xrootd  --server y  --user xrootd  --xrdr-host  redirector.yourdomain.org  --xrdr-storage-path /storage/path   --xrdr-storage-cache /storage/cache  --public-cache-size 1
</pre>

Now =vdt-control -list= shows the Xrootd service is installed:
<pre class="screen">
#vdt-control --list
Service                 | Type   | Desired State
------------------------+--------+--------------
xrootd                  | init   | enable
</pre>


Startup the data server:
<pre class="screen">
vdt-control --on
</pre>

Here is an example showing running Xrootd data services (=xrootd= is the Xrootd user):
<pre  class="screen">
ps -fu xrootd
UID        PID  PPID  C STIME TTY          TIME CMD
xrootd   26443     1  0 10:29 ?        00:00:00 /opt/xrootd/xrootd/bin//xrootd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/etc/xrootd.cfg
xrootd   26494     1  0 10:29 ?        00:00:00 /opt/xrootd/xrootd/bin//cmsd -l /opt/xrootd/xrootd/var/logs/cmslog -c /opt/xrootd/xrootd/etc/xrootd.cfg
xrootd   26515 26443  0 10:29 ?        00:00:00 /opt/xrootd/xrootd/bin/XrdCnsd -d -l /opt/xrootd/xrootd/var/logs/cnsd.log root://gc1-xrdr.uchicago.edu:209
<!--
ps a | grep xrootd
16072 pts/1    Sl     0:00 /opt/xrootd/xrootd/bin//xrootd -l /opt/xrootd/xrootd/var/logs/xrdlog -c /opt/xrootd/xrootd/etc/xrootd.cfg
16091 pts/1    Sl     0:00 /opt/xrootd/xrootd/bin/XrdCnsd -d -l /opt/xrootd/xrootd/var/logs/cnsd.log root://itb2.uchicago.edu:2094
16144 pts/1    Sl     0:00 /opt/xrootd/xrootd/bin//cmsd -l /opt/xrootd/xrootd/var/logs/cmslog -c /opt/xrootd/xrootd/etc/xrootd.cfg
-->
</pre>


---++ Start/Stop !Xrootd
On each node where  !Xrootd is installed (redirector or data server) to start/stop Xrtood use the following. 

To start: <pre class="screen">
cd /opt/xrootd
source setup.sh
vdt-control --on
</pre>

To stop: <pre class="screen">
cd /opt/xrootd
source setup.sh
vdt-control --off
</pre>

---++ Testing your system
 Login on the redirector node (e.g. =redirector.yourdomain.org=) as an ordinary user.

<pre class="screen">cd /opt/xrootd 
source setup.sh 
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$VDT_LOCATION/xrootd/lib 
export LD_PRELOAD=$VDT_LOCATION/xrootd/lib/libXrdPosixPreload.so 
echo &ldquo;This is a test&rdquo; &gt;/tmp/test 
cp /tmp/test xroot://redirector.yourdomain.org:1094///storage/path/test 
cp xroot://redirector.yourdomain.org:1094///storage/path/test /tmp/test1 
diff /tmp/test1 /tmp/test 
</pre>

---++ Advanced installation options and issues

---+++ Disabling !Xrootd's CNS
CNS (Composite Name Space) is an Xrootd service (*XrdCnsd*) that runs on data server nodes to provide a unified name space for all the data in the system.  With this service a client on the redirector can have a composite view of files from each of the data servers.  There have been some reports of problems/issues with  *XrdCnsd* and its use no longer recommended by the Xrootd developers (it will be removed in future releases). 

To disable CNS do the following:
   1. Login to the *redirector* host and stop xrootd: <pre class="screen">
cd /opt/xrootd 
source setup.sh
vdt-control --off
</pre>
   1. Modify =$VDT_LOCATION/post-install/xrootd= deleteing the following lines: <pre class="file">
 if test "`hostname`" = $XRDR_HOST;then
            $XROOTD_HOME/etc/StartXRD -c $XROOTD_HOME/etc/xrootd_cnd.cfg -n xrootd_cnd
 fi
</pre>
   1. Modify Xrootd's configuration (=$VDT_LOCATION/xrootd/etc/xrootd.cfg=) commenting the following line:<pre class="file">
#ofs.forward 3way $(xrdr):2094 mv rm rmdir trunc
</pre>
   1. Restart xrootd: <pre class="screen">
cd /opt/xrootd 
. setup.sh
vdt-control --on
</pre>
   1. Login to each *data server* and stop xrootd::<pre class="screen">
cd /opt/xrootd 
source setup.sh
vdt-control --off
</pre>
   1. Modify Xrootd's configuration (=$VDT_LOCATION/xrootd/etc/xrootd.cfg=) commenting the following line:<pre class="file">
#ofs.notify create closew mkdir | ${xrootdlocation}/bin/XrdCnsd -d -l ${xrootdlocation}/var/logs/cnsd.log root://$(xrdr):2094
</pre>
   1. Restart xrootd: <pre class="screen">
cd /opt/xrootd 
source setup.sh
vdt-control --on
</pre>

---+++ Optimizing disk partitions for performance
To maximize the performance of your Xrootd installation, you should place the _storage_cache_ and _storage_path_  on separate disk partitions.  The partition used for the storage cache should be optimized for the kinds of files anticipated by the application.  In general, you can use the defaults for the  =mke2fs= program since that will tend to optimize for large files. 

The partition used for the _storage_path_ should be optimized to hold a large number of inodes and files.  Since the _storage_path_ is used to hold symlinks to the actual data files, the file system should have a large number of inodes since each symlink will require an inode to hold an entry for the symlink.  In addition, the size of the data blocks used by the file system on the partition should be made as small as possible since symlinks will typically consume an entire block regardless of the size of data block; a small data block size results in less space being wasted.  For an [[http://en.wikipedia.org/wiki/Ext3][ext3 file system]], passing =-b 1024= to =mke2fs= will set the block size to 1024 bytes which is the smallest allowed block size.

Sometimes =mke2fs= is invoked using the wrapper =mkfs -t ext3 ...= that passes the options along. Use =man mke2fs= to see all the options.

%NOTE% Xrootd inherits the characteristics of the underlying file system.  E.g. if you use and ext3 file system you are limited to 31998 subdirectories per one directory, stemming from its limit of 32000 links per inode.  

---+++ Creating disk partitions for the data servers
Here is a general example (there are many approaches taken in practice):
   * Choose your disk or RAID array
   * Create a partition of type Linux using =fdisk= (answer prompts as appropriate): <pre class="screen">
/sbin/fdisk /dev/sda
</pre>
   * Format the partition into an ext3 file system: <pre class="screen">
mkfs -t ext3 /dev/sda1
</pre>
   * Mount the disk in your root file system, e.g. by adding to =/etc/fstab= a line like:<pre class="file">
/dev/sda1               /storage                ext3    defaults        1 2
</pre>
   * Create the needed Xrootd directories as usual:<pre class="screen">
mkdir -p /storage/path
mkdir -p /storage/cache
</pre>

---+++ Increasing the maximum number of file descriptors
For bigger systems the default number of file descriptors (=ulimit -n=, 1024) is insufficient to assure access by many clients simultaneously
The limit should be increased to 65500 descriptors on the redirector and all data servers, for root and for the user under which xrootd is running (=xrootd=).
The procedure below is valid for RHEL 4/5 based Linux distributions (SL[C/F], !CentOS, ...):

Configure the system to accept the desired value for maximum number of open files. Check the value in =/proc/sys/fs/file-max= (=cat /proc/sys/fs/file-max=)to see if it is larger than the value needed for the maximum number of open files.
To increase to 65500 the number of file descriptors:<pre class="screen">echo 65500 > /proc/sys/fs/file-max</pre>
and, to make it persistent across reboots, edit =/etc/sysctl.conf=  to include the line: <pre class="file">fs.file-max = 65500</pre>

Set the value for maximum number of open files (both hard and soft limit) in the file =/etc/security/limits.conf=. To do that add the following line below the commented line that should already be there:<pre class="file"># domain type item value
* - nofile 65500</pre>
<!-- Explanation, not necessary --
This line sets the default number of open file descriptors for every user on the system to 65500. Note that the "nofile" item has two possible limit values under the header: hard and soft. Both types of limits must be set before the change in the maximum number of open files will take effect. By using the "-" character, both hard and soft limits are set simultaneously. The hard limit represents the maximum value a soft limit may have and the soft limit represents the limit being actively enforced on the system at that time. Hard limits can be lowered by normal users, but not raised and soft limits cannot be set higher than hard limits. Only root may raise hard limits.
-->

<!-- These step should be necessary but not sure --
Modify the SSH daemon to remove privilege separation. Edit =/etc/ssh/sshd_config= and find the line
<pre class="file"># UsePrivilegeSeparation yes</pre>
Change it to read:
<pre class="file">UsePrivilegeSeparation no</pre>
Similarly
<pre class="file"># PAMAuthenticationViaKbdInt no</pre>
should be changed to read
<pre class="file">PAMAuthenticationViaKbdInt yes</pre>
For the change to take effect, you'll need to restart the SSH service:
<pre class="screen">service sshd restart</pre>
After making this change, when users log in via SSH they will automatically have the maximum number of open files that was set in =/etc/security/limits.conf=. No additional work is necessary.
-->

---+++ Firewall configuration for the Xrootd redirector

Edit the =/etc/sysconfig/iptables= file to add these lines *ahead* of the REJECT line (if your host has reject rules in its [[http://en.wikipedia.org/wiki/Iptables][iptables]] configuration):
<pre class="file"># begin xrootd-rdr # Xrootd connections  (from anywhere) 
-A RH-Firewall-1-INPUT -m state --state NEW,ESTABLISHED -p tcp -m tcp --dport 1094 -j ACCEPT 
-A RH-Firewall-1-INPUT -m state --state NEW,ESTABLISHED -p tcp -m tcp --dport 1213 -j ACCEPT 
-A RH-Firewall-1-INPUT -m state --state NEW,ESTABLISHED -p tcp -m tcp --dport 2094 -j ACCEPT 
-A RH-Firewall-1-INPUT -m state --state NEW,ESTABLISHED -p tcp -m tcp --dport 2525 -j ACCEPT 
# end xrootd-rdr #</pre>

Check the status of iptables: <pre class="screen">
/etc/init.d/iptables status</pre> 
Restart iptables: <pre class="screen">
/etc/init.d/iptables restart</pre> 
Check the status of the iptables to see the changes: <pre class="screen">
/etc/init.d/iptables status</pre>


---+++ Firewall configuration for an Xrootd data server

Edit the =/etc/sysconfig/iptables= file and add these lines ahead of the REJECT line:
<pre class="file"># begin xrootd-ds # Xrootd connections  (from anywhere) 
-A RH-Firewall-1-INPUT -m state --state NEW,ESTABLISHED -p tcp -m tcp --dport 2525 -j ACCEPT 
# end xrootd-ds #</pre>

Check the status of iptables: <pre class="screen">/etc/init.d/iptables status</pre> 
Restart iptables: <pre class="screen">/etc/init.d/iptables restart </pre> 
Check the status of the iptables to see the changes:  <pre class="screen">/etc/init.d/iptables status</pre>
Now edit the file =$VDT_LOCATION/xrootd/etc/xrootd.cfg= and change =xrd.port any= to =xrd.port 2525=.
 
---+++ !XrootdFS - a POSIX file system for Xrootd
[[http://wt2.slac.stanford.edu/xrootdfs/xrootdfs.html][XrootdFS]] is a POSIX file system for an Xrootd storage cluster based on [[http://en.wikipedia.org/wiki/Filesystem_in_Userspace][FUSE]] (Filesystem in Userspace). !FUSE is a kernel module that intercepts and services requests to non-privileged user space file systems like !XrootdFS.    Install !XrootdFS on nodes where you want a single Xrootd file system to appear, e.g. on an *interactive user node* or a !GridFTP server connected to an Xrootd system.  
%NOTE% For [[http://xrootd.slac.stanford.edu/doc/cvshead/updates.html][releases]] older than _xrootdfs 3.0rcX_ , !XrootdFS requires CNS.


---++++!! Install FUSE 
Three rpm packages must be installed:
   * fuse
   * fuse-libs
   * kernel-module-fuse

This can be checked by using rpm (e.g. =rpm -q [package-name]=) and verifying that the package name and version are returned.  If the package is not installed, rpm will print a message saying that the package is not installed.  This can be done via the *yum* utility (e.g. =yum install fuse fuse-libs=) or rpm commands directly.  Using yum is preferable since it will bring in any dependencies that fuse or the fuse-libs requires and automatically install the correct versions of the fuse kernel modules. Alternatively (for those familiar with building kernels) FUSE can be  downloaded from http://sourceforge.net/projects/fuse/ and built according to instructions provided there.  Note that root privileges are required.  It is essential that the FUSE version ( &gt; 2.7.3) and flavor match your kernel. 

---++++!! Install !XrootdFS 
Setup Pacman and make an installation directory (e.g. =/opt/xrootdfs=).  Install !XrootdFS from the %CACHE% cache. The installation described here is done as root even though services do not  run as root.  Pacman will ask whether you want to trust the cache (answer =yall=).
<pre class="screen">cd /opt/xrootdfs
pacman -get %CACHE%:XrootdFS 
</pre> 

Configure !XrootdFS with the following command where:
   * =$VDT_LOCATION= is the installation directory of the !XrootdFS package (=/opt/xrootdfs=)
   * =xrootd= is the non-privileged user that runs the !XrootdFS service (it must have a login shell)
   * FQDN of the redirector host, e.g.  =redirector.yourdomain.org= 
   * =/xrootd= is the mount point for the file system
   * =/storage/path= is the _storage_path_ directory on the redirector host, as discussed above
<pre class="screen">
cd /opt/xrootdfs
source setup.sh
$VDT_LOCATION/vdt/setup/configure_xrootdfs \
 --user xrootd \
 --cache /xrootd \
 --xrdr-host redirector.yourdomain.org  \
 --xrdr-storage-path /storage/path
</pre>
 
Start !XrootdFS: <pre class="screen">
cd /opt/xrootdfs
source setup.sh
vdt-control --on
</pre>

%NOTE% !XrootdFS will be started automatically with server reboots. 

---++ Troubleshooting
This section lists troubleshooting information and the solutions to some possible errors.

---+++ Log and configuration file locations 
 If any of the tests described above have failed or you are just curious to see what’s going on,  you can find log and configuration files in the following locations:
|*Host*|*Configuration files*| *Log files*| 
|Xrootd – redirector |$VDT_LOCATION/xrootd/etc/xrootd.cfg  <br/>$VDT_LOCATION/xrootd/etc/xrootd_2.cfg  <br/>$VDT_LOCATION/xrootd/etc/StartXRD.cf  <br/>|$VDT_LOCATION/xrootd/var/logs/xrdlog  <br/>$VDT_LOCATION/xrootd/var/logs/cmslog  <br/>$VDT_LOCATION/xrootd/var/logs/xrootd_cnd/xrdlog |
|Xrootd – data server| $VDT_LOCATION/xrootd/etc/xrootd.cfg  <br/>$VDT_LOCATION/xrootd/etc/StartXRD.cf |$VDT_LOCATION/xrootd/var/logs/xrdlog  <br/>$VDT_LOCATION/xrootd/var/logs/cmslog |


---+++ Network problems
If copy commands return errors like: <pre class="screen">(cp from xrootd) cp: cannot stat `xroot://gc1-xrdr.uchicago.edu:1094///storage/path/test-root': Operation canceled
(cp to xrootd) cp: cannot create regular file `xroot://gc1-xrdr.uchicago.edu:1094///storage/path/test-root2': Unknown error 4294967295
</pre>Check the firewall configuration on the data server. Rules are mentioned above.

---+++ Disk problem on small test site
The copy error:
<pre class="screen">(cp to xrootd) cp: cannot create regular file `xroot://gc1-xrdr.uchicago.edu:1094///storage/path/test-xrootd': Unknown error 4294967295</pre> is a generic error used by Xrootd and may have multiple causes.
One is if the free space on data servers is less that the minimum space configured in Xrootd (11GB by default). When this happens =$VDT_LOCATION/xrootd/var/logs/cmslog= on the data server has the following line:<pre class="file">100421 00:05:55 23723 Meter: Insufficient space;  6GB available < 11GB high watermark</pre>
To fix this add to =$VDT_LOCATION/xrootd/etc/xrootd.cfg= a line with smaller minimum space:<pre class="file">cms.space linger 0 recalc 15 min 2% 3g 5% 5g</pre>

---++ References
*Scalla Xrootd:*
   * [[http://xrootd.slac.stanford.edu/][Scalla home]] and [[http://xrootd.slac.stanford.edu/papers/Scalla-Intro.htm][Scalla introduction]]
   * http://xrootd.slac.stanford.edu/doc/dev/ - references from the development release
   * [[http://xrootd.slac.stanford.edu/doc/dev/xrd_config.pdf][Configuration reference]] (PDF), [[http://xrootd.slac.stanford.edu/doc/dev/xrd_config.htm][html]]
   * Update notes to CVS, http://xrootd.slac.stanford.edu/doc/cvshead/updates.html
   * [[http://project-arda-dev.web.cern.ch/project-arda-dev/xrootd/site/index.html][Xrootd/Scalla @ CERN (ARDA)]]

*OSG releated:*
   * A. Baranovski, [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=744&version=2][OSG Document 744-v2: Xrootd evaluation and feature review]]
   * Using Xrootd with SRM: ReleaseDocumentation.BestmanGatewayXrootd or with just !GridFTP, ReleaseDocumentation.GridFTPXrootd


%BR%

---++ *Comments*
%COMMENT{type="tableappend"}%

%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 22 Apr 2010 %BR%
%REVIEW%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = RobGardner
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = SuchandraThapa
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->
