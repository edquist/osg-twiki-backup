%META:TOPICINFO{author="KyleGross" date="1481048003" format="1.1" version="1.16"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%

---++ Introduction
The Tier 3 cluster requires some accounts in order to be usable.   Note what we are describing here is the simplest method to setup accounts across the cluster using password files.  Accounts on clusters are usually managed with a service such as NIS or LDAP.

---++ Controlling password lookups
The =/etc/nsswitch.conf= file controls both host lookups and password lookups.
   * Here is a snippet for options that will work for this example (note simplification: only file lookup is used):<pre class="file">
#passwd: db files ldap nis
passwd:     files
shadow:     files
group:      files

#hosts:     db files nisplus nis dns
hosts:      files dns</pre>

---++ Accounts creation
Since all hosts in the cluster share the user home directories, the user and group IDs must be consistent. 

<!--Make sure that system (existing) accounts/groups have IDs < 500.

User accounts/groups or accounts/groups used for the OSG grid software (*rsvuser*) will have IDs >= 500.
-->

Host are supposed to have the same accounts and passwords.

To ease consistency accounts and groups are created on one host, e.g. =gc1-hn= and replicated to the other hosts in the cluster.
 
   * Home directories must be mounted. *Please test that shared home directories are mounted correctly! NFS must be working  (see ClusterNFSSetup)*
   * Accounts are created with =useradd= utility
   * Accounts to be created:
      * =condor= used by the condor scheduler 
      * =gcuser= (a normal interactive user) 
      * =osgedu= and other accounts, such as the vo group accounts, as needed
   * e.g. you can use the following lines to create the accounts mentioned above:<pre class="screen">useradd condor
useradd gsuser
useradd osgedu 
</pre>
<!--   * (optional) By default, the =useradd= utility creates a separate group for each user.  We may want to define two groups:
      * users - for local, interactive user accounts
      * gridvo - for group accounts for grid users from VOs  
   * Creation of ¨addon¨ files extracting groups/accounts with IDs > 500 (to append to other hosts):
      * =group._addon.gc1= 
      * =passwd._addon.gc1=  
      * =shadow._addon.gc1= (note that shadow is write protected)
      * =gshadow._addon.gc1= (note that gshadow is write protected)
   * Push the files to each of the hosts in the cluster, appending them to the corresponding files as appropriate.
   * Check the files manually to verify that they were updated correctly.
-->
   * Replicate the files to the other machines (as root) using =/home= shared with NFS. If you have no NFS you must copy the files, e.g. using scp:
      * on the host (=gc1-hn=) where you created the user accounts:<pre class="screen">
mkdir /home/rootshared
cd /etc/
cp passwd group shadow gshadow /home/rootshared
</pre>
      * on all the other machines (=gc1-nfs=, =gc1-se=, ...) as root:<pre class="screen">
cd /home/rootshared
cp passwd group shadow gshadow /etc/
chmod 400 /etc/shadow /etc/gshadow
</pre>
      * once you are done replicating the accounts on all the machines, remove the directory used for sharing:<pre class="screen">
rm -rf /home/rootshared
</pre>
      *  Note that user directory have not been created because home directories shared via NFS are assumed

---++ Other consideration
A reminded at the beginning, NIS or LDAP allow a better centralized management of accounts. If you use the replication described in this document you will have to consider also:

   * password update: you may allow password updates only from a specific machine and propagate the proper section of =/etc/shadow= (the one with the users passwords) to the other nodes. A replacement of =passwd= on the other nodes will print a message to direct the users to the correct node.
   * you may limit the users account only on interactive and batch nodes to improve security (no user accounts on server nodes like CE, SE, ...)
   * you may move the home directory of service accounts to a local directory to improve performance and reliability:
      * local directory has to be created with the right permission and ownership on each node using that service account
      * account creation has to point to the right directory (e.g. =adduser -d /local/home/condor condor=)
      * the path of this local home directory has to be the same in all the cluster

%BR%


---++ *Comments*
| Add all other accounts that may be used, also for optional installs (xroot, CE:mis,..) | Main.MarcoMambelli | 12 Feb 2010 - 20:05 |
| PM2RPM_TASK = CE&#60;br /&#62; | Main.RobertEngel | 28 Aug 2011 - 06:13 |
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################ 
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       =  Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DocWorkshop
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->