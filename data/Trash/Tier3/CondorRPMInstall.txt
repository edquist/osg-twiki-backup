%META:TOPICINFO{author="MarcoMambelli" date="1259090960" format="1.1" version="1.4"}%
%META:TOPICPARENT{name="Tier3DocDev"}%
---++ Installing Condor using the RPM distribution
%TOC{depth="3"}%

---++ Introduction
We will use the latest stable release of Condor 7.4.0, released Nov 09, 2009.  

This installation uses the Condor RPM distribution. It can be downloaded form the Condor site or installed using a RPM or yum repository.
US-ATLAS setup a temporary yum repository that can be used for this installation.

Certain operations, like the RPM installation and some system configuration, has to be repeated on each node.
Some steps like the configuration of _condor_config_ are performed only once, e.g. on the head node =osg-ce=, others like the customization of the local condor configuration file is different for each node.

Sharing at least the directory hosting the configuration files allows to simplify a bit the configuration by making it easy to make cluster-wide configuration changes.
Anyway this installation is possible also having no shared directories if the customized _condor_config_ file is replicated on all nodes.

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.4/3_2Installation.html

%BR%
*A note about the directory structure:*
This Condor installation was structured to facilitate upgrades to Condor with minimal effort.  The Condor release directory on each host is =/opt/condor= which is a soft link to the actual Condor installation directory: =/nfs/condor/condor-7.4.0=, which includes the version number and is created during the RPM installation.  The motivation for this choice will hopefully become clear in the [[#Upgrades][Upgrades]] section below.


---++ Preparing the yum install
Create the file =/etc/yum.repos.d/condor.repo= with the following content.
<verbatim>
[condor]
name=Condor Repository for RHEL/CentOS/SL $releasever $basearch
failovermethod=priority
baseurl=http://dukpc23.fnal.gov/~benjamin/condor/v7.4.0/RHEL/5/
enabled=1
gpgcheck=0
</verbatim>

---++ Condor Installation and Configuration
*On each node* Install Condor from the repository (answer y to question)
<verbatim>
yum install condor
ln -s /opt/condor-7.4.0 /opt/condor
</verbatim>

---+++Fix some files
*Only on one node, e.g. gc1-ce* Edit =/opt/condor/etc/condor_config= and copy to correct area.  Using a shared configuration directory, this has to be done for one node only, e.g. the *batch head node* =gc1-ce=.   No need to repeat this for the *worker nodes* in the batch queue.
   * Edit =condor_config= and replace the following attributes. <verbatim>
RELEASE_DIR = /opt/condor
LOCAL_DIR = /scratch/condor
LOCAL_CONFIG_FILE = /nfs/condor/condor-etc/condor_config.$(HOSTNAME).local
CONDOR_ADMIN = root@$(FULL_HOSTNAME)
# The following should be your T3 domain
UID_DOMAIN = yourdomain.org
FILESYSTEM_DOMAIN = $(UID_DOMAIN)
ALLOW_WRITE = *.$(UID_DOMAIN)</verbatim>
   * at the end of the file add<verbatim>
IN_HIGHPORT = 9999
IN_LOWPORT = 9000
# The following should be the full name of the head node
CONDOR_HOST = gc1-ce.yourdomain.org    
SEC_DAEMON_AUTHENTICATION = required
SEC_DAEMON_AUTHENTICATION_METHODS = password 
SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi,kerberos
SEC_PASSWORD_FILE = /local/condor/condor_credential 
ALLOW_DAEMON = condor_pool@* </verbatim>
   * Now copy to the proper area. (This file is shared by all nodes).  If the node is not also the NFS server, you will have to use scp instead of cp because of root squashing. <verbatim>
scp /opt/condor/etc/condor_config  gc1-nfs:/nfs/condor/condor-etc/condor_config </verbatim>

*This is different for each node* Edit =/opt/condor/local.&lt;hostname>/condor_config.local= and put in proper place. The editing of this file depends on the type of node. Choose the appropriate one from below.
   * For *Batch Head Node*, e.g. =gc1-ce=: 
      * comment out the following lines<verbatim>
#CONDOR_HOST
#RELEASE_DIR
#LOCAL_DIR
#CONDOR_ADMIN
</verbatim>
      * edit the following attribute<verbatim>
DAEMON_LIST= COLLECTOR, MASTER, NEGOTIATOR
</verbatim>
   * For *Worker Nodes* of the batch queue, like =gc1-cXXX=:
      * comment out the following lines<verbatim>
#CONDOR_HOST
#RELEASE_DIR
#LOCAL_DIR
#CONDOR_ADMIN
</verbatim>
      * edit the following attribute<verbatim>
DAEMON_LIST=MASTER,STARTD</verbatim>
   * For *Interactive Nodes*, like =gc1-ui=:
      * comment out the following lines<verbatim>
#CONDOR_HOST
#RELEASE_DIR
#LOCAL_DIR
#CONDOR_ADMIN
</verbatim>
      * edit the following attribute<verbatim>
DAEMON_LIST= MASTER, SCHEDD
</verbatim>
   * For all of them. Copy =/opt/condor/local.&lt;hostname>/condor_config.local= to the proper area. If the node is not also the NFS server, you will have to use scp instead of cp <verbatim>
scp  /opt/condor/local.<hostname>/condor_config.local  gc1-nfs:/nfs/condor/condor-etc/condor_config.<hostname>.local
</verbatim>

*On all nodes* Edit =/opt/condor/condor.sh= and copy it to correct area.  This is for all batch and interactive nodes.
   * =/opt/condor/condor.sh= should look like:<verbatim>
CONDOR_CONFIG="/nfs/condor/condor-etc/condor_config"
export CONDOR_CONFIG
PATH="/opt/condor/bin:/opt/condor/sbin:$PATH"
export PATH
</verbatim>
   * Now copy to the proper area <verbatim>
cp /opt/condor/condor.sh /etc/sysconfig/condor
</verbatim>
Optionally =condor.sh= can also be copied to =/etc/profile.d/condor.sh= of the interactive nodes, to have it execute on login and have Condor commands in the default environment for all the users.

*For all nodes* Edit /opt/condor/etc/examples/condor.init and copy to the correct area.  All nodes.
   * change =/usr/sbin/$prog= to =/opt/condor/sbin/$prog= in the file (search for it)       
   * copy to correct area<verbatim>
cp /opt/condor/etc/examples/condor.init /etc/init.d/condor
chmod a+x /etc/init.d/condor
chkconfig --level 235 condor on
</verbatim>

Create local directories:<verbatim>
mkdir /var/run/condor
chown condor:condor /var/run/condor
mkdir /scratch/condor
chown condor:condor /scratch/condor
cd /opt/condor/local.<hostname>/
mv execute log spool /scratch/condor/
</verbatim>
set password <verbatim>
source /etc/sysconfig/condor
condor_store_cred -c add </verbatim> (enter password. The password should be same on all nodes of the T3 cluster.) 

---++ Changes to iptables
Edit the */etc/sysconfig/iptables* file to add these lines ahead of the REJECT line and the COMMIT statement<verbatim>
-A RH-Firewall-1-INPUT  -s <network_address> -m state --state ESTABLISHED,NEW -p tcp -m tcp --dport 9000:10000 -j ACCEPT  
-A RH-Firewall-1-INPUT  -s <network_address> -m state --state ESTABLISHED,NEW -p udp -m udp --dport 9000:10000 -j ACCEPT </verbatim>
where the network_address is the address of the intranet of the T3 cluster, e.g. 192.168.192.0/18 (Same as the extranet if your T3 does not have a separate intranet, e.g. 146.139.33.0/16 for Argonne).

Restart the firewall:<verbatim>
/etc/init.d/iptables restart
</verbatim>

---++ Start and test Condor
Condor is starting automatically during reboots. You can start it manually typing <verbatim>
/etc/init.d/condor start </verbatim>  (should say ok)

You can check if Condor is running correctly<verbatim>
condor_config_val log   # (should be /scratch/condor/log)
cd /scratch/condor/log
#check master log file
less MasterLog
# verify the status of the negotiator
condor_status -negotiator</verbatim>

More test below.

---++ Upgrades
As explained in the section about the directory structure, this Condor installation was structured to facilitate upgrades with minimal effort.  The Condor directory on each host is =/opt/condor=. This is a link to the real Condor installation directory that has also the version number in the name (e.g. =/opt/condor/condor-7.4.0=).

Only one version of Condor will be used at the time but linking =/opt/condor= to a new directory, e.g. =/opt/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if desired simply changing one link (e.g. if the new installation is not working properly). 

---++ Setup
Executing <pre>source /opt/condor/condor.sh</pre> will add the condor commands to your path and set the =CONDOR_LOCATION= variable.
If you edited and copied the =condor.sh= and =condor.csh= files in =/etc/profile.d/= as described above, Condor will be in the environment of every user automatically, no need to source any file.


---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status= and submit test jobs with =condor_submit=. 
Check CondorTest for more.

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 17 Nov 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!--
# ---++ Additional notes
 <verbatim>mkdir /export/home/shared
cd /export/home/shared
wget http://newbio.cs.wisc.edu/zkm/condor-7.4.8-linux-x86-rel5-dymamic-1.x86_64.rpm
cd /root/
rpm -i --test /export/home/shared/condor-7.4.0-linux-x86_64-....
rpm -i /export/home/share/condor-7.4.0 ....
cd /opt/condor-7.4.0/etc/examples
#edit condor.init</verbatim>

look for "error program not found"

<font face="#mce_temp_font#"> /usr/sbin/$prog --&gt; goes to /opt/condor/sbin/$prog</font>
<verbatim>cp /opt/condor-7.4.0/condor.sh /etc/sysconfig/condor
#edit condor
 CONDOR_CONFIG="/export/share/condor-etc/condor_config"
cd ../../ 

cp etc/examples/condor.init /etc/init.d/condor
/sbin/chkconfig --level 235 condor on
/sbin/chkconfig --list condor
chmod 755 /etc/init.d/condor
mkdir /var/run/condor
chown condor:condor /var/run/condor
cd etc/
# edit condor_config
  RELEASE_DIR = /opt/condor
  LOCAL_DIR = /local/condor
  LOCAL_CONFIG_FILE = /export/share/condor-etc/condor_config.$(HOSTNAME).local
  CONDOR_ADMIN = <admin email>
  UID_DOMAIN = cs.wisc.edu
  FILESYSTEM_DOMAIN = $(UID_DOMAIN)
  ALLOW_WRITE = *.$(UID_DOMAIN)
  at the end of the file add
  SEC_DAEMON_AUTHENTICATION = required
  SEC_DAEMON_AUTHENTICATION_METHODS = password #change this to different method
  SEC_CLIENT_AUTHENTICATION_METHODS = password,fs,gsi,kerberos
  SEC_PASSWORD_FILE = /local/condor/condor_credential 
  ALLOW_DAEMON = condor_pool@* 

cd ../local.glow-223 (condor manager)
# edit condor_config.local
  CONDOR_HOST = glow-c223.es.wisc.edu 
  #LOCAL_DIR
  #CONDOR_ADMIN
  DAEMON_LIST= COLLECTOR, MASTER, NEGOTIATOR (for condor master)
cp condor_config.local /export/share/condor-etc/condor_config.glow-c223.local 
cp /etc/sysconfig/condor /opt/condor/condor.sh 

#checking
source /opt/condor/condor.sh
echo $CONDOR_CONFIG  should be /export/share/condor-etc/condor_config
condor_config_val RELEASE_DIR  should be /opt/condor
condor_config_val LOCAL_DIR   should be /local/condor
#set up local space
mkdir /local/condor
chown condor:condor /local/condor
cd /opt/condor/local.glow-c223/
mv execute/ log/ spool/ /local/condor/
#set password
condor_store_cred -c add  (enter password) 

#start condor
/etc/init.d/condor start   (should say ok)
#check
condor_config_val log   (should be /local/condor/log)
cd /local/condor/log
#check Master log
condor_status -negotiator
 
 </verbatim> 


Instructions - transcript v2:

<verbatim>
#rpm -i condor.rpm
# make sure that there is a repository configuration file /etc/yum.repos.d/condor.repo

yum install condor

ln -s /opt/condor-7.4.0 /opt/condor

## set up init.d so condor starts automatically

vi /opt/condor/condor.sh
+ fix CONDOR_CONFIG to /export/share/condor-etc
+ fix path to /opt/condor/bin /opt/condor/sbin 
cp /opt/condor/condor.sh /etc/sysconfig/condor

chmod 755 /opt/condor/etc/examples/condor.init
vi /opt/condor/etc/examples/condor.init
+ change /usr/sbin/$prog to /opt/condor/sbin/$prog
cp /opt/condor/etc/examples/condor.init /etc/init.d/condor
chkconfig --level 235 condor on

mkdir /var/run/condor
chown condor:condor /var/run/condor


## central manager only:
RELEASE_DIR = /opt/condor
LOCAL_DIR = /local/condor
LOCAL_CONFIG_FILE = /export/share/condor-etc/condor_config.$(HOSTNAME).local
#CONDOR_ADMIN =
#UID_DOMAIN = cs.wisc.edu
#FILE_SYSTEM_DOMAIN = $(UID_DOMAIN)
COLLECTOR_NAME =
ALLOW_WRITE = *.$(UID_DOMAIN)

# search for LOWPORT and add
IN_HIGHPORT = 9999
IN_LOWPORT = 9000


at end of file:
SEC_DAEMON_AUTHENTICATION = Required
SEC_DAEMON_AUTHENTICATION_METHODS = PASSWORD
SEC_CLIENT_AUTHENTICATION_METHODS = PASSWORD,FS,GSI,KERBEROS
SEC_PASSWORD_FILE=/local/condor/condor_credential
ALLOW_DAEMON=condor_pool@*

## On all condor nodes
vi /opt/condor/local.*/condor_config.local
#CONDOR_HOST
#RELEASE_DIR
#CONDOR_ADMIN
DAEMON_LIST

cp /opt/condor/local.*/condor_config.local /export/share/condor-etc/condor_config.&lt;name>.local
#this will not work on hosts not owning the NFS shared directory (root squash)
#from the NFS master
scp &lt;name>:'/opt/condor/local.*/condor_config.local' /export/share/condor-etc/condor_config.&lt;name>.local
#E.g. scp glow-c226:'/opt/condor/local.*/condor_config.local' /export/share/condor-etc/condor_config.glow-c226.local

## set up condor's local space
mkdir /local/condor
chown condor:condor /local/condor
cd /opt/condor/local.&lt;name>/
mv execute log spool /local/condor

## set up password authentication
source /opt/condor/condor.sh
condor_store_cred -c add

## iptables may interfere with condor:
# set
-->