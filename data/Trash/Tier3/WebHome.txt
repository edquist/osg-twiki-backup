%META:TOPICINFO{author="MarcoMambelli" date="1258993426" format="1.1" version="1.34"}%
---+!! %MAKETEXT{"Welcome to the OSG [_1] Twiki" args="<nop>%WEB%"}%
%TOC%

This page documents a variety of setup and configuration options that are likely to be used by US Atlas and CMS Tier3 sites and is intended to be used in support of Atlas and CMS specific site configurations that will be posted on designated Atlas and CMS wikis. When complete, the following documentation will be reviewed by the development teams to ensure that these instructions follow best practices as recommended by the developers and the VDT team. Components include:

|| Security(Gridmap, GUMS) || Compute Element + RSV | Storage Element | Local Storage | SQUID ||
|| Condor Cluster (Tarfile/RPM + Hawkeye + Quill) | Cluster Manager (TBD) ||
|| SL 5.4 ||

Storage Element Options: 
   1 !BeStMan-Gateway/NFS
   1 !BeStMan-Xrootd
   1 !BeStMan-Hadoop

Local Storage Options:
   1 !GridFTP
   1 !GridFTP-Xrootd
   1 Xrootd stand alone

---++ !! Documentation
More information coming soon!
This section is currently a work in progress with mainly a skeleton and links to related documents. 

This section describes how to deploy a simple Tier 3 cluster starting from the OS and building different components on top of it.
The documents are in a hands-on format and present tested examples based on specific choices. Reference documents are cited to allow admins to utilize alternative configurations.


---+++ Site planning
The [[ReleaseDocumentation.SitePlanning][OSG SitePlanning]] document helps understanding OSG components and presents information on possible configurations.  


---+++ What is at Tier 3 Cluster?
An OSG Tier-3 (T3), in general, is a small to medium IT resource managed directly by a scientist group, therefore it should not require high technical expertise and be easy to maintain.

It will enable one or more of the following:
   * provide computers for interactive computing
   * access computing resources on the Grid
   * transfer data to and from the Grid
   * preform local computation using a batch queue
   * store big amount of data using a distributed file system

A T3 aims to limit the effort necessary to setup and maintain it.
   * Less than 1 week FTE to set up a T3g, after equipment arrives and infrastructure is ready.
   * Small fraction of an FTE for maintenance.

A T3 may come in a big variety of possible configurations. 
These documents are presenting a set of components that can be installed in a T3 and also a couple of typical configurations.
While users are free to use these documents and change whatever they prefer, it is recommended  that they follow the provided examples or at least use the components described and tested in these documents.

Further recommendations and examples for the scientists may come from their collaborations or their campuses.


---+++Components of a Tier 3 Cluster
A cluster is a set of computers, nodes, that work together providing different functionalities.
The following sections will describe different elements that you may find in a Tier 3 (T3): interactive nodes, computers allowing the users to log in and run their applications, a batch queue, allows user to schedule jobs that will run on one or more of the batch nodes, storage nodes, computers allowing to store efficiently big amounts of data.
[[ClusterComponents][Read more]]

---+++ Cluster description 

This section collects a brief explanation of some concepts, in case the user is not familiar with them. These are necessary to understand these documents.

The hosts are organized in a cluster: machines with different functions and a network infrastructure.

We suppose that all machine are networked together:
   * the intranet is the network connecting all the hosts
   * the outside network is the network where user hosts and clients accessing the cluster reside, this can be the public internet or a private network
   * at least some machines, headnodes, are directly visible from the outside network
   * intranet and outside network may be the same (no need to have two networks)

The hosts have different functions:
   * head node - node with services accessible from the outside netwok
   * worker nodes - series of machines on the intranet, all identical (except the name), used to run jobs
   * servers - servers with specific functions useful for the cluster
Distinct functions may be on the same host.

A Local Resource Manager (LRM) allows to schedule jobs on the worker nodes without accessing them directly. Users can contact the head node providing the LRM service. 

A shared file system is a disk space shared among the hosts of the cluster, e.g. NFS, xrootd.

Clusters come in different sizes. We arbitrarily define a cluster to be small if it has less than 50 job slots.

Clusters can both provide to the Grid and/or access from the Grid several OSG services. Examples of OSG services are:
   * Compute element (CE)
   * Storage element (SE)
      * Classic SE - gsiftp
      * Storage Resource Manager (SRM)
   * GUMS

For clarity we give a name to the cluster: =gc1= (Grid Cluster 1). Each host in the cluster will have a unique fully qualified domain name (FQDN).  The FQDN includes a local hostname and a parent domain name (here "yourdomain.org"). 
The following is a list of roles that the hosts may have within the cluster. In this example, we use the roles to define the hostnames. This means that the host that is your compute element (if you have a CE) has an FQDN =gc1-ce.yourdomain.org=.
<!-- del>There is no one-to-one correspondence between these names and the hosts. If you have no host covering that role, then there will be no host with that name. If a host is covering two or more roles, then all those names will refer to the same host, e.g. they may be _cnames_. The list is:
</del -->
There is no one-to-one correspondence between these names and the hosts. If you have no host covering that role, then there will be no host with that name. If a host is covering two or more roles, then all those names will refer to the same host and can be _cnames_, e.g. the Storage Element and the Xrootd redirector may be the same host. The list is:
   * =gc1-ce.yourdomain.org= a Compute Element (and cluster head-node) 
   * =gc1-se.yourdomain.org= a Storage Element 
   * =gc1-xrdr.yourdomain.org= xrootd redirector
   * =gc1-gums.yourdomain.org= a GUMS server
   * =gc1-nfs.yourdomain.org= an NFSv4 server 
   * =gc1-ui.yourdomain.org=  your user interface machine
   * =gc1-c001.yourdomain.org= compute node 1
   * =gc1-c002.yourdomain.org= compute node 2
   * =gc1-c003.yourdomain.org= compute node 3
   
In the documents the machines will sometimes be referred to by their local hostname, e.g. =gc1-ce= instead of =gc1-ce.yourdomain.org=


---+++ Getting started
Hosts setup

---++++ Base systems
Hosts are supposed to have an operating system.
The reference OS used in this document is Scientific Linux 5.4, a linux distribution based on RHEL 5.4.
Anyway these instructions and examples should work for any RHEL based OS (e.g. RHEL, SL variants, !CentOS, ...) with little or no variation.
The OS installation is not covered by this document. ClusterOSInstall contains pointers to some documentation for your convenience.

---++++ Customization of the base system
To ease the following installation steps and to have a common base here are established some conventions and customizations of the base OS, like user accounts, a shared file system and others.

File system structure:
<pre>
/exports - root of the NFSv4 pseudo filesystem
/nfs/ - collects the directories shared using NFS
/home - home directories (shared)
/storage - data disk used for distributed file system (xrootd)
</pre>

A first option, suitable for small Tier 3s makes use of shared directories to ease administration.
ClusterNFSSetup describes how to setup the server and the clients for NFSv4

A second option will explore the installation without the use of shared disk space.

---++++ Installing Pacman
Pacman is a package management program used to instal most of OSG software.
ReleaseDocumentation.PacmanInstall describes how to install Pacman.
Choose  =/nfs/osg/app/pacman= as installation directory:
<pre>
cd /nfs/osg/app
wget http://atlas.bu.edu/~youssef/pacman/sample_cache/tarballs/pacman-3.29.tar.gz
tar --no-same-owner -xzvf pacman-3.29.tar.gz
cd pacman-3.29
source setup.sh
cd ..
ln -s pacman-3.29 pacman
</pre>
Once Pacman is installed you can  =source /nfs/osg/app/pacman/setup.sh= and you are ready to install OSG software using Pacman.


---+++ Distributed file system 
xrootd is a shared file system distributed across several hosts, each one serving part of it. It is optimized to store data files, not for programs or system files.
It is used at its best if the jobs are running on the host serving their input files.

You may want to install xrootd if you have big data files and would like an uniform file system space using the disk space available on several nodes (dedicated data servers or the worker nodes).

XrootdInstall describes the installation of the xrootd file system


---+++ Quick Guide for setting up a Condor Cluster 
Each cluster needs a _Local Resource Manager_ (LRM, also called _queue manager_ or _scheduler_) to schedule the jobs on the worker nodes. Common LRMs are Condor, LSF, PBS, SGE; if you have one installed or a preferred one you can use that one. 
If not, this section describes how to install and configure Condor 7.4, the latest production version of Condor.
Here are two different ways to install Condor:
   * CondorSharedInstall - an easy way to add Condor to small clusters using a shared file system 
      * it is easy to add new nodes
      * configuration changes and updates are also very quick
      * cons: shared installation may slow down several nodes using it
   * CondorRPMInstall - Condor is installed on each node using the RPM packages of Condor
      * no contention for shared files
      * only option if no shared file system
      * cons: some more work to add nodes 
      * cons: updates need to be installed on all nodes 

The base configuration of condor can be modified to add some extra features:
   * CondorServiceJobSlotSetup describe how to add a service jobs slot to each node. This will allow to install application software or to run monitoring jobs on the worker nodes without modify the scheduling of the regular jobs
   * CondorHawkeyeSetup describes how to setup Condor's Hawkeye and shows an example using Hawkeye to conditionally schedule  or suspend jobs depending on the activity on the worker node


---+++ User client
The user client machine is a host that allows users to login and provides them with software to access Grid services and specific VO software. This document covers only the installation of Grid clients, leaving to the VOs the documentation of their software.

SiteCoordination.GridClientTutorial shows the installation of the Grid client software.
Note that users will need to [[ReleaseDocumentation.GridCertificateRequestTutorial][request a certificate]] and [[ReleaseDocumentation.RequestVOMembershipTutorial][register in a VO]] before being able to use the Grid.

Examples from VOs:
   * ATLAS VO: https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SoftwareInstallation
 

---+++ Worker node client
The worker node client is used to allow non interactive jobs (the ones submitted using the Grid or Condor) to access the Grid.
We are installing the worker node client in =/opt/wn=, linked to =/nfs/osg/wn= using the instruction in the OSG release documents: ReleaseDocumentation.WorkerNodeClient

In short, on the node chosen for the installation, e.g. =gc1-ce=, you can use the following:<pre>
source /nfs/osg/app/pacman/setup.sh
mkdir /nfs/osg/wn
ln -s /nfs/osg/wn /opt/osg
cd /opt/wn
pacman -get http://software.grid.iu.edu/osg-1.2:wn-client
ln -s /etc/grid-security/certificates /opt/wn/globus/TRUSTED_CA
</pre>

On all other nodes that may run jobs, e.g. the worker nodes:<pre>
ln -s /nfs/osg/wn /opt/osg
</pre>

Alternatively, if there is no shared file system or if there is a load concern, the worker node client can be installed locally, in =/opt/wn= on all the worker nodes.

---+++ Grid users authentication
There are 3 options:
   * use a gridmap file
   * rely on an external GUMS server
   * install a GUMS server

Links:
   * ReleaseDocumentation.GUMSHandsOn
   * ReleaseDocumentation.InstallConfigureAndManageGUMS
   * ReleaseDocumentation.GridColombiaInstallGUMS

---+++ Installing a CE
Installing the OSG CE

Links:
   * ReleaseDocumentation.InstallCETutorial

---+++ Installing a SE

The T3 sites would probably want to install !BeStMan-gateway on top of NFS or some other FS in order to receive  datasets at  their site automatically via some the data subscription mechanism. A grid storage element allows authenticated and authorized connections from the Tier 1 and Tier 2 sites to deposit data at  Tier 3. 

Please follow [[BeStManGateway][this installation guide]] to setup !BeStMan-gateway.

If you need to install a standalone !GridFTP server or an additional !GridFTP server for your !BeStMan installation , please, follow [[ReleaseDocumentation/GsiFtpStandAlone][this Installation guide]]

!BeStMan-gateway server could be installed on top of !xrootd distributed file system. The installation procedure and configuration is somewhat different from the procedure described above.  Please follow [[BeStManGatewayXrootd][this installation guide]] to setup !BeStMan-gateway for !Xrootd.

Links:
   * ReleaseDocumentation.BestmanGateway
   * ReleaseDocumentation.InstallSETutorial
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SetupSE
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/ThroughputCleanup

---+++ Installing RSV
 
---+++ Squid Web proxy

---+++ Network performance

Links:
   * ReleaseDocumentation.NetworkPerformanceToolkit
   * ReleaseDocumentation.NetworkPerformanceTutorial


Dump of information: Tier3DocDev
 

---++ !! <nop>%WEB% Web Utilities
<form action='%SCRIPTURLPATH{"search"}%/%WEB%/'>
   * <input type="text" name="search" size="22" />&nbsp;<input type="submit" class="twikiSubmit" value="%MAKETEXT{"Search"}%" /> - [[WebSearchAdvanced][%MAKETEXT{"advanced search"}%]]
   * WebTopicList - all topics in alphabetical order
   * WebChanges - recent topic changes in this web
   * WebNotify - subscribe to an e-mail alert sent when topics change
   * WebRss, WebAtom - RSS and ATOM news feeds of topic changes
   * WebStatistics - listing popular topics and top contributors
   * WebPreferences - preferences of this web
</form>


   * Fig. 1- Tier 3 cluster components: <br />
     <img src="%ATTACHURLPATH%/gc-dependencies-wb.png" alt="gc-dependencies-wb.png" width='775' height='629' />    

%META:FILEATTACHMENT{name="bestman-gateway.jpg" attachment="bestman-gateway.jpg" attr="" comment="" date="1258476644" path="bestman-gateway.jpg" size="38240" stream="bestman-gateway.jpg" tmpFilename="/usr/tmp/CGItemp5500" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="gc-dependencies-wb.png" attachment="gc-dependencies-wb.png" attr="" comment="Fig. 1- Tier 3 cluster components" date="1258993425" path="gc-dependencies-wb.png" size="100313" stream="gc-dependencies-wb.png" tmpFilename="/usr/tmp/CGItemp11663" user="MarcoMambelli" version="1"}%
