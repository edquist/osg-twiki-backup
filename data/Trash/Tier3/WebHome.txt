%META:TOPICINFO{author="DanFraser" date="1259692683" format="1.1" reprev="1.50" version="1.50"}%
---+!! %MAKETEXT{"Welcome to the OSG [_1] Twiki" args="<nop>%WEB%"}%


%RED% Note: This document is under development and changing rapidly %ENDCOLOR%

%TOC%

---++ Document Purpose and Overview

This document was designed as a hands-on guide to enable US Atlas and US CMS Tier-3 system administrators to deploy a new cluster/grid. The basic cluster/grid installation creates a baseline infrastructure from which experiment specific application components can be layered on top of to complete the intended functionality of the Tier-3 system. Experiment specific software (e.g. Phedex, DQ2, CRAB, Athena, ...) use and installation is NOT covered in this documentation and the interested reader may wish to consult Atlas and CMS websites for this information.

Users of this document are expected to have a basic understanding of UNIX system administration but do not need to be familiar with cluster/grid systems; there are many explanations throughout this guide to assist the first time cluster/grid installer. As such, this document also serves as an introductory entry point into the larger set of OSG documentation.

The documents below are in a hands-on format and present tested examples based on specific choices. Reference documents are cited to provide further background into each of the components as well as to allow admins to customize their configurations. 

---++Tier 3 Concepts and Components
This section explains what a Tier-3 is and introduces concepts and defaults that are useful to understand the Tier 3 documentation.

---+++ What is a Tier-3 Cluster?
An OSG Tier-3 (T3), in general, is a small to medium IT cluster/grid resource targeted at supporting a small group of scientists. 

T3 systems typically provide one or more of the following capabilities:
   * access to local computational resources using a batch queue
   * interactive access to local computational resources
   * storage of large amounts of data using a distributed file system
   * access to external computing resources on the Grid
   * the ability to transfer large datasets to and from the Grid
  
T3s can also offer computing resources and data to fellow grid users

A T3 is not always managed by a professional system administrator, so an important goal of this documentation is to try and minimize system administrator effort required to set up and operate one of these systems. Even with minimal settings however, system administration should be taken seriously and it is not un-reasonable to dedicate at least 1/4th of an FTE for this task, not including initial setup, which can take a week or longer depending upon site policies and capabilities.

The nodes of a T3 are organized in a cluster: computers (often with different functions) tightly connected via a network infrastructure.
Since there are many ways that a cluster can be configured, these documents present a typical configuration [link here] utilizing recommended best practices together with component by component instructions for installing a T3. Further recommendations and examples for the administrators may come from the collaborations or from their campuses. 

This guide is broken into two parts: Basic Cluster Configuration and T3 Middleware Components. (A third part that is not included in this guide is the scientific applications.)

---+++Components of a Tier 3 Cluster
In a Tier-3 cluster, there are three classes of cluster nodes:
   1 Batch queue worker nodes that execute jobs submitted via the batch queue. (Are there any cases where interactive nodes are also worker nodes).
   1 Shared Interactive nodes where users can log in directly and run their applications. 
   1 Nodes that serve various roles such as batch queues, file systems, or other middleware components (see below). In some cases one node can host multiple roles while in other cases for a variety of reasons (including security), nodes should be set aside for single purpose uses.

The ratio of interactive nodes to worker nodes is an important consideration when setting up a cluster. In general, the most efficient use of resources is to submit the vast majority of the workloads into the batch queues. This is the setup for the vast majority of CMS and Atlas T2 and T3 sites.  Shared interactive nodes are more difficult to administer, since there are fewer pre-defined constraints on the nodes, and these can easily take up a significant portion of a system administrators time with unpredictable users. Still, interactive nodes are often useful for scientists and may be required for example for those who need to login to submit batch jobs or to configure and debug short running jobs in an interactive environment, prior to submitting longer running jobs to the batch queue system. In general, administrators should determine the needs of their users and configure as few shared interactive nodes as possible. 

A list of important components (or roles) for the T3 architecture is as follows:

   * NFSv4 Server -- Using NFS to create a shared file system is the easiest way to set up and maintain a T3.
   * Condor Batch Queue -- A batch queue system is strongly recommended for T3s. Condor was chosen as being one of the most familiar and easiest to support by the OSG team. 
   * 


[[ClusterComponents][Read more on Cluster Component descriptions]]

---++++Naming Convention for Tier 3 cluster nodes
To simplify documentation and examples we give a name to the cluster: =gc1= (Grid Cluster 1). Each node in the cluster, has a unique fully qualified domain name (FQDN).  The FQDN includes a local hostname and a parent domain name (here "yourdomain.org"). 
The following is a list of roles (abstract functionalities) that nodes may have within the cluster. We use the role to define the hostnames. This means that the host that is your compute element (if you have a CE) has an FQDN =gc1-ce.yourdomain.org=.
There is no one-to-one correspondence between these names and the nodes. If there are no nodes covering a role, then there will be no node with that name. If a node is covering two or more roles, then all those names will refer to the same node and can be _cnames_, e.g. the Storage Element and the Xrootd redirector may be the same host, making gc1-se and gc1-xrdr two names of the same node. 
A basic list is:
   * =gc1-ui.yourdomain.org=  The user interface machine, can also be referred to as Interactive node 1
   * =gc1-wn001.yourdomain.org=  Worker Node 1
   * =gc1-wn002.yourdomain.org=  Worker Node 2
   * =gc1-wn003.yourdomain.org=  Worker Node 3
   * =gc1-nfs.yourdomain.org=  the NFSv4 server
   * =gc1-hn.yourdomain.org=  the queue head-node 
   * =gc1-se.yourdomain.org=  the Storage Element 
   * =gc1-ce.yourdomain.org=  the Compute Element 
   * =gc1-xrdr.yourdomain.org= the xrootd redirector
   * =gc1-gums.yourdomain.org=  the GUMS server 

In the documents the machines will sometimes be referred to by their local hostname, e.g. =gc1-ce= instead of =gc1-ce.yourdomain.org=

---+++Network configuration
Your campus or your department will provide you a network connection that in these documents we refer as _extranet_, i.e. the network that connects also to the world outside of your Tier 3 cluster.

[[ClusterNetwork][Read more]] about the different topologies and the default configuration, including subnet IP address, for the network connecting the nodes of a Tier 3 used in the examples.

---+++ Site planning
The [[ReleaseDocumentation.SitePlanning][OSG SitePlanning]] document helps understanding OSG components and presents information on possible configurations.  


---++Component Installation and Setup
All the modules in this section can be installed separately or grouped together on different nodes of the cluster.
Fig.1 shows the dependencies between different set of instructions.
   * Fig. 1- Tier 3 cluster components and dependencies: <br />
     <img src="%ATTACHURLPATH%/gc-dependencies-wb-sm.png" alt="gc-dependencies-wb-sm.png" width='387' height='314' />    

The instructions in this section are organized by topic, not by node.
The next section, [[][Cluster Examples]] will describe how to install from the ground up the nodes of a cluster.

---+++Preliminaries
Presents instructions regarding the installation of all the nodes and their initial setup

To ease the following installation steps and to have a common base here are established some conventions and customizations of the base OS, like user accounts, a shared file system and others.

---++++ Base systems
Hosts are supposed to have an operating system.
The reference OS used in this document is Scientific Linux 5.4, a linux distribution based on RHEL 5.4.
Anyway these instructions and examples should work for any RHEL based OS (e.g. RHEL, SL variants, !CentOS, ...) with little or no variation.
The OS installation is not covered by this document. ClusterOSInstall contains pointers to some documentation for your convenience.

---++++ Customization and setup of the file system (included eventual NFS)
Here part of the standard file system structure in a Tier 3:
   * =/exports= - root of the NFSv4 pseudo filesystem (directory tree exported by NFS)
   * =/nfs= - collects the directories mounted from NFS
   * =/home= - home directories (shared)
   * =/opt= - grid software
   * =/scratch= - local scratch area
   * =/storage= - data disk used for distributed file system (xrootd)

Read more about the file system structure in ClusterFileSystem.

A first option, suitable for small Tier 3s makes use of shared directories to ease administration. ClusterNFSSetup describes how to setup the server and the clients for NFSv4

A second option performs the installation without the use of shared disk space.
If you decide not to have any NFS file system you will be limited in the possible options (e.g. no shared Condor installation) and some of the administrative tasks will be more difficult.

---++++SSH Configuration
To make the cluster secure we'll use a host-based SSH key infrastructure. This section will introduce keys, agents, how to configure them and how to use them to ease in a safe way the administration of the T3.

ClusterSSHSetup describes an example of SSH configuration and use. That is a suggested setup, make sure to follow the policies and recommendation in place at your university or laboratory. 

---++++ Users account setup

ClusterAccountsSetup describes the configuration of the user accounts

---+++ Installing Pacman
Pacman is a package management program used to instal most of OSG software.
ReleaseDocumentation.PacmanInstall describes how to install Pacman.
Choose  =/nfs/osg/app/pacman= as installation directory:
<pre>
cd /nfs/osg/app
wget http://atlas.bu.edu/~youssef/pacman/sample_cache/tarballs/pacman-3.29.tar.gz
tar --no-same-owner -xzvf pacman-3.29.tar.gz
cd pacman-3.29
source setup.sh
cd ..
ln -s pacman-3.29 pacman
</pre>
Once Pacman is installed you can  =source /nfs/osg/app/pacman/setup.sh= and you are ready to install OSG software using Pacman.


---+++ Distributed file system 
xrootd is a shared file system distributed across several hosts, each one serving part of it. It is optimized to store data files, not for programs or system files.
It is used at its best if the jobs are running on the host serving their input files.

You may want to install xrootd if you have big data files and would like an uniform file system space using the disk space available on several nodes (dedicated data servers or the worker nodes).

XrootdInstall describes the installation of the xrootd file system


---+++ Quick Guide for setting up a Condor Cluster 
Each cluster needs a _Local Resource Manager_ (LRM, also called _queue manager_ or _scheduler_) to schedule the jobs on the worker nodes. Common LRMs are Condor, LSF, PBS, SGE; if you have one installed or a preferred one you can use that one. 
If not, this section describes how to install and configure Condor 7.4, the latest production version of Condor.
Here are two different ways to install Condor:
   * CondorSharedInstall - an easy way to add Condor to small clusters using a shared file system 
      * it is easy to add new nodes
      * configuration changes and updates are also very quick
      * cons: shared installation may slow down several nodes using it
   * CondorRPMInstall - Condor is installed on each node using the RPM packages of Condor
      * no contention for shared files
      * only option if no shared file system
      * cons: some more work to add nodes 
      * cons: updates need to be installed on all nodes 

The base configuration of condor can be modified to add some extra features:
   * CondorServiceJobSlotSetup describe how to add a service jobs slot to each node. This will allow to install application software or to run monitoring jobs on the worker nodes without modify the scheduling of the regular jobs
   * CondorHawkeyeSetup describes how to setup Condor's Hawkeye and shows an example using Hawkeye to conditionally schedule  or suspend jobs depending on the activity on the worker node

CondorTest show how to check the status of the Condor installation and how to submit some simple job.

---+++ User client
The user client machine is a host that allows users to login and provides them with software to access Grid services and specific VO software. This document covers only the installation of Grid clients, leaving to the VOs the documentation of their software.

SiteCoordination.GridClientTutorial shows the installation of the Grid client software.
Note that users will need to [[ReleaseDocumentation.GridCertificateRequestTutorial][request a certificate]] and [[ReleaseDocumentation.RequestVOMembershipTutorial][register in a VO]] before being able to use the Grid.

Examples from VOs:
   * ATLAS VO: https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SoftwareInstallation
 

---+++ Worker node client
The worker node client is used to allow non interactive jobs (the ones submitted using the Grid or Condor) to access the Grid.
We are installing the worker node client in =/opt/wn=, linked to =/nfs/osg/wn= using the instruction in the OSG release documents: ReleaseDocumentation.WorkerNodeClient

In short, on the node chosen for the installation, e.g. =gc1-ce=, you can use the following:<pre>
source /nfs/osg/app/pacman/setup.sh
mkdir /nfs/osg/wn
ln -s /nfs/osg/wn /opt/osg
cd /opt/wn
pacman -get http://software.grid.iu.edu/osg-1.2:wn-client
ln -s /etc/grid-security/certificates /opt/wn/globus/TRUSTED_CA
</pre>

On all other nodes that may run jobs, e.g. the worker nodes:<pre>
ln -s /nfs/osg/wn /opt/osg
</pre>

Alternatively, if there is no shared file system or if there is a load concern, the worker node client can be installed locally, in =/opt/wn= on all the worker nodes.

---+++ Grid users authentication
There are 3 options:
   * use a gridmap file
   * rely on an external GUMS server
   * install a GUMS server

Links:
   * ReleaseDocumentation.GUMSHandsOn
   * ReleaseDocumentation.InstallConfigureAndManageGUMS
   * ReleaseDocumentation.GridColombiaInstallGUMS

---+++ Installing a CE
Installing the OSG CE

Links:
   * ReleaseDocumentation.InstallCETutorial

---+++ Installing a SE

The T3 sites would probably want to install !BeStMan-gateway on top of NFS or some other FS in order to receive  datasets at  their site automatically via some the data subscription mechanism. A grid storage element allows authenticated and authorized connections from the Tier 1 and Tier 2 sites to deposit data at  Tier 3. 

Please follow [[BeStManGateway][this installation guide]] to setup !BeStMan-gateway.

If you need to install a standalone !GridFTP server or an additional !GridFTP server for your !BeStMan installation , please, follow [[ReleaseDocumentation/GsiFtpStandAlone][this Installation guide]].

!BeStMan-gateway server could be installed on top of !xrootd distributed file system. The installation procedure and configuration is somewhat different from the procedure described above.  Please follow [[BeStManGatewayXrootd][this installation guide]] to setup !BeStMan-gateway for !Xrootd. 

If you need to install a standalone !GridFTP-Xrootd server in addition to !GridFTP-Xrootd server in your !BeStMan-Xrootd installation  or just to provide means for authenticated and authorized file  transfers on top of your !Xrootd installation , please, follow [[ReleaseDocumentation/GridFTPXrootd][this Installation guide]].

Links:
   * ReleaseDocumentation.BestmanGateway
   * ReleaseDocumentation.InstallSETutorial
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/SetupSE
      * https://atlaswww.hep.anl.gov/twiki/bin/view/Tier3Setup/ThroughputCleanup
 
---+++ Squid Web proxy

---+++ Network performance

Links:
   * ReleaseDocumentation.NetworkPerformanceToolkit
   * ReleaseDocumentation.NetworkPerformanceTutorial

---++ Tier 3 installation                                                                                                                                                                                                                        


%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 25 Nov 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

Dump of information: Tier3DocDev 

---++ !! <nop>%WEB% Web Utilities
<form action='%SCRIPTURLPATH{"search"}%/%WEB%/'>
   * <input type="text" name="search" size="22" />&nbsp;<input type="submit" class="twikiSubmit" value="%MAKETEXT{"Search"}%" /> - [[WebSearchAdvanced][%MAKETEXT{"advanced search"}%]]
   * WebTopicList - all topics in alphabetical order
   * WebChanges - recent topic changes in this web
   * WebNotify - subscribe to an e-mail alert sent when topics change
   * WebRss, WebAtom - RSS and ATOM news feeds of topic changes
   * WebStatistics - listing popular topics and top contributors
   * WebPreferences - preferences of this web
</form>




%META:FILEATTACHMENT{name="bestman-gateway.jpg" attachment="bestman-gateway.jpg" attr="" comment="" date="1258476644" path="bestman-gateway.jpg" size="38240" stream="bestman-gateway.jpg" tmpFilename="/usr/tmp/CGItemp5500" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="gc-dependencies-wb.png" attachment="gc-dependencies-wb.png" attr="" comment="Fig. 1- Tier 3 cluster components" date="1258993425" path="gc-dependencies-wb.png" size="100313" stream="gc-dependencies-wb.png" tmpFilename="/usr/tmp/CGItemp11663" user="MarcoMambelli" version="1"}%
%META:FILEATTACHMENT{name="gc-dependencies-wb-sm.png" attachment="gc-dependencies-wb-sm.png" attr="" comment="Fig. 1- Tier 3 cluster components and dependencies" date="1259084660" path="gc-dependencies-wb-sm.png" size="46411" stream="gc-dependencies-wb-sm.png" tmpFilename="/usr/tmp/CGItemp11814" user="MarcoMambelli" version="1"}%
