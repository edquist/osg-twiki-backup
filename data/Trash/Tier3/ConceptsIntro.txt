%META:TOPICINFO{author="KyleGross" date="1329928282" format="1.1" version="1.16"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! Introduction to Concepts and Components
%DOC_STATUS_TABLE%
%TOC%

---++ Introduction

This document gives access to a series of pages that introduce some important concepts in Grid computing and in cluster managements.
This is not an exhaustive manual but should give you all the elements to understand all the documents in the Tier 3 Web.
It further introduces some conventions used in other documents, e.g. the correspondence between hostnames and roles within the cluster.

If you prefer an how-to document go to the ExamplesIntro

%STARTINCLUDE% 

---++ Tier 3 Concepts and Components

This section explains what a Tier 3 is and introduces concepts and defaults that are useful to understand the Tier 3 documentation.

---+++ What is a Tier-3 Cluster?
An OSG Tier 3, in general, is a small to medium cluster/grid resource targeted at supporting a small group of scientists. 

Tier 3 systems typically provide one or more of the following capabilities:
   * access to local computational resources using a batch queue
   * interactive access to local computational resources
   * storage of large amounts of data using a distributed file system
   * access to external computing resources on the Grid
   * the ability to transfer large datasets to and from the Grid
  
Tier 3s can also offer computing resources and data to fellow grid users

A Tier 3 is not always managed by a professional system administrator, so an important goal of this documentation is to try and minimize system administrator effort required to set up and operate one of these systems. Even with minimal settings however, system administration should be taken seriously and it is not unreasonable to dedicate at least 1/4th of an FTE for this task, not including initial setup, which can take a week or longer depending upon site policies and capabilities.

The nodes of a Tier 3 are organized in a cluster: computers (often with different functions) tightly connected via a network infrastructure.
Since there are many ways that a cluster can be configured, these documents present a [[ClusterNetwork][typical network]] utilizing recommended best practices together with component by component instructions for installing a Tier 3. Further recommendations and examples for the administrators may come from the collaborations or from their campuses. 

This guide is broken into two parts: Basic Cluster Configuration and Tier 3 Middleware Components. (A third part that is not included in this guide is the scientific applications provided by the ATLAS and CMS projects.)

---+++ Components of a Tier 3 Cluster
In a Tier-3 cluster, there are three classes of cluster nodes:
   1 Batch queue worker nodes that execute jobs submitted via the batch queue. 
   1 Shared Interactive nodes where users can log in directly and run their applications. 
   1 Nodes that serve various roles such as batch queues, file systems, or other middleware components (see below). In some cases one node can host multiple roles while in other cases for a variety of reasons (including security or performance), nodes should be set aside for single purpose uses.
A list of important components (or roles) for the Tier 3 architecture is as follows:

   * NFSv4 Server -- Using NFS to create a shared file system is the easiest way to set up and maintain a Tier 3. This documentation describes how to setup NFSv4, but NFSv3 can also be used.
   * Condor Batch Queue -- A batch queue system is strongly recommended for Tier 3s. This document only provides the installation of Condor (selected because it is one of the most familiar internally to the OSG and hence easily supported by the OSG), but other systems can be used and may be preferable, for example if there is local expertise available in another batch queuing system. The general OSG documentation provides some help for different systems.
   * Distributed File System -- An optional capability that can be helpful for moving efficiently VO data and other files across the worker nodes. It may also provide data-locality performance improvements to scientific applications. This document covers the installation of Xrootd, a DFS optimized for ROOT files used in the HEP community, although other systems may be used. 
   * Storage Element (SE) -- Provides the capability of high performance data transfers over the grid and is standard fare on every Tier 2 and Tier 3 system.
   * Compute Element (CE) -- Provides the capability of sharing your local resources with fellow grid users. CMS Tier 3s typically install a CE while ATLAS Tier 3s typically do not install a CE for their smallest sites.
   * GUMS -- An optional Grid Identity Mapping Service that provides automatic capabilities for managing user lists as well as enabling users to have additional "groups" and "role" privileges. For sites that already have a GUMS installed somewhere on campus, it is recommended to tie into this pre-existing capability if possible. For new Tier 3 sites, unless there is a strong requirement for "groups" and "roles" it is recommended that Tier 3s *not* set up this service and use Grid mapfiles for identity mapping instead. 

[[ClusterComponents][Read more on Cluster Component descriptions]]

---++++ Naming Convention for Tier 3 cluster nodes
To simplify documentation and examples we give a name to the cluster: =gc1= (Grid Cluster 1). Each node in the cluster, has a unique fully qualified domain name (FQDN).  The FQDN includes a local hostname and a parent domain name (here "yourdomain.org"). 
The following is a list of roles (abstract functionalities) that nodes may have within the cluster. We use the role to define the hostnames. This means that the host that is your compute element (if you have a CE) has an FQDN =gc1-ce.yourdomain.org=.
A node may have more than one name if it runs more than one service; in that case  If a node is covering two or more roles, then all those names will refer to the same node and can be _cnames_, e.g. the Storage Element and the Xrootd redirector may be colocated on the same node, making gc1-se and gc1-xrdr represent different names of the same node. If there are no nodes covering a role then there will be no node with that name. 
A basic list is:
   * =gc1-ui.yourdomain.org=  The user interface machine, can also be referred to as Interactive node 1
   * =gc1-wn001.yourdomain.org=  Worker Node 1
   * =gc1-wn002.yourdomain.org=  Worker Node 2 ...
   * =gc1-sn001.yourdomain.org=  Storage Node 1 ..., usually colocated with the Worker Node
   * =gc1-nfs.yourdomain.org=  the NFSv4 server
   * =gc1-hn.yourdomain.org=  the queue head-node 
   * =gc1-se.yourdomain.org=  the Storage Element 
   * =gc1-ce.yourdomain.org=  the Compute Element 
   * =gc1-xrdr.yourdomain.org= the xrootd redirector 
   * =gc1-gums.yourdomain.org=  the GUMS server 
   * =gc1-proxy.yourdomain.org= Squid (proxy) server
   * =gc1-net.yourdomain.org=  Firewall/Router (e.g. NAT server)

In the documents the machines will sometimes be referred to by their local hostname, e.g. =gc1-ce= instead of =gc1-ce.yourdomain.org=

---+++ Security
Security is a key aspect of Tier 3 site administration. Below is a short list of items for good security administration.  All of the items below can be summarized with one word *communication*.  As long as a site admin keeps his communication channels open and functional with OSG, he will have achieved the basics of site security.  This could be as simple as reading emails and asking questions.

The five most important things for a site administrator to do concerning security are as follows:

   1 Designate a security contact and a backup contact for the site.  A security contact can be the site admin. A back-up person could be anyone who is familiar with the site and fill in should there be an emergency and we cannot reach the primary contact. If the primary security contact is a student, post-doc or someone with a temporary affiliation with the site, the back-up person should be someone with a permanent relationship with the site. For example, if you are a grad student, give your adviser or supervisor's contact information as back-up. So when you graduate and move on, we will still have someone to connect with. [[https://twiki.grid.iu.edu/bin/view/Documentation/OIMSecurityContactInfo][Security contact information instructions.]]
   1 Read and stay current on the security bulletins issued by the OSG.  We send emails to the security contacts when we make security announcements. All of the public announcements are archived on the OSG Security Blog (http://osgsec.blogspot.com/). The email states the level of the security risk and whether immediate action is required or not. If you are not sure whether you are affected by an announcement or do not know what to do, email osg-security-team@opensciencegrid.org.  [[https://twiki.grid.iu.edu/bin/view/Documentation/OSGSecurityNotifications][More information on security notices]]
   1 Know who your institutions security officer(s) are and develop a relationship with them.  First and foremost you are bound by your local site's security policies. You should have an idea on how to operate a site securely according to your local site policies. If your local site security officer has questions about the grid configuration an OSG security person will gladly help you. We can communicate with the site security professionals and/or help you answer their questions. Just make a request to osg-security-team@opensciencegrid.org.  [[https://twiki.grid.iu.edu/bin/view/Documentation/LocalSecurityOfficer][Information on getting to know your site security officer.]]
   1 Make sure your systems are up to date on security patches.  We will send you alerts on active system vulnerabilities and can advise you with upgrades. But we will need your help too. There are many T3 sites and many different OSes at these sites. We will do our best, but you should keep a keen eye on your system.  Keeping your VDT software up-to-date will be part of keeping your system patched and we have [[https://twiki.grid.iu.edu/bin/view/Documentation/ApplySecurityUpdates][more information on VDT/OSG sofware updates here.]]
   1 Maintaining current OSG authentication services at your site.  This will entail [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/MaintainCAInformation][maintaining up-to-date CA information]] as well as [[https://twiki.grid.iu.edu/bin/view/Documentation/MaintainVOInformation][up-to-date VO and user access information]].  These are the components that allow and restrict OSG access to your site and is very important to keep the information current.

We have detailed documentation on how to perform each of the above items, as well as other security responsibilities sites may want to address, at [[https://twiki.grid.iu.edu/bin/view/Documentation/SecuritySiteResponsibilities][Site Responsibilities for Security]] page.

There is also an [[https://twiki.grid.iu.edu/bin/view/Documentation/SecurityBestPractices][OSG Security Best Practices]] page which covers other OSG related security issues as well as more general security topics.

Further security information can be found at the [[https://twiki.grid.iu.edu/bin/view/Security/WebHome][OSG Security Page]].<BR>

---+++ Site planning
The [[ReleaseDocumentation.SitePlanning][OSG SitePlanning]] guide is a general purpose OSG document that you may find helpful for a general overview. It describes many situations however that may not necessarily apply to small sites such as Tier 3s.

To use the grid and to register your site you need a [[ReleaseDocumentation.CertificateUserGet][grid user certificate]].

All grid services need [[ReleaseDocumentation.GetHostServiceCertificates][host or service certificates]]. 
Instead of using separate certificates sometime it is possible to [[http://www.globus.org/toolkit/docs/5.0/5.0.3/security/gsic/admin/#gsic-admin-security_considerations][use a single one]]: during host authorization, the Globus toolkit treats host names of the form "hostname-ANYTHING.edu" as equivalent to "hostname.edu". This means that if a service was set up to do host authorization and hence accept the certificate "hostname.edu", it would also accept certificates with DNs "hostname-ANYTHING.edu".

---+++ Network configuration
Your campus or your department will provide you a network connection that in these documents we refer as _extranet_, i.e. the network that connects also to the world outside of your Tier 3 cluster. 
[[ClusterNetwork][Read more about different network topologies]] and the default configuration, including subnet IP address, for the network connecting the nodes of a Tier 3 used in the examples. 


%STOPINCLUDE% 
%BR%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = MarcoMambelli

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Tier3

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Planning
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = MarcoMambelli
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################ 
-->

-- Main.MarcoMambelli - 20 May 2010