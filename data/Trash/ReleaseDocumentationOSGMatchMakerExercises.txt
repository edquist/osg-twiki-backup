%META:TOPICINFO{author="MatsRynge" date="1256061091" format="1.1" version="1.5"}%
%META:TOPICPARENT{name="GridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---++ Overview

For this demo we will be using Povray, which is a raytracer used to create three-dimensional graphics. A scene (including objects, textures, lights, cameras, ...) is described in a scene description language, and Povray then follow the light rays and creates a image with shadows, transparency and so on. This final output we are going to do in this demo is an image of a Bonsai tree:

<img src="%ATTACHURLPATH%/rendered.png" alt="rendered.png" width='400' height='300' />


Running Povray can be pretty compute intensive. Ho intensive depends on scene complexity and output size of the image. Running on a grid like OSG, one of the things you will have to do is break your problem up into sub problems. For our rendering problem, we will split the task of rendering the full image up into the problems of rendering tiles of the image, and then when we have all the tiles, we will put the tiles together into the final image. How many tiles you want to do is a user setting, but one possible break down would be 8x6 (48 tiles).

<img src="%ATTACHURLPATH%/rendered_split_lines.png" alt="rendered_split_lines.png" width='500' height='375' />


The way to describe a set of jobs in OSGMM is as a [[http://www.cs.wisc.edu/condor/manual/v7.2/2_10DAGMan_Applications.html][Condor DAGMan]]. The normal use case for DAGMans is that you have job dependencies, but in the case of rendering Povray tiles, the jobs are fully independent (you can render each tile without knowing what is going on with the neighboring tiles). But there are other advantages with DAGMan. It has hooks for pre/post scripts than can run locally before and after a job, and if case of job failures, DAGMan can resubmit the job. The steps necessary to create a workflow is:

   * for each tile, create a Condor submit file to describe the job and pass the needed parameters to the job
   * add all the Condor jobs to the DAGMan
   * submit the DAGMan


---++ Starting a Run

To start a run, user your assigned training account and ssh to gs-mm.uchicago.edu. Then, copy the example to your home directory, and see what files and directories exists in the example directory:

<pre class="screen">
gs-mm$ <b>cp -r ~osgmm/povray-example ~/</b>
gs-mm$ <b>cd povray-example</b>
gs-mm$ <b>ls</b>
helpers
inputs
local-post-job
local-pre-job
remote-povray-wrapper
runs
submit
</pre>

Description of the files / directories:

   * *helpers* - This is a directory holding helper scripts. In the case of the povray example, there is only one helper, tiles-combine, which is used to combine all the individual tiles into one final image

   * *inputs* - Directory containing inputs for the jobs, which include the Povray scene description for the Bonsai scene

   * *local-pre-job* - It is a script which DAGMan runs before each job. It is mostly a placeholder in the Povray example.

   * *local-post-job* - It is a script which DAGMan runs after each job completes. This is a very important part of detecting job failures. Failures should be expected in any distributed system, so it is important to detect and handle failures accordingly. The local-post-job script checks the job output to determine if the job was successful or not, and if a failure is detected, the script exists with exit code of 1, which signals to the DAGMan that the job should be resubmitted.

   * *remote-povray-wrapper* - This is the actual job that runs on the remote side. In most cases you will have to wrap your executable in a job wrapper to do the staging in/out, work dir handling (moving to $OSG_WN_TMP for example for local disk I/O) and to do some extra error detection. In the Povray example, we use the _pull_ staging method, the job starts on some compute node somewhere, and pulls in the inputs using globus-url-copy. When the job is done, the output is staged back using globus-url-copy again. These staging tasks are defined in the remote-povray-wrapper.

   * *runs* - This is a directory in which is used to old each "run". When you start a new run, a timestamped directory will be created in here to hold logs and outputs.

   * *submit* - This is the submit script. It creates the Condor jobs based on inputs/parameters, and a DAGMan description. Then the run is submitted to Condor for execution.


Let's start a run with the default parameters:

<pre class="screen">
gs-mm$ <b>./submit</b>
Generating job 1 X: 1 {1:160}, Y: 1 {1:120}
....
Generating job 25 X: 5 {641:800}, Y: 5 {481:600}

Checking all your submit files for log file names.
This might take a while...
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : master.dag.condor.sub
Log of DAGMan debugging messages                 : master.dag.dagman.out
Log of Condor library output                     : master.dag.lib.out
Log of Condor library error messages             : master.dag.lib.err
Log of the life of condor_dagman itself          : master.dag.dagman.log

Condor Log file for all jobs of this DAG         : runs/2009-10-20_135040/alljobs.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 849162.
-----------------------------------------------------------------------

</pre>



---++ Checking on the Jobs

---++ Outputs

---++ Details (Run Directory)

---++ More information

Povray:
http://en.wikipedia.org/wiki/POV-Ray

Bonsai Povray scene:
http://www.zazzle.com/bonsais_poster-228003235318786172

%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.MatsRynge - 16 Oct 2009 %BR%
%REVIEW%


%META:FILEATTACHMENT{name="rendered.png" attachment="rendered.png" attr="" comment="" date="1255974825" path="rendered.png" size="201944" stream="rendered.png" tmpFilename="/usr/tmp/CGItemp18620" user="MatsRynge" version="1"}%
%META:FILEATTACHMENT{name="rendered_split_lines.png" attachment="rendered_split_lines.png" attr="" comment="" date="1255974838" path="rendered_split_lines.png" size="178227" stream="rendered_split_lines.png" tmpFilename="/usr/tmp/CGItemp18648" user="MatsRynge" version="1"}%
