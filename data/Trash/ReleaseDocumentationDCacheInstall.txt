%META:TOPICINFO{author="TedHesselroth" date="1266601314" format="1.1" reprev="1.12" version="1.12"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%

*Purpose*: The purpose of this document is to provide dCache based SE administrators the information on how to prepare, install and validate the SE.

---++ Preparation
---+++ Introduction

dCache is a disk-caching system jointly developed by Deutsches Elektronen-Synchrotron (DESY) and Fermi National Accelerator Laboratory (Fermilab). Although it was initially developed to provide disk caching for tertiary storage, dCache makes an excellent grid Storage Element (SE) &ndash; even without tape backend &ndash; because it has a [[https://srm.fnal.gov/twiki/bin/view/SrmProject/WebHome][Storage Resource Manager]] interface and supports multiple transfer protocols: gridftp, xrootd, dcap, and http. dCache also provides a single name space across the entire pool of disk servers, making it look like a single file system to the users. It simplifies administration by providing mechanisms for internal clean-up and load balancing, which allows efficient use of storage space and handling of users' requests.

dCache is a storage element implementation that meets the SRM v2.2 specification. dCache is not packaged with the base Virtual Data Toolkit installation, but is available through the VDT as a separately downloadable package. In addition, OSG provides Gratia storage and transfer probes for site and grid-wide monitoring. Recently, subset of [[http://datagrid.ucsd.edu/toolkit][community toolkit]] was added to vdt-dcache distribution.

The dCache distributed components are implemented with the Cells package. Cells communicate with each other and components of dCache are implemented as cells. Hardware resources on a dCache node depend on what cells are running on that node. For a detailed description of dCache cells and the resources they require, see <a target="_top" href="https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions/">Descriptive Cell Listing</a>. !dCache also uses !PostgreSQL databases for SRM and [[https://twiki.grid.iu.edu/bin/view/Documentation/StorageDcachePnfs][Pretty Normal File System (PNFS)]] to provide transparent file access.

---+++ Architecture
The detailed dcache architecture is provided at [[http://www.dcache.org][dCache Project Home Page]].
Main dCache components are listed in the table below:
| *Component Name* | *Purpose* |
|Location Manager|Instructs a newly started domains to which domain they should connect |
|PNFS, Chimera| Name servers|
|PNFS Manager|Manages the name space functionality|
|Info Provider|Provides a description of a !dCache instance using the GLUE information model|
|!gPlasma|Manages authorization of the clients|
|!PoolManager|Selects which pool is used for an incoming request|
|!ResilientManager|Keeps track of the number of replicas of each file within a certain subset of pools and makes sure this number is always withina specified range|
|!SrmSpaceManager|Provides a standardized webservice interface for managing a storage resource. Provides means to reserve space, initiate file storage or retrieve, and replicate files to another SRM|
|Doors ||
|  dcap door|Supports dcap protocol|
|  gsiftp door|Supports !GridFTP protocols v1 and !GridFTP v2. |
|  gsidcap door|Supports dCap protocol with a GSI authentication wrapper|
|  xrootd door|Supports xroot file transfer protocol|
|  NFSv4.1| Supports NFSv4.1 protocol|
|Pools| Store retrieved files and provide access to that data. Data access is supported via movers. A machine may have multiple pools| 


<div class="figure">
  <p><img src="%ATTACHURLPATH%/dcache_tier2.png" class="scaled" alt="Medium-sized dCache installation (click to enlarge)" /></a></p>
  <p>[[%ATTACHURLPATH%/dcache_tier2.png][Medium-sized dCache installation (click to enlarge)]] %BR% </p></div>

The following are the recommended system minimums for an OSG dCache installation. All nodes should have Scientific Linux 4.2 or later.

*Admin Nodes*

   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) with a hot spare on the Admin Nodes with the billing or srm databases running the XFS file system. These should be backed up regularly.</p> 
   * <p>Gigabit or better network links</p> 

*Pool Nodes*

   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better or equivalent Opteron (e.g., quad Opteron 270)</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) hot swappable with hot spare recommended running the XFS file system. External RAIDs are highly recommended.</p> 
   * <p>Gigabit or better network links</p> 

*PNFS Node*

   * <p>8 GB RAM</p> 
   * <p>Postgres databases on raided disk (RAID 5) with back-up performed regularly.</p> 
   * <p>Disk should be used exclusively for the pnfs database.</p> 

*Note:*

See [[http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=1837&version=1&filename=dcache_tuning.pdf"][ _dCache Tuning_ ]] for recommended Linux TCP/IP parameter tuning

[[https://indico.desy.de/getFile.py/access?contribId=19&sessionId=5&resId=0&materialId=slides&confId=138][dCache hardware layouts]] provides a good guide for the layout of dCache hardware in a variety of scales. The [[http://www.dcache.org/manuals/index.shtml"][dCache manual]] also has more information.

Please check [[http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server][Tuning PostgreSQL]] has recommendations on tuning !PostgreSQL. 

---++ Installation

VDT provides a package for installing dCache on an Open Science Grid site. The latest version  is available for download from the [[http://vdt.cs.wisc.edu/components/dcache.html][VDT-dCache website]]. There are two ways to read the installation/setup/configuration instructions. You can either read [[http://vdt.cs.wisc.edu/extras/2.3.5/InstallingDcacheForOSG.README.html][Installation Procedure]] or once you have downloaded and untarred [[http://vdt.cs.wisc.edu/software/dcache/server/2.3.5/][the tarball]] appropriate for your machine, you can read the README file (present under INSTALL_LOCATION/install/ directory). If you are planning to do upgrade of existing dCache installation, please, follow [[http://vdt.cs.wisc.edu/extras//2.3.5/UpgradeDcacheForOSG.html][Upgrade Procedure]].

---+++ Integration with the information system
 Integration of the SE with the central information systems takes place during the Compute Element installation/configuration. See the topic [[ReleaseDocumentation/GenericInformationProviders][Generic Information Providers]]. The SE does not collect or publish information independently.

---+++ Integration with Gratia dCache probes
The probes report storage related information to the central Gratia collector. There are two types of probes:
   * The dCache-transfer probe reports to Gratia the details of each file transfer into or out of a dCache file server.
   * The dCahche-storage probe is responsible for reporting storage capacity and storage usage to the central Gratia collector. The information reported is:
      * The storage capacity and amount used for each dCache pool.
      * The storage capacity and amount used for each SRM Space reservation
Please follow [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes][Gratia dCache Probes Installation]] to install and configure Gratia dCache Probes.
---++ Validation

Please perform the following tests to validate your SE before declaring it to be functional:

<pre class="screen">
srmping
</pre>

<pre class="screen">
srmcp
</pre>

---+++++Setting up gPlazma

After dCache is installed, authorization for access must be configured. Please refer to the [[http://www.dcache.org/manuals/Book/config/cf-gplazma.shtml][gPlazma chapter]] of the dCache book on how to configure authorization for your site's policies.   

---+++++ Setting up the Replica Manager

The Replica Manager feature of dCache automatically makes extra copies of files that are written into the storage element. The purpose is to increase performance for the subsequent reading of files and to offer some protection against data loss in non tape-backed systems. In the VDT package for dCache the Replica Manager is turned on by default with parameters of a minimim of two and a maximum of three replicas per file. However, dCache pools must be explicitly added to the "ResilientPools" group in the <nop>PoolManager.conf file in order for replication to take effect. See [[https://twiki.grid.iu.edu/twiki/bin/view/Documentation/StorageDcacheOverview][Overview of Storage Resource Manager (SRM) and dCache for OSG]] for details of the dCache pools and <nop>PoolManager. Only files written to resilient pools will be replicated.  

For instructions on the replica manager setup, please see [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/ReplicaManagerSetup][Configuration of Storage Elements for Replica Management]].

---+++++ Setting up Space Reservation for Opportunistic Use

The Space Reservation feature of dCache allows users to have a guarantee of a given amount of storage for a period of time. For opportunistic use, the concept is for users to make a space reservation in the context of supporting a job running on a compute element. The reservations will be for relatively small amounts of storage for short duration. See the [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/StorageInOSGDraft][Use of Storage in OSG]] document for details. In the VDT packaging of dCache, Space Reservation is turned on by default. However, dCache pools must be explicitly added to the "public" pool group in the <nop>PoolManager.conf file in order for space reservation to take effect. Files written using space reservations will always be written to the associated pools.

For instructions on the opportunistic storage setup, please see [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/OpportunisticStorageSetup][ Configuration of Storage Elements for Opportunistic Use]], or, if using tags, [[Storage.OpportunisticStorageUse]]

[[Storage/OpportunisticStorageSetup]]

[[Storage/SpaceResServerSetup]]

[[Storage/OpportunisticStorageUse]]

---+++Gratia

---++++ dCache Gratia probes
The probes report storage related information to the central Gratia collector. The probes are installed on their correct nodes by the VDT install scripts during an initial dCache installation or upgrade. 


      * [[GratiaDcacheProbes][Preparing, Installing and Validating Gratia dCache probes]]


 There are two types of probes:

   * The transfer probe reports to Gratia the details of each file transfer into or out of a dCache file server.

The probe gets this information from the dCache "billing" database and should run on the dcache node on which the dCache http domain is running. For performance reasons, sites with large dCache billing databases are advised to alter the "billinginfo" table by adding an index on the pair of columns (datestamp, transaction) and to alter the "doorinfo" table by adding an index on the (transaction) column.  This should speed up the search for newly added records. 

The transfer probe will start automatically at boot time if the installation option to do so is selected. Otherwise, start it with

<verbatim>
service start gratia-dcache-transfer
</verbatim>

The probe will then run continuously in the background 

   * The storage probe is responsible for reporting storage capacity and storage usage to the central Gratia repository. The information reported is:
      * The storage capacity and amount used for each dCache pool.
      * The storage capacity and amount used for each SRM Space reservation.<br>

It gets the pool information from the dCache admin server. It gets the SRM information from the SRM tables in the SRM "srmdcache" database. Therefore it should run on the dCache node on which srm is running. The probe runs as a cron job on the host running SRM, so no action need be taken to start it. 

Information how to verify the reports generated by the probes is provided in their respective README files (under nder /opt/d-cache/gratia/probe/dCache-transfer /opt/d-cache/gratia/probe/dCache-storage). In brief, to verify that the Gratia probes are working, run some transfers, including the use of space reservation. A convenient way to do that is to run the validation test suite (see below) and then check Gratia collector information.

The details how to install and configure dCache Gratia probes are provided in [[https://twiki.grid.iu.edu/bin/view/Integration/ITB092/GratiaDcacheProbes][Gratia dCache Installation Procedure]].

---+++GIP
[[InformationServices/DcacheGip]]


---++++ dCache Generic Information Provider

The purpose of the dCache Generic Information Provider is to discover and publish all storage related information corresponding to your dCache based Storage Element. After you have installed the OSG CE, please refer to documentation on how to configure the GIP for the SE on the [[https://twiki.grid.iu.edu/twiki/bin/view/InformationServices/DcacheGip][dCacheGIP]] page. 

*Note:* At the moment, to get the most up-to-date, bug-free version, you'll need to switch from the GIP distributed with ITB 0.9.0 to SVN. The instructions on how to do this are available on the [[https://twiki.grid.iu.edu/twiki/bin/view/InformationServices/GipInstall][GipInstall]] page. 


---+++RSV

---++Validation

---++++ dCache Validation Suite
The dCache validation test suite is designed to test basic dcache functionality as well as more advanced dcache features. It is developed and maintained at Fermilab. This test suite is mainly intended for use by the Storage Element admins and is a good first-level check of the SE. If all tests within the testsuite are successful, it means the basic functionality exists on the server side. 

 The suite consists of three major parts:

   * Fermi SRM Client Test Suite. The main purpose of the Fermi SRM Client Test Suite is to run various srmclient commands developed at Fermilab against a dCache based Storage Element (SRM V2.2 only). Various tests that are run as part of this test suite include:
      * <u>srmcp</u> (Get/Put operations, with and without Space Reservation)
      * <u>srmmkdir</u> (Create new directories)
      * <u>srmmv</u>  (Move directory from one location to another)
      * <u>srmls</u> (List contents of a directory)
      * <u>srm-get-permission</u> (Get permissions on a file/directory)
      * <u>srm-check-permission</u> (Check permissions on a file/directory)
      * <u>srm-set-permission</u> (Set permissions on a file/directory)
      * <u>srm-reserve-space</u> (Make a space reservation)
      * <u>srm-release-space</u> (Release a space reservation)
      * <u>srmrm</u> (Remove file)
      * <u>srmrmdir</u> (Remove directory)

%ICON{warning}%  If your dcache configuration doesn't support opportunistic storage some of the commands should fail (e.g srmreservespace, srmreleasespace, srmcp with space token option). See the above section on how to set up opportunistic storage.

   * SRM Space Management test suite. It tests space allocation/release with various options including:
      * retention_policy
      * guaranteed_size
      * desired_size
      * lifetime
      * access_latency

%ICON{warning}%If your dcache configuration doesn't support opportunistic storage you should skip this test. See the above section on how to set up the replica manager.

   * Replica Manager test suite. It performs the following tasks:
      * copy multiple files into storage
      * verify report of disk usage
      * check the number of replicas for each files
      * delete these files from storage
      * verify consistency of information in Pnfs, pools and Replica Manager
      * verify report of disk usage

%ICON{warning}%If your dcache configuration doesn't support the replica manager you should skip this test. See the above section on how to set up the replica manager.

The validation suite is an rpm package that can be downloaded from  [[http://vdt.cs.wisc.edu/software/dcache/tools//testing/][VDT dcache tools]]. You will have to modify the test configuration according to your dcache installation. A detailed description of configuration is provided in README file contained in the rpm.

---+++++ Installing the Validation Test Suite
The validation suite is an rpm package. After installing it, change the ownership of the installation root directory to the uid of the user who will be running the test with voms-proxy certificate.
<verbatim>
wget http://vdt.cs.wisc.edu/software/dcache/tools//testing/dcache_validation-0.1-0.noarch.rpm
rpm -i --prefix <your_home_area> dcache_validation-0.1-0.noarch.rpm
chown -R <your_user_name>.<your_group_name> <your_home_area>/dcache_validation-0.1
</verbatim>

---+++++ Running the Validation Test Suite

The detailed instructions how to run the test, what software should be installed on your machine and how to see the results are provided in  dcache_validation-0.1/README. 
To see the results of the Validation Test Suite on the web follow the instructions provided in README of the suite's installation. an example of the test suite results can be
found [[https://osg-ress-2.fnal.gov:8443/dcache_validation_tests][here]]. 

%ICON{warning}% In order to see this example you have to have your user certificate installed in your browser.

---+++ Registration for SRM Monitoring at LBNL

To register your SE with LBNL Monitoring system:
go to http://datagrid.lbl.gov/sitereg/
and follow the instructions to register. 

Daily functional monitoring for all SRM interfaces is done around 9am Pacific time.


---+++ Testing Opportunistic Storage

The Fermi client commands for using space reservations are described in [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/SpaceResClientCommands][Using Opportunistic Storage]]. These commands can be run from any client machine which has them installed (see above). When there is a Compute Element associated with a Storage Element, jobs containing the commands described therein should be created and run on worker nodes. To test opportunistic storage from a Compute element please run the [[%ATTACHURL%/oppstor_test.py.txt][oppstor_test.py]] script (remove the .txt extension) from a worker node. This script does several tests of making, using, and releasing space reservations using the Fermi clients. To run it, you only need a proxy and to have the Fermi clients in the path. Use the command

<verbatim>
jython oppstor_test.py  srm://srmnode.oursite.edu:8443 /pnfs/oursite.edu/data/oppstorage/test
</verbatim>

with the URL of the srm server and desired path for the written files.

Removal of files used in opportunistic storage that are no longer needed allows the space to be recycled. To do this please use the SRM PNFS Space Reclaimer of the OSG Storage Operations Toolkit (see below).



[[ReleaseDocumentation.ValidateDcacheGratia]]

[[Integration/ITB090/InstallationITBStorageElement]]

[[ReleaseDocumentation/GPlazmaInteropTesting]]


---++ References

[[http://s-2.sourceforge.net/][S2]] - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp

<br />%STOPINCLUDE% 
%BR% 
%COMPLETE3% %BR% 
%RESPONSIBLE% Main.NehaSharma - 12 Sept 2008 %BR% 
%REVIEW% Main.TanyaLevshina - 21 Jul 2009 %BR%
%REVFLAG% %X% %BR%

---++ *Comments*
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:27 |
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:42 |
| I hope that I have fixed all the links | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
| We will need to add simple validation test commands | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
%COMMENT{type="tableappend"}%

%META:FILEATTACHMENT{name="dcache_tier2.png" attachment="dcache_tier2.png" attr="" comment="" date="1257462558" path="dcache_tier2.png" size="157327" stream="dcache_tier2.png" tmpFilename="/usr/tmp/CGItemp11636" user="TanyaLevshina" version="1"}%
