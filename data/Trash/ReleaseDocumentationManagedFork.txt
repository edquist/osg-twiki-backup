%META:TOPICINFO{author="SuchandraThapa" date="1305231815" format="1.1" reprev="1.42" version="1.42"}%
%META:TOPICPARENT{name="ComputeElementInstall"}%
%DOC_STATUS_TABLE%

---+!! Managed Fork
%TOC%

---+ About this Document

%ICON{hand}% This document is for system administrator of a %LINK_GLOSSARY_CE%. It describes the function and installation of the !ManagedFork job manager.

---+ About !ManagedFork

[[Documentation/GlossaryC#DefsManagedFork][!ManagedFork]] is an optional service which replaces the default fork jobmanager with Condor to manage incoming fork requests.  Commands such as =condor_q= and =condor_history= can be used to see the actual command lines of the fork jobs during and after execution, providing an important logging capacity.  More importantly, the number of fork jobs can be controlled with a configurable policy to help ensure that the CE is not overwhelmed by fork jobs.  This is a very important consideration: the standard fork manager allows a user to accidentally or maliciously "fork bomb" a CE.  As such, Managed Fork is highly recommended.

We will cover the steps of installing ManagedFork, configuring ManagedFork, enabling ManagedFork, and using ManagedFork.

%IMPORTANT% The Managed Fork job manager does not schedule fork jobs onto compute nodes in the execution pool. Using a Condor local universe, the jobs are still scheduled to the CE headnode, but since the local universe is used they should run quickly and without delay unless a scheduling limit for the CE has been reached. 


---+ Requirements

The base OSG compute element install (http://software.grid.iu.edu/osg-1.2:ce) must be installed with Pacman before the managed fork
job manager is installed.  See ComputeElementInstall for this procedure.  [[Documentation/GlossaryC#DefsCondor][Condor]] is also required but if you do not have it the VDT will install it automatically.  Both cases are covered below.

---+ Install using Condor from the VDT
If you want to use Condor from the VDT, installation is simple:
<pre class="screen">
cd $VDT_LOCATION
source $VDT_LOCATION/setup.sh
pacman -get  %CACHE%:ManagedFork
source $VDT_LOCATION/setup.sh
</pre>
Sourcing setup.sh a second time is to load the Condor environment after !ManagedFork is installed.  

Note that installing %CACHE%:ManagedFork will install a local Condor that is only used for ManagedFork jobs.

---+ Install using an existing Condor

Many site administrators have a pre-existing Condor (for the cluster's batch system, eg.) and wish to use it, rather than another copy of Condor, for Managed Fork jobmanger. The installation process is similar, but make sure you have =VDTSETUP_CONDOR_LOCATION= and =VDTSETUP_CONDOR_CONFIG= defined as discussed in PreparingComputeElement in order to specify your Condor installation.  The installation is done in same directory as the OSG CE software.   Setup these variables, then do:
<pre class="rootscreen">
# The variables in %RED%red%ENDCOLOR% are the likely defaults if you installed Condor via RPM.  Adjust them if necessary
VDTSETUP_CONDOR_LOCATION=%RED%/usr%ENDCOLOR%
VDTSETUP_CONDOR_CONFIG=%RED%/etc/condor/condor_config%ENDCOLOR%

cd $VDT_LOCATION
source $VDT_LOCATION/setup.sh
pacman -get  %CACHE%:ManagedFork
</pre>

The above step will give you all the files needed to run Managed Fork jobs but it does not make
Managed Fork the default.


---+ Enabling Managed Fork 

---++ Enabling using configure-osg
In the [Managed Fork] section of the config.ini file, 
just put in 

=enabled = %(enable)s=

and the correct commands to enable the managed fork job manager will be executed for you via the configure-osg utility.  



---++ Enabling manually

To configure the default jobmanager to be the Managed Fork jobmanager, execute the following command.

<pre class="rootscreen">
source $VDT_LOCATION/setup.sh
$VDT_LOCATION/vdt/setup/configure_globus_gatekeeper --managed-fork y --server y
</pre>

The =--server y= option ensures that the globus-gatekeeper file is copied to /etc/xinetd.d when vdt-control --on globus-gatekeeper
is done.  The =--managed-fork y= option modifies the service definition in $VDT_LOCATION/globus/etc/services/jobmanager-fork
such that it points to the managed fork jobmanager perl modules rather than the regular fork modules, making it the default job manager.
Until this step is done, the default job manager is the fork job manager, allowing the user to run as many arbitrary commands at once 
as they want.


---++ Configuration suggestions
By default, the Managed Fork jobmanager will behave just like the fork jobmanager.  If you wish to restrict it, you need to modify your local Condor configuration.  If you're using Condor from the VDT this can be done by editing =$VDT_LOCATION/condor/local.&lt;hostname&gt;/condor_config.local=.   Here are some configuration suggestions:

   * Only allow 20 local universe jobs to execute concurrently: 
<pre class="programlisting">
   START_LOCAL_UNIVERSE = TotalLocalJobsRunning < 20
</pre>
   * Set a hard limit on most jobs, but always let grid monitor jobs run (strongly recommended):
<pre class="programlisting">
   START_LOCAL_UNIVERSE = TotalLocalJobsRunning < 20 || GridMonitorJob =?= TRUE
</pre>

If condor is already running when you make these modifications, then execute the command condor_reconfig
to pick up the new setting.


---+ Disabling Managed Fork
To put back in place the default fork jobmanager (i.e. to disable the Managed Fork), execute the following command:
<pre class="screen">
# source $VDT_LOCATION/setup.sh
# $VDT_LOCATION/vdt/setup/configure_globus_gatekeeper --managed-fork n --server y
</pre>

Also, if you are using configure-osg, set the following in the [Managed Fork] section of the config.ini file to prevent ManagedFork from being enabled the next time you run configure-osg:

=enabled = %(disable)s=

---+ Further Details on Managed Fork
For more details on setup and configuration, refer to the [[%VDT_DOCS_URL%/notes/Globus-ManagedFork-Setup.html][VDT Managed Fork Jobmanager Release Notes]].


---+ Verify the Correct Operation of ManagedFork
As a client with appropriate user proxy
<pre class="userscreen">
globus-job-run ce/jobmanager-fork /bin/sleep 900
</pre>
While this job is running, log into your CE as root and execute the following commands
<pre class="rootscreen">
source $VDT_LOCATION/setup.sh
condor_q -constraint 'JobUniverse==12'
</pre>
This will give a listing of all Managed Fork jobs that are currently running. One should look
like this:
<pre class="screen">
2784794.0   fnalgrid       10/7  22:19   0+00:00:11 R  0   0.0  sleep 900 
</pre>



---+ FAQ
* Can you use the same condor installation to run regular jobs on a condor cluster and managed fork jobs?  Yes.  If
VDTSETUP_CONDOR_LOCATION is set during installation of ManagedFork and during Globus-Condor-Setup then
both kinds of jobs will run in the same condor installation.



---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = StevenTimm

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%
  
 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = SuchandraThapa
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%

 DEAR DOCUMENT TESTER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local TESTER       = SuchandraThapa
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED  = %YES%


############################################################################################################
-->

%META:TOPICMOVED{by="ForrestChristian" date="1166051931" from="Integration.ManagedFork050" to="Integration/ITB_0_5.ManagedFork"}%
