%META:TOPICINFO{author="BrianBockelman" date="1486493608" format="1.1" reprev="1.29" version="1.29"}%
%META:TOPICPARENT{name="Trash.ReleaseDocumentationWorkerNodeClient"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++What is gLexec and why do you need it
gLexec is adapted from apache's suexec to use grid credentials to run jobs as another 
user.  It is meant to be used by VO's that run pilot jobs.  In a pilot job one
user submits a job, either PANDA or Condor Glide-in, which then contacts the 
VO's server and requests a workload once it has gotten the slot.  gLexec is 
used to authenticate the workload and verify that it is an authorized user from 
the VO who has submitted the workload. 

gLexec has a number of authentication plugins and can be used both by 
EGEE LCAS/LCMAPS and by OSG with GUMS and SAZ.   It is an suid
program and thus must be installed on every single worker node individually. 
It cannot and must not be NFS-exported.  

***Note, however, that clever sysadmins can install gLexec on an NFS partition with the rest of the worker node client, 
and then copy the nfs-mounted gLexec and related libraries to a local disk and point the pilots to this new binary.***

---++Pre-install steps for gLexec

<b>gLexec must be installed as root</b>.

You must create a <b>"glexec" local user</b> before you attempt the installation. gLexec uses it for privilege separation and the installation will fail without it.

Starting in OSG 1.0.0, gLexec also requires a range of <b>group ids</b> for tracking purposes. 
Define at least 4 group ids per batch slot per worker node. They must be consecutive and in any range 
(default range is 65000-65049, configured in post-install section below).   The same group ids can be used on every
worker node, so a good way to handle this is to multiply the number of batch slots on the largest worker node by 6
and then share the group ids between all the worker nodes.

For example to create group ids matching the default range (for 8 batch slots with a little extra):
<pre class="screen">
# groupadd -g 65000 glexec00
# groupadd -g 65001 glexec01
# groupadd -g 65002 glexec02
     :
     :
# groupadd -g 65049 glexec49
# useradd -c "gLexec user" -s /sbin/nologin glexec
</pre>

These group ids will be added to a gLexec configuration file during the post-installation configuration process.

---++Installing the gLexec package

It is recommended that the gLexec package be installed within the OSG
Worker Node Client area because most of its dependencies are the same 
as those within the worker node client.   It must be installed in the *same* 
location on all worker nodes managed by the same Compute Element.
The reason for this is that the location of the gLexec executable is defined on the CE.
Details are described in the [[Trash.ReleaseDocumentationGlexecInstall#Configuring_the_CE_node][Configuring the CE node section]] of this document.
To install:

%STARTOSG12%
<pre class="screen">
# pacman -get %CACHE%:Glexec
# source ./setup.sh
# vdt-post-install
</pre>
%ENDOSG12%

<pre class="screen">
# pacman -get %CACHE%:Glexec
</pre>

Note that the =setup.sh= file is not changed to put the =glexec= binary into your path.
The install will create config files under =/etc/glexec= if the directory does not already exist, create the =/var/log/glexec=
directory if it does not exist, and change the =glexec-osg/sbin/glexec= binary to be suid. 
(If you do not see these, check the vdt-install.log and the post-install README file.

Please also remember to install the CA Certificates as for every VDT install (procedure described in the post-install README file
and also in the [[CaCertificatesUpdates][Install the CA Certificate Updater document]]) or arrange to share them
from a site-central distribution point.
If you use site-centralized CA Certificates distribution,
modify the =/etc/grid-security/certificates= directory to point to a set of current CA Certificates and CRL files.

---++Post-install Configuration of gLexec
The following steps need to be executed manually after the 
=glexec= pacman install completes:

---+++Configure each node with host cert or proxy
Each node which is executing =glexec= needs to have either a host
certificate or a host proxy distributed from the gatekeeper.


If you don't have host certificates for the worker nodes,
there is a host proxy distribution script located at 
=http://fermigrid.fnal.gov/files/glexec/host_dist_latest.tgz=
which can be installed on your gatekeeper and will automatically create
a proxy from your gatekeeper's host certificate and push it out to =/etc/grid-security/hostproxy.pem= and =/etc/grid-security/hostproxykey.pem=.

---+++Modify /etc/glexec/contrib/gums_interface/getmapping.cfg

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg=
to point to the correct location of your GUMS server.  

The =gums_url= variable is the only variable you should have to change unless you
want to use the XACML2 interface; see the "Using the new XACML2 GUMS interface" section below. 

If you're using host proxies instead of host certificates
as described above, you will also need to change =host_x509_file= and =host_key_file= .

<pre class="screen">
# gums_interface/getmapping config file
#
# this is a shell script that gets sourced
#

vdt_location=/opt/vdt
log_dir=/var/log/glexec

########################
# GUMS config parameters
########################

# Use this if you are using host proxies
#host_x509_file="/etc/grid-security/hostproxy.pem"
#host_key_file="/etc/grid-security/hostproxykey.pem"

# Use this if you are using host certs
host_x509_file="/etc/grid-security/hostcert.pem"
host_key_file="/etc/grid-security/hostkey.pem"

# Use this if you have a host cert mapfile
#host_mapfile=/etc/glexec/contrib/gums_interface/host_mapfile

##################################
# Mapping program
map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"
# Point it to your GUMS machine
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"

#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface
#map_exe="/opt/vdt/prima/bin/prima_scas_map_args"
# Point it to your GUMS machine
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"


######################
# Optional SAZ command
# Leave it blank if you don't use SAZ
sazcmd_location=
sazcfg=

# if set to 1, will map even if SAZ fails
saz_warn_only=0

####################
# monitoring program
# leave empty if none should be used
# if set, it will accept arguments
#   mon_pid log_dir job_id target_uid user_DN [vo issuer [user_FQAN]*]
monitor_exe=/opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor

# Should monitoring errors be nonfatal?
continue_on_monitor_error=0

</pre>

---+++ Using the new XACML2 GUMS interface 
Starting with this version of VDT, GUMS and PRIMA can use two different ways to communicate with each other  
   * the old PRIMA legacy protocol, based on SAML v1
   * the new XACML2 protocol defined by the OSG-EGEE interoperability group

The default installation (describe above) will, for backward compatibility reasons, use the old PRIMA legacy protocol.
However, if you installed GUMS v1.3.0 or later, you should switch to the new interface.

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg=, 
commenting out the default =map_exe= and =gums_url=, then
uncomment and configure the alternative one.

<pre class="screen">
##################################
# Mapping program
#map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"
# Point it to your GUMS machine
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"

#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface
map_exe="/opt/vdt/prima/bin/prima_scas_map_args"
# Point it to your GUMS machine
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"
</pre>

---+++Modify /etc/glexec/tracking_groups.cfg

Modify the file =/etc/glexec/tracking_groups.cfg=
with the correct tracking group id range that you created in the pre-installation section.
<pre class="screen">
# This file defines the range of GIDs
# used for tracking purposes
#
# Make sure this range of GIDs is not used for any other purpose
#
min_gid=65000
max_gid=65049
</pre>

---+++Testing gLexec locally on the worker node

Now as a non-privileged user, do the following (where "yourvo" is your vo, etc.):
<pre class="screen">
> bash
>  cd $VDT_LOCATION
>  source setup.sh
>  voms-proxy-init -voms yourvo:/yourvo -file /tmp/x509_xyz
> exit
> export GLEXEC_CLIENT_CERT=/tmp/x509_xyz
> $VDT_LOCATION/glexec-osg/sbin/glexec /usr/bin/id
It appears that the value of pthread_mutex_init is 10603520
uid=13160(fnalgrid) gid=9767(fnalgrid)
</pre>
If gLexec is successful, it will print out the uid and gid that your
proxy would normally be mapped to by your gums server.
The gLexec utility also has a process called glexec_monitor which 
runs as root through the life of the glexec'ed process..

---+++About the gLexec log files
There are four gLexec log files that are made on each worker node and they have 
four different timestamp formats.  They all live by default in /var/log/glexec:
glexec_log, the main log of each glexec stamp, logs in GMT:
<pre>
glexec[14507]: 20080109T173203Z glexec pid: 14507
glexec[14507]: 20080109T173203Z Reading glexec configuration file failed, continue with default values.
glexec[14507]: 20080109T173203Z uid: real/eff cdffgrid(3377)/root(0), gid: real/eff cdf(3200)/root(0)
</pre>
gums_interface.log, the record of getting the gums mapping, logs in local time:
<pre>
glegumsi[23138#23137] 2008-12-02T13:58:17Z Caller account (500,500)
glegumsi[23138#23137] 2008-12-02T13:58:17Z Base subject  '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872/CN=proxy'
glegumsi[23138#23137] 2008-12-02T13:58:17Z Found subject '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Primary   FQAN '/cms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Secondary FQAN '/cms/uscms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:18Z GUMS mapped to user 'uscms466'
glegumsi[23138#23137] 2008-12-02T13:58:18Z Mapped as (504,504)
glegumsi[23138#23137] 2008-12-02T13:58:18Z Launching monitor /opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor
glegumsi[23138#23137] 2008-12-02T13:58:18Z Using tracking GID 65002
</pre>
lcas_lcmaps.log, log of the lcas and lcmap portions, logs with a %RED%zero-indexed month%ENDCOLOR%  (January=0, February-1, etc.).  Time is local time.
<pre>
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 5: 2008-00-09.11:32:03-14507-glexec_get_accounts :       lcmaps_plugin_gu
ms-plugin_run(): Adding POOL_INDEX :  (void)
LCMAPS 0: 2008-00-09.11:32:03-14507-glexec_get_accounts :       lcmaps_plugin_gu
ms-plugin_run(): gums plugin succeeded
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_dummy_good.mod
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-runPlugin()
: running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_dummy_good.mod
LCMAPS 0: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-lcmaps_run_
with_pem_and_return_account(): succeeded
LCMAPS 7: 2008-00-09.11:32:03-14507-glexec_get_accounts : Termination LCMAPS
LCMAPS 1: 2008-00-09.11:32:03-14507-glexec_get_accounts : lcmaps.mod-lcmaps_term
(): terminating
</pre>
For completeness, glexec_monitor.log logs using the GMT format
<pre>
glemon[23093#22904]: 2008-12-02T19:57:06Z Tracking gid: 65001
glemon[23093#22904]: 2008-12-02T19:57:06Z Monitor pid: 23095
glemon[23093#22904]: 2008-12-02T19:57:07Z Parents: 137980749#22904,137980747#22903,137477937#14901,137477234#14871,3906#3203
glemon[23093#22904]: 2008-12-02T19:57:07Z System boot time: 1226868018
glemon[23327#23137]: 2008-12-02T19:58:18Z Started, should use uid 504
glemon[23327#23137]: 2008-12-02T19:58:18Z Used DN: "/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used VO: "cms" Issuer: "/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used FQAN: "/cms/Role=NULL/Capability=NULL"
glemon[23327#23137]: 2008-12-02T19:58:18Z Tracking gid: 65002
glemon[23327#23137]: 2008-12-02T19:58:18Z Monitor pid: 23329
glemon[23327#23137]: 2008-12-02T19:58:19Z Parents: 137987916#23137,137987916#23136,137477937#14901,137477234#14871,3906#3203
glemon[23327#23137]: 2008-12-02T19:58:19Z System boot time: 1226868018
glemon[23093#22904]: 2008-12-02T19:59:07Z Terminated, CPU user 0 system 0
glemon[23093#22904]: 2008-12-02T19:59:08Z Killing procd 23096
glemon[23093#22904]: 2008-12-02T19:59:08Z All childs terminated
</pre>

---+++ Configuring the CE node

After all worker nodes have been configured for glexec, the CE node 
=$VDT_LOCATION/monitoring/config.ini= file needs to be modified to tell the
location of the glexec executable:

<pre class="screen">
[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = YOUR_WN_VDT_LOCATION/sbin/glexec
</pre>

To effect the change, you need to run this on the CE node:
%STARTOSG12%
<pre class="screen">
# source YOUR_VDT_LOCATION/setup.sh
# configure-osg -c
</pre>
%ENDOSG12%

<pre class="screen">
# source YOUR_VDT_LOCATION/setup.sh
# $VDT_LOCATION/monitoring/configure-osg.py -c
</pre>

This will populate the =OSG_GLEXEC_LOCATION= variable in
$VDT_LOCATION/monitoring/osg-job-environment.conf which contains variables
available to all grid jobs.  This is how pilot jobs invoke glexec.

To test this, create a script like this to run on a worker node via your batch system:
<pre class="screen">
#!/bin/bash
echo "OSG_GLEXEC_LOCATION=$OSG_GLEXEC_LOCATION"
export GLEXEC_CLIENT_CERT=$X509_USER_PROXY
"$OSG_GLEXEC_LOCATION" /usr/bin/id
echo glexec result: $?
</pre>

The result should be something like this:
<pre class="screen">
OSG_GLEXEC_LOCATION=/opt/osg-wn-client/glexec-osg/sbin/glexec
uid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)
glexec result: 0
It appears that the value of pthread_mutex_init is -43132256
</pre>

%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.DaveDykstra - 5 May 2009 %BR%
%REVIEW% Main.JohnWeigand - 21 Jul 2009 %BR%
%REVFLAG% %Y% %BR%


---++ *Comments*
%COMMENT{type="tableappend"}%
