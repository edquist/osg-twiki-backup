%META:TOPICINFO{author="BrianLin" date="1497450406" format="1.1" version="1.8"}%
%META:TOPICPARENT{name="Trash.Documentation_Release3TrashReleaseDocumentationPreparingComputeElement"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%
%BR%
---+ %SPACEOUT{ "%TOPIC%" }%

---++ Introduction
*THIS PAGE IS STILL HEAVY WORK IN PROGRESS*


This page is about the installation of a simple Condor batch system, with a head node and worker nodes, that can be used for an OSG site, a Computing Element (CE), or a Tier 3. 
This is not about Trash.ReleaseDocumentationManagedFork and it is not intended as a general [[http://www.cs.wisc.edu/condor/][Condor guide]]; OSG is not providing support for installing cluster fabrics, Operating Systems and management software. This is also not about Condor-cron or Condor-G used in client installations. It is a simple installation and collection of examples targeting Tier 3s not familiar with cluster computing. If you have already a cluster with a local resource manager (PBS, LSF, Condor, SGE, ...) you do not need this. Condor has been chosen over other LRMs not because of any special features but because we have in house expertise and distribution packages available. (Note: Condor does come installed as part of the VDT, but this is only recommended for Trash.ReleaseDocumentationManagedFork jobs or for installing Condor clients and these are beyond the scope of these directions. For installing a Condor batch system, we do _not_ recommend using the version the VDT packaged version.)

For our purpose a cluster is a set of computers connected on a (local) network. The cluster is used to run jobs. One of the nodes, head node, has to be also on a _public_ network . _public_ means visible from the hosts used to submit jobs. The other nodes may be on either a public or private network and are referred to as Worker Nodes (WNs).

The software coordinating the jobs in a cluster is a Local Resource Manager, Condor for our purposes. In Condor the configuration and the components running on the head node are different form those running on worker nodes.

The installation below will take care of both.

---++ Preparing the installation
A cluster of machines is assumed
   * one machine available also on the public network is selected as head node
   * other machines are worker nodes

Condor is not coming automatically with VDT (except when Trash.ReleaseDocumentationManagedFork is installed and no local condor is provided) but it available in VDT and can be added to any OSG Pacman installation.

---++ Installation
There are different options:
   * use a single installation on the head node (e.g. with the OSG CE) and nfs-export it to all the cluster. This is OK for small clusters. Some medium clusters like OU (50-100 nodes) are using it successfully
   * install condor on the CE (as before), and install condor also on the worker nodes using Pacman (e.g. with the WN-client)
   * install condor on the CE (as before), and install Condor as RPM packages on all the worker nodes. This is preferred by big production clusters that have management software like Rocks and CF-Engine 


---+++ Installation only on head node
                                                                                                                                                                                     
Install condor on the CE as explained in Trash.ReleaseDocumentationStandaloneCondorInstallation and NFS-export to all nodes.
In that configuration all you have to do is 

Now make a =local.hostname= directory for each worker node and a condor_config.local for each with the right =DAEMON_LIST=, presumably =MASTER, STARTD=

This is not ideal because Condor runs much better if all the condor binaries are local to the node.                                                                                                                                                      

---+++Installation on head node and worker nodes using Pacman

The head node can be installed as before

You can use the WN-client and install Condor on top of that.

Config file modification would be necessary because by default the VDT condor installs Personal condor which is a full-fledged one node condor installation, with =COLLECTOR, NEGOTIATOR, MASTER, SCHEDD=, and =STARTD=.  
For worker nodes only =MASTER= and =STARTD= are needed.                                                                                                             
                                                                                                                                                                                     
---+++Installation using RPMs

Condor can be installed using RPMs

What we actually do is 
The install of Condor on the worker nodes via rpm can become even easier with systems like Rocks that push the packages for you.
Alternatively the package can be installed and the condor config files can be pushed out  via CFengine.

A side benefit of having condor outside the VDT is that you can get the latest Condor version (sometime _"VDT just doesn't update condor fast enough"_) you can keep all the condor jobs running when you are upgrading or updating your vdt via the Pacman updater script.

                                                                                                                                                                          
Notes from emails
<pre>                                                                                                                                               
>                                                                                                                                                                         
> I think providing information on how to configure condor to not evict                                                                                                   
> jobs would be useful, since I imagine most software run by VO's is not                                                                                                  
> suspendable without loss (certainly we can't suspend CMS software).                                                                                                     
>                                                                                                                                                                         
</pre>

The Condor team has a wonderful set of How-To recipes for condor's admins:
   * http://nmi.cs.wisc.edu/node/1465

---++ Links
OSG Condor update guide. what to do with OSG services when you update an existing condor used by OSG? 
   * coming soon

---++ User examples
UMD, contributed by Malina Kirn
   * http://hep-t3.physics.umd.edu/HowToForAdmins.html#softwareCondor
ANL T3g setup guide based on condor, contributed by Sergei Chekanov
   * https://atlaswww.hep.anl.gov/twiki/bin/view/Trash/Tier3Setup/WebHome

---++ References and related pages
   1 Cluster head node installation: Trash.ReleaseDocumentationStandaloneCondorInstallation
   1 Condor hints: Trash.ReleaseDocumentationMinimalCondorHints
   1 Condor hints 2: ReleaseDocumentation.CondorBatchSystemHints


%STOPINCLUDE%
%BR%
---++ *Comments*
%COMMENT{type="tableappend"}%

%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.MarcoMambelli - 14 Aug 2009 %BR%
%REVIEW%