%META:TOPICINFO{author="RobQ" date="1172852392" format="1.1" version="1.12"}%
%META:TOPICPARENT{name="DocumentationTable"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>%TOPIC%
%TOC%

%STARTINCLUDE%

---++ Introduction

The goal for the Open Science Grid software stack is to provide a uniform interface to sets of scientists organized into virtual organizations for doing science that requires High Throughput Computing across many independently managed computing and storage clusters.

As administrator responsible for deployment of the OSG software stack, your goal will be to make your existing computing and storage cluster available to some or all registered virtual organizations in the OSG. For you to understand what you are in for, it is probably a reasonable start with a description of the expectations of the scientists who will use your cluster via the OSG software stack.

Scientists will send _jobs_ into your cluster's batch system via something called a _Compute Element (CE)_. Some of these scientists will require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in something called a _Storage Element (SE)_. 

---++ Compute Element (CE)

At present, the CE is fairly unambiguously specified, with few choices left to you as site administrator. The standard installation includes a GRAM and gftp interface, both of which are currently required on the same CE host for the =condor_g= client to successfully stage out (smallish) output files that are defined as part of the job's jdl. The host for the CE needs to include a sizeable local disk (~100GB or more) as scientists tend to specify a few (smallish) input and output files as part of their jdl. Those files are spooled onto your cluster via the CE. In addition, stdout and stderr of all jobs submitted via the CE are spooled via the CE out of your cluster.

The main CE related choice for you as site admin is to decide on your security policy with regard to group accounts, dynamic accounts for all users, in short, uid management on the cluster. Other choices are the OS (most if not all OSG clusters are some RHEL derivative), batch system (condor, pbs, lsf, and sge are presently supported), and the network architecture (default assumption is public/private with NAT &mdash; you will need to advertise your architecture by changing some settings by hand if yours isn't like this) of your cluster. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster.

The CE also hosts information provider(s) and monitoring services, most of which are configured correctly by default. Starting with OSG 0.6, we require each and every site to deploy GRATIA, the OSG accounting system. Your site is thus sending accounting records to OSG. Aggregated summaries of this information can be viewed via the [[http://gratia-osg.fnal.gov:8881/gratia-reporting/index.jsp?link=admin.jsp][GRATIA displays]].


---++ Storage Element (SE)

The SE is much less well defined. Scientists expect three types of functionality:
   * an area where they can install application software releases into via the CE, and  that is then read-only accessible from all batch slots on your cluster.
   * an area where they can stage data to using gftp that is then readable from all batch slots.
   * an area where they can stage output files to from all batch slots, for later asynchronous retrieval using gftp.

In addition, two other types of disk space must be available at each batch slot:
   * a set of "client tools" that are part of the OSG software stack. These can be either exported from the CE host, a seperate host, or be deployed locally on each compute node.
   * an area that is strictly local to each batch slot, from within which they run their job(s) during the time they have "leased" the batch slot. You or your batch system is expected to clean this area after the scientists release the batch slot to you.

Requirements, configurations, and expected use are discussed at:
   * [[LocalStorageRequirements][Local Storage Requirements]]
   * [[LocalStorageConfiguration][Local Storage Configuration]]
   * [[UsingLocalStorage][Local Storage Use]]

Some implementation choices, and related information can be found at:
   * [[StorageElementAdmins][Storage Element Admins]]


---++ Documenting Your Site's Policies

Last but not least, a word about policy:

We expect you to specify your site's policy via some web page that becomes part of your site registration, and is available via gridcat at the GOC. This policy should clearly specify your policies with regard to resource access. We suggest that you allow all virtual organizations registered with the OSG at least "opportunistic use" of your resources. This may mean that you preempt those jobs when higher priority jobs come around. The scientists using the OSG generally prefer having access to your site and be subject to preemption over not having any access at all.

---++ Current Documentation Set

   * DocumentationTable


%STOPINCLUDE%

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.RobGardner - 22 Mar 2006<br>
-- Main.FkW - 19 Apr 2006<br>
-- Main.RobQ - 01 May 2006  %BR% 

%WHU%

%META:TOPICMOVED{by="ForrestChristian" date="1166055949" from="Integration.OverviewGuide050" to="Integration/ITB_0_5.OverviewGuide"}%
