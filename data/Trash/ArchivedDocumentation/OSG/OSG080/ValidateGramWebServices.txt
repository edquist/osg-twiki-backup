%META:TOPICINFO{author="BrianBockelman" date="1486491116" format="1.1" version="1.17"}%
%META:TOPICPARENT{name="Trash.ReleaseDocumentationSiteValidationTable"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%
%BR%
---+ _%INCLUDEHEADING%  %SPACEOUT{ "%TOPIC%" }%_
%EDITTHIS%
%BR%

----++ Quick server-side checks
   *  Check that container is running:  ==ps auxw | grep org.globus.wsrf.container.ServiceContainer== 
   *  Check permissions are configured:  ==grep $VDT_LOCATION /etc/sudoers==
   *  Check log file records connections ..e.g.  ==tail -f $VDT_LOCATION/globus/var/container-real.log==


----++ Client-side test of WS-GRAM services

Documentation from Globus on how to execute jobs via WS-GRAM can be found [[http://www-unix.globus.org/toolkit/docs/4.0/execution/wsgram/][here]].

-----+++ !!Command Line tests
   * Test container functions:  ==globusrun-ws -submit -F <i>hostname</i>:9443 -c /bin/true== <pre class="screen">
<verbatim>
ruly % globusrun-ws -submit -F hostname:9443 -c /bin/true
Submitting job...Done.
Job ID: uuid:cc1b94f0-6c61-11dc-8201-000476f3dd75
Termination time: 09/27/2007 18:53 GMT
Current job state: Done
Destroying job...Done.
ruly % echo $?
0</verbatim></pre>
You can also submit a /bin/false job to verify that the value of $? is 1 after the command returns.  This test will verify that the
container on host <i>hostname</i> is up and running, and that the Fork jobmanager can run your jobs.  If it fails, verify that
the container is up on the server using the ps auxw command above or lsof on port 9443, and double-check that the container is
not blocked by a firewall.
   * add batch jobmanager:    ==globusrun-ws -submit -F <i>hostname</i>:9443 -Ft [Condor|PBS|SGE] -c /bin/true== <pre class="screen">
<verbatim>
ruly % globusrun-ws -submit -F hostname:9443 -Ft PBS -c /bin/true
Submitting job...Done.
Job ID: uuid:6523ba32-6c63-11dc-a6b7-000476f3dd75
Termination time: 09/27/2007 19:05 GMT
Current job state: Pending
Current job state: Active
Current job state: Done
Destroying job...Done.
ruly % echo $?
0</verbatim></pre>
This tests that the container is capable of routing jobs to the batch system of your choice.  If you do not see the Pending and
Active states, it could be that there is a firewall between your client and the server, and that your client is not receiving notifications back from the server.  In that case, the client will fall back to polling for status, and may only catch the job once it is done.  The next test will help double-check that.
   * add delegation (output returns to screen):               ==globusrun-ws -submit -F <i>hostname</i>:9443 -s -c /bin/hostname== <pre class="screen">
<verbatim>
ruly % globusrun-ws -submit -F hostname:8443 -s -c /bin/hostname 
Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:4618e4f4-6c64-11dc-9d97-0007e9d81215
Termination time: 09/27/2007 19:11 GMT
Current job state: Active
Current job state: CleanUp-Hold
hostname
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.
</verbatim>
</pre>
This test adds a delegation step where your client delegates a proxy to the container.  When the job is finished running, the job
enters a state of "CleanUp-Hold", which means that the output is being saved until the client retrieves it.  The output is sent back using GridFTP to a port opened by the client.  Once the client is finished getting the output, the job proceeds to a cleanup step where the saved output is deleted, then the job finishes and the delegated credentials are destroyed.

If your client is behind a firewall and your port ranges are not setup, you will instead get an error during Cleanup-Hold like: <pre class="screen">
<verbatim>
globusrun-ws: ignoring error while streaming gsiftp://hostname:2811/home/user/5056668a-6c64-11dc-828f-000476f3dd75.0.stdout:
globus_ftp_client_state.c:globus_i_ftp_client_response_callback:3260:
the server responded with an error
500 500-Command failed. : globus_xio: Unable to connect to client.ip.address:59459
500-globus_xio: System error in connect: No route to host
500-globus_xio: A system call failed: No route to host
500 End.
</verbatim>
</pre>

   * add delegation + batch (output returns to screen):  ==globusrun-ws -submit -F <i>hostname</i>:9443 -Ft [Condor|PBS|SGE] -s -c /bin/hostname== <pre class="screen">
<verbatim>
Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:54ab01a4-6c65-11dc-8e39-0007e9d81215
Termination time: 09/27/2007 19:18 GMT
Current job state: Pending
Current job state: Active
Current job state: CleanUp-Hold
compute-node-hostname
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.
</verbatim>
</pre>
This test is the same as the last, except the job is routed through the batch system of your choice.
   * add simple RSL job file:  ==globusrun-ws -submit -F <i>hostname</i>:9443 -Ft [Condor|PBS|SGE] -s -f hellogrid.xml==
      * <b> hellogrid.xml </b> <pre class="screen">
<verbatim>
<?xml version="1.0"?>
<!-- Simple Job Request With Arguments -->
<job>
    <executable>/bin/echo</executable>
    <argument>Hello,</argument>
    <argument>Grid</argument>
</job>
</verbatim>
</pre>
      * <pre class="screen"><verbatim>Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:c8078226-6c65-11dc-93bd-0007e9d81215
Termination time: 09/27/2007 19:22 GMT
Current job state: Pending
Current job state: Active
Current job state: CleanUp-Hold
Hello, Grid
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.</verbatim></pre>
      This test doesn't stress any new elements of the system, but serves as an introduction to the GRAM4 RSL syntax.
   * add file-staging to remote host:  ==globusrun-ws -submit -F <i>hostname</i>:9443 -Ft [Condor|PBS|SGE] -S -f hostname_file_stage.xml==
      *  <b>hostname_file_stage.xml</b> (Change <i>YourDestinationHostName</i> below to a machine that is running a GridFTP server)<pre class="screen">
<verbatim>
<job>
     <executable>/bin/hostname</executable>
     <directory>${GLOBUS_USER_HOME}</directory>
     <stdout>${GLOBUS_USER_HOME}/hostname_stdout</stdout>
     <fileStageOut>
         <transfer>
          <sourceUrl>file:///${GLOBUS_USER_HOME}/hostname_stdout</sourceUrl>
          <destinationUrl>gsiftp://YourDestinationHostName:2811/tmp/hostname_stdout</destinationUrl>
        </transfer>
     </fileStageOut>
     <fileCleanUp>
         <deletion>
             <file>file:///${GLOBUS_USER_HOME}/hostname_stdout</file>
         </deletion>
     </fileCleanUp>
</job>
</verbatim>
</pre>
<pre class="screen"><verbatim>Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:77df3432-6c66-11dc-a2f3-0007e9d81215
Termination time: 09/27/2007 19:27 GMT
Current job state: Active
Current job state: StageOut
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.

YourDestinationHostName $ cat /tmp/hostname_stdout 
hostname</verbatim></pre>
Note that you used -S in the client command, not -s.  In this example we don't use streaming back to the client at all.  Instead,
we stage out the results to <i> YourDestinationHostName</i>.  However, we still need to delegate a staging credential, and the
-S flag makes that happen.  Because there is no streaming, there is no Cleanup-Hold stage.  Instead, there is a StageOut stage where the file staging directives make the requested transfer.

-----+++ !!Client-side test using Condor-G with WS-GRAM 
 
   * <b>Condor version must be greater than 6.8.3 for using Condor-G with WS-GRAM of GT4.0.5 on clients with local firewalls restricting ephemeral port access </b>
      * To check your version:  ==condor_version== 
      * OSG-0.6. Clients, VDT-1.6.1 have Condor versions of 6.8.3
   * Create and submit condor submit file using grid universe and gt4 host:  ==condor_submit hostname_submit_file.cmd==
      * <b> hostname_submit_file.cmd </b> (Specify <i>hostname</i> and batch resource) <pre class="screen">
#specify resource destination
Universe            = grid
grid_resource       = gt4 https://<i>hostname</i>:9443 [Condor|PBS|SGE]

# specify executable
Executable          = /bin/hostname
Transfer_Executable = false
 
# copy stdout stderr to local files, referenced by job (Process) and submission id (Cluster)
output  =  mytest.out.$(Cluster).$(Process)
error   =    mytest.err.$(Cluster).$(Process)
 
#a single local log file for tracking Condor-g submission
log    = mytest.log
 
# do not send email notification
notification=Never
 
# submit 2 identical jobs: Process=0 and Process=1
Queue 2
</pre>
      * monitor jobs progress with ==condor_q==



-----+++ !!Known Problems & Solutions using Condor-G with WS-GRAM 

   * As mentioned above, Condor installation must above <b>Version 6.8.3</b> to work with WS-GRAM Globus-4.05
   * Condor-g will start a gridftp server on the client machine. If however the Client machine is configured with a gridftp server,  it will use that server which may resolve user authentication differently.  This is fixed by inserting the following lined into the client's condor/libexec/gridftp_wrapper.sh:
      * <pre class="screen">
GSI_AUTHZ_CONF=/doesnotexist
export GSI_AUTHZ_CONF
</pre>
  
%STOPINCLUDE%
%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.SuchandraThapa - 26 Oct 2007 %BR%
%REVIEW% Main.CharlesBacon

%META:TOPICMOVED{by="AnneHeavey" date="1192741310" from="Integration/ITB_0_7.ValWSGram" to="Integration/ITB_0_7.ValidateGramWebServices"}%
