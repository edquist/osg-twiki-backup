%META:TOPICINFO{author="BrianBockelman" date="1486494334" format="1.1" reprev="1.13" version="1.13"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction

OSG_DATA is a storage area for transient / volatile files that are shared between jobs that are executing on the worker nodes. It provides similar functionality to the comibination of [[Trash.ReleaseDocumentationStorageParameterOsgSiteRead][OSG_SITE_READ]] and [[Trash.ReleaseDocumentationStorageParameterOsgSiteWrite][OSG_SITE_WRITE]] and may be easier for small sites to deploy. However, [[Trash.ReleaseDocumentationStorageParameterOsgSiteRead][OSG_SITE_READ]] and [[Trash.ReleaseDocumentationStorageParameterOsgSiteWrite][OSG_SITE_WRITE]] are usually efficient for big production sites with specialized hardware, because it forces the separation between input and output.

Currently, we discourage both users from using OSG_DATA and site administrators from removing it. Users may need time to ween themselves from using OSG_DATA.

Relative paths must resolve consistently between gatekeeper and worker nodes.  

This area is intended to hold:
   * data shared among different jobs running on different worker nodes
   * data that has to outlive the execution of the jobs on the worker nodes

Regular programs should have open/read/write permissions on OSG_DATA, because they may use it transparently as a local disk space --  NFS, <!-- dcap, drm, --> AFS, CIFS are OK). This is a subset of a POSIX-compatible filesystem that excludes features such as creating special files (e.g., pipes, sockets, locks, links) and modifying file metadata.

Gridftp or "SE-like" access from outside of the cluster is required to allow an efficient use of OSG_DATA as a staging area.

Since the allocation of this space is transient, it is important that users remove unused data and/or that a simple mechanism to allow cleanups is added.

The use of OSG_DATA as a writable area from compute nodes is one of the most significant performance bottlenecks in an OSG cluster. Use of OSG_DATA as the "hold all read/write area" (working area for the jobs) is discouraged.

---++ Using OSG_DATA to provide more scalable and reliable data access
   * Write data to OSG_DATA via gridftp or, if necessary, fork jobs (unpack tarballs, etc.)</li>
   * Stage the job into the cluster</li>
   * The job copies its data to the compute node ([[StorageParameterOsgWnTmp][OSG_WN_TMP]]) or reads data sequentially from OSG_DATA, if the data is read once. 
%WARNING% Reading data sequentially should be avoided. It significantly affect performance on typical network files systems if many random reads are necessary. Random data access over large data sets is where grid storage shows its potential &mdash; its distributed nature is better suited for handling that type of data access scalably and reliably.
   * Job output is placed in [[StorageParameterOsgWnTmp][OSG_WN_TMP]]</li>
   * At the end of job, the results from [[StorageParameterOsgWnTmp][OSG_WN_TMP]] are packaged, staged to OSG_DATA and picked up via gridftp or staged out via other mechanisms (such as srmcp).
%WARNING% If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if your home directories are local or it is another OSG_DATA de facto. Again, at present we discourage site administrators from removing OSG_DATA and discourage users from using OSG_DATA.

Typical uses of this area are:
   * Input datasets for jobs executing on the worker nodes
   * Datasets produced by the jobs executing on the worker nodes
   * Shared data for MPI applications
   * Data staged in or waiting to be staged out

---++ Notes
   1. The ability to modify file permissions may be required in future revisions -- it is available in SRMv2
   1. The [[StorageParameterOsgDefaultSe][OSG_DEFAULT_SE]] entry may be used to publish the base GSIftp URL of the SE viewing that space.
   1. An example of a simple space management solution could be a file in each directory ( =.keep=) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested), all files in that directory and subdirectories may be removed. This is a "gentleman's agreement"; keep in mind that none of the data in a transient / volatile storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
   1. For example, the current Condor jobmanager uses a shared space (=gass_cache= by default) when it processes requests to transfer the executable or some data. You should not assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, removing a shared space like OSG_DATA for performance issues is useless if you only reintroduce it as a shared home directory, as that will probably also adversely affect performance because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate shared home directories at the same time. See the reference below to using nfs lite for more information.

---++ Further Reading
   * [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382][NFS lite CE Installation]]

%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.SuchandraThapa - 19 Oct 2007 %BR%
%REVIEW% 

%META:TOPICMOVED{by="SuchandraThapa" date="1192832937" from="Integration/ITB_0_7.AboutStorageAreaOsgData" to="Integration/ITB_0_7.StorageParameterOsgData"}%
