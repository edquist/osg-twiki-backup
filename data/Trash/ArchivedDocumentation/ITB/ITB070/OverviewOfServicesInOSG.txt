%META:TOPICINFO{author="KyleGross" date="1225985925" format="1.1" version="1.27"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Background
The goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters.  Scientists organized into virtual organizations (VOs) and requiring High Throughput Computing are the users of the interface and consumers of the CPU cycles and storage. 

Your site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them. 

As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support.  The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. As end-user scientists send jobs into your cluster's batch system, your CE  receives them and parses them out to Worker Nodes (WN) for execution. Some VOs and scientists require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE) on which more OSG storage-specific software is installed. A site is not required to provide both a CE and an SE.

---++ Site Policies

OSG expects you to clearly specify your site's policies regarding resource access.  Please write them on a web page, make this page part of your site registration, and make it available via VirtualOrganizations/VOInfoRS. 

We encourage you to allow all virtual organizations registered with the OSG at least "opportunistic use" of your resources. You may need to preempt those jobs when higher priority jobs come around. The scientists using the OSG generally prefer having access to your site subject to preemption over
having no access at all.

---++ Compute Element (CE)

The standard installation includes a GRAM and gftp interface; both are currently required on the same CE host for the condor_g client to successfully stage out (smallish) output files that are defined as part of the job's jdl. The CE host needs to include a sizeable local disk (~100GB or more) as scientists tend to specify a few (smallish) input and output files as part of their jdl. Those files in addition to stdout and stderr of all jobs submitted via the CE are spooled via the CE out of your cluster. 

You must determine your security policy with regard to uid management on the cluster. You may choose group accounts and/or dynamic accounts for all users.

You must choose the OS (most if not all OSG clusters are some RHEL derivative), the batch system (condor, pbs, lsf, and sge are presently supported), and the network architecture (default assumption is public/private with NAT - you will need to advertise your architecture by changing some settings by hand if yours isn't like this) of your cluster. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster. 

The CE hosts information provider(s) and monitoring services, most of which are configured correctly by default. Starting with OSG 0.6, we require all OSG sites to deploy GRATIA, the OSG accounting system. Your site thus sends accounting records to OSG. Aggregated summaries of this information can be viewed via the 
[[http://gratia-osg.fnal.gov:8881/gratia-reporting/index.jsp?link=admin.jsp][GRATIA displays]].

---++ Storage Element (SE)
%INCLUDE{ "Integration.StorageElementAdmins" section="SEintro" }%

<!-- The SE is currently much less well defined than a  CE. Scientists expect three types of areas: 

   * an area into which they can install application software releases via the CE; this must be read-only accessible from all batch slots on your cluster. 
   * an area to which they can stage data using gftp; this must be readable from all batch slots. 
   * an area to which they can stage output files from all batch slots, for later asynchronous retrieval using gftp. 


In addition, the following must be available at each batch slot: 

   * a set of "client tools" that are part of the OSG software stack. These can be either exported from the CE host, from a separate host, or be deployed locally on each compute node. 
   * an area that is strictly local to each batch slot, from within which they run their job(s) during the time they have "leased" the batch slot. You or your batch system is expected to clean this area after the scientists release the batch slot to you. 
-->

---++ Optional site services

---++ VirtualOrganizations/VOInfo management services

---++ OSG user software


%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.RobGardner %BR%
%REVIEW%

%META:TOPICMOVED{by="RobGardner" date="1192827019" from="Integration/ITB_0_7.OverviewServicesOSG" to="Integration/ITB_0_7.OverviewOfServicesInOSG"}%
