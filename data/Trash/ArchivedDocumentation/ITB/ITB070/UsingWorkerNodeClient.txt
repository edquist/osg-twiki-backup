%META:TOPICINFO{author="BrianBockelman" date="1486489745" format="1.1" version="1.7"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%RED%This document is in the process of being morphed from what used to be the "Using Local Storage" 
document, to being a "using the worker node client" document.%ENDCOLOR%

%WARNING% Storage definitions and implementations are not consistent across OSG sites.

---++ Storage Types Supported at OSG Sites
All OSG sites should support the following storage:

%EDITTABLE{ header="| *Name* | *What you can do* | *Notes* |" format="| text | textarea, 2x30 | textarea, 2x30 |" changerows="on" }%
| *Name* | *What you can do* | *Notes* |
| =$OSG_APP= | Install application software releases into via the CE. | After install, area is read-only accessible from all batch slots on your cluster. |
| ?? | Stage data to using gftp that is then readable from all batch slots. | %RED% OSG sites have not consistently deployed this area. %ENDCOLOR% |
| ?? | Stage output files to from all batch slots, for later asynchronous retrieval using gftp. | %RED% OSG sites have not consistently deployed this area. %ENDCOLOR% |
| =$OSG_GRID= | Set of "client tools" that are part of the OSG software stack. | Available at batch slot |
| =$OSG_WN_TMP<sup>[[#FootOsgWnTmpImplementedPoorly][1]]</sup>= | Temporary storage area in which your job(s) run | Local to each batch slot |

#FootOsgWnTmpImplementedPoorly
   1. %RED%OSG sites have inconsistently implemented $OSG_WN_TMP.%ENDCOLOR% The standard is to start your job(s) in a dedicated directory on one of the worker node's local disks and configure $OSG_WN_TMP to point to that directory. However, many OSG sites simply place your jobs on some shared filesystem and expect you to do the following before starting the job: %BR% =export mydir=$OSG_WN_TMP/myWorkDir$RANDOM ; mkdir $mydir ; cd $mydir= %BR% . This is particularly important if your job has significant I/O.

   * *%RED% How do I know which type of system I am coming into? Why is it particularly important if you have big IO? %ENDCOLOR%*


---++ Input and Output Files Specified via condor-g jdl
Condor-g allows specification of the following:

| =executable= | one file |
| =output= | one file - stdout | 
| =error= | one file - stderr |
| =transfer_input_Files= <sup>[[#ThirdNote][1]]</sup> | comma seperated list of files |
| =transfer_output_files= <sup>[[#FourthNote][2]]</sup> | comma seperated list of files |

%WARNING% <a name="ThirdNote"></a>Do not use to transfer more than a few megabytes: these files are transfered via the CE headnode and can cause serious loads, which can bring down the cluster. 
%WARNING% <a name="FourthNote"></a>Do not spool gigabyte-sized files via the CE headnode by condor file transfer. Space on the headnode tends to be limited, and some sites severely quota the gram scratch area via which these files are spooled. Instead, store them in the dedicated stage-out spaces and pull them from the outside as part of a DAG. 

In the remainder of this document we describe how to find your way around these various storage areas both from outside the site before you submit your job, as well as from inside the site after your job starts inside a batch slot.

---++ Finding your way around
%NOTE% A definition of the concepts used in this document can be found in [[Trash.ReleaseDocumentationLocalStorageConfiguration][Local Storage Configuration]], including a section describing minimal requirements and some sample configurations.

Here we describe how to find the various storage locations. We start by describing how to find things after your job starts, and then complete the discussion by describing what you can determine from the outside, before you submit a job to the site. We deliberately do not describe what these various storage implementations are. That is done in the [[Trash.ReleaseDocumentationLocalStorageConfiguration][Local Storage Configuration]] document.

---++ Setting Up $OSG_GRID
First see [[RunningSourceOsgGridSetupSh][Running "source osg_grid_setup.sh"]].

---+++ !!Getting Information About CE Storage Before You Submit a Job
OSG sites advertise their properties via the _Generic Information Provider (GIP)_. This information can be querried from outside the site. It is meant to be used to select sites that support the functionality you need for your applications. 

<!-- Pulls definition from Documentation.GlossaryOfTerms and puts it into a blue-bordered centered box. -->
%INCLUDE{ "Documentation.ToolsInsetInclude" section="GlossaryTerm" INCLUDETOPIC="Documentation.GlossaryG" TERM="Generic Information Provider" }%

The GIP uses the GLUE schema, and all information may be read via LDAP queries. The following fields are of particular importance in the context of storage:

%TABLE{ valign="top" }%  
| *GLUE Schema name* | *OSG Correspondence* | *Notes*  | 
| !GlueCEInfoApplicationDir | =$OSG_APP= |    |
| !GlueCEInfoDataDir | =$OSG_DATA= |     |
| Worker node client directory | =$OSG_GRID= | _Published with:_  %BR% !GlueLocationLocalID=OSG_GRID  %BR%   !GlueLocationName=OSG_GRID  %BR%   !GlueLocationPath= the path to the directory |

%NOTE% The GLUE schema allows the site to configure different paths for SITE_READ and SITE_WRITE and DEFAULT_SE for each VO. However, few sites are likely to do this as it requires manual editing of the schema file.

%INCLUDE{ "Documentation.ToolsInsetInclude" section="GlossaryTerm" INCLUDETOPIC="Documentation.GlossaryG" TERM="Grid Laboratory" }%

---+++ !!Selecting GLUE Schema Attribures Using =ldapsearch=
The following syntax for ldapsearch works to get all !GlueLocationLocalID &mdash; there are several &mdash; from fngp-osg.fnal.gov.

<pre class="screen">
$ <b>ldapsearch -x -h fngp-osg.fnal.gov -p 2135 \
  -b GlueClusterUniqueId=fngp-osg.fnal.gov,mds-vo-name=local,o=grid \
  GlueLocationLocalID </b>

   version: 2
   
   #
   # filter: (objectclass=*)
   # requesting: GlueLocationLocalID
   #
   
   # fngp-osg.fnal.gov, local, grid
   dn: GlueClusterUniqueID=fngp-osg.fnal.gov, mds-vo-name=local,o=grid
   
   # fngp-osg.fnal.gov, fngp-osg.fnal.gov, local, grid
   dn: GlueSubClusterUniqueID=fngp-osg.fnal.gov, GlueClusterUniqueID=fngp-osg.fna
    l.gov, mds-vo-name=local,o=grid
   
   # OSG_SITE_READ, fngp-osg.fnal.gov, fngp-osg.fnal.gov, local, grid
   dn: GlueLocationLocalID=OSG_SITE_READ, GlueSubClusterUniqueID=fngp-osg.fnal.go
    v, GlueClusterUniqueID=fngp-osg.fnal.gov, mds-vo-name=local,o=grid
   GlueLocationLocalID: OSG_SITE_READ
   
   # OSG_SITE_WRITE, fngp-osg.fnal.gov, fngp-osg.fnal.gov, local, grid
   dn: GlueLocationLocalID=OSG_SITE_WRITE, GlueSubClusterUniqueID=fngp-osg.fnal.g
    ov, GlueClusterUniqueID=fngp-osg.fnal.gov, mds-vo-name=local,o=grid
   GlueLocationLocalID: OSG_SITE_WRITE
   
   # OSG_GRID, fngp-osg.fnal.gov, fngp-osg.fnal.gov, local, grid
   dn: GlueLocationLocalID=OSG_GRID, GlueSubClusterUniqueID=fngp-osg.fnal.gov, Gl
    ueClusterUniqueID=fngp-osg.fnal.gov, mds-vo-name=local,o=grid
   GlueLocationLocalID: OSG_GRID
   
   # search result
   search: 2
   result: 0 Success
   
   # numResponses: 6
   # numEntries: 5
</pre>


---++ Examples
[[%SCRIPTURL%/Trash/Trash/Trash/Trash/Integration/LocalStorageUse/bdii_query.pl.txt][This PERL script]] %RED%Bad link -- where is this file?%ENDCOLOR% can be used to query either the GRIS or the BDII to get the OSG_GRID area.

%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.StevenTimm - 18 Oct 2007 %BR%
%REVIEW% Main.AlainRoy  - 19 Oct 2007