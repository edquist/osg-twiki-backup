%META:TOPICINFO{author="KyleGross" date="1225985928" format="1.1" version="1.17"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---+ Introduction

_Please note_: This is a guide to planning an OSG installation that provides access to your site for jobs and data. It is not intended for clients. 

Installing a large OSG site can be daunting for a couple of reasons. First of all, you will almost certainly need to install software on multiple computers. Second, you will need to have a clear plan before you proceed. Please don&rsquo;t just dive into the installation.

There are different kinds of installations you might perform. They include:

   * *A compute element (CE)* The computing element is the front-end to your computer cluster. Users submit jobs to the CE, which submits the jobs to your local batch system. The OSG software assumes that you have installed your local batch system yourself and provides no help with that. 

   * *A worker node (WN)* A worker node is a single computer (perhaps with multiple CPUs or cores) in your computer cluster on which jobs will run and/or data will be stored.

   * *A user mapping service* When user jobs arrive at your site, they are identified by a certificate containing the user's name. There are two ways to map a user to the local user id. One of them is a service called _GUMS_. It is a good idea to run GUMS on a separate computer from your CE. (The other method is not a service and so is never run on a separate computer. See _edg-mkgridmap_ below.)

   * *A virtual organization management service (VirtualOrganizations/VOInfoMS)* If you are running a virtual organization (VO), you need to control who is a member of your VO. You do this with VirtualOrganizations/VOInfoMS. It is a very good idea to run this on a separate computer from your CE. 

   * *A storage element (SE)* A storage element manages and controls access to storage at your site. Storage management can be one of the more complicated systems to set up. OSG provides two different SEs: one based on dCache and one based on Bestman. dCache is often installed on multiple computers; it should NOT be installed on your CE. Bestman is more appropriate for smaller sites, but it is still recommended to run it on a machine other than your CE.

   * *A web proxy* Some users submit jobs that download their executables or data files from a web site. In order to conserve bandwidth and to help applications run faster, you might wish to install a web proxy. OSG provides Squid for your use. It is a good idea to run this on a separate computer from your CE.

   * *A shared file system* OSG recommends the use of a shared file system so that some files are accessible on all worker nodes. We do not provide instructions on how to set up a shared fileystem, but instead expect you to do that yourself. However, we will tell you what needs to be shared.

   * *A user mapping service* A mapping service maps global user identifiers (as taken from certificate distinguished names) to local user identities. OSG uses GUMS. GUMS is effectively an authorization service as well: all users that are mapped, and only those users, are allowed access. 

If you install all of these services at your site, you could easily have several computers (and possibly many more) running OSG software. 

---++ What should you install? 

If you want to allow users to run jobs on your computing cluster, at a minimum you need to install the computing element (CE) and worker node (WN). 

Depending on how you want to control access to your site, you might want to install GUMS. 

If your users are likely to access large amounts of data (many gigabytes or perhaps even terabytes), you need an SE.

If you run a virtual organization (VO), you should install VirtualOrganizations/VOInfoMS (and VirtualOrganizations/VOInfoMRS with it, to make it easier to manage).  

---++ The simplest possible OSG compute cluster

     <img src="%ATTACHURLPATH%/BasicCE.gif" alt="BasicCE.gif" width='616' height='407' />

There are a lot of possible variations here: this is just one example of how it might be set up. Two major notes:

   1. The worker node client software must be installed on all worker nodes. Most sites install this just once using a shared file system for simplicity, but you are not required to use a shared file system. You can choose to install it on each node if you prefer. 
   1. At a minimum, the shared file system must include =$OSG_APP=, which is the location that users put applications shared between jobs. It may also include other directories, [[#SharedFS][See below for more information]].

---++ What a small site might look like

<img src="%ATTACHURLPATH%/SmallSite.gif" alt="SmallSite.gif" width='616' height='443' />

Although the picture doesn't show it clearly, the shared filesystem is accessible by all cluster nodes as well as the CE. 

This structure might be sufficient, but it is starting to push its limits. The problem is the number of services running on the CE: because this is your interface to OSG, you want to keep it from being overloaded. We recommend breaking things up so that not so many things are on the same computer. It would be more scalable to add two computers to this site:

<img src="%ATTACHURLPATH%/SmallSite-Modified.gif" alt="SmallSite-Modified.gif" width='616' height='672' />

Note that there is also likely to be a !GridFTP server running on the CE, but the majority of file transfers should go through the SE. People who submit to your site but are unaware of the SE may use the !GridFTP server on the CE. 

But now there is a problem: you have two different computers doing authorization/user mapping independently. We can add a GUMS service so that all of your computers are synchronized. 

---++ What a medium size site might look like

This is very similar to the previous example, except that we have added a GUMS server:

<img src="%ATTACHURLPATH%/MediumSite.gif" alt="MediumSite.gif" width='673' height='667' />

GUMS has two significant advantages over static grid mapfiles.

   1. User mappings are maintained in a single location. 
   1. GUMS dynamically handles VirtualOrganizations/VOInfoMS information, while grid mapfiles don&rsquo;t. This is explained further in [[#Mapping_users][Mapping users]].

---++ What a large site might look like

<img src="%ATTACHURLPATH%/LargeSite.gif" alt="LargeSite.gif" width='808' height='657' />

Notice we've used dCache in this diagram rather than Bestman: it's better for a large site. There are different ways to set up dCache: don&rsquo;t take this picture as suggesting that this is the only way. For example, in some setups, there are one or more pool nodes that are distinct from the cluster nodes. In other setups each of the cluster nodes is also a pool node.

---+ What you need to do

---++ Certificates

In general, your CE will need two certificates: a host certificate and an http certificate. A squid proxy doesn't need any certificates. 

Your installation will go most smoothly if you get your certificates first. 

[[GetGridCertificates][More information about the certificates you need]]

#SharedFS
---++ Shared file system directories

You need to run a shared file system if you support job submission to your site. There are four common directories that are shared: not all of them are required. 

| *Name* | *Required?* | *Purpose* |
| OSG_APP | Yes| Store applications used by multiple jobs |
| OSG_DATA | No, but highly recommended | Store data used by jobs |
| OSG_GRID | No | Location of worker node client |
| HOME | Usually | Users' home directories. Required unless you are using Condor NFS-Lite job manager |

More details can be found in [[LocalStorageConfiguration][Local Storage Configuration]]

---++ Mapping users

There are two methods for mapping users to local userids: _edg-mkgridmap_ and _GUMS_. If you can afford the time and complexity, we recommend using GUMS. 

OSG has many users that are members of multiple VOs. These users can distinguish which VirtualOrganizations/VOInfo under which they submit a job by creating a _VOMS proxy_ instead of the more common _grid proxy_. A VirtualOrganizations/VOInfoMS proxy is a just a grid proxy with extra information (an _attribute certificate_ that identifies the VirtualOrganizations/VOInfo and optionally the user's role within it). 

*edg-mkgridmap* is a small program that runs four times a day. Each time it runs, it contacts all of the VirtualOrganizations/VOInfoMS servers for the VOs that you support to find out all of the users in the VOs, then recreates a _grid-mapfile_. This is a file that is used by Globus to map users. It simply lists each users _distinguished name (DN)_ followed by the account that they map to. 

edg-mkgridmap easy to configure and use. It is trivial to see the mappings that will be used. However, there are a several problems with edg-mkgridmap:

   * A grid-mapfile does not interpret any of the information in a VirtualOrganizations/VOInfoMS proxy. The end result is that a multiple-VirtualOrganizations/VOInfo user has no ability to control what VirtualOrganizations/VOInfo their job appears under, and it may get mapped incorrectly at your site. 
   * If there are several computers in your setup that do authorization, and edg-mkgridmap fails at some of them (or simply runs out of sync), the list of users mapped at each of your computers may be different and thus incorrect.

*GUMS* is a service that is performs user mapping on behalf of other software. It periodically contacts the VirtualOrganizations/VOInfoMS servers that represent the VOs you support and sets up mappings. GUMS is a centralized service at your site, so each computer at your site will make the same mapping decisions. In addition, GUMS understands the information from a VirtualOrganizations/VOInfoMS proxy, so it will use that information when mapping users. On the other hand, GUMS is an additional complexity for your site: it is yet one more service to maintain and monitor. Because it is a fundamental piece of your security infrastructure, we recommend that it run on a different computer from your CE. Since users can run jobs directly on your CE (via the _fork_ interface), this avoids interference with GUMS if a security flaw were found. 

[[AboutAuthorizationForCE][More details on Authorization]]

---++ Registering with OSG
Note that you will need to register your site with the OSG Grid Operations Center (GOC). This can be done at the [[OIM][https://oim.grid.iu.edu]] registration pages. You will need to have a valid IGTF certificate loaded into your browser to access OIM.

---++ Advanced notes

---+++ glexec
*glexec* is a service to help so-called _glide-in_ or _pilot_ jobs interact with the security at your site. 

Some users on OSG use a technique called _glide-in_: they run a job that doesn&rsquo;t do the actual computation. Instead, it pulls in the actual computation to do the work when it is ready. Glide-in is conceptually simple but there are many complexities in practice: a full discussion is beyond the scope of this document. 

glexec solves one particular problem with glide-in jobs when they pull in work from a user other than the glide-in job submitter. Your local policy might require accounting and/or security measures. Glexec will assist with this by ensuring the the work job is properly authenticated and authorized. Details on installing glexec are in the CE installation guide. Note that glexec is a setuid script so it must be installed locally, not on NFS. 

   * *For more information about glexec* see GlexecInstall.

---+++ Site Best Practices

We have some technical details about [[SiteFabricBestPractices][site fabric best practices]]. 


%STOPINCLUDE%
%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.AlainRoy - 18 Oct 2007 %BR%
%REVIEW% Main.RobGardner - 06 Jun 2008 %BR%
%REVFLAG% %Y% %BR%

%META:FILEATTACHMENT{name="BasicCE.gif" attr="" autoattached="1" comment="" date="1193861491" path="BasicCE.gif" size="19744" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="MediumSite.gif" attr="" autoattached="1" comment="" date="1195511759" path="MediumSite.gif" size="24847" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SmallSite.gif" attr="" autoattached="1" comment="" date="1195511660" path="SmallSite.gif" size="24073" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="LargeSite.gif" attr="" autoattached="1" comment="" date="1195503184" path="LargeSite.gif" size="27102" user="UnknownUser" version=""}%
%META:FILEATTACHMENT{name="SmallSite-Modified.gif" attr="" autoattached="1" comment="" date="1195511683" path="SmallSite-Modified.gif" size="29954" user="UnknownUser" version=""}%
