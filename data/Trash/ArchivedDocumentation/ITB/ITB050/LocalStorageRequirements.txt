%META:TOPICINFO{author="RobQ" date="1154712287" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="DocumentationTable050"}%
---+!! Local Storage Requirements

%TOC%

---++ Introduction

The goal of this page is to provide a recommendation for the definitions and environment variables for the Local Storage accessible to Compute Elements on OSG. It covers the definition of these spaces and correspondences to external schemas (e.g. GLUE, Grid3/OSG).
The paths defined with the names below are sometimes also called _CE storages_. They are disk spaces or SEs accessible from within the CE .
<!-- (the same from each jobmanager of that CE: jobs on the headnode using _fork_, jobs on the worker nodes usong _pbs/condor/lsf/..._, multiple gatekeeper added to the same CE for reliability or scalability reasons will have to have the same values). -->
The keyword <i>UNAVAILABLE</i> will be provided instead of the path when the CE is not supporting a particular CE storage, distinguishing an unconfigured site from one which provides support for only certain CE storages.
The values provided do not refer to any disk space as viewed from outside (SE, !GridFTP server, ...).

<!-- In this document there are some suggestions on how to encourage proper use and deployment (how to make this information available), but in general this is the responsibility of other groups in OSG.
-->
Possible technologies to deploy CE storage include (but are not limited to):
   1. Variables defined in the environment that resolve to the correct path or URL
   2. Path or URLs consistent across the CE (headnodes and WN), published using information provider (e.g. GIP/BDII)
   3. *DEPRECATED* !GridCat

Solutions 2 and 3 may involve a lookup to the information system that can be done before submitting the job (in that case the job will carry along the information to be able to use the CE).

<!-- This page has a sister page, LocalStorageRequirementsDiscussion, that includes the discussion that led to this document. Please read it and feel free to add your ideas to that page if you are interested in proposing changes.
-->

LocalStorageConfiguration provides installation/configuration notes and examples.
LocalStorageUse is a document for users containing best practices, notes and examples.

In the following table there is a name matrix:
   * CE Storage corresponds to the names in this document and the variables in =osg-attributes.conf=
   * GLUE is the attribute name as in GLUE Schema 1.2 (The same attribute may appear in more than one place in the Schema)
   * Grid3/OSG is the LDAP attribute name as in Grid3 Schema

| *CE Storage*    |*GLUE*|*Grid3/OSG*|
| OSG_GRID  | Location.Path (*2)  | Gri3Dir    | 
| OSG_APP   | CE.Info.ApplicationDir (!CE.Info.ApplicationDir) (*1)    | Gri3AppDir    | 
| OSG_DATA  | CE.Info.DataDir (!CE.VOView.DataDir) (*1)                | Gri3DataDir   | 
| OSG_SITE_WRITE | Location.Path (*2) |   |
| OSG_SITE_READ  | Location.Path (*2) |   |
| OSG_WN_TMP | CE.Cluster.WNTmpDir (!CE.SubCluster.WNTmpDir)   |  |
| OSG_DEFAULT_SE | CE.Info.DefaultSE (!CE.VOView.DefaultSE)   |     |
d52 1
*1. As visible from the table above, GLUE provides the possibility to have multiple values for some of the CE storage, depending on the VO and the Role (VOMS FQAN). In OSG these are currently sitewide information.<br>
*2. The GLUE Schema does not have an specific attribute for SITE_WRITE or SITE_READ, but it provides the location entity (Name/Version/Path sets) to accommodate additional CE local storage. In order to accommodate that, two locations will have to be defined through the GIP:
   1. !LocalID: GRID+OSG, Name:GRID, Version: OSG, Path: <value of GRID>
   1. !LocalID: SITE_WRITE+OSG, Name:SITE_WRITE, Version: OSG, Path: <value of SITE_WRITE>
   1. !LocalID: SITE_READ+OSG, Name:SITE_READ, Version: OSG, Path: <value of SITE_READ>

Each CE administrator will provide for the correct functioning of the client software (Globus and SRM: access to the users proxy, gass_cache mechanism) for all the jobs running on the CE.  Common practice is to use a shared $HOME directory, but the administrator is free to use other mechanisms transparent to the users. Users should have no other assumption about the CE different from what is stated in this document. It is unsafe to make assumptions about the existence and characteristics (size, being shared, ...) of the $HOME directory. In particular, site admins are free to deploy configurations that do not
include any NFS exports from the CE, e.g. as described in OSG document 
<a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382">382</a>.

The following sections describe the different storage areas that may be declared local to a CE. Each section includes:
The following sections describe the different storage areas that may be declared local to a CE.
Each section includes:
   * brief description
   * detailed description
   * use cases (informal)
   * notes

---++ Known Problems

---++ OSG_GRID
%INCLUDE{ "AboutStorageAreaOsgGrid" }%
This area is read-only, intended for client software provided by OSG.
The CE administrators will install the client software included in the OSG-WN-Client package in this area (see WorkerNodeClient for details.)  It is required that relative paths and content are consistent between gatekeeper and worker nodes even if OSG_GRID itself (the base dir) differs between the two.  It is up to the site administrator to either provide a shared directory, or to locally install it on each machine. OSG_GRID may be included directly into the PATH defined for jobs running at the CE.
In cases when WorkerNodeClient installation leads to OSG_GRID that differs between CE (i.e. gatekeeper)
In cases when [[WorkerNodeClient050][Worker Node Client]] installation leads to OSG_GRID that differs between CE (i.e. gatekeeper)
and worker nodes, it is required to add appropriate softlinks on the worker node such that jobs that
assume the same base path will work.
Relative paths and content must be consistent between the gatekeeper and worker nodes, even if the base OSG_GRID directory is different. If OSG_GRID differs between the CE/gatekeeper and worker nodes, create softlinks on the worker node that will recreate the gatekeeper's base path for OSG_GRID.
Typical uses of this area are:
---+++ Typical uses of OSG_GRID
   * Provide a common set of OSG client software (_globus-job-run_, _globus-url-copy_, _srmcp_, etc.)
None.
d66 1
---++ OSG_APP
This area is intended for VO-wide software installations.
It is required that relative paths resolve consistently between gatekeeper and worker nodes even if OSG_APP itself (the base dir) differs between the two.  It is strongly recommended that the base dir itself is the same as well, or some legacy software (*1) will not function properly.  This area may be writable only by a subset of users and read-only for all the others: there is no guarantee that every user will have write access.  OSG_APP must point to a POSIX-compliant filesystem for software installation. 
It is required that relative paths resolve consistently between gatekeeper and worker nodes even if OSG_APP itself (the base dir) differs between the two.  It is strongly recommended that the base dir itself is the same as well, or some legacy software<sup>[[#Foot1Table2][1]]</sup> will not function properly.  This area may be writable only by a subset of users and read-only for all the others: there is no guarantee that every user will have write access.  OSG_APP must point to a POSIX-compliant filesystem for software installation. 
It is recommended that OSG_APP is writable only via specific users/VO roles in order to improve basic security and robustness. (*2)
It is recommended that OSG_APP is writable only via specific users/VO roles in order to improve basic security and robustness.<sup>[[#Foot2Table2][2]]</sup>

Typical uses of this area are:

Notes (*#):
   1. Pacman resolves variables an symbolic links saving the full real path. A suggested procedure to avoid problems with it is:
   1. <a name="Foot1Table2"></a>Pacman resolves variables an symbolic links saving the full real path. A suggested procedure to avoid problems with it is:
---+++ Notes:
   1. <a name="Foot1Table2"></a>Pacman resolves variables an symbolic links saving the full real path. The following may help you avoid problems with it:
   1. The 'sticky bit' enabled on OSG_APP is recommended for all shared CE storages.
   1. <a name="Foot2Table2"></a>The 'sticky bit' enabled on OSG_APP is recommended for all shared CE storages.
      * Install applications only using the fork jobmanager.
   1. <a name="Foot2Table2"></a>We recommend enabling the 'sticky bit' on OSG_APP for all shared CE storages.


---++ OSG_DATA
This area is a transient storage shared between jobs executing on the worker nodes.
OSG_DATA is a storage area for transient / volatile files that are shared between jobs that are executing on the worker nodes. It provides similar functionality to the comibination of OSG_SITE_READ and OSG_SITE_WRITE and may be easier for small sites to deploy. However, OSG_SITE_READ and OSG_SITE_WRITE are usually efficient for big production sites with specialized hardware, because it forces the separation between input and output.
It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
AFS, CIFS are OK). This is a subset of a POSIX-compatible filesystem, excluding features such as special file creation (pipes, sockets, locks, links) and the ability to modify file metadata (permissions)(*1).
AFS, CIFS are OK). This is a subset of a POSIX-compatible filesystem, excluding features such as special file creation (pipes, sockets, locks, links) and the ability to modify file metadata (permissions)<sup>[[#Foot1Table3][1]]</sup>.
Gridftp or 'SE like' access from outside of the cluster is required to allow an efficient use of OSG_DATA as staging area (*2).
Gridftp or 'SE like' access from outside of the cluster is required to allow an efficient use of OSG_DATA as staging area.<sup>[[#Foot2Table3][2]]</sup>
Since the allocation of this space is transient, it is important that users remove unused data and/or that a simple mechanism to allow cleanups (*3) is added.


A suggested dataflow using OSG_DATA and providing more scalable and reliable data access is the following:
   * Data is written to OSG_DATA via gridftp or if necessary fork jobs (unpack tarballs, etc.)
   * Job is staged into the cluster
   * Job copies its data to the compute node (OSG_WN_TMP) or reads data sequentially from OSG_DATA if the data is read once. The latter is a significant performance issue if many random reads are necessary on typical network file systems and this should be avoided. It is worth noting that random data access over large data sets is where grid storage shows its potential -- its distributed nature is better suited for handling that type of data access scalably and reliably.
   * Stage the job into the cluster
   * The job copies its data to the compute node (OSG_WN_TMP) or reads data sequentially from OSG_DATA, if the data is read once. 
If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another OSG_DATA de facto (*4). At present, we discourage site admins from removing OSG_DATA and discourage users from using OSG_DATA. We believe that many users need some more transition time to ween themselves off of their use of OSG_DATA.
If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another OSG_DATA de facto.<sup>[[#Foot4Table3][4]]</sup> At present, we discourage site admins from removing OSG_DATA and discourage users from using OSG_DATA. We believe that many users need some more transition time to ween themselves off of their use of OSG_DATA.
d133 1
Typical uses of this area are:
   $ %X% *WARNING*: If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another OSG_DATA de facto.<sup>[[#Foot4Table3][4]]</sup> Again, at present we discourage site administrators from removing OSG_DATA and discourage users from using OSG_DATA.
   
---+++ Typical uses of OSG_DATA
   * Input datasets for jobs executing on the worker nodes
Notes (*#):
   1. The ability to modify file permissions may be required in future revisions -- it is available in SRMv2
   1. The OSG_DEFAULT_SE entry may be used to publish the base GSIftp URL of the SE viewing that space.
   1. An example of a simple space management solution could be a file in each directory ( _.keep_) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested), all files in that directory and subdirectories may be removed. This is a "gentleman's agreement" -- keep in mind that none of the data in a transient storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
   1. E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate $HOME at the same time, e.g. as described in OSG document <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382">382</a>.
   1. <a name="Foot3Table3"></a>An example of a simple space management solution could be a file in each directory ( _.keep_) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested), all files in that directory and subdirectories may be removed. This is a "gentleman's agreement" -- keep in mind that none of the data in a transient storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
   1. <a name="Foot4Table3"></a>E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate $HOME at the same time, e.g. as described in [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382][OSG document 382]].
   1. <a name="Foot2Table3"></a>The OSG_DEFAULT_SE entry may be used to publish the base GSIftp URL of the SE viewing that space.
   1. <a name="Foot4Table3"></a>For example, the current Condor jobmanager uses a shared space (=gass_cache= by default) when it processes requests to transfer the executable or some data. You should not assume that =$HOME= is shared between gatekeeper and worker nodes. Furthermore, removing a shared space like OSG_DATA for performance issues is useless if you only reintroduce it as =$HOME=, as that will probably also adversely affect performance because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate =$HOME= at the same time. See [[http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382][OSG document 382]] for more information.
d74 1
This area is a transient storage visible from all worker nodes and optimized for high-performance read operations.
%INCLUDE{ "AboutStorageAreaOsgSiteRead" }%
It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This read-only area is intended to hold input data shared among different applications running on different worker nodes.
files will be placed there by site administrators or by writing from the Grid side of the SE.  (*1).<br>
If the gatekeeper cannot write to this area, there may be problems with jobmanagers using a shared directory to transfer the executable or some data (*2). 
files will be placed there by site administrators or by writing from the Grid side of the SE.<sup>[[#Note1Sec4][1]]</sup>.<br>
If the gatekeeper cannot write to this area, there may be problems with jobmanagers using a shared directory to transfer the executable or some data.<sup>[[#Note2Sec4][2]]</sup>

Users have no write access to this area from the worker node. Users can write from the grid side of the Storage Element. However, site administrators can write files to the area. <sup>[[#Note1Sec4][1]]</sup>.

If the gatekeeper cannot write to this area, jobmanagers may have problems using a shared directory to transfer the executable or some data.<sup>[[#Foot4Table3][4]]</sup>

<!-- This follows the LCG model. This last option may cause problems to many current applications that count on normal file access to a shared space and it may require non-trivial changes to them to use special client programs to access the DEFAULT_SE. Furthermore it may cause problems with jobmanagers using a shared directory to transfer the executable or some data. A dataflow example could be: 
Typical uses of this area are:
Its features cover the read part of DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.
-->
   1. If GSIftp can be used to put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes then the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported in that case.
   1. E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads)
   1. <a name="Note1Sec4"></a>If GSIftp can be used to put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes then the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported in that case.
   1. <a name="Note1Sec4"></a>E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads)
   * Data staged in 
---+++ Notes (*#):
This area is a transient storage visible from all worker nodes and optimized for high-performance write operations.

It allows normal random-access file write operations: open, set flags, write (sequential, with multiple write operations), and close. It may not be possible to modify a file once closed. This is provided through a grid file access library specific for the underlying storage, and programs may use it transparently (*1) as a local disk space.<br>
Users may not have read access to this area; in such cases files will be accessed with the help of the site administrator, or through mechanisms external to the CE (e.g., a SE can read from that area (*2)).<br>
It allows normal random-access file write operations: open, set flags, write (sequential, with multiple write operations), and close. It may not be possible to modify a file once closed. This is provided through a grid file access library specific for the underlying storage, and programs may use it transparently <sup>[[#Note1Sec5][1]]</sup> as a local disk space.<br>
Users may not have read access to this area; in such cases files will be accessed with the help of the site administrator, or through mechanisms external to the CE (e.g., a SE can read from that area <sup>[[#Note2Sec5][2]]</sup>).<br>
<!-- Its features cover the write part of OSG_DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.
-->
Typical uses of this area are:
OSG_SITE_WRITE is provided through a grid file access library specific for the underlying storage, and programs may use it transparently <sup>[[#Note1Sec5][1]]</sup> as a local disk space.

   1. Limitations should be avoided when possible, and be clearly stated.
   1. If GSIftp can be used to put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes then the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported in that case. 
---+++ Typical uses of OSG_SITE_WRITE
---++ TMP [OBSOLETE!]
   * Storage of datasets produced by the jobs executing on the worker nodes
   * Data waiting to be staged out
This area was intended as a shared temporary work area. Because of its similarities with OSG_DATA, the reduced interest in multimode applications (that anyway can use OSG_DATA), and the possibility of the application abusing of a shared space, this
*has been removed* in the most recent OSG version.
---+++ Notes
   1. <a name="Note1Sec5"></a>Limitations should be avoided when possible, and be clearly stated.
This area is a temporary work area that may be purged when the job completes.
It is required that OSG_WN_TMP points to a POSIX-compliant filesystem that supports links, sockets, locks, pipes and other special files as required. Good I/O is essential for job performance, so a local filesystem is recommended.
Ideally, it should be a temporary directory (or partition) assigned empty for the job, with a well defined amount of space (many jobs require at least 1-2 GB of disk space) and purged at job completion.  The space provided should be dedicated and isolated: jobs overusing their space should not affect each other or affect the OS.<br>
It is required that OSG_WN_TMP points to a POSIX-compliant filesystem that supports links, sockets, locks, pipes and other special files as required. Good I/O is essential for job performance, so a local filesystem is recommended. Ideally, it should be a temporary directory (or partition) assigned empty for the job, with a well defined amount of space (many jobs require at least 1-2 GB of disk space) and purged at job completion.  The space provided should be dedicated and isolated: jobs overusing their space should not affect each other or affect the OS.
<br>
Sites using condor as their batch system typically implement this by letting condor create the
jobs directory prior to launching the job. In those cases OSG_WN_TMP may be defined in the GIP implicitly by pointing to another environment variable, rather than explicitly by pointing to a full path.
d86 1
%INCLUDE{ "AboutStorageAreaOsgWnTmp" }%
Sites using condor as their batch system typically implement this by letting condor create the jobs directory prior to launching the job. In those cases OSG_WN_TMP may be defined in the GIP implicitly by pointing to another environment variable, rather than explicitly by pointing to a full path.

Typical uses of this area are:

In most cases, OSG_WN_TMP points to a space shared among all jobs that are currently executing at the worker node. Therefore, we recommend each job create a unique subdirectory of OSG_WN_TMP (e.g., =OSG_WN_TMP/somestring$JOBSLOT=) that becomes its working area. The job should then remove this subdirectory before it completes,
---+++ Typical uses of OSG_WN_TMP
Notes (*#):
   1. A mechanism to help automatic cleanup would be the introduction of a 'lock' directories with files named according the temporary directory, containing the PID of the job. If that process is terminated, the directory can be removed


None. 
It is accessible only using SE access methods (Gridftp, SRM). It is accessible from within the cluster (visible from the worker nodes). If accessible from outside, it is the preferred SE for the CE and should be used when doing 2(or more)-step copy of input datasets to the CE (e.g. 3rd party transfer to OSG_DEFAULT_SE, then copy to OSG_WN_TMP using a client program).<br>
It is accessible only using SE access methods (Gridftp, SRM). It is accessible from within the cluster (visible from the worker nodes). If accessible from outside, it is the preferred SE for the CE and should be used when doing 2(or more)-step copy of input datasets to the CE (e.g. 3rd party transfer to OSG_DEFAULT_SE, then copy to OSG_WN_TMP using a client program).

In a simple cluster this could be a SE visible from both inside and outside and serving an internally shared space like OSG_DATA. In sites with no shared space, this would allow I/O via grid tools.

Typical uses of this area are:
---++ OSG_DEFAULT_SE
%INCLUDE{ "AboutStorageAreaOsgDefaultSe" }%
The OSG_DEFAULT_SE variable represents a Storage Element closely related to the CE, accessible from within the cluster (visible to worker nodes) only via SE access methods such as Gridftp or SRM. If OSG_DEFAULT_SE is accessible from outside, it is the preferred SE for the CE and should be used when doing a two- or more step copy of input datasets to the CE. For example, doing a 3rd-party transfer to OSG_DEFAULT_SE, then copying to OSG_WN_TMP using a client program.

Notes (*#):
---+++ Typical uses of OSG_DEFAULT_SE
   * Staging in of large input files (e.g. datasets)
   * Staging out of large output files (e.g. datasets)
---++ Minimum requirements
An OSG site is not required to provide every single area described above but must implement at least one of the options below.
-->

Mandatory set options (one of the following):

---+++ 1. OSG_GRID, OSG_APP, OSG_DATA, OSG_WN_TMP 
This is the Grid3 model.  A simple CE (one or few nodes, not a production CE) may even decide to use a single shared disk, pointing both OSG_DATA and OSG_WN_TMP to the same directory, but this is not recommended.
See LocalStorageConfiguration050 for installation/configuration notes and examples.
---+++ 2. OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_SITE_WRITE, OSG_WN_TMP
Areas that are not provided should be labelled as "UNAVAILABLE".


---+++<a name="Grid3Model"></a> OSG_GRID, OSG_APP, OSG_DATA, OSG_WN_TMP  (Grid3 Model)
A simple CE (one or few nodes, not a production CE) may even decide to use a single shared disk, pointing both OSG_DATA and OSG_WN_TMP to the same directory, but this is not recommended.

---+++<a name="Grid3ModelWithReadWriteInsteadOfData"></a> OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_SITE_WRITE, OSG_WN_TMP
---+++ 3. OSG_GRID, OSG_APP, OSG_DEFAULT_SE, OSG_WN_TMP 
This is the LCG model.  Current applications depending on normal file access to a shared space may require non-trivial changes -- there may also be issues with jobmanagers that use a shared directory to transfer the executable or data.
   * cp: OSG_WN_TMP->OSG_SITE_WRITE 
   * external stage-out

#LcgModel
---+++ OSG_GRID, OSG_APP, OSG_DEFAULT_SE, OSG_WN_TMP  (LCG model)
Current applications depending on normal file access to a shared space may require non-trivial changes -- there may also be issues with jobmanagers that use a shared directory to transfer the executable or data.

---+++ 4. OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_DEFAULT_SE, OSG_WN_TMP 
This is the SRM/dCache model. This is actually a superset of 3, so this CE satisfies 3.  In this model read access is via dcap, while write access is via srmcp only. (Write access via dcap is possible in principle and coulde provide OSG_SITE_WRITE; however, in practice dCache does not allow modification of a file once it is closed).  A typical deployment and use case is thus to allow writes only via srmcp (i.e. OSG_DEFAULT_SE) while reads may use either srmcp (OSG_DEFAULT_SE) or dcap (OSG_SITE_READ). 
The current SRM implementation in dCache does not allow overwriting of logical files, thus guaranteeing that all physical file replicas of the same logical file remain the same. 

In the SRM/dCache model, read access is via dcap, while write access is via srmcp only. DCache does not allow modification of a file once it is closed, although write access via dcap is possible in principle and could provide OSG_SITE_WRITE. 

A typical deployment allows writes only via srmcp (i.e., OSG_DEFAULT_SE) while reads may use either srmcp (OSG_DEFAULT_SE) or dcap (OSG_SITE_READ). 

The current SRM implementation in dCache does not allow overwriting of logical files, This guarantees that all physical file replicas of the same logical file remain the same. 
The set of CE storages provided by a CE *must include* at least one of these four sets (*1 OR 2 OR 3* - 4 includes 3).
A dataflow example may be: 
Of course, providing a wider selection of CE storages would allow the jobs to select the most proper for their needs -- but it could also allow the jobs to adopt inefficient execution models that could negatively affect the performance of the whole cluster.

See LocalStorageConfiguration for installation/configuration notes and examples.

a270 5
   * site admin intervention or external transfer 
   * job execution (open/seek/read from OSG_SITE_READ using dcap)
   * srmcp: OSG_WN_TMP->OSG_DEFAULT_SE
   * external stage-out

-- Main.BurtHolzman - 04 Aug 2005
-- Main.FkW - 25 Apr 2006
-- Main.RobQ - 01 May 2006
-- MainForrestChristian - 06 Nov 2006 (editing only)
 -- Main.BurtHolzman - 04 Aug 2005 %BR%
 -- Main.MarcoMambelli - 23 Sep 2005 %BR%
 -- Main.RobQ - 01 May 2006 %BR%
 -- Main.Forrest.Christian - 06 Nov 2006 (editing only) %BR%
 
