%META:TOPICINFO{author="ForrestChristian" date="1172255797" format="1.1" reprev="1.14" version="1.14"}%
%META:TOPICPARENT{name="DocumentationTable"}%
%LINKCSS%

---+!! Configuring Local Storage
%TOC%

%STARTINCLUDE%
%EDITTHIS%

This document describes how a CE administrator can configure the OSG attributes, including ones referencing "CE storage", during the installation and afterwards (if he needs to change the layout of his CE). 

[[LocalStorageRequirements][Local Storage Requirements]] describes what the different CE storage are and [[LocalStorageRequirements#MandatorySetOptions][Mandatory Set Options]] specifies the minimum requirements for being OSG-compliant. It is not mandatory to provide all the CE storages.  Also, see [[UsingLocalStorage][Local Storage Use]] for a very draft idea on how these should be used.


---++Configuring the OSG Attributes
This section covers the configuration of information about your CE that you must make known to OSG. 

These will be published as pat of the GLUE schema using the GIP [or the Grid3 schema] and used directly or indirectly by other OSG applications and users submitting jobs. <!-- The current version also provides backward compatibility with applications using Grid3 conventions. -->

At the core is =$VDT_LOCATION/monitoring/osg-attributes.conf=, a standard configuration file that can be edited or reviewed directly. The configuration script =$VDT_LOCATION/monitoring/configure-osg.sh=  automates much of the configuration.

The meaning and purpose of the various elements of the configuration attributes are documented further in [[LocalStorageRequirements][Local Storage Requirements]] and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. 

New resource administrators may want to read that information carefully and determine how to map those elements onto their Resource before proceeding. 

Guidance on the basic elements and common defaults is provided below. 


---+++!!Gather configuration information
OSG strives to make resources available with minimal requirements; however, the grid requires certain information about files and filesystem mount points to provide a basic execution environment. 

For applications to be installed and to be executed correctly, filesystem sharing and the filesystem mount points available for a cluster must be specifically coordination. For this purpose, administrators must define special directory hierarchies (mount points) and allocate them in the OSG environment. Many of these mount points should be available on the head / gatekeeper node and, using the exact path, on each of the worker nodes. Generally, they do _not_ have to be made available in form of a shared filesystem across the whole cluster. 

Read-only spaces can generally be provisioned with or without a shared filesystem as long as you provide consistent paths.


---+++ OSG Location: =$OSG_LOCATION=
Where OSG software will be installed. It must be writable by root.  This attribute is automatically setup by the configure_osg.sh script and users are not asked for this value.  

$OSG_LOCATION contains the OSG-specific software as well as the Globus middleware and other middleware applications, including server and client utilities used by the system itself.  Users will use a different client installation.  This
directory should not be exported to the worker nodes.


---+++ OSG Grid: =$OSG_GRID=
Where OSG Client Software will be installed &mdash; see [[WorkerNodeClient][Worker Node Client]] for description.  

$OSG_GRID includes client utilities for Grid middleware, such as VDS and srmcp.  It should be writable by root and readable by all users. It must be accessible by both gatekeeper and worker nodes via a shared filesystem, or different installations on local disks using a consistent pathname.


---+++ Application Directory: =$OSG_APP=
Base location for VO-specific application software. 

$OSG_APP is read-only mounted on all worker nodes in the cluster. Only users with software installation privileges in their VO should have write privileges to this directories. At least 10 GB of space should be allocated per VO.


---+++ Data Directories: =$OSG_DATA= __or__ =$OSG_SITE_READ= & =$OSG_SITE_WRITE=
The data directories are intended as the spaces for applications to write input and output data files with persistency that must exceed the lifetime of the job which created it. 
   * These directories should be writable by all users.  
   * Users will be able to create sub-directories which are private, as provided by the filesystem. 
   * At least 10 GB of space should be allocated per worker node; some VOs require much larger allocations.

The following different options are possible: 
   * $OSG_DATA: shared directory with read-write access for all users
   * $OSG_SITE_READ: shared directory with read-only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
   * $OSG_SITE_WRITE: shared directory with write-only access for all users (data may be staged out by the administrator or using a SE pointing to the same space)

A CE can provide $OSG_DATA, both $OSG_SITE_READ and $OSG_SITE_WRITE, or none of them if it has a local SE specified in $OSG_DEFAULT_SE. 

If a particular hierarchy is not available on your CE, provide the keyword =UNAVAILABLE=.

For additional details please refer to [[LocalStorageRequirements][Local Storage Requirements]].

%IMPORTANT% The $OSG_DATA, $OSG_SITE_READ and $OSG_SITE_WRITE directories must be accessible from the head node as well as each of the worker nodes. 


---+++ Temporary Directory: =$OSG_WN_TMP=
A temporary directory local to the worker node, used as a working directory.  
   * At least 10 GB per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs that can run up to 4 jobs, should have 40GB). 
   * Files placed in this area by a job may be deleted upon completion of the job.


---+++ Default Storage Element: =$OSG_DEFAULT_SE=
A storage element that is close and visible from all the nodes of the CE, both worker and head node. <!-- Usually, it is local to the CE and accessible from outside with the same or a different URL. --> 

The value to be specfied in $OSG_DEFAULT_SE is the full URL, including method, host/port and path of the base dir. This full URL must be reachable from inside as well as outside the cluster. The $OSG_DEFAULT_SE generally supports only put and get, rather than open/read/write/close as discussed in [[LocalStorageRequirements][Local Storage Requirements]]. 

If the CE has no default SE it can use the value UNAVAILABLE for $OSG_DEFAULT_SE. 

%NOTE% The current release supports SRM and gftp for $OSG_DEFAULT_SE.


---+++ VO Sponsor
To determine what Virtual Organization(s) are paying for the resources of this cluster. The notation incorporates a VO name followed by a percentage, so that CEs are able to denote multiple VO partners. 


---+++!!Execute the configuration script
Run the following script as root to execute the configuration script.

<pre class="screen">
> <b>cd $VDT_LOCATION/monitoring</b>
> <b>./configure-osg.sh</b>
</pre>

Sample output from the configure-osg.sh script:  
%STARTMore%
%INCLUDE{"SampleOutputFromConfigureOsgSh" }%
%ENDMore%

The =configure-osg.sh= script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file and =grid3-info.conf= that is a link to it.  The script also creates =$VDT_LOCATION/monitoring/osg-job-environment.conf= which duplicates some of the attributes from =$VDT_LOCATION/monitoring/osg-attributes.conf= so that they can be palced in the job environment as well as =$VDT_LOCATION/monitoring/osg-local-job-environment.conf=. The =$VDT_LOCATION/monitoring/osg-local-job-environment.conf= file is to allow site admins to set site specific environment variables that should be present for jobs that are running on the cluster. The This file is used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CEs.

The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.


---++Reconfiguring an Existing OSG Site
An already installed OSG site can be reconfigured by editing =osg-attributes.conf=, or by rerunning the =configure-osg= script.


---++Examples
These examples make assumptions about relationships between what is visible from inside of the CE (from the jobs) and what is available from the outside (SE, !GridFTP) for staging operations.

Your configuration may differ considerably.

   * [[InstallingCompleteOSGSiteOnSingleComputer][Installing a Complete OSG Site on a Single Computer]]
   * [[InstallingOSGInMinimalSuggestedConfiguration][Installing OSG in the Suggested Minimal Configuration]]
   * [[InstallingOSGOnSingleHeadnodeCluster][Installing OSG on a Single Headnode Cluster]]
   * [[InstallingOSGWithSRMbasedCE][Installing OSG with an SRM-based CE]]


---++ Configuring the Generic Information Provider (GIP)
See [[GenericInformationProviders][Generic Information Providers]].
%STOPINCLUDE%

%INCLUDE{ "Documentation.ToolsBottomMatter" }%

<!-- For significant updates to the topic, consider adding your 'signature' (beneath this editing box) -->
<!--Future editors should add their signatures beneath yours -->
-- Main.MarcoMambelli - 11 Nov 2005 %BR%
-- Main.FkW - 25 Apr 2006 %BR%
-- Main.RobQ - 01 May 2006 %BR%
-- Main.ForrestChristian - 06 Nov 2006 (editing only) %BR%
-- Main.ForrestChristian - 10 Nov 2006  %BR%
 %BR%
 %BR%

%META:TOPICMOVED{by="ForrestChristian" date="1164649583" from="Integration/ITB_0_5.ConfiguringLocalStorage" to="Integration/ITB_0_5.LocalStorageConfiguration"}%
