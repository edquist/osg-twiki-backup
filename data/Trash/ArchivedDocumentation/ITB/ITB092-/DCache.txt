%META:TOPICINFO{author="NehaSharma" date="1222802869" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%

*Purpose*: The purpose of this document is to provide dCache based SE administrators the information on how to prepare, install and validate the SE.

---++ Preparation
---+++ Introduction

dCache is a disk-caching system jointly developed by Deutsches Elektronen-Synchrotron (DESY) and Fermi National Accelerator Laboratory (Fermilab). Although it was initially developed to provide disk caching for tertiary storage, dCache makes an excellent grid Storage Element (SE) &ndash; even without tape backend &ndash; because it has a [[https://srm.fnal.gov/twiki/bin/view/SrmProject/WebHome][Storage Resource Manager]] interface and supports multiple transfer protocols: gridftp, xrootd, dcap, and http. dCache also provides a single name space across the entire pool of disk servers, making it look like a single file system to the users. It simplifies administration by providing mechanisms for internal clean-up and load balancing, which allows efficient use of storage space and handling of users' requests.

The dCache distributed components are implemented with the Cells package. Cells communicate with each other and components of dCache are implemented as cells. Hardware resources on a dCache node depend on what cells are running on that node. For a detailed description of dCache cells and the resources they require, see <a target="_top" href="https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions/">Descriptive Cell Listing</a>.dCache also uses !PostgreSQL databases for SRM and pnfs.

The following are the recommended system minimums for an OSG dCache installation. All nodes should have Scientific Linux 4.2 or later.

*Admin Nodes*

   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) with a hot spare on the Admin Nodes with the billing or srm databases running the XFS file system. These should be backed up regularly.</p> 
   * <p>Gigabit or better network links</p> 

*Pool Nodes*

   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better or equivalent Opteron (e.g., quad Opteron 270)</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) hot swappable with hot spare recommended running the XFS file system. External RAIDs are highly recommended.</p> 
   * <p>Gigabit or better network links</p> 

*PNFS Node*

   * <p>8 GB RAM</p> 
   * <p>Postgres databases on raided disk (RAID 5) with back-up performed regularly.</p> 
   * <p>Disk should be used exclusively for the pnfs database.</p> 

*Note:*

See [[http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=1837&version=1&filename=dcache_tuning.pdf"][ _dCache Tuning_ ]] for recommended Linux TCP/IP parameter tuning. [[http://www.powerpostgresql.com/PerfList"][Power PostgreSQL - PerfList]] and [[http://www.varlena.com/varlena/GeneralBits/Tidbits/perf.html#shbuf"][Tuning PostgreSQL]] have recommendations on tuning !PostgreSQL. The latter also has recommendations for shared buffers for your PNFS node.

[[https://indico.desy.de/getFile.py/access?contribId=19&sessionId=5&resId=0&materialId=slides&confId=138][dCache hardware layouts]] provides a good guide for the layout of dCache hardware in a variety of scales. The [[http://www.dcache.org/manuals/index.shtml"][dCache manual]] also has more information.

---++ Installation

VDT provides a package for installing dCache on an Open Science Grid site. The package is available for download from the [[http://vdt.cs.wisc.edu/software/dcache/server/][VDT-dCache website]]. There are two ways to read the installation/setup/configuration instructions. You can either read them from [[http://vdt.cs.wisc.edu/extras/InstallingDcacheForOSG.README.html][VDT-dCache website]] or once you have downloaded and untarred the tarball appropriate for your machine, you can read the README file (present under INSTALL_LOCATION/install/ directory).

---+++ Integration with the information system
 Integration of the SE with the central information systems takes place during the Compute Element installation/configuration. See the topic [[ReleaseDocumentation/GenericInformationProviders][Generic Information Providers]]. The SE does not collect or publish information independently.

---++ Validation

Please perform [[https://twiki.grid.iu.edu/twiki/bin/view/Integration/ITB090/InstallationITBStorageElement#dCache_Validation_Suite][these simple tests]] to validate your SE before declaring it to be functional.
---++ References

[[http://s-2.sourceforge.net/][S2]] - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp

<br />%STOPINCLUDE% 
%BR% 
%COMPLETE3% %BR% 
%RESPONSIBLE% Main.NehaSharma - 12 Sept 2008 %BR% 
%REVIEW%
