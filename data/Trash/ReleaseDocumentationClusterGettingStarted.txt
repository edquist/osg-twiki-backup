%META:TOPICINFO{author="BrianBockelman" date="1486500267" format="1.1" version="1.8"}%
%META:TOPICPARENT{name="Trash.ReleaseDocumentationGridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction
   * The purpose of this exercise is to setup a small test cluster that can be  used to illustrate creating a site capable of receiving Grid jobs and data.
   * It will have a minimal number of "head nodes" that will host various services such as a Condor job manager, an OSG gatekeeper and storage element.
   * It will have a two or more "compute nodes" that will receive and execute jobs.

*NOTE - DISCLAIMER:*
   * The Grid cluster setup described here is meant only to be suggestive for a (*small*) test-lab environment.  There are many factors to consider when deploying a production cluster in a university's or laboratory's IT environment including the local networking environment, which other services may be available to the cluster, and local site policies for security-related issues. 


---++ Site planning
   * Take a look at ReleaseDocumentation.SitePlanning to get an idea of the options available for setting of a Grid cluster. 
   * One first considers what is appropriate for the scale of the resource, and the support that must be given for users.
   * In this tutorial, our goal will be to use as few services as possible to make a simple Grid site;  then we'll add more to increase capabilities.   For example, we will start by using a *gridmap* file rather than the GUMS service for authorizing Grid users.
   * The next step is to take inventory of your resources and identify which software (service) will be installed on which host.


   

---++ Identify the hosts and and assign their roles
Lets say your test grid cluster is going to have in its name,  "gc1",  and you have 8 machines. The fully qualified domain name (FQDN) includes a domain (here "yourdomain.org"). In the documents the machines will sometimes be referred to by their short name, e.g. =gc1-ce= instead of =gc1-ce.yourdomain.org=.

   * =gc1-ce.yourdomain.org= a Compute Element (and cluster head-node) 
   * =gc1-se.yourdomain.org= a Storage Element 
   * =gc1-gums.yourdomain.org= a GUMS server
   * =gc1-nfs.yourdomain.org= an NFS server 
   * =gc1-ui.yourdomain.org=  your user interface machine
   * =gc1-c001.yourdomain.org= compute node 1
   * =gc1-c002.yourdomain.org= compute node 2
   * =gc1-c003.yourdomain.org= compute node 3
   
We may double up on services on some of the head nodes, and may add/subtract compute nodes (but you need at least one).  But this would be the basic host-role assignment.   You might call this your *node matrix*.  You can use *cnames* to alias more than one name to a single host (for example, *gc1-nfs* and *gc1-gums* could be installed on the same computer).   Later, you can achieve this using virtual machine technology such as !VMWare or Xen.

---++ Machine, operating system characteristics
For machines hosting Condor, the OSG compute and storage elements, it is good to have a good amount of memory and local disk (in the case of the storage node) as these services can sometime be quite resource hungry.

In this tutorial the machines used are assumed to have a Linux operating system installed.  Different Linux distributions differ for configuration and placement of files.  These documents refer to Scientic Linux 5.3.  However, any Red Hat based Linux distribution (Fedora, !CentOS, etc.) will work fine for the material presented here.   You may have to adapt these instructions if you have something different (SUSE, Debian, or Gentoo for example).  A basic server installation is assumed.  Here are some of our preferences:
   * A stripped down installation with as few of the "extras" that are offered as part of a Scientific Linux (Red Hat) distribution.
   * The SELinux package which provides additional security controls at the kernel level is *disabled*
   * =iptables= (firewall) is disabled <pre>
chkconfig --list iptables
chkconfig iptables off
</pre>
   * The automounter =autofs= is disabled

---++ YUM update
   * We like to use the =yum= utility to keep things up to date.  So before embarking on the rest of the tutorial, do a =yum update=.
   * However, we *do not like* having yum automatically update itself on a periodic basis as this leads to unknown changes entering onto our platform.  So make sure to<pre>
/etc/init.d/yum stop
chkconfig yum off</pre>


---++ More information
   * More about SELinux and how to disable it, see [[http://www.crypt.gen.nz/selinux/disable_selinux.html][here]].
   * Firewall configuration options during installation, see [[https://www.scientificlinux.org/distributions/4x/installing/firewall.configuration.html][here]]



%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.RobGardner - 23 Oct 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%

<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/Trash/ReleaseDocumentationGridColombiaWorkshop2009/centerpageborder.css
-->
%META:TOPICMOVED{by="BrianBockelman" date="1486500267" from="ReleaseDocumentation.ClusterGettingStarted" to="Trash.ReleaseDocumentationClusterGettingStarted"}%
