%META:TOPICINFO{author="RobGardner" date="1115912468" format="1.0" version="1.78"}%
a1 7
%META:TOPICINFO{author="DougOlson" date="1128643339" format="1.0" version="1.145"}%
%META:TOPICPARENT{name="WebHome"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 
d3 1
a3 924
---+!!<nop>%TOPIC%
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>OSG ITB CE Install Guide

%TOC%
%STARTINCLUDE%

This document is intended for administrators responsible for installing and configuring an OSG Compute Element (CE). 
It is not meant as an all-inclusive guide to Grid computing or even all the options for configuring a CE. It provide a recipe for a default configuration of the OSG provided reference implementation. It makes recommendations that have been found to be sensible starting points by OSG participants and may exceed the minimum requirements. Parties which wish to adapt to some local requirements/configuration should expect to have to adapt these instructions to their use.
Those interested in the formal requirements of a Compute Element are referred to the OSG Deployment Document.


This document is intended for administrators responsible for installing and configuring an OSG Compute Element (CE) into the OSG Integration !TestBed (ITB). It is not meant as an all-inclusive guide to Grid computing or even all the options for configuring a CE. Instructions for installing a CE intended for the OSG Production grid can be found [[Documentation.OsgCEInstallGuide][here]].

---+++ Operating Systems

The instructions assume you're using one of the following Linux distributions:

	* Red Hat 7.x
	* Red Hat 9.0
	* Red Hat Enterprise Linux 3
	* Fedora Core 3 
	* Debian Linux 3.1 (Sarge)

Installation may be successful with other Linux distributions, but they have not been tested by the maintainers of Pacman and the VDT.  Several "binary-compatible"
%RED% Can we add the right comment for Rocks? %ENDCOLOR%
distributions such as Scientific Linux Fermi 3.0.x (x=3,4) and Rocks 3.x (x>=1)
have been successfully installed by members of the OSG-ITB treating them 

The proposed installation and configuration method is based on <a href="http://physics.bu.edu/pacman/">Pacman</a>.  Pacman is a package manager like RPM or dpkg, but is able to work on and support multiple platforms.  Specific Pacman directions are given below, and the pre-installation procedures.

---+++Installation Method
The installation and configuration method is based on <a href="http://physics.bu.edu/pacman/">Pacman</a>.  Pacman is a package manager like RPM or dpkg, but is able to work on and support multiple platforms.  Specific Pacman directions are given below, and in the pre-installation procedures.


---+++ Conventions used
The following conventions are used in these pages:
	* =monospaced text= indicates terminal output 
	* ==bold monospaced text== indicates terminal input (by the user)
	* =<i>monospaced text in italics</i>= indicates variable data that may differ on your installation
	* the =&gt;= prompt indicates commands that do not require a specific shell or root access
	* the =#= prompt indicates commands that require root access

	* the =$= prompt indicates sh- or bash-specific commands
	* the =%= prompt indicates csh- or tcsh-specific commands
It is assumed that your hardware is already running one of the operating systems previously mentioned. If your system is a cluster, the batch queuing system should be likewise previously installed and configured, unless you intend to use the Condor system that can optionally be installed with VDT.	
It is assumed that your hardware is already running one of the operating systems previously mentioned. If your system is a cluster, the batch queuing system should be likewise previously installed and configured, unless you intend to use the Condor system that can optionally be installed with VDT.

If you would like to preserve your existing Condor installation, you can setup the environment so Pacman will recognize this. Two environment variables are defined which result in:
	* Condor being installed but not configured. 
	* VDT middleware services pointing to appropriate locations in your external Condor installation.

To specify an external Condor installation you need to set the following two environment variables before starting the installation:
---+++ Preserving a pre-existing Condor installation?
(Does this pertain to an external-to-VDT Condor installation?) If you would like to preserve your existing Condor configuration, you can set two environment variables so Pacman will recognize this. 

For example, if you have Condor installed in the standard location, the following should be sufficient to let the VDT installation scripts know to preserve your existing installation:
This will result in:
<pre>[root@mysite root]# env | grep CONDOR 
CONDOR_IDS=501.501
CONDOR_CONFIG=/opt/condor/etc/condor_config
CONDOR_ROOT=/opt/condor
[root@mysite root]# export VDTSETUP_CONDOR_LOCATION=$CONDOR_ROOT</pre>
	* VDT middleware services pointing to appropriate locations in your external Condor installation.
It is NOT necessary to execute the setup.sh from any previously existing VDT installation in order to do an upgrade.
==$ export VDTSETUP_CONDOR_LOCATION=$CONDOR_ROOT==
---+++ Existing reliable network?
It is assumed that your hardware is connected to a reliable network connection through which packages may be retrieved and from which services may contact your system.
It is NOT necessary to execute the setup.sh from any previously existing VDT installation in order to do an upgrade.  
---+++ Firewall?
It is assumed your installation is not inside a site firewall. (*What if it is?*)

---+++ Time synchronization (NTP) and reverse name lookups (DNS)

Each system should be setup to support network time protocol. Lack of synchronization generally complicates troubleshooting and can cause problems with the security infrastructures evaluation of eg. proxy lifetimes.

Also for the middleware to correctly function both the forward and reverse lookups as configured through a local DNS service are required for the IP address of the system.

#UniqueName 
---+++ You'll need a unique OSG site name
Each OSG site will need to designate a *unique name* by which services and resources may refer to the site. This name will be displayed on the Site Catalog and used in tables for other monitoring and accounting.  For example, the University of New Mexico has a unique name of UNM_HPC; the University of Buffalo is Buffalo_CCR.
#UniqueName 
---+++ You'll need a unique OSG resource name
 If there is a _previous installation_ of the OSG or Grid3 environment or 
 other Globus middleware please _stop the processes_ which are currently running.
 This includes the Globus Resource Information Service (GRIS), MonALISA and other 
 services configured to start upon initialization on your system.
 More information is provided in OSGShutdownGuide.

---+++ Creation and setup of local user accounts for VOs
UNIX user accounts need to be created by the system administrator for the VOs.  The accounts are: 
You will need to create at least one local user account (with the appropriate configuration) for each VO to which you wish to provide resources.  The uid and name of the account can be locally determined. You will be asked to provide a mapping from local accounts to VOs later in the installation, but there are assumed defaults detailed in OsgVoAccounts. Those defaults are assumed for examples used in this installation guide.

The accounts are: 
	* cdf
	* btev
	* fmri
	* gadu
	* mis
	* uscms01
	* uscms02
	* usatlas1
	* usatlas2
	* usatlas3
	* sdss
	* lsc01
	* ivdgl
You will need to create at least one local user account (with the appropriate configuration) for each VO to which you wish to provide resources. (This is a requirement for VO separation that should be documented in the CE requirements)

The uid and name of the account can be locally determined, you will be asked to provide a mapping from local accounts to VOs later in the installation, but there are assumed defaults detailed in OsgVoAccounts. These defaults are assumed for purposes of this installation guide.

	* star
You will need to have the ability to get your personal grid proxy on the machine to run the validation tests and for debugging purposes. If you don't have a user certificate, or don't know how to generate a grid proxy, see your VO support center (or do what if this IS the VO expert ?).

mailing list osg-general at opensciencegrid dot org is available for you to submit
Instructions for installing and setting up Pacman for your specific platform can be found [[https://uimon.cern.ch/twiki/bin/view/Atlas/GetPacman][here]].

Instructions for installing and setting up Pacman for your specific platform can be found at PacmanInfo. 

	* *Decide on an installation directory for the software*.
This is typically <b>/usr/local/grid</b>, but whatever suits the
local resource structure is fine. If you are installing on a
cluster, this directory must be available on all the nodes. 
If this directory is not shared additional installation of
software will be required on each of the worker nodes. Nearly all of
the software to be installed is from the Virtual Data Toolkit (VDT).
This is typically something like <tt>/usr/local/grid</tt>, but whatever suits the local resource structure is fine. If you are installing on a
For example,

<pre>
export VDT_LOCATION=/usr/local/grid
cd $VDT_LOCATION
</pre>
 $  export VDT_LOCATION=/usr/local/grid
The installation described here is done as root.  However, non-root
installs are supported.  Not all services will run as root; Condor, 
MonALISA and the GRIS will run as the user "daemon". If you prefer to run
MonALISA and/or Condor with its own UID, you need only to create a 
<b>monalisa</b> and/or <b>condor</b> user before proceeding with installation the VDT will configure the daemons appropriately.  _Verify that the umask is set to "0022" prior to installation.  Failure to do so may render the installation unusable._
The installation described here is done as root.  However, non-root installs are supported. (%RED% There should probably be some caveat or supplemental reference here otherwise why have the "as root" default ? It IS dangerous if unnecessary. %ENDCOLOR%) Not all services will run as root; Condor, 
A few questions regarding trust of the caches from which the software is
downloaded will be displayed. Please answer *y* (yes) so that the
software can be retrieved. (Note that
it may be necessary to set your platform if you are installing a Linux version based upon Red Hat Enterprise Linux 3.)
Pacman 3.11 doesn't support Scientific Linux 3.0.x although it's help text says it does.  Also
make sure there are no non-standard versions of perl, tcsh, or
bash in your $PATH

=&gt; <b>pacman -pretend-platform:linux-rhel-3</b>=


<pre>
&gt; <b>pacman -get iVDGL:osg-0.1.5</B> 
Do you want to add [iVDGL] to [trusted.caches]? (y or n): <b>y</b>
Package [osg-0.1.5] found in [iVDGL]...
Package [osg-auto-0.1.5] found in [iVDGL]...
Package [VDT_135] found in [iVDGL]...
Do you want to add [http://www.cs.wisc.edu/vdt/vdt_135_cache] to [trusted.caches]? (y or n): <b>y</b>
...
...
...
</pre>
In the case of platforms such as Scientific Linux Fermi 3.0.4 you may get
the following question:
<pre>
The VDT is unable to recognize your platform.  This version of the VDT is
supported on several Linux releases:

	 RedHat 7
	 RedHat 9
	 RedHat Enterprise Linux 3
	 Fedora Core 3
	 Debian 3.1

The VDT may work on other platforms, but it's not guaranteed.  If you
encounter problems and send mail to the VDT team for support please mention
that you are installing the VDT on an unsupported platform.

Do you wish to continue? [y/n]
</pre>
(Answer yes).

If you plan to use Condor, answer <b>y</b> (yes) to the question posed regarding setting up the Condor job manager.  
By default, a personal-condor instance is installed to facilitate testing.<br>

<pre>If you would like, we can set up a Globus jobmanager interface
to Condor.

This will allow Globus Gatekeeper to run jobs on Condor.

Would you like to enable Globus jobmanager for Condor?
Choices: y (yes), n(no), s (skip this question) <b>y</b>
</pre>

 Other questions may be asked it is safe to answer <b>n</b> (no).


<b>This will take between 10 and 60 minutes to complete, depending
upon the system and network connection.</b> The installation should complete
with the following message. During this time you may open a second 
terminal and watch the progress by monitoring the <b>vdt-install.log</b> file.
Untarring [site-verify.tar.gz]...
	* *Set up your environment*  
Assuming the Pacman install completed without
fatal errors, you should now be able to source the OSG setup environment.

---+++Setup OSG Install Environment
 Assuming the Pacman install completed without fatal errors, you should now be able to source the OSG setup environment.
<pre>  $ <b>source setup.sh</b>
</pre> 
or
If you are using Kerberos locally, note that $VDT_LOCATION/jdk1.4/bin has been prefixed to your $PATH, which contains (among other things) kinit, klist, and ktab.  This is almost certainly not what you want, and will be corrected in VDT 1.3.6.

	* *Read the $VDT_LOCATION/post-install/README file*
Make changes as needed. (Ignore instruction to put users that need access to MDS into the gridmap file if you will be implementing GUMS.  It's a good idea to create a gridmap-file with yourself in it to test the gatekeeper -- this happens before GUMS/PRIMA installation.)

	* *Optional extra packages for PBS, LSF, or FBSNG setup*

	  An extra package may be required to setup access to an
existing PBS, LSF, or FBSNG installation. If you plan on using condor no action is necessary.
<b>Ensure that that the command-line utilities for your batch system 
are in your path,</b> then install the appropriate package (for PBS, LSF, or 
FBSNG, respectively): 

<pre>&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/vdt_135_cache:Globus-PBS-Setup</b><br><br>
&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/vdt_135_cache:Globus-LSF-Setup</b><br><br>
&gt; <b>pacman -get !http://www.cs.wisc.edu/vdt/vdt_135_cache:Globus-FBSNG-Setup</b><br>	</pre>

---++ Obtaining a Host Certificate
> pacman -get http://www.cs.wisc.edu/vdt/vdt_136_cache:Globus-FBSNG-Setup
Minimally, you will need a host certificate in order to verify the identity of your CE.

---++Configure the Public Key Infrastructure
	  The default Certificate Authority should be configured to be
the DOEGrids CA. You will find a <a href="http://www.cs.wisc.edu/vdt/releases/1.3.5/certificate_authorities.html">list of CAs</a>
which were added as authorized CAs on your system. 
Please review the list of authorized CAs.  
The VDT installed
the daemon <b>edg-crl-upgrade</b>
which should be running at all times. 
<verbatim>
 ps axwww | grep edg-crl-upgrade
</verbatim>
> ps axwww | grep edg-crl-upgrade
<verbatim>
 /etc/init.d/edg-crl-upgraded start
</verbatim>
This program checks for and
updates the certificate revocation lists (CRLs) for each of the
CAs installed. If CRLs are not kept current
incoming connections will fail. 
> /etc/init.d/edg-crl-upgraded start
The CertScripts package can assist you with choosing CA's to trust and
and periodically checking that the CRLs have not expired.

Configure the DOEGrids CA to be used by default by running
 the utility below. <I>If there is an option "<b>choose the option 
 which matches "1c3f2ca8 : ...</b>", then enter "<b>q</b>" at the prompts.</I>
		  <pre>&gt; <b>$VDT_LOCATION/vdt/setup/setup-cert-request</b>
Reading from /g3dev/globus/TRUSTED_CA
Using hash: 1c3f2ca8
Setting up grid-cert-request
Running grid-security-config...
<br>...<br></pre>

---+++ Request and install the host certificate for the resource

	 To authorize this resource for use, a host certificate needs to
be
requested from an appropriate Certificate Authority. Currently the
<a href="http://igoc.ivdgl.indiana.edu/grid-install/documentation/www.doegrids.org"> DOEGrids CA </a>is in use.
iVDGL and PPDG project instructions for getting a certificate are available on the <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/">iVDGL RA</a>
and <a href="http://www.ppdg.net/RA">PPDG RA</a> sites. 
	 Here is a brief guide to the process assuming you have a valid
User Certificate. 
iVDGL and PPDG project instructions for getting a certificate are available on the <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/">iVDGL RA</a> and <a href="http://www.ppdg.net/RA">PPDG RA</a> sites (RA is Registration Authority). 
	* Run grid-cert-request to generate your host's private key 
(<b>hostkey.pem</b>) and the certificate request.
		  <pre>&gt; <b>cd $VDT_LOCATION</b>
&gt; <b>. ./setup.sh</b>
&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i></b>
	&gt; <b>. ./setup.sh</b>
	&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i></b>

Using configuration from <I>/usr/local/grid</I>/globus/etc/globus-host-ssl.conf
writing new private key to '<I>/usr/local/grid</I>/globus/etc/hostkey.pem'<br>-----<br>...<br></pre> 
.........................................................++++++
	* Copy the <b>hostkey.pem</b> in <b>$VDT_LOCATION/globus/etc</b> to the
<b>/etc/grid-security</b> directory <i>preserving
the read only for root permissions.</i>
</pre> 

	* The certificate request is stored as the file <b>hostcert_request.pem</b>
in <B>$VDT_LOCATION/globus/etc</b> .
The important part (referred to as the PKCS#10 request) will look similar to the follwing:

	* The certificate request is stored as the file <tt>$VDT_LOCATION/globus/etc/hostcert_request.pem</tt> .
The important part (referred to as the PKCS#10 request) will look similar to the following:
<pre>-----BEGIN CERTIFICATE REQUEST-----
MIIBmzCCAQQCAQAwWzETMBEGCgmSJomT8ixkARkTA29yZzEYMBYGCgmSJomT8ixk
ARkTCGRvZWdyaWRzMREwDwYDVQQLEwhTZXJ2aWNlczEXMBUGA1UEAxMObXlob3N0
ZS5pdS5lZHUwgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBALMM/3jhhkGUA3uz
5h4dzUzluDbCdBonv3UjeWbBFyh1JUA3d2148WBTtHAfQuB+f61tRcia2j5eEQEg
TwQnx86VaG8mAOk6gKe/zZlfhVkYR6OY1ssmc3dtkFoIprVoUUJCviq9DUp3DRO/
57/AtDAJsPhAl/UY5d2jFWl9jYGtAgMBAAGgADANBgkqhkiG9w0BAQQFAAOBgQAr
ZS88q8AgsOJ3n0EJoiAvr6dws4mr94Xg9uohNogmGsdjsdM3LG6Q+Qb22YdPPEH9
3HeLtcBlsjEnBq/a+M4IsfnPCn/6hvJs0HS7PxSR98bE116Ik4zuM1dtQ1Ce8Q/h
<nop>YtfstZj7FRZPZP1Lg2uftazDst4dkuu0msBP7dUAZw==</pre>
<pre>-----END CERTIFICATE REQUEST-----</pre>

Connect to the DOEGrids CA server and paste this into the
area provided in the form at the URL &lt;<a href="https://pki1.doegrids.org/ManServerEnroll.html">https://pki1.doegrids.org/ManServerEnroll.html</a>&gt;.
		  <i> All of the information must be filled out and in the
additional comments
	* Email will arrive indicating a link to retrieve the host
certificate <b>hostcert.pem</b> .  Goto the link and download it onto the 
host.
institutional sponsor</a></b></i>
	* After obtaining the host certificate install it into the directory
<b>/etc/grid-security</b> as <b>hostcert.pem</b> . Use the following <b>openssl</b> command
to verify the certificate.
	* Email will arrive indicating a link to retrieve the host certificate <tt>hostcert.pem</tt> .  Goto the link and download the certificate to <tt>/etc/grid-security/hostcert.pem</tt> on the host.
</pre>
The output is something like:
	* Or, to get an LDAP service certificate for authenticated MDS (optional):
	 
---+++ Get an LDAP service certificate to run authenticated MDS (optional)
MDS is the monitoring and directory service. The GRIS (Grid Resource info service, a sensor that collects site data and publishes it to MDS) is configured to allow only authenticated and authorized access. Access requires an LDAP service certificate (two files), owned by the MDS owner (if you installed as root, the owner is probably daemon -- check).
And follow the instructions, which are very similar to the host cert instructions, except that you are creating <b>ldapkey.pem</b> and <b>ldapcert.pem</b>. More info on service certificates can be found at <a href="http://www.cs.wisc.edu/vdt/releases/1.3.5/installation_post.html#servicecert">http://www.cs.wisc.edu/vdt/releases/1.3.5/installation_post.html#servicecert</a>
&gt; <b>. ./setup.sh</b>
&gt; <b>./globus/bin/grid-cert-request -host <i>hostname.domain.tld</i> -service ldap</b></pre>
And follow the instructions, which are very similar to the host cert instructions, except that you are creating <b>ldapkey.pem</b> and <b>ldapcert.pem</b>. More info on service certificates can be found at <a href="http://www.cs.wisc.edu/vdt/releases/1.3.6/installation_post.html#servicecert">http://www.cs.wisc.edu/vdt/releases/1.3.6/installation_post.html#servicecert</a>
---+++Globus Configuration
---++ Configuration and Setup of OSG CE Services

---+++!!Globus Configuration
the <b>$VDT_LOCATION/vdt/setup/configure_globus.sh</b> script. 
reconfigure it, please review the <a href="http://www.cs.wisc.edu/vdt/globus_config.html">
Review the configuration files <b>/etc/xinetd.d/globus-gatekeeper</b> and
<b>/etc/xinetd.d/gsiftp</b> (or in <b>/etc/inetd.conf</b>). Additionally 
these services must be listed in the <b>/etc/services</b> file.  When you are happy,
Review the configuration files <tt>/etc/xinetd.d/globus-gatekeeper</tt> and
<pre>
[root@mysite grid]# /etc/rc.d/init.d/xinetd restart
restart the xinetd (or inetd) daemon to pick up the configuration changes:
<pre><b>
[root@mysite grid]# /etc/rc.d/init.d/xinetd restart</b>
Stopping xinetd:														 [  OK  ]
To verify that the gatekeeper is running at this point, you should be able to telnet to the public IP address of your site on port 2119 and get a response. Run "telnet <i>hostname port</i>".  It should return "Connected to...". The same should be true of the gsiftp port (2811 by default, or 2812 for the new server, gsiftp2).
</pre>

To verify that the gatekeeper is running at this point, you should be able to telnet to the public IP address of your site on port 2119 and get a response. Run <tt>telnet <i>hostname port</i></tt>.  It should return <tt>Connected to...</tt>. The same should be true of the gsiftp port(s) (2811 by default, or 2812 for the new server, gsiftp2).
The setup of Grid3 Information Provider (also known as MDS) is partly accomplished with the assistance of the configuration script <b>$VDT_LOCATION/monitoring/configure-grid3.sh</b> and partly
by modifying a few files with detailed information. Enter the 
<a href="#UniqueName"><b>unique name</b></a> when asked for the <B>GRID3 SITE NAME</B>.


The Grid3 schema strives to make a minimum number of assumptions and requirements on a site. The filesystem sharing and filesystem mount points available for a cluster is an area which requires specific coordination for applications to be installed and to be executed correctly. To this purpose four special directory hierarchies (mount points) will be required to be defined and allocated to the  environment. These directories will be required to be available on the head node or gatekeeper node and also available, using the exact path, available on each of the worker nodes or simply shared filespace.

---++++!!Gather configuration information

OSG strives to make the minimum requirements on a resource, however, to provide a basic execution environment that applications can build upon, certain information about file/filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. To this purpose, four special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node or gatekeeper node and also available, using the exact path, on each of the worker nodes or simply shared filespace.

* %RED%Grid Directory%ENDCOLOR%

	 This directory is where the Grid environment will be installed. This directory will contain the Globus middleware and other middleware applications. It should be writable for the user root. This directory contains both server and client utilities for the middleware and therefore should be shared between gatekeeper and worker nodes.

* %RED%Temporary Directories%ENDCOLOR%

	 There will be two temporary directories one will be local to the worker node and the other shared. The shared temporary directory will be the current working directory of a running application and persist only as long as the job is executing. The files in this directory should be managed by the running application Automated clean up may be required to be handled by the site administrator. All users should 
be able to use this directory for writing and reading files. At least 10G byte of space should be allocated per worker node. The local temporary directory may be used by applications to reduce latency to data. At least 10G should be available in this directory. 
	 The data directory will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 100G byte of space should be allocated per worker node. 
* %RED%Data Directory%ENDCOLOR%

	 The data directory will be required to be shared from the head node (gatekeeper node) to each of the worker nodes. This will be the directory to which applications will write input and output data files for running jobs. This directory should be writable by all users. Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10G byte of space should be allocated per worker node. One VO would like 100G+ per worker node. 

* %RED%Application Directory%ENDCOLOR%

One of the questions to be answered is regarding the <b>VO sponsor</b> of this site.
This is attempting to determine the VO ownership of the current cluster. The notation
incorperates a VOname followed by a percentage so that clusters are able to split ownership between VO's. 
One of the questions enquires about the <b>VO sponsor</b> of this site.
incorperates a VOname followed by a percentage so that clusters are able to note multiple VO partners. 
Change into the VDT root directory.

=&gt; <b>cd $VDT_LOCATION</b>=
Source the environment; for sh and bash:
=$ <b>. ./setup.sh</b>=
For csh and tcsh:
=% <b>source ./setup.csh</b>=
The *configure-grid3.sh* script is located in the <b>monitoring</b>
directory.  (It must be run as root.)
---++++!!Execute the configuration script

# <b>cd monitoring</b>
# <b>./configure-grid3.sh</b>
</pre>
For a typical installation, the script will look something like this:
Please specify your GRID3 SITE NAME [_hostname.domain.tld_]: <b><i><a href="#UniqueName">UNIQUE_NAME</a></i></b>
<pre>
Please specify your GRID3 BASE_DIR [/usr/local/grid]:
Please specify your OSG SITE NAME [_hostname.domain.tld_]: <b><i><a href="#UniqueName">UNIQUE_NAME</a></i></b>
Please specify your GRID3 APP_DIR [/app]:
Please specify your OSG BASE_DIR [/usr/local/grid]:
Please specify your GRID3 DATA_DIR [/data]:
Please specify your OSG APP_DIR [/app]:
Please specify your GRID3 TMP_DIR [/scratch]:
Please specify your OSG DATA_DIR [/data]:
Please specify your GRID3 TMP_WN_DIR [/tmp]:
Please specify your OSG TMP_DIR [/scratch]:
Possible Sponsors are usatlas, ivdgl ligo uscms sdss.

the following notation. 'usatlas:50 ivdgl:10 uscms:20 local:20'
You can express the percentage of sponsorship using
the following notation:<br/>
 myvo:50 yourvo:10 othervo:20 local:20
Please specify the VO sponsor of this site [iVDGL]:

Please specify the Batch Queuing to be used [condor]:


Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
Grid3 Location:  <i>/usr/local/grid</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Shared Temp:	  <i>/scratch</i>
WorkerNode Temp: <i>/tmp</i>
JOB Manager:	  <i>condor</i>
This configure script creates a $VDT_LOCATION/monitoring/grid3-info.conf file. <font color="red">We don't know where this file is used,or why gris comes next.  And how do we tell if gris is working?</font>

Start the information service daemon (optional):
<pre># <b>/etc/init.d/gris start</b>
</pre> 

This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The configuration of a production queue manager is beyond the scope of this document. Since there will be periodic validation submitted by DNs in the (which ? Has this been rationalized ?) VO, sites are strongly encouraged to provide at least two levels of priority and assign the lower priority to the "osg" grid user.

The command
The configuration of a production queue manager is beyond the scope of this document. Since there will be periodic validation submitted by DNs in the "mis" VO, it is recommended sites provide at least two levels of priority and assign the lowest priority to user mapped to the "mis" VO.

The commands
<pre># <b>CONDOR_CONFIG="$VDT_LOCATION/condor/etc/condor_config"</b>
# <b>export CONDOR_CONFIG</b>
# <b>$VDT_LOCATION/condor/sbin/condor_master</b></pre>
---+++ Authorizing users

The first step is to configure the service to allow access using your own Grid credentials. Once that's done you'll be able to run local tests and verify operation of your CE locally.
---++++!!Test Configuration (using local grid mapfile)
Make sure you have a Grid proxy for yourself. (Do we need instructions here -- they vary by CA/site.)

Make sure you have a Grid proxy for yourself. 
$ grid-proxy-info

<pre>
> grid-proxy-info
</pre>
should return something like 
<pre>
subject  : /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane
issuer	: /DC=gov/DC=fnal/O=Fermilab/OU=Certificate Authorities/CN=Kerberized CA
identity : /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane
...
</pre>
 $ cat /etc/grid-security/grid-mapfile
Take the subject string and prepend it to the file /etc/grid-security/grid-mapfile and assign it to a local user account (you can use any of the VO accounts you've created at the beginning to test). So the grid-mapfile should have at least one entry like:
<pre>
 > cat /etc/grid-security/grid-mapfile
"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane" usatlas1
OsgCEAuthorization contains further instructions on how to configure the CE to allow others to access it. You can come back to this step after verifying basic operations of your CE. 
---++++!!Osg CE Authorization (using VOMS/GUMS/PRIMA)
---++ Initial Site Verification
The site verify script should be run as a regular user (not *root*), so login as yourself and get a grid proxy 
(Run a grid-proxy-init or voms-proxy-init are the usual methods)  

To verify many of the installed OSG software components
execute the <b>site_verify.pl</b> (Perl) script.

The script itself is available in the $VDT_LOCATION/verify/ directory.
Documentation can be found at &lt;<a href="http://griddev.uchicago.edu/download/grid3/doc.pkg/WIP/site_verify_pl.html">http://griddev.uchicago.edu/download/grid3/doc.pkg/WIP/site_verify_pl.html</a>&gt;.

Note, the site_verify.pl script will not work until you have enabled
access for some VO users.  See the section on Authorizing VO users below.
  
<tt>
$ cd $VDT_LOCATION<br>
$ source ./setup.sh<br>
$ grid-proxy-init <br>
Enter "<i>Your Passphrase</i>"<br>
$ cd verify<br>
$ perl site_verify.pl --host=<i>hostname.domain.tld</i><br>
</tt>

[[SiteVerify Troubleshooting]]


OsgCEAuthorization contains further instructions on how to configure the CE to allow others to access it. You can come back to that step after verifying basic operations of your CE. 
		(Here &lt;hostname&gt; is the CE hostname.)
%RED%This section is redundant: go to MonALISA for configuration instructions. AH
%ENDCOLOR%

The setup of MonALISA is done with the assistance of the
configuration script. However there are two files which need to be edited.
The <b>$VDT_LOCATION/MonaLisa/Service/CMD/site_env</b> script must be modified 
manually to set the location of the batch queue.  In addition, the 
<b>$VDT_LOCATION/MonaLisa/Service/VDTFarm/vdtFarm.conf</b> file must be updated
for the new <b>TracePath</b> module.  Instructions on how to edit these two 
<i>The name <b>OSG-ITB</b> group should be used when queried for the "monitor group name."</i>
  
<pre>&gt; <b>./configure_monalisa.sh</b>
<i>For the ITB machines, the name <b>OSG-ITB</b> group should be used when queried for the "monitor group name."</i>
<br>
<pre>&gt; <b>$VDT_LOCATION/vdt/setup/configure_monalisa.sh</b>


Please specify user account to run MonaLisa daemons as: [daemon]: 

This is the name you will be seen by the world, so please choose
a name that represents you. Make sure this name is unique in the 
MonaLisa environment.

in the global site list. Grid3 users should enter "grid3".

Your Monitor Group name is important to group your site correctly
in the global site list. OSG ITB users should enter "OSG-ITB".

Please enter your monitor group name [<i>hostname.domain.tld</i>]: <b>OSG-ITB</b>
...
...
...</pre>

Fill in your site specific information. *NOTE: Check your latitude and longitude entries!  
If you are in the Americas, for example, your longitude will be a negative number.	
Typical continental U.S. latitudes range from 25 to 55 degrees and longitudes range from 
-60 to -120 degrees.  If you get it wrong, your site will not show up properly in MonALISA.*

<pre>
Please review the information:

MonaLisa user: daemon 
Farm name: <i><a href="#UniqueName">UNIQUE_NAME</a></i>
Monitor group: OSG-ITB
...
Is this information correct (y/n)? [n]: <b>y</b></pre>

Is this information correct (y/n)? [n]: <b>y</b>
</pre>
	 
	* Edit the *site_env* file
			To have MonALISA interact with the local batch queuing system, 
			the file <b>site_env</b> will need to specify the location of the local 
			installation directories. 
<pre>&gt; <b>cd $VDT_LOCATION/MonaLisa/Service/CMD</b>
&gt; <b>vi site_env</b></pre>

 <pre><b>PBS_LOCATION="/usr/local/pbs";
export PBS_LOCATION;</b></pre>
 <pre><b>
	  > PBS_LOCATION="/usr/local/pbs";
 <pre>&gt; <b>cd $VDT_LOCATION/MonaLisa/Service/VDTFarm</b>
&gt; <b>vi vdtFarm.conf</b></pre>
 <pre>
	  &gt; <b>cd $VDT_LOCATION/MonaLisa/Service/VDTFarm</b>
	  &gt; <b>vi vdtFarm.conf</b></pre>

	  To have MonALISA use the *Tracepath* module the vdtFarm.conf
	  will need to be edited with the following line added:
	* What's the Tracepath module, why do we need it, and why isn't it automatically installed?
 ==*Tracepath{monTracepath, localhost, " "}==
	* Start Monalisa
	* Tracepath module lets you trace network activity near your cluster.  %RED%Why isn't it automatically installed?%ENDCOLOR%

	* Start Monalisa %RED%Do you have to be root?%ENDCOLOR%
---++++ Testing MonALISA and its Information Providers
	  <tt># <b>/etc/init.d/MLD start</b></tt> 
From a web browser, launch the MonALISA client tool from the URL &lt;<a href="http://gocmon.uits.iupui.edu:8888/index.html">http://gocmon.uits.iupui.edu:8888/index.html</a>&gt;.
---++++!!Testing MonALISA and its Information Providers
top of the left hand menu. On the running MonALISA (ML) client, check
that your site is reporting to ML and that it is correct geographical
location (mouse over the dot and it will display the site name, make
sure it is the name you picked out that conforms to the naming
convention). 
location. Mouse over the dot and it will display the site name. Make
sure it is the name you picked out and that it conforms to the naming
reporting to ML go to the !TabPan view (on the left hand menu bar) and

information, compare with other sites. If there are "Unknown" entries
in the fields then most likely there is error in the ML configuration
check that all of the table entries for your site are providing
contact your local Support Center if you need help. 
in the fields, then most likely there is error in the ML configuration

---+++Configure CoreMIS
For OSG-ITB 0.1.5, MIS-CI release is 0.2.5. If this version of MIS-CI is installed.
Two patches should be applied. These patches will not be necessary from 0.2.6.
The procesure to patch these patches is as follows:  **Note--it appears that 
in the revised VDT1.3.5 there is a version 0.2.5p1 may already have these patches applied...at least the patch command returns an error 
message that the patch has already been applied to uninstall-misci.sh.0.2.5
(S. Timm 4/19/05). Also note that these instructions don't actually say 
to run the uninstall-misci.sh script before the configure script and
it looks like you should.

<pre>
			 cd $VDT_LOCATION/MIS-CI
			 wget http://gdsuf.phys.ufl.edu:8080/releases/mis-ci/uninstall-misci.sh.0.2.5.patch
			 patch -p0 < uninstall-misci.sh.0.2.5.patch
			 cd $VDT_LOCATION/MIS-CI/etc/misci
			 wget http://gdsuf.phys.ufl.edu:8080/releases/mis-ci/mis-ci-functions.0.2.5.patch
			 patch -p0 < mis-ci-functions.0.2.5.patch
</pre>

In order to configure CoreMIS, you should follow the instructions located at 
[[http://osg.ivdgl.org/twiki/bin/view/Integration/CoreMIS#MIS_CI_Post_Install]] or run the command below.

			 Precheck 1: ivdgl account valid ?
<pre><b>			 ls -al $VDT_LOCATION/MIS-CI/share/sqlite/MIS-CI.db</b></pre> should look something like:<br>
<pre>			 -rw-r--r--	 1 ivdgl grid	 28672 Apr 15 10:40 /usr/local/osgint-0.1.5/MIS-CI/share/sqlite/MIS-CI.db<br></pre>

			 Precheck 2: 'crontab -u ivdgl -l' works ?

<pre><b>			 # $VDT_LOCATION/MIS-CI/configure-misci.sh</b> 
					 Editing site configuration...
					 ...
					 ...
					 ...
					 Would you like to set up MIS-CI cron now? (y/n) <b>y</b>
					 At what frequency (in minutes) would you like to run MIS-CI ? [10]
					 ...
					 ...
					 ...
					 Would you like to add MIS-CI crontab to this ? (y/n) <b>y</b>
					 ...
					 ...
					 ...
					 configure--misci Done
					 Please read $VDT_LOCATION/MIS-CI/README </pre>


---++ Installing Extra Software

The OSG ITB installation mostly installs server software such as the Gatekeeper, GridFTP, VOMS, etc. Only a few client tools are installed. An easy way to get more client tools is to install the VDT-Client package:

<pre>
&gt; <b>pacman -get http://www.cs.wisc.edu/vdt/%OSG_ITB_VDT_CACHE%:VDT-Client</b> 
</pre>

---++ Final Site Verification
<a href="http://www.ivdgl.org/MIS-CI/grid-site-state-info">grid-site-state-info</a>

If you haven't completed the Authorization configuration to add access to VOs, you may want to go to
OsgCEAuthorization and do that now. 


---++ Site Verification
(Remember: To run the site verify script you should not be *root* )
Now you're ready to run the full CE site verification test suite. All elements of this test should now pass.
<tt>
(NB: To run the site verify script you should not be *root* )

$ grid-proxy-init <br>
Enter "<i>Your Passphrase</i>"<br>
$ source ./setup.sh<br>
$ grid-proxy-init <br></b>
</tt>
$ cd verify<br>
---++ OSG-ITB Registration
The earlier test case only authorized yourself as a local user.  You should now go to OsgCEAuthorization and authorize other users before registering your service (otherwise no one but you will be able to access it!) 
To register the site into the Grid Catalog please 
compose an email which contains the following information
and send it to igoc@ivdgl.org - copy/paste the text
below in the body:
<pre>

  Information about you and your machine(s)

  Short_Site_Name (a.k.a <a href="#UniqueName">UNIQUE_NAME</a>)
  Host_Name
  SystemAdmin email
  System Admin phone

  If System Admin is a person rather than a mail list System Admin Name

	Location information for installation:
  $app path name
  $data path name 
  $tmp path name
  $wntmp path name

</pre>
A minimal amount of information is needed for the OSG Grid Operations Center (GOC) to publish a site to the monitoring and operational infrastructure. This includes organization name, organization manager or designated representative name and email, security contact name and email, resource URL, support center, and form submitter. 

While this minimal information will allow the GOC to publish your information to the monitoring tools, more information is requested to make site and support center communication easier. Please take time to fill out the form completely.

---++ Firewalls

Grid components are distributed throughout the network, and services 
such as gatekeepers and data movement utilties are required to be 
accessible to the dynamic cloud of clients and peer services.
This ditributed and dynamic requirement places the burden
of the security on the implementation of the application.

Due to the discovery of significant vulnerabilities in recent years, network-based applications are untrusted by default. To solve the application
problem effort has focused on developing and deploying firewalls which
restricts full and free network access.  (You might say that this is analogous to building 
a house with no doors. Is it safe? Yes. Is it useful? No.)

Some essential network-based applications have been
"hardened," such as mail relay services, web servers, and secure
shell daemons. These are further protected further by IP address 
filtering to prevent access from unknown hosts or domains.

Grid components which are located behind network firewall 
face special challenges for Grid setup and operations. 

There are two styles of firewalls usually encountered. 

	* A network firewall which is upstream from your server 
(usually centrally maintained). This blocks all traffic to your 
host. 

	* Host-based firewalls which are setup and maintained 
by individual host administrators. This is usually setup and configured
by the <b>iptables</b> program which filters incoming network packets which 
arrive for the host.

 In addition to host-based firewalls, hosts can choose to 
 implement host based access rules (usually setup with the <b>tcp_wrapper</b>
  or <b>hosts_allow</b> utilties.

Network traffic can be blocked at the firewall for both incoming and outgoing dataflow
depending on hostnames, ip addresses, ports and protocols. 


A common setup is to allow any outgoing connection,
while significantly (if not completely) restricting 
incoming connections.  The Globus project provides thorough documentation on this subject which will not be repeated here.  It is strong encouraged that you read the document <A HREF="http://www.globus.org/security/v2.0/firewalls.html">Globus Toolkit Firewall Requirements</A> to avoid issues which may arise from firewall configuration.


IP port usage which may require firewall updates:

	* MDS: 2135/tcp
	* GRAM: 2119/tcp
	* GridFTP: 2811/tcp
GRAM and GridFTP need to know the port range that you've opened up. You need to set two environment variables, GLOBUS_TCP_PORT_RANGE and GLOBUS_TCP_SOURCE_RANGE. Some people set these in the xinetd configuration, but you can also set it in $VDT_LOCATION/vdt/etc/vdt-local-setup.sh. Then these will be used by GRAM, GridFTP, and any clients that require them. This file is only present in VDT 1.3.5. 
	* Monalisa: 9000/udp (for ABping measurements).  These are specified in the file <B>$VDT_LOCATION/MonaLisa/Service/<I>cluster_name</I>/ml.properties</B>
	
GRAM and GridFTP need to know the port range that you've opened up. You need to set two environment variables, GLOBUS_TCP_PORT_RANGE and GLOBUS_TCP_SOURCE_RANGE. Some people set these in the xinetd configuration, but you can also set it in $VDT_LOCATION/vdt/etc/vdt-local-setup.sh. Then these will be used by GRAM, GridFTP, and any clients that require them.

By the way, the VDT is working on a better solution for setting these variables, and it will hopefully be in an upcoming VDT release. 

These ports and protocols <i>must be open</i> to all grid clients and
server machines participating in the grid in order to provide minimal
functionality.

You also may need to open the following optional ports for additional Grid services:

	* GIIS: 2136/tcp
	* GSISSH: 22/tcp
	* MyProxy: 7512/tcp
Here is a sample of the a /etc/hosts.allow with the GLOBUS services opened:
	* RLS server: 39281/tcp

Here is a sample of the a <tt>/etc/hosts.allow</tt> with the GLOBUS services opened:
<verbatim>

# cat /etc/hosts.allow
#
# hosts.allow	This file describes the names of the hosts which are
#					allowed to use the local INET services, as decided
#					by the '/usr/sbin/tcpd' server.
#
sshd: 129.79.6.113 
ALL : localhost
vdt-run-gsiftp.sh : ALL
vdt-run-gsiftf2.sh : ALL
vdt-run-globus-gatekeeper.sh : ALL
</verbatim>

*For RH9, RHEL3 or compatible iptables systems*

The default firewall configuration for Red Hat's iptables sets the system up with a stateful packet filter. This is different than some legacy RH7 systems as by default no ports that are not explicitly opened by the iptables script will be open. This includes high numbered ports that are often used by grid services. 

If your preference is to leave as much of the stateful packet filtering in place but enable just those grid services you want to deploy then you can use the following instructions. 

Two changes need to be made to an OSG gateway with a host based iptables stateful firewall. 

First is the configuration of the firewall itself. On RHEL or similar systems this is done in /etc/sysconfig/iptables

The Chain RH-Firewall-1-INPUT is a default chain for RHEL3. This chain is also sometimes called INPUT. Make sure the following rules use the chain that other rules in /etc/sysconfig/iptables do. 

Note: For GSISSH this port is often already open for systems. You can use either this rule or the default rule setup at install time if you selected custom firewall and enabled ssh. 

<verbatim>
# Globus: Requires addition configuration in /etc/xinetd.d/globus-gatekeeper
# set: env = GLOBUS_TCP_PORT_RANGE=40000,50000
# This allows up to 10,000 ports and matches the globus config.
# How globus is configured to use these ports is subject to change in an upcoming
# release
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 40000:50000 -j ACCEPT
# Monalisa, grabs 3 ports from the following range
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 9000:9010 -j ACCEPT
# Gridftp
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2811 -j ACCEPT
# MDS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2135 -j ACCEPT
# GRAM
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2119 -j ACCEPT

# Optional Services
# RLS Server
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 39281 -j ACCEPT
# MyProxy
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 7512 -j ACCEPT
# GSISSH/SSH
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT
# GIIS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2136 -j ACCEPT
# GUMS/VOMS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 8443 -j ACCEPT
</verbatim>


Second is the configuration of globus to use the above port range. _This may change in upcoming VDT releases, see above_


/etc/xinetd.d/globus-gatekeeper
<verbatim>
service globus-gatekeeper
{
			socket_type = stream
			protocol = tcp
			wait = no
			user = root
			instances = UNLIMITED
			cps = 400 10
			server = $VDT_LOCATION/vdt/sbin/vdt-run-globus-gatekeeper.sh
			env	 = GLOBUS_TCP_PORT_RANGE=40000,50000
			disable = no
		  -globus-tcp-port-range 40000,50000
</verbatim>

_Note: $VDT_LOCATION should be set by the pacman installer_
<verbatim>
# /etc/rc.d/init.d/iptables restart

<verbatim><b>
# /etc/rc.d/init.d/iptables restart</b>
Flushing firewall rules:											  [  OK  ]
# /etc/rc.d/init.d/xinetd reload
Reloading configuration:											  [  OK  ]
# 
Applying iptables firewall rules:								  [  OK  ]


</pre>
---++Shutdown Guide
Please see the [[OSG Shutdown Guide]].


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.LeighGrund - 10 Mar 2005

-- Main.RobGardner - 25 Mar 2005
-- Main.BockjooKim - 18 Apr 2005
-- Main.RobQ - 15 Apr 2005

-- Main.BockjooKim - 30 Jun 2005

-- Main.LeighGrund - 25 Apr 2005

-- Main.TerrenceMartin - 29 Apr 2005

-- Main.DaneSkow - 11 May 2005

-- Main.BurtHolzman - 11 May 2005 
%STOPINCLUDE%

%META:TOPICMOVED{by="DaneSkow" date="1115753408" from="Integration.OSGCoreInstallGuide" to="Integration.OSGCEInstallGuide"}%
