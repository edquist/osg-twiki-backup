%META:TOPICINFO{author="RobQ" date="1142535699" format="1.0" version="1.192"}%
%META:TOPICPARENT{name="WebHome"}%
---+!!<nop>ITB CE 0.3.6 / OSG CE 0.4.1 Install Instructions
%TOC%
%STARTINCLUDE%

---++ Introduction

This document is intended for administrators responsible for installing and configuring:
<UL>
<LI>ITB Compute Element (CE) version 0.3.6 onto OSG Integration Resources
<LI>OSG Compute Element (CE) version 0.4.1 onto OSG Production Resources
</UL>
It is not meant as an all-inclusive guide to Grid computing or even all the options for configuring a CE. 
<!-- Instructions for installing a CE intended for the OSG version 0.2.1 Production grid can be found [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=67&Itemid=65][here]]. 
-->

---+++ Conventions used

The following conventions are used in these pages:
	* =monospaced text= indicates terminal output 
	* ==bold monospaced text== indicates terminal input (by the user)
	* =<i>monospaced text in italics</i>= indicates variable data that may differ on your installation
	* the =&gt;= prompt indicates commands that do not require a specific shell or root access
	* the =#= prompt indicates commands that require root access
	* the =$= prompt indicates sh- or bash-specific commands
	* the =%= prompt indicates csh- or tcsh-specific commands

---+++ Operating Systems

The base software from VDT 1.3.10 is supported on:

	* Red Hat 7
	* Red Hat 9
	* Debian 3.1 (Sarge)
	* Red Hat Enterprise Linux 3 AS
	* Red Hat Enterprise Linux 4 AS
	* Fedora Core 3
	* Fedora Core 4
	* ROCKS Linux 3.0/3.2/3.3/4.0
	* Scientific Linux 3.0/4.0 (Fermi)
	* Red Hat Enterprise Linux 3 AS ia64
	* !SuSE Linux 9 ia64
	* Red Hat Enterprise Linux 3 AS amd64

If you experience problems with the installation of VDT supported software or for general  system requirements,	the basic [[http://vdt.cs.wisc.edu/releases/1.3.10/requirements.html][VDT 1.3.10 System requirements page]] may be useful.  All OS above supported on i386 architecture unless noted specifically otherwise.

---+++Installation Method

The installation and configuration method is based on <a href="http://physics.bu.edu/pacman/">Pacman</a>.  Pacman is a package manager like RPM or dpkg, but is able to work on and support multiple platforms.  Specific Pacman directions are given below, and in the pre-installation procedures.

---++Known Problems

%RED% Ambiguity in instructions for Worker Node Client install. 
The Worker Node Client line item says that it must be installed.
But just below that, under "Choose an Installation Directory"
it says "This (the $VDT_LOCATION) must be exported to all the worker
nodes, otherwise additional installation of software on the worker 
nodes is required."  Need to resolve this ambiguity.  I propose
doing so by saying that Worker Node Client installation is 
required, and it can either be exported via NFS to the workers 
or installed on each one.  OSG CE directory doesn't need to be
nfs-exported to the workers under any circumstance.  S. Timm 3/16/06 %ENDCOLOR%
%RED% Table of contents is malformed, missing Configuring OSG Attributes,
Job Queuing System, Authorizing Users, S. Timm 3/16/06 %ENDCOLOR%
---++ Pre-installation Checklist

---+++ System or Cluster Already Setup including a functioning batch system
It is assumed that your hardware is already running one of the operating systems previously mentioned. If your system is a cluster, the batch queuing system should likewise previously be installed and configured. If you intend to use the [[http://www.cs.wisc.edu/condor/][Condor]] system, that can be installed with pacman from the VDT cache before this grid middleware.  It 
is strongly recommended to install Condor in a different directory
than your main CE installation. 

To pull Condor from the VDT cache, use:

<pre>pacman -get http://vdt.cs.wisc.edu/vdt_1310_cache:Condor</pre>

The installation will ask for local storage space for 3 separate purposes: 
	* OSG applications
	* data
	* temporary caching.

 It is recommended that you have separate, dedicated partitions for each purpose. 

For details on the storage use and configuration, please reference the section on 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/LocalStorageConfiguration">Configuring the OSG Attributes</a> in this guide.

---+++ Preserving a pre-existing Condor installation?
If you have installed Condor as your batch queueing system, you are required to set two environment variables so the Condor path is correctly configured. 

	* *VDTSETUP_CONDOR_LOCATION* - the location of your Condor installation  (e.g. _/opt/condor_). The Condor _bin/_, _bin_, _etc/_, _lib/_... directories should be directly under this location.
	* *VDTSETUP_CONDOR_CONFIG* (optional): The location of your Condor configuration file (if non-standard). Default is _$VDTSETUP_CONDOR_LOCATION/etc/condor_config_.

This will result in:
	* VDT middleware services pointing to appropriate locations in your external Condor installation.

For example, if you wish to preserve a Condor installed in the standard location,
<pre>
  $ export VDTSETUP_CONDOR_LOCATION=$CONDOR_ROOT
</pre>
will suffice (provided $CONDOR_ROOT is set, per the usual Condor config).

---+++ Compilers
The VDT installation presumes that you have a working C and C++ 
compiler as part of the install.  The gcc and g++ that come as
rpms with most Linux distributions are fine.

---+++ Network setup?
It is assumed that your hardware has a working network connection through which packages may be retrieved and from which services may contact your system. The full OSG CE package is ~700MB.

---++++!! Firewall?
If your installation is inside a site firewall, has a host firewall (ipchains/iptables), or is using TCP wrappers (=/etc/hosts.{allow,deny}=), you'll need to review the 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/InstallInstructions#Firewalls">Firewall section</a> of this guide and arrange for appropriate Internet access.

---++++!! Time synchronization (NTP) 
Each system should be setup to support network time protocol. Lack of synchronization generally complicates troubleshooting and can cause problems with the security infrastructures evaluation of eg. proxy lifetimes. 

---++++!! Reverse name lookups (DNS)
For the middleware to correctly function both the forward and reverse lookups as configured through a local DNS service are required for the IP address of the system.

#UniqueName 
---+++ Assigning a unique OSG resource name

%INCLUDE{"OSGResourceName"}%

---+++ Previous OSG or Grid3 Installations
If there is a _previous installation_ of the OSG or Grid3 environment or other Globus middleware, please _stop the processes_ which are currently running. This includes the Globus Resource Information Service (GRIS), !MonALISA and other services configured to start upon initialization on your system. Also, if you've sourced the $VDT_LOCATION/setup.[c]sh on your previous VDT install, just log out and log back in to clear out your environment.

More information is provided in the
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/OSGShutdownGuide">OSG Shutdown Guide</a>.

---+++ Creation and setup of local user accounts for VOs for OSG 
UNIX user accounts need to be created by the system administrator for the VOs.  
You will need to create at least one local user account (with the appropriate configuration) for each VO to which you wish to provide resources.  The uid and name of the account can be locally determined. You will be asked to provide a mapping from local accounts to VOs later in the installation, the following default accounts are assumed for examples in this document.

The accounts are: 
	* cdf
	* fermilab
	* grase
	* fmri
	* gadu
	* mis
	* sdss
	* ivdgl
	* star
	* usatlas1
	* uscms01
	* ligo
	* sam
	* samgrid
	* dosar
	* des
	* glow
	* grow
	* gridex
	* nanohub
	* geant4
	* i2u2

In addition, you need a globus user account for web services to run.

---+++ Do you have a user certificate?
You will need a personal grid certificate to run validation tests.  If you don't have a personal certificate, or don't know how to generate a grid proxy (grid-proxy-init, etc), contact your <a href="http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support">VO Support Center</a>.

---+++ Assistance during the installation

If you have questions during the installation of this software the first line of support is your 
<a href="http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=37&elMenu=Grid%20Support">VO Support Center</a>, as an alternative the
mailing list osg-general@opensciencegrid.org is available for you to submit
questions.  More information about the mailing lists is available at <a href="http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=75&Itemid=88">OSG Mailing Lists</a>.

---++ Installing and Setting up Pacman
Instructions for installing and setting up Pacman for your specific platform can be found in the 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/PacmanInfo">Pacman Guide</a>.

---++ Installing OSG CE Services

Now you're ready to start installing the Compute Element services. We distinguish between software that must be installed on the server/gatekeeper (eg., osg-0.4.0.pacman), and software which must be executable from the worker nodes.

---+++Installing the OSG Worker Node Client Package

This must be installed in a location visible to jobs executing on the worker node. Refer to the 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/WorkerNodeClient">Worker Node Client Guide</a>  for additional information.

---+++ Choose an installation directory 
This is typically something like <tt>/usr/local/grid</tt>, but whatever suits the local resource structure is fine. If you are installing on a
cluster, this directory must be available on all the nodes. If this directory is not shared, additional installation of
software will be required on each of the worker nodes. Nearly all of the software to be installed is from the VDT.

For example (shown for sh, bash):
<pre>
  $  export VDT_LOCATION=/usr/local/grid
  $  cd $VDT_LOCATION
</pre>

---+++Installing the OSG CE software package
The installation described here is done as root. Not all services will run as root:
	* Condor and the GRIS will run as the user "daemon".  
	* The VDT will configure the daemons appropriately.  
	* _Verify that the umask is set to "0022" prior to installation.  Failure to do so may render the installation unusable._
	* A few questions regarding trust of the caches from which the software is downloaded will be displayed. Please answer *y* (yes) so that the software can be retrieved.
	*  Finally, make sure there are no non-standard versions of perl, tcsh, or bash in your $PATH variable.

The following pacman commands will install the OSG software packages.
<pre>
	&gt;  pacman -get OSG:osg-0.4.1
</pre>
For OSG-ITB work, you should instead do:
<pre>
	 &gt; pacman -get iVDGL:ce-0.3.6
</pre>
See the [[#Installing_and_Setting_up_Pacman][Installing and Setting up Pacman]] section of this guide if you encounter an 'unsupported' platform message.

<b>This will take between 10 and 60 minutes to complete, depending
upon the system and network connection.</b> During this time you may open a second 
terminal and watch the progress by monitoring the <tt>$VDT_LOCATION/vdt-install.log</tt> file.

You should not be asked any other questions during the installation process.

The installation should complete with the following message. 
<pre>
Downloading [site-verify.tar.gz] from [hep.uchicago.edu]...
Untarring [site-verify.tar.gz]...
Downloading [srmclient-1.21.tar.gz] from [hep.uchicago.edu]...
			6/6 MB downloaded...										 
Untarring [srmclient-1.21.tar.gz]...

</pre>

---+++ Setup the OSG environment
 Assuming the Pacman install completed without fatal errors, you should now be able to source the OSG setup environment. In your installation directory, from here forward known as the $VDT_LOCATION and source the setup script.
<pre>
  $ source setup.sh
			or
  % source setup.csh
</pre> 

---+++ Optional packages for Condor, PBS, LSF, FBSNG, or SGE
An extra package may be required to setup access to an existing Condor, PBS, LSF, FBSNG, or SGE installation. *Ensure that that the command-line utilities for your batch system are in your path*, then install the appropriate package in your $VDT_LOCATION directory (for Condor, PBS, LSF, FBSNG, or SGE respectively):  
<pre>
	&gt; pacman -get OSG:Globus-Condor-Setup
	&gt; pacman -get OSG:Globus-PBS-Setup
	&gt; pacman -get OSG:Globus-LSF-Setup
	&gt; pacman -get OSG:Globus-FBSNG-Setup
	&gt; pacman -get OSG:Globus-SGE-Setup
</pre>
For OSG-ITB substitute
<pre>
&gt; pacman -get iVDGL:Globus-Condor-Setup-0.3.6
</pre>
  and so forth.

This guide will not go into the detail on the installation of any of these optional packages.

---+++ Post install README
Read the <tt>$VDT_LOCATION/post-install/README file</tt>.	
We suggest that you read the file for information only, and follow the instructions in the various OSG installation guides presented in these pages, rather than those in the VDT README file.  

<b>Note: Do not be concerned with any error messages you may see in this README file.  They are usually not a problem.</b>

This page and others that it links to will take you through all the steps necessary to configure your installation.

---++ Configure the Public Key Infrastructure
Minimally, you will need a host certificate and private key to identify of your CE.

---+++ Setup and maintenance of Certificate Authority connection
Configure the DOEGrids Certificate Authority (CA) to be used by default by running the utility below. 

If you are given the option "<b>choose the option  which matches "1c3f2ca8 : ...</b>", <br>
then enter "<b>q</b>" at the prompts.
<pre>
  &gt; $VDT_LOCATION/vdt/setup/setup-cert-request
  Reading from /g3dev/globus/TRUSTED_CA
  Using hash: 1c3f2ca8
  Setting up grid-cert-request
  Running grid-security-config...
  ......
</pre>

A <a href="http://www.cs.wisc.edu/vdt/releases/1.3.10/certificate_authorities.html">list of CAs</a> were added as authorized CAs on your system. 

<b>Important Note:</b> In the past, you had the option of putting these certificates in the 
<i>/etc/grid-security/certificates</i> directory or in the local <i>$VDT_LOCATION/globus/TRUSTED_CA</i> directory.
<b>This is no longer an option.</b>  
The OSG installation is preconfigured to place the certificates in 
the local <i>TRUSTED_CA</i> directory.  The <i>edg-crl-upgrade</i> daemon will be updating CRLs in this local directory only. 
 If you want to maintain certificates in the <i>/etc/grid-security/certificates</i> directory you should 
 link it to the local <i>TRUSTED_CA</i> location (symlink in either direction) and copy the CA files appropriately.

Please review the list of authorized CAs and modify the set in $X509_CERT_DIR= $VDT_LOCATION/globus/TRUSTED_CA as needed to match your local policy.  

The daemon <i>edg-crl-upgrade</i> should be running at all times in the background to refresh the CRLs from these CAs.  If CRLs are not kept current, incoming connections will fail. 

To check that it's running:
<pre>
&gt; ps axwww | grep edg-crl-upgrade
</pre>

If not running, do
<pre>
&gt; /etc/init.d/edg-crl-upgraded start
</pre>


The <a href="http://osg.ivdgl.org/twiki/bin/view/Integration/CertScripts">Certificate Scripts Package Guide</a> which has been installed can assist you with choosing Certificate Authorities to trust and and periodically checking that the CRLs (Certificate Revocation Lists) have not expired.

---+++ Request and Install Host Certificate
After the pacman installation of the OSG CE software completes you should have the PPDG-Cert-Scripts  package installed (in $VDT_LOCATION/cert-scripts) and the instructions below show the use of those scripts.

To authorize this resource for use, a host certificate needs to be obtained from an appropriate Certificate Authority. 
<UL>
<LI>Currently the  <a href="http://www.doegrids.org"> DOEGrids CA</a> is the most common CA in use for OSG participants. 
<LI>The iVDGL and PPDG project instructions for getting a certificate are available on the <a href="http://igoc.ivdgl.indiana.edu/RAinfo/newra/">iVDGL RA</a> and <a href="http://www.ppdg.net/RA">PPDG RA</a> sites (RA is Registration Authority). 
</UL>
	 
Here is a brief guide to the process assuming you have a valid User Certificate. 
 
Run the <i>cert-request</i> script  to generate your host's private key (<tt>hostkey.pem</tt>) and submit the certificate request.
<pre>
	$ cd $VDT_LOCATION
	$ source  ./setup.sh
	$ cert-request -ou s -dir .
	Processing OU=Services request.
	Give reason (1 line) you qualify for certificate, such as
	  member of CMS experiment	 or
	  collaborating with Condor team,  etc.
	 reason: <b><i>testing for PPDG</i></b>
	input full hostname: <b><i>goofy.looney.tunes</i></b>
	Generating a 2048 bit RSA private key
.	....................................+++
	 ..................+++
	 writing new private key to './5842key.pem'
	 -----
	 input your email address: <b><i>address@your.email.server</i></b>
	 input your complete phone number: <b><i>9995551212</i></b>
	Choose a registration authority to which you are affiliated.
	_Enter__this____for this registration authority
		  anl	  ANL: Argonne National Lab
		  epa	  NCC-EPA: Environmental Protection Agency
		  esg	  ESG: Earth System Grid
		  esnet	ESnet: DOE Science network
		  fnal	 FNAL: Fermilab host and service certificates
		  ivdgl	iVDGL:  see www.ivdgl.org
		  lbl	  LBNL: Berkeley Lab
		  lcg	  LCG: LHC Computing Grid
		  nersc	NERSC: computer center, see www.nersc.gov
		  o		 Other: if you can not tell what to choose
		  ornl	 ORNL: Oak Ridge National Lab
		  pnnl	 PNNL: Pacific Northwest National Lab
		  ppdg	 PPDG: includes BNL, JLab, SLAC and many HEP & NP experiments
	(choose from left column): <b><i>ppdg</i></b>
	ppdg
	PPDG
	You must agree to abide by the DOEGrids policies,
	at http://www.doegrids.org/Docs/CP-CPS.pdf
	Do you agree (y,N): <b><i>y</i></b>

	Your Certificate Request has been successfully submitted
	Your Certificate Request id: 2005

		  You will receive a notification email from the CA when your certificate
		  has been issued. Please disregard the instructions to download your
		  certificate though a web browser and use the cert-retrieve script instead.
</pre>

After the certificate is approved you will receive an email which includes the serial number of the new certificate.  Use that serial number to retrieve the certificate and move it to the installation directory.
<pre>
  &gt; cert-retrieve -dir . -certnum 0x299
	using CA doegrids
	Checking that the usercert and ./5842key.pem match
	writing RSA key
	./usercert.pem and ./userkey.pem now contain your Globus credential

	&gt; mv ./usercert.pem /etc/grid-security/hostcert.pem
	&gt; mv ./userkey.pem /etc/grid-security/hostkey.pem
	&gt; chmod 444 /etc/grid-security/hostcert.pem
	&gt; chmod 400 /etc/grid-security/hostkey.pem
</pre>


The following command will verify the certificate is readable.  The output shown will be similar, but specific to your request.
<pre>
	&gt; openssl x509 -text -noout -in /etc/grid-security/hostcert.pem

	Certificate:
		 Data:
			  Version: 3 (0x2)
			  Serial Number: 665 (0x299)
			  Signature Algorithm: sha1WithRSAEncryption
			 Issuer: DC=org, DC=DOEGrids, OU=Certificate Authorities, CN=DOEGrids CA 1
			 Validity
				  Not Before: Dec 13 23:55:14 2005 GMT
				  Not After : Dec 13 23:55:14 2006 GMT
			 Subject: DC=org, DC=doegrids, OU=Services, CN=goofey.looney.tunes
			 .........
.</pre>
	 
---+++ Get an LDAP service certificate to run authenticated MDS (optional)
MDS is the monitoring and directory service. The GRIS (Grid Resource info service, a sensor that collects site data and publishes it to MDS) is 
configured by default to allow anonymous access only.  Instructions are in the 
$VDT_LOCATION/post-install/README on how to allow only authenticated and authorized access. Access requires an LDAP service certificate (two files), owned by the MDS owner (if you installed as root, the owner is probably daemon -- check).
<pre>
	$ source $VDT_LOCATION/setup.sh
	$ cert-request -ou s -dir . -host <i>hostname.domain.tld</i> -service ldap
</pre>
And follow the instructions, which are very similar to the host cert instructions, except that you are creating <b>ldapkey.pem</b> and <b>ldapcert.pem</b>. More info on service certificates can be found at the <a href="http://www.cs.wisc.edu/vdt/releases/1.3.9/installation_post.html#servicecert">VDT Post Installation Site</a>

---++ Configuration and Setup of OSG CE Services

---+++ Globus Configuration

Globus has been pre-configured for this installation. If you wish
reconfigure it, please review the <a href="http://vdt.cs.wisc.edu/releases/1.3.10/config/configure_globus_gatekeeper.html"> VDT
Globus Configuration Script</a> document on using
the =$VDT_LOCATION/vdt/setup/configure_globus_gatekeeper= script. 

Review the configuration files =/etc/xinetd.d/globus-gatekeeper= and
=/etc/xinetd.d/gsiftp= (or in =/etc/inetd.conf=) that were created during the pacman installation.
Additionally, =/etc/services= file was updated with the following entries:
<pre>  gsiftp						2811/tcp		 # Added by the VDT
  globus-gatekeeper		 2119/tcp		 # Added by the VDT</pre>

If you are satisfied with this configuration, restart the xinetd (or inetd) daemon to pick up the configuration changes:
<pre>  /etc/rc.d/init.d/xinetd restart
  Stopping xinetd:							  [  OK  ]
  Starting xinetd:							  [  OK  ]</pre>

To verify that the gatekeeper is running at this point, you should be able to telnet to the public IP address of your site on port 2119 and get a response.
=telnet _hostname port_=

It should return =Connected to...=. The same should be true of the gsiftp port (2811 by default).

---+++ Configuring the OSG Attributes

At this point, there are several OSG attributes (environmental variables) that need to be established for required and optional CE services to run effectively.

These attributes are collected by executing the =$VDT_LOCATION/monitoring/configure-osg.sh= script which is described in the [[./LocalStorageConfiguration][Local Storage Configuration]] document. <b>Do not skip this step.</b>

---+++ Job Policy and the Batch Queuing System 

The configuration of a production queue manager is beyond the scope of this document. Since there will be periodic validation submitted by DNs in the "mis" VO, it is recommended sites provide at least two levels of priority and assign the lowest priority to user mapped to the "mis" VO.

---++ Authorizing Users

---+++ Test Configuration (using local grid mapfile)

The first step is to configure the CE to allow access using your own Grid credentials. Once that's done you'll be able to run local tests and verify operation of your CE locally.

Make sure you have a grid proxy for yourself. (this is based on your certificate):
<pre>
	&gt; source VDT_LOCATION/setup.(c)sh
	&gt; grid-proxy-init
	 (you will be prompted for your GRID pass phrase)
 </pre>
	
Then, to get the subject (DN) of your proxy, run: 
<pre>
	&gt;  grid-proxy-info -subject
Output....
	/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane
</pre>

Take the subject string and prepend it to the /etc/grid-security/grid-mapfile and assign it to a local user account (you can use any of the VO accounts you've created at the beginning to test). So the grid-mapfile should have at least one entry like:
<pre>
 > cat /etc/grid-security/grid-mapfile
"/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane" usatlas1
</pre>

This is enough to enable the rest of the installation testing.

---++++ Simple Test of Fork Queue
Using the grid proxy you just created, you can test the Globus gatekeeper fork queue by executing:
<pre>
	  &gt;  globus-job-run $(hostname):2119/jobmanager-fork  /usr/bin/id 
Output....
	  uid=9872(usatlas1) gid=9872(usatlas1) groups=9872(usatlas1)
</pre>
You should see the UNIX user account assigned  based on the <i>gridmap-file</i> you created previously..

You can also view the <i>$VDT_LOCATION/globus/var/globus-gatekeeper.log</i> file to see the authorization messages:
<pre>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 6: Got connection 131.225.207.100 at Sun Jan  8 23:12:43 2006
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 5: Authenticated globus user:	 <b>/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane</b>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 0: GRID_SECURITY_HTTP_BODY_FD=7
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 5: <b>Requested service: jobmanager-fork</b>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 5: <b>Authorized as local user: usatlas1</b>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 5: <b>Authorized as local uid: 9872</b>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 5:			  <b>and local gid: 9872</b>
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 0: executing /storage/local/data1/osg/globus/libexec/globus-job-manager
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 0: GATEKEEPER_JM_ID 2006-01-08.23:12:43.0000029703.0000000000 for /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Dane Skow/UID=dane on 131.225.207.100
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 0: GRID_SECURITY_CONTEXT_FD=10
  TIME: Sun Jan  8 23:12:43 2006
	PID: 29703 -- Notice: 0: Child 29808 started
</pre>

---++++ Simple Test Of The  Job Manager Queue
Now you can test the Globus gatekeeper job subbmissions for the job manager queue you are using. The example below is for the Condor job manager.
<pre>
	  &gt;  globus-job-run $(hostname):2119/jobmanager-condor /usr/bin/id 
Output....
	  uid=9872(usatlas1) gid=9872(usatlas1) groups=9872(usatlas1)
</pre>
You should see the same results as with the fork queue submission.

The <i>globus-gatekeeper.log</i> file will look identical except for the "Request service" message:
<pre>
	PID: 29703 -- Notice: 5: Requested service: jobmanager-condor
</pre>

---++++ Simple Test Of GSIFTP Services
A simple test of the gsiftp services requires creating a simple file and then copying it from one location on your machine to a location known to your CE node OSG implemenation. When you configured your OSG attributes (<a href="#Configuring_the_OSG_Attributes">Configuring the OSG Attributes</a> section of this document),  you defined an OSG_DATA variable as a shared directory with read-write access for all users.  We will use this as the destination directory for the file  we are copying to CE node.

Create a temporary file to be copied:
<pre>
	$ echo "My test gsiftp file" > /tmp/gsiftp.test
</pre>
Copy the file to the $OSG_DATA directory:
<pre>
	  &gt; source $VDT_LOCATION/monitoring/osg-attributes.conf	 <i>(This is simply to get the OSG_DATA variable)</i>
	  $ globus-url-copy file:/tmp/gsiftp.test gsiftp://$(hostname)${OSG_DATA}/gsiftp.test
</pre>
Verify the file was copied to the $OSG_DATA directory:
<pre>
	 &gt; ls -l  $OSG_DATA/gsiftp.test
	 -rw-r--r--  1 usatlas1 usatlas1  20 Jan  9 13:29 /storage/local/data1/osg/OSG.DIRS/data/gsiftp.test
</pre>
To verify the accounting information was captured, you can view the <i>$VDT_LOCATION/globus/var/log/gridftp.log</i> for the ftp copy operation you just performed.  You should see an entry similar to this for the transfer statistics collected by various systems like !MonALISA and ACDC::
<pre>
	<b>DATE=20060110211714.329666</b> HOST=cmssrv09.fnal.gov PROG=globus-gridftp-server NL.EVNT=FTP_INFO START=20060110211714.247555 <b>USER=ivdgl </b>FILE=/tmp/gridcat-gsiftp-test.gridcat.21602.remote BUFFER=0 BLOCK=262144 <b>NBYTES=28 </b>VOLUME=/ STREAMS=1 STRIPES=1 DEST=[129.79.4.64] TYPE=RETR CODE=226
</pre>
The authorization and other informational messages are captured in the <i>$VDT_LOCATION/globus/var/log/gridftp-auth.log</i>:
<pre>
	[2834] Tue Jan 10 15:21:37 2006 :: Server started in inetd mode.
	[2834] Tue Jan 10 15:21:37 2006 :: Configuration read from /storage/local/data1/osg/globus/etc/gridftp.conf
	[2834] Tue Jan 10 15:21:37 2006 :: New connection from: cmssrv09.fnal.gov:38376
	[2834] Tue Jan 10 15:21:38 2006 :: <b>User uscms112 successfully authorized</b>
	[2834] Tue Jan 10 15:21:38 2006 :: Starting to transfer "/storage/local/data1/osg/OSG.DIRS/app/weigand.jgw.sh.2707".
	[2834] Tue Jan 10 15:21:38 2006 :: Finished transferring "/storage/local/data1/osg/OSG.DIRS/app/weigand.jgw.sh.2707".
	[2834] Tue Jan 10 15:21:38 2006 :: Closed connection from cmssrv09.fnal.gov:38376
</pre>



---+++ OSG CE Authorization (using VOMS/GUMS/PRIMA)

The <a href="http://osg.ivdgl.org/twiki/bin/view/Integration/OsgCEAuthorization">OSG CE Authorization</a> document contains instructions on how to configure the CE to allow others to access it. 

It is suggested you come back to this step after performing the monitoring and optional services for your OSG CE node.

---++Monitoring Setup

---+++ Configure MIS-CI
The Monitoring and Information Services Core Infrastructure (MIS-CI) provides 
information on the site environment and computing resources. The OSG-CE package includes MIS-CI. 
This section describes how to configure MIS-CI if you wish to enable it.

The $VDT_LOCATION/MIS-CI/configure-misci.sh script performs the configuration. It
creates or adds a crontab for the MIS-CI information collectors. The Unix account for the MIS-CI scripts should be <b>mis</b>. By default, the script assumes the !GridCat DN is mapped to the ivdgl user.  You will need to use the <b>--choose_user</b> option to change to <b>mis</b>.
<pre>
	$  cd $VDT_LOCATION
	$  SOURCE  ./setup.sh
	$  $VDT_LOCATION/MIS-CI/configure-misci.sh --choose_user
		 Editing site configuration...
		 Creating MIS-CI.db
			  :
			<i>( a lot of information on the tables it is creating will appear before any questions are asked)</i>
				:
		 Would you like to set up MIS-CI cron now? (y/n) <b>y</b>
		 At what frequency (in minutes) would you like to run MIS-CI ? [10] <b>10</b>
		 Under which account the cron should run ? [ivdgl] <b>mis</b>
		 Frequency 10
		 User mis
		 Would you like to create MIS-CI crontab  ? (y/n) <b>y</b>
		 Updating crontab
		 Configuring MIS jobmanager
		 /storage/local/data1/osg/MIS-CI/share/misci/globus/jobmanager-mis is created

		 Your site configuration :
		 sitename ITB_INSTALL_TEST
		 dollarapp /storage/local/data1/osg/OSG.DIRS/app
		 dollardat /storage/local/data1/osg/OSG.DIRS/data
		 dollartmp /storage/local/data1/osg/OSG.DIRS/data
		 dollarwnt /storage/local/data1/osg/OSG.DIRS/wn_tmp
		 dollargrd /storage/local/data1/osg
		 batcheS condor
		 vouserS uscms01 ivdgl sdss usatlas1 cdf grase fmri gadu
		 End of your site configuration


		If you would like to add more vo users,
		you should edit /storage/local/data1/osg/MIS-CI/etc/misci/mis-ci-site-info.cfg.
		You have additional batch managers :  condor .
		If you would like to add these,
		you should edit /storage/local/data1/osg/MIS-CI/etc/misci/mis-ci-site-info.cfg.

		configure--misci Done
		Please read /storage/local/data1/osg/MIS-CI/README
</pre>


MIS-CI is collecting information using crontab as the user mis (or ivdgl if you left it as the default). Therefore, in order to stop
MIS-CI processes, crontab should be removed. The script $VDT_LOCATION/MIS-CI/uninstall-misci.sh
is provided for this purpose:
<pre>
		&gt;  cd $VDT_LOCATION
		&gt;  source setup.(c)sh
		&gt;  cd MIS-CI
		&gt;  ./uninstall-misci.sh
</pre>

After finishing configuring the MIS-CI, a few checks might be necessary:
<OL>
<LI> Verify the crontab was created for the <i>mis</i> user.
 <pre>
		&gt;  crontab -u mis -l
</pre>
<LI> If you want to force an MIS-CI table update (due to fresh install or update), then as the MIS-CI user (<i>mis</i>), execute:
<pre>
	&gt;  $VDT_LOCATION/MIS-CI/sbin/run-mis-ci.sh
</pre>
<LI>  As a non-root user, verify that at least one table is filled.<br>
			  <b>If you chose not to force an update, it might take 10 minutes or so before the tables are filled with current information.</b>
<pre>
	 &gt;	source $VDT_LOCATION/.setup.(c)sh
	 &gt;	grid-proxy-init
	  <i>(enter your password)</i>
	 &gt;  globus-job-run &lt;hostname&gt;/jobmanager-mis /bin/sh siteinfo
	  <i> (Here &lt;hostname&gt; is the CE hostname.)</i>
	 <i>...... sample output ....
		 id							  1 
		ymdt							Wed Jan 11 19:00:01 UTC 2006 
		sitename					  ITB_INSTALL_TEST 
		hostname					  localhost 
		VOname						 local:100 
		appdir						 /storage/local/data1/osg/OSG.DIRS/app 
		datadir						/storage/local/data1/osg/OSG.DIRS/data 
		tmpdir						 /storage/local/data1/osg/OSG.DIRS/data 
		wntmpdir					  /storage/local/data1/osg/OSG.DIRS/wn_tmp 
		grid3dir					  /storage/local/data1/osg 
		jobcon						  condor 
		utilcon						fork 
		locpname1					  
		locpname2					  
		ncpurunning				  0 
		ncpus						  4 </i>
</pre>
</OL>


---+++ Configure MDS (GRIS)

The Globus information system is called MDS and is preconfigured to read the osg-attributes.conf information file.	

The configuration script (<i>VDT_LOCATION/vdt/setup/configure_mds</i>) is executed automatically during the initialdownload with default values.

To test the GRIS:
<UL>
<LI>first you need an <a href="#Get_an_LDAP_service_certificate">LDAP service certificate</a> installed in /etc/grid-security/ldap. Both the directory and certificate files should be owned by the user daemon.
<pre>&gt;  chown -R daemon /etc/grid-security/ldap</pre>

<LI>One may (optionally) start the Globus information service daemon (GRIS) with the following command (as root):
<pre>&gt; /etc/init.d/gris start</pre> 

<LI>MDS should be configured for anonymous bind.  This  sends a test query to your local host and does no authentication on the user submitting the request .  Verify you have no proxy certificate (/tmp/x509u_(your_PID)).  If one exists,  remove it first.
<pre>
&gt;  grid-info-search -anonymous
<i>... your screen should scroll for a while showing a lot of data... 
....you can redirect the output to validate</i>
</pre>

<LI>Then, using <u>your</u> user account, create a grid proxy and run the grid-info-search request again with the following security-enabled query to output the details of all objects on the GRIS:
<pre>
&gt; source $VDT_LOCATION/setup.(c)sh
&gt; grid-proxy-init
&gt;  grid-info-search -b "mds-vo-name=local, o=grid"
<i>... your screen should scroll for a while showing a lot of data... 
....you can redirect the output to validate</i>
</pre>
</UL>



---++Activate Your Site
The !GridCat system maintains status and other information on all OSG sites as can be viewed 
<a href="http://osg-cat.grid.iu.edu/">here</a>. 

Sites added to !GridCat are presumed to be inactive if a site state bit is not set to be 1 (see below).
<UL>
<LI><b>Inactive</b> sites will have the site status dot with the grey color. 
<LI> Once the site becomes <b>active</b>, the site status dot will become either green or red, depending on the 
<a href="http://osg-cat.grid.iu.edu/">GridCat</a>.
test results.
</UL>
Since the default site state is presumed to be inactive, the CE site administrator  has to proactively switch the site state to be active. The activation is done by modifying the file <i>$VDT_LOCATION/MIS-CI/etc/grid-site-state-info</i>. and setting the value to 1 for the variable below:
<pre>
export grid_site_state_bit =  1
</pre>

<b>NOTE: It might take up to 2 hours for registered sites to take effect in the !GridCat display. If your site is not registered with the OSG-GOC see the instructions in the 
<a href="#OSG_Registration">OSG Registration section</a> of this document. Until your site is registered, it will not appear in 
<a href="http://osg-cat.grid.iu.edu/">GridCat</a></b>

If your site decides to become inactive for various reasons, e.g., site maintenance, the site administrator  should set the value of <i>grid_site_state_bit</i> to be other than 1. 

Example <a href="http://www.ivdgl.org/MIS-CI/grid-site-state-info">grid-site-state-info</a> file.



---++ Optional Components
---+++ MonALISA
To configure this optional component, see the 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/MonALISA">
MonALISA</a> document in this guide.

<!--  Discovery Service  To configure this optional component, see the  <a href="http://osg.ivdgl.org/twiki/bin/view/Integration/DiscoveryServiceInstallation"> Discovery Service MDS)</a> document in this guide. -->

---+++ Generic Information Providers (GIP)
To configure this optional component, see the 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/GenericInformationProviders">
Generic Information Providers</a> document in this guide.


---++ Site Verification
---+++site-verify
Now you're ready to run the full CE site verification test suite. All elements of this test should now pass.

Note: To run the site verify script you should not be <b>root</b> .
<pre>
&gt;	cd $VDT_LOCATION
&gt;	source ./setup.sh
&gt;	grid-proxy-init 
	  <i>....enter your passphrase</i>
&gt;	cd verify
&gt;	./site_verify.pl 
</pre>
The results will indicate the various tests that are performed with results indicating  FAILED, UNTESTED,  NOT WORKING,  NONE or  NO. conditions.

---++ Authorizing Users: Operational Configuration
The earlier test case only authorized yourself as a local user.  You should now go to 
<a href="http://osg.ivdgl.org/twiki/bin/view/Integration/OsgCEAuthorization">
Osg CE Authorization</a> document and authorize other users before performing the
<a href="#OSG Registration">OSG Registration</a> of your service (otherwise no one but you will be able to access it!).

---++ OSG Registration

To register the site with the OSG Grid Operations Center and into the Grid Catalog please use the [[http://www.opensciencegrid.org/index.php?option=com_wrapper&Itemid=68&elMenu=Grid%20Support][webform]] located under the OSG [[http://www.opensciencegrid.org/index.php?option=com_content&task=view&id=47&Itemid=65][Administrator Support page]]. If you are registering into the OSG, be sure to check the appropriate box for which grid catalog you are registering with. You should receive an email response automatically back from the GOC to the operations contact you supplied. If this response doesn't arrive within a reasonable delay, please resubmit your registration.

A minimal amount of information is needed for the OSG Grid Operations Center (GOC) to publish a site to the monitoring and operational infrastructure. This includes organization name, organization manager or designated representative name and email, security contact name and email, resource URL, support center, and form submitter. 

While this minimal information will allow the GOC to publish your information to the monitoring tools, more information is requested to make site and support center communication easier. Please take time to fill out the form completely.

---++ Firewalls

Grid components are distributed throughout the network, and services such as gatekeepers and data movement utilties are required to be accessible to the dynamic cloud of clients and peer services.
This distributed and dynamic requirement places the burden of the security on the implementation of the application.

Due to the discovery of significant vulnerabilities in recent years, network-based applications are untrusted by default. To solve the application problem effort has focused on developing and deploying firewalls which
restricts full and free network access.  (You might say that this is analogous to building  a house with no doors. Is it safe? Yes. Is it useful? No.)

Some essential network-based applications have been "hardened," such as mail relay services, web servers, and secure shell daemons. These are further protected further by IP address  filtering to prevent access from unknown hosts or domains.

Grid components which are located behind network firewall face special challenges for Grid setup and operations. 

There are two styles of firewalls usually encountered. 
	* A network firewall which is upstream from your server  (usually centrally maintained). This blocks all traffic to your host. 
	* Host-based firewalls which are setup and maintained  by individual host administrators. This is usually setup and configured by the <b>iptables</b> program which filters incoming network packets which arrive for the host.

 In addition to host-based firewalls, hosts can choose to  implement host based access rules (usually setup with the <b>tcp_wrapper</b>	or <b>hosts_allow</b> utilties.

Network traffic can be blocked at the firewall for both incoming and outgoing dataflow depending on hostnames, ip addresses, ports and protocols. 

A common setup is to allow any outgoing connection, while significantly (if not completely) restricting 
incoming connections.  The Globus project provides thorough documentation, including but not limited to
the 
<a href="http://www.globus.org/toolkit/security/firewalls/">Globus Toolkit firewall requirements</a> document.
Also remember that you may need to open ports on your firewall for your batch system (Condor, LSF, etc.!)

Port usage which may require firewall updates:
	* Inbound:
		* MDS: 2135/tcp inbound
		* GRAM: 2119/tcp inbound
		* [[http://www.globus.org/grid_software/data/gridftp.php][GridFTP]]: 2811/tcp inbound
		* GRAM callback: user-defined contiguous range (set via the environment variable =GLOBUS_TCP_PORT_RANGE=beginport,endport=).  It should span at least 100 ports for a small site. 
		* Monalisa: 9000/udp (for ABping measurements).  These are specified in the file =$VDT_LOCATION/MonaLisa/Service/VDTFarm/ml.properties=
	* Outbound:
		* GRAM callback: user-defined contiguous range (set via the environment variable =GLOBUS_TCP_SOURCE_RANGE=beginport,endport=). 
	
GRAM and [[http://www.globus.org/grid_software/data/gridftp.php][GridFTP]] need to know the port range that you've opened up via environment variables.  You need to set the =GLOBUS_TCP_PORT_RANGE=beginport,endport= for inbound
ports.  If you restrict outbound connections, you will also need to set =GLOBUS_TCP_SOURCE_RANGE=beginport,endport=.  These may be set either in $VDT_LOCATION/vdt/etc/vdt-local-setup.sh=, or in the xinetd configuration files -- the examples below
use xinetd.  The variables will be used by GRAM, 
<a href="http://www.globus.org/grid_software/data/gridftp.php">GridFTP</a>, and any clients that require them.

These ports and protocols <i>must be open</i> to all grid clients and server machines participating in the grid in order to provide minimal functionality.

You also may need to open the following optional ports for additional Grid services:
	* GIIS: 2136/tcp
	* GSISSH: 22/tcp
	* <a href="http://grid.ncsa.uiuc.edu/myproxy/">MyProxy</a>: 7512/tcp
	* VOMS: 8443/tcp
	* RLS server: 39281/tcp

Here is a sample of the a <tt>/etc/hosts.allow</tt> with the GLOBUS services opened:
<pre>
&gt;  cat /etc/hosts.allow
<i>
  #
  # hosts.allow	This file describes the names of the hosts which are
  #					allowed to use the local INET services, as decided
  #					by the '/usr/sbin/tcpd' server.
  #
  sshd: 129.79.6.113 
  ALL : localhost
  vdt-run-gsiftp.sh : ALL
  vdt-run-globus-gatekeeper.sh : ALL</i>
</pre>

*For RH9, RHEL3 or compatible iptables systems*

The default firewall configuration for Red Hat's iptables sets the system up with a stateful packet filter. This is different than some legacy RH7 systems as by default no ports that are not explicitly opened by the iptables script will be open. This includes high numbered ports that are often used by grid services. 

If your preference is to leave as much of the stateful packet filtering in place but enable just those grid services you want to deploy then you can use the following instructions. 

Two changes need to be made to an OSG gateway with a host based iptables stateful firewall. 

First is the configuration of the firewall itself. On RHEL or similar systems this is done in /etc/sysconfig/iptables

The Chain RH-Firewall-1-INPUT is a default chain for RHEL3. This chain is also sometimes called INPUT. Make sure the following rules use the chain that other rules in /etc/sysconfig/iptables do. 

Note: For GSISSH this port is often already open for systems. You can use either this rule or the default rule setup at install time if you selected custom firewall and enabled ssh. 
  <verbatim>
# Globus: Requires addition configuration in /etc/xinetd.d/globus-gatekeeper
# set: env = GLOBUS_TCP_PORT_RANGE=40000,50000
# This allows up to 10,000 ports and matches the globus config.
# How globus is configured to use these ports is subject to change in an upcoming
# release
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 40000:50000 -j ACCEPT
# Monalisa, grabs 3 ports from the following range
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 9000:9010 -j ACCEPT
# Gridftp
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2811 -j ACCEPT
# MDS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2135 -j ACCEPT
# GRAM
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2119 -j ACCEPT

# Optional Services
# RLS Server
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 39281 -j ACCEPT
# MyProxy
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 7512 -j ACCEPT
# GSISSH/SSH
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 22 -j ACCEPT
# GIIS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 2136 -j ACCEPT
# GUMS/VOMS
-A RH-Firewall-1-INPUT  -m state --state NEW -p tcp -m tcp --dport 8443 -j ACCEPT
  </verbatim>

Second, we configure Globus to use the allowed inbound port range.

/etc/xinetd.d/globus-gatekeeper
<verbatim>
  service globus-gatekeeper
  {
			socket_type = stream
			protocol = tcp
			wait = no
			user = root
			instances = UNLIMITED
			cps = 400 10
			server = $VDT_LOCATION/vdt/sbin/vdt-run-globus-gatekeeper.sh
			env	 = GLOBUS_TCP_PORT_RANGE=40000,50000
			disable = no
  }
</verbatim>

<b>If</b> you restrict outbound connections (to the same port range, for example), you should also modify the gsiftp config file.

/etc/xinetd.d/globus-gsiftp
<verbatim>
service gsiftp
{
			socket_type = stream
			protocol = tcp
			wait = no
			user = root
			instances = UNLIMITED
			cps = 400 10
			server = $VDT_LOCATION/vdt/sbin/vdt-run-gsiftp.sh
			env += GLOBUS_TCP_SOURCE_RANGE=40000,50000
			disable = no
}
</verbatim>

Finally, add the port range(s) to
<tt>$VDT_LOCATION/globus/etc/globus-job-manager.conf</tt> to ensure that it is picked up by other services by
adding the following lines (omit the globus-tcp-source-range line if you do not restrict outbound connections):
<verbatim>
		  -globus-tcp-port-range 40000,50000
		  -globus-tcp-source-range 40000,50000
</verbatim>

_Note: $VDT_LOCATION should be set by the pacman installer_

After editing the above files run the following commands
  <pre>
  # /etc/rc.d/init.d/iptables restart
  Flushing firewall rules:											  [  OK  ]
  Setting chains to policy ACCEPT: filter						  [  OK  ]
  Unloading iptables modules:										  [  OK  ]
  Applying iptables firewall rules:								  [  OK  ]
  # /etc/rc.d/init.d/xinetd reload
  Reloading configuration:											  [  OK  ] 
  </pre>

---++Troubleshooting Guide
As you install, monitor the $VDT_LOCATION/vdt-install.log.  

	* If pacman tries to retrieve something from a website that's having problems, you'll get an error message that's unrelated to the real problem because pacman can't recognize 404 errors when downloading tarballs.  For example, when the PRIMA download site was down, it told us the file wasn't in the correct format:
<pre>
vdt-untar is untarring prima-0.3.x86_rh_9.tar.gz
gzip: stdin: not in gzip format 
</pre>
---++Shutdown Guide
Please see the <a href="http://osg.ivdgl.org/twiki/bin/view/Integration/OSGShutdownGuide">
OSG Shutdown Guide</a>.


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->

%STOPINCLUDE%



-- Main.RobQ - 06 Dec 2005<br>
-- Main.JohnWeigand - 05 Jan 2006<br>
-- Main.JohnWeigand - 12 Jan 2006<br>


%META:TOPICMOVED{by="DaneSkow" date="1115753408" from="Integration.OSGCoreInstallGuide" to="Integration.OSGCEInstallGuide"}%
