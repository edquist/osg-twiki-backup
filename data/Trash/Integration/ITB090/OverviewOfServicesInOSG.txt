%META:TOPICINFO{author="JamesWeichel" date="1317326906" format="1.1" version="1.39"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Background
The goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters.  Scientists, researchers,  and students, organized as virtual organizations (VOs), are the consumers of the CPU cycles and storage. 

Your site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them. 

As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support.  The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. The end-user sends jobs into your cluster's batch system, your CE  receives them and passes them out to Worker Nodes (WN) for execution. Some VOs and end-users require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE).  A site is not required to provide both a CE and an SE.

---++ Site Policies

OSG expects you to clearly specify your site's policies regarding resource access.  Please write them on a web page, make this page part of your site registration, and make it available via the GOC monitoring [[http://vors.grid.iu.edu/cgi-bin/index.cgi][VORS]] (you will answer a question about this during configuration) and the OSG information management system, OIM.  We encourage you to allow all virtual organizations registered with the OSG at least "opportunistic use" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all.

---++ Compute Element (CE)

The standard installation includes a GRAM and !GridFTP interface; both are currently required on the same CE host for the Condor-G client to successfully stage out (smallish) output files that are defined as part of the job's JDL.  The CE host needs to include a sizeable local disk (~100GB or more) as scientists tend to specify a few (smallish) input and output files as part of their JDL. Those files, in addition to stdout and stderr of all jobs submitted via the CE, are spooled via the CE out to your cluster. 

You must determine your security policy with regard to Unix ID management on the cluster.  You may choose group accounts and/or dynamic accounts for all users.

You must choose the OS (most if not all OSG clusters are some Red Hat Enterprise Linux derivative), the batch system (Condor, PBS, LSF, and SGE are presently supported), and the network architecture (default assumption is public/private with NAT - you will need to advertise your architecture by changing some settings by hand if yours isn't like this) of your cluster. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster (NFS-lite). 

The CE hosts information provider(s) and monitoring services, most of which are configured correctly by default.  We require all OSG sites to deploy Gratia, the OSG accounting system. Your site thus sends accounting records to OSG about jobs run on your site and data transfers involving your site.  Aggregated summaries of this information can be viewed via the 
[[http://gratia-osg.fnal.gov:8880/gratia-reporting/][Gratia displays]].

There are other services the run off the CE that are required:
   * CEMon which provides the basis for the information services in OSG. The Grid Information Providers much be run and configured to correctly publish information about the resource to the OSG information service.
   * RSV the Resource Service Validation system which schedules execution of local "probes" of your CE (and SE), and reports the results up to the GOC.  This is important for service availablity monitoring of OSG sites.
   * The package of tools available to the OSG job environment: the OSG worker-node client package.  Many site admins install this package on a single server and NFS export it to compute nodes.  

---++ Storage Element (SE)
There are two types of storage element services provided by OSG which implement SRM v2.  See pointers to instructions for these services:
   * [[ReleaseDocumentation.DCacheInstall][dCache]]  - generally useful for aggregating disks from lots of servers
   * [[ReleaseDocumentation.BeStMan]] - sits in front of any POSIX filesystem.  There is also a version which supports xrootd filesystems. 

An SE must run correctly configured Grid Information Providers, Gratia accounting and RSV probes.
  
These services are supported by the OSG Storage group.  Please email osg-storage@opensciencegrid.org for installation and support questions for these services.


<!-- 

%INCLUDE{ "Trash.IntegrationStorageElementAdmins" section="SEintro" }%

-->


<!-- The SE is currently much less well defined than a  CE. Scientists expect three types of areas: 

   * an area into which they can install application software releases via the CE; this must be read-only accessible from all batch slots on your cluster. 
   * an area to which they can stage data using gftp; this must be readable from all batch slots. 
   * an area to which they can stage output files from all batch slots, for later asynchronous retrieval using gftp. 


In addition, the following must be available at each batch slot: 

   * a set of "client tools" that are part of the OSG software stack. These can be either exported from the CE host, from a separate host, or be deployed locally on each compute node. 
   * an area that is strictly local to each batch slot, from within which they run their job(s) during the time they have "leased" the batch slot. You or your batch system is expected to clean this area after the scientists release the batch slot to you. 
-->

---++ Optional site services
There are additionally a number of optional site services you may wish to install, including:
   * GUMS - the Grid User Management Service, for managing Grid accounts on your cluster
   * Squid - a web caching service
   * Syslog-ng - a logging service that can be used to forward CE logfiles to the GOC for troubleshooting purposes.
   * Monalisa - a monitoring program

---++ VO management services
There are two services that are used by VO administrators for management of VO information:
   * VOMS the VO management service
   * VOMRS the administrative interface to VOMS

Usually these are installed and operated by a designated central group affiliated with a given VO.

---++ OSG user software
The OSG user software consists of
   * OSG-Client (which is basically VDT-Client) and include tools like Condor-G for submission and management of jobs on the OSG.
   * Worker-node Client.  This package contains things like Globus client tools, wget, Myproxy and LCG-Utils.   It needs to be available to every compute node, either installed on local disk or exported from a shared location.


%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.RobGardner %BR%
%REVIEW% Main.AlainRoy -  12 Jun 2008 %BR%
%REVCOM%  %BR%
%REVFLAG%  %Y%  %BR%

%META:TOPICMOVED{by="RobGardner" date="1192827019" from="Integration/ITB_0_7.OverviewServicesOSG" to="Integration/ITB_0_7.OverviewOfServicesInOSG"}%
