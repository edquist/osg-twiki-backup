%META:TOPICINFO{author="JeffPorter" date="1176227229" format="1.1" reprev="1.8" version="1.8"}%
%META:TOPICPARENT{name="MinutesWSGramMar19"}%
%LINKCSS%

<!-- This is the default OSG Integration template. Please modify it in the sections indicated to create your topic! --> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!! %SPACEOUT{ "GT4 testing with Condor-G" }%
%TOC%

%STARTINCLUDE%
%EDITTHIS%

---++ Comparison of GT4 and GT2 using Condor-g 

Shown below are the submit files for running direct gt4/gt2 comparisons. These examples are for =queue 25=.  This current tests were carried out for =queue = 10,25,50,75,100,150,200,400, and 600=.

*Condor submit example file for GT4 tests:*

<pre class="screen"> <b>
 Universe=grid
 Grid_Type = gt4
 Jobmanager_Type = Condor  
 GlobusScheduler=https://osg-itb.ligo.caltech.edu:9443
 executable=/usr2/shared/app/star_app/uptime.pl
 transfer_executable=False
 stream_output = False
 stream_error  = False
 output = test25/test.out.$(Process)
 error  = test25/test.err.$(Process)
 log    = test25/test.log
 notification=Never
 queue 25
</b></pre>

*Condor submit file example for GT2 tests:*
 
<pre class="screen"> <b> 
 Universe=grid
 grid_resource = gt2 osg-itb.ligo.caltech.edu/jobmanager-condor
 executable=/usr2/shared/app/star_app/uptime.pl
 transfer_executable=False
 stream_output = False
 stream_error  = False
 output = gt2_test25/test.out.$(Process)
 error  = gt2_test25/test.err.$(Process)
 log    = gt2_test25/test.log
 notification=Never
 queue 25
</b></pre>
 
---++ Event timing from Condor-g log file

The logfiles produced from running multiple jobs with Condor-g contain 4 separate events types with each event for each job marked by a timestamp. I will use these timestamps to break up the jobs' life cycle into these three time-periods (t1-t0, t2-t1, and t3-t2) for performance comparisons between gt4 and gt2.  An example log file is attached to this page and the event-types are listed here.

| *event kind* |  *Log-file timestamped entry*  |  *my interpretation*   |  
|  0  |  Job submitted from host: <ip-address:port>  |  submission to client submit-host  | 
|  1  |  Job submitted to Globus  |  submission to remote site  |  
|  2  |  Job executing on host:   |  submission successful to worker node  | 
|  3  |  Job terminated  |  job is fully completed and reported back to client submit-host  | 



---++ Timing Measurements

The following two plots show the timing of jobs submitted to globus for =queue 600= and for GT4 and GT2 separately. The x-axis is t1-t0 in seconds.  Here we can see a similar structure in the gt4 and gt2 results where the 600 jobs are submitted over a few thousand seconds and show a sub-structure that indicates a broad clumping by 100 jobs.  However, the gt4 result is more than twice as extended in time as the gt2 test. 


|    <img src="%ATTACHURLPATH%/zero_2_sub_600.gif" alt="zero_2_sub_600.gif" width=305' height='205' />  |       <img src="%ATTACHURLPATH%/zero_2_sub_600_gt2.gif" alt="zero_2_sub_600_gt2.gif" width='305' height='205' />
   | 

In the following two plots, both gt4 and gt2 results are overlayed. The left plot is mean-time between remote-submission and execution plotted against the =queue= sample. The right plot has the mean-time between execution and finish.  In both plots the gt4 result begin comparable to the gt2 results at low =queue= number but rise quickly with increasing number reaching plateaus 2-4 times that of the gt2 result.  The gt2 results are more stable over the entire range. 

I suspect that the increased times from gt4 over these 2 periods are the cause of the extended gt4 period in the first plots (t1-t0 above). That is, I saw little or no difference in the load on the client submit-host between the gt4 and gt2 tests with minute-averaged loads ranging from 1.0 to 2.5. Thus, the longer remote submission times was likely due to the longer completion times experienced during gt4 submission and execution.


|       <img src="%ATTACHURLPATH%/sub_2_exec_time.gif" alt="sub_2_exec_time.gif" width='305' height='205' />
  |       <img src="%ATTACHURLPATH%/exec_2_finis_time.gif" alt="exec_2_finis_time.gif" width='305' height='205' />
  |

---++ Next steps


It seems pretty clear that the plateau region in the above to plots is due to Condor-g's handling of the job submission. The sub-structure in the first plots (6 bumps for queue=600) and the starting of the plateau at =queue = 100= are related and surely due to the submit-host configuration (though just now I don't see which parameters are involved).  The next steps would be:

   * modify Condor-g's submission handling to allow more jobs sent to the remote site  
   * verify that the plateau region turns on at higher =queue= number
   * measure slope to plateau (can do this now) and use slope measurement as a metric for tuning of or feedback to WS-GRAM

These tests are fairly automated now so can be done with relative ease.  The biggest challenges are the time it takes to run the tests (~2 hrs for =queue ~ 600=) and ensuring the load on the test site is not being thrashed by other testing. 

---++ Updates

After removing the default attenuation in Condor-g by setting the GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE configuration parameter to 1000, I began to re-do the tests.  I started with =queue 250= and was able to kill the ws-gram service on the ligo itb site.  I've attached the log and output files here as 'killer250.tgz'.

I reran the scan test for both gt2 and gt4. The result of the t2-t1 and t3-t2 vs =queue= number are shown here. These results compare well with the previous test.  The gt4 test killed the ws gram service on the ligo-itb site at =queue 150=.
 

|       <img src="%ATTACHURLPATH%/sub_2_exec_unlimited.gif" alt="sub_2_exec_unlimited.gif" width='305' height='205' />
  |       <img src="%ATTACHURLPATH%/exec_2_finis_unlimited.gif" alt="exec_2_finis_unlimited.gif" width='305' height='205' />
  |


---++ Additional tests

I was asked to try two items in order to help investigate this failure:

   * Check whether a known threading-fix was in my version of Condor.
      * Martin sent me an axis.jar file which I compared with that on my machine: no difference
   * Check the responsiveness of the service via 'globusrun-ws' during the scaling tests
      * I found the 'globusrun-ws' would appear hang though eventually report back (as long as I didn't kill the remote server) 
      * my ping looped over uptime and 'ps -elf | wc -l'.  I saw load rise steadily during the during the process, peaking at 10-15. The #-of-processes remained fairly constant.

---++ Scaling tests with data movement

 The next set of tests include moving data into and out of the compute site using tools native to Condor-g for both gt2 and gt4.  Initial tests were done with jobs that took one ~2MB input file and produced one ~200KB output file.  The submit files were the same as above but with the additional lines:

<pre class="screen"> <b> 

should_transfer_files = YES
when_to_transfer_output = ON_EXIT
transfer_input_files = inputFiles/rawdata_$(Process).dat 
transfer_output_files = reduceddata_$(Process).dat
Arguments = -p $(Process)

</pre></b>


 The results were similar to those without data movement but not exactly the same.  A couple of observations:

   * It looks like the submission times in gt2 are flat until =queue 60= at which point perhaps the number of jobs start being delayed by the batch system while the gt4 rates appear to be linearly dependent on =queue=.  
   * the exec-to-finish times are better in this scenario for the gt4 service than in the previous tests with no data flow - though a depence on =queue= is still observed.
   * I believe I killed the system again with gt4 for =queue 120=


|       <img src="%ATTACHURLPATH%/sub_2_exec_datamv.gif" alt="sub_2_exec_datamv.gif" width='305' height='205' />
  |       <img src="%ATTACHURLPATH%/exec_2_finish_datamv.gif" alt="exec_2_finish_datamv.gif" width='305' height='205' />
  |



---++ Scaling tests on UC's T2DEV-OSG

These tests included input data (~2MB/job) but no output data and the results seem much more reasonable than on the ligo-itb site. Specifically, there is not that dependence on =queue= seen on the ligo site. However, the gt4 service is still killed with =queue 120=

|       <img src="%ATTACHURLPATH%/sub-2-exec-t2dev.gif" alt="sub-2-exec-t2dev.gif" width='305' height='205' />
  |       <img src="%ATTACHURLPATH%/exec-2-finish-t2dev.gif" alt="exec-2-finish-t2dev.gif" width='305' height='205' />
  |




%STOPINCLUDE%

%BOTTOMMATTER%

-- Main.JeffPorter - 19 Mar 2007

%META:FILEATTACHMENT{name="exec_2_finis_unlimited.gif" attr="" autoattached="1" comment="execution-to-finsh-timing-unlimited" date="1174873984" path="exec_2_finis_unlimited.gif" size="6852" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="condor_config" attr="" autoattached="1" comment="condor config of submit host" date="1174342496" path="condor_config" size="79707" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="zero_2_sub_600_gt2.gif" attr="" autoattached="1" comment="gt2-queue600-sub-timing" date="1174339503" path="zero_2_sub_600_gt2.gif" size="8847" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="sub_2_exec_unlimited.gif" attr="" autoattached="1" comment="submission-to-execution-timing-unlimited" date="1174874026" path="sub_2_exec_unlimited.gif" size="6595" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="sub_2_exec_time.gif" attr="" autoattached="1" comment="submission-to-execution-timing" date="1174340112" path="sub_2_exec_time.gif" size="5242" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="sub_2_exec_datamv.gif" attr="" autoattached="1" comment="submission to exec with data movement" date="1175798984" path="sub_2_exec_datamv.gif" size="7682" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="exec_2_finis_time.gif" attr="" autoattached="1" comment="execution-to-finish-timing" date="1174340142" path="exec_2_finis_time.gif" size="5739" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="gt4_queue25.log.log" attr="" autoattached="1" comment="gt4-queue25-log file" date="1174341261" path="gt4_queue25.log.log" size="28850" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="killer250.tgz" attr="" autoattached="1" comment="log+out+err of condor-g job that killed ws-gram" date="1174503255" path="killer250.tgz" size="15389" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="sub-2-exec-t2dev.gif" attr="" autoattached="1" comment="submit 2 exec on t2dev with input data" date="1176226765" path="sub-2-exec-t2dev.gif" size="8629" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="exec_2_finish_datamv.gif" attr="" autoattached="1" comment="exec-2-finish with data movement" date="1175798956" path="exec_2_finish_datamv.gif" size="7454" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="exec-2-finish-t2dev.gif" attr="" autoattached="1" comment="exec to finish on t2dev with input data" date="1176226799" path="exec-2-finish-t2dev.gif" size="7156" user="Main.JeffPorter" version="1"}%
%META:FILEATTACHMENT{name="zero_2_sub_600.gif" attr="" autoattached="1" comment="gt4-queue600-sub-timing" date="1174339447" path="zero_2_sub_600.gif" size="8257" user="Main.JeffPorter" version="1"}%
