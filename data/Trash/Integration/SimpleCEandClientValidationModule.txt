%META:TOPICINFO{author="JeffPorter" date="1226477123" format="1.1" version="1.6"}%
%META:TOPICPARENT{name="ValidationTestbed"}%
---+!! %SPACEOUT{ "%TOPIC%" }%


%TOC%

---++ Requirements

This page includes a series of tests to check the basic functionality of a CE from an OSG Client. It assumes you have:

   * access to an OSG Client summit host.  For many of the test this should include running Condor for Condor-G submissions.
   * a target CE with known/knowable characteristics
      * hostname
      * jobmanager types
   * access a personal grid certificate in order to retrieve a valid grid/voms proxy with privilege to run on the target CE
      * see [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Included_topic_CESimple_Test][ simple tests]] for obtaining a grid or voms proxy (for either running tests from the CE or from the OSG client below) OR
      * ==source $VDT_LOCATION/setup.sh; grid-proxy-init==  (assuming your credentials are in =$HOME/.globus=)
      * ==source $VDT_LOCATION/setup.sh; voms-proxy-init [-voms <i>your-vo</i>]== (assuming your credentials are in =$HOME/.globus=)

---++ Basic CE Testing From the CE

Several component tests of the CE installation are documented in the [[ReleaseDocumentation/ValidatingComputeElement][Validation]] section of the Release documentation.  Since we're not requiring that the site resources be registered, we will stick to testing functionality of the services not the information repositories. 

   * it's a good idea to check your host and service certs, particularly to keep up with the expiration date: 
      * ==openssl x509 -in /etc/grid-security/hostcert.pem -noout -subject -dates==
      * same for =/etc/grid-security/containercert.pem= and =/etc/grid-security/http/httpcert.pem= and any others as needed 
   * [[ReleaseDocumentation/ValidatingComputeElement#Included_topic_Validate_Site_Ver][Validate with Site Verify]] : a script that runs a series of validation tests on the CE
   * [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Included_topic_Validate_GUMS][Validate Gums Client]] : Your CE is a GUMS client, these steps confirm it is configured as such
   * [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Quick_server_side_checks][ WS Gram Server tests]] : A quick check to see if the server is running and configured
   * [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Included_topic_Validate_RSV_Prob][ Validate RSV Probes]] goes into some detail on how to test individual probes. But also:
      * run ==condor_cron_q== to see that probes are running/scheduled
      * go to https://yource_hostname:8443/rsv & review the status of the probes (they may take a while before showing up).  
         * Drill down into web page to diagnose problems
         * look at probe out/err/log files in =$VDT_LOCATION/osg-rsv/logs/probes/=
   * [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Included_topic_Validate_Vo_Acces][ Validate VO Access]] : review what is needed for supported VOs to have access.  Similar steps as checking that your CE is a GUMS client above. 


---++ Basic CE Testing from OSG Client

In this sub-section, we'll run a series of tests of the CE from the OSG Client. The tests will include both jobs sent to both the fork and LRM via GT2 and GT4 interfaces and test data transfers.  One initial tests of the GT2 jobmanager checks the user environment on the CE for running fork jobs.

   * ==globus-job-run <i>yource_hostname</i>/jobmanager-fork /usr/bin/env==

From that output you can review the environment variable settings.  It's worthwhile to review and become familiar with these environment variables and you can use this information, specifically the variable =$OSG_LOCATION=, to copy over the site_verify.pl script from your CE while testing it's gsiftp functionality. Specifically:

   * ==globus-url-copy gsiftp://<i>yource_hostname/yource_osg_location</i>/verify/site_verify.pl== ==file:///tmp/site_verify.pl==

And then run the =site_verify.pl= script against the CE, but now remotely from the OSG Client:

   * ==chmod +x /tmp/site_verify.pl; /tmp/site_verify.pl --host=<i>yource_hostname</i>==

Additional basic tests should include submitting the job to the LRM, which in this example will show what environment variables are set on the worker nodes. 

   * ==globus-job-run <i>yource_hostname</i>/jobmanager-xxx /usr/bin/env==  (where xxx = condor or pbs or sge)

And finally you should run a similar set of tests against the GT4 service: (see [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Client_side_test_of_WS_GRAM_serv][Client side WS GRAM testing]] for more detailed tests including examples with files staging via rsl).

   * ==globusrun-ws -submit -F <i>yource_hostname</i>:9443 -Ft Fork -s -c /usr/bin/env==  (where the "-s" option streams stdout back - not recommended for general use)
   * ==globusrun-ws -submit -F <i>yource_hostname</i>:9443 -Ft XXX -s -c /usr/bin/env== (where XXX = Condor or PBS or SGE )

---++ Testing the CE from the OSG Client with Condor-G

The first step here is to verify that Condor-G on the client is working.  A simple test job is shown at our [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ValidatingComputeElement#Condor_G_submission][ testing client condor g submissions]] page.  The example uses the GT4 (WS GRAM) service. The only difference when submitting to GT2 is in the contact string for the =grid_resource=.

   * ==grid_resource gt2 <i>yource_hostname</i>/jobmanager-xxx== where xxx=fork|condor|pbs|sge for pre-WS GRAM
   * ==grid_resource gt4 <i>yource_hostname</i>:9443 XXX==  where XXX=Fork|Condor|PBS|SGE for WS GRAM

The linked example shows the submit file, how to submit a job, how to monitor the job's status, and checking the finished job where the job is a simple =/bin/hostname= example. Next we'd like to run a series of tests to check the environment of for the user.  The example here was developed from what the Engage group has found useful in determining site availability.

One set of tests is run directly on the CE via the fork job manager. Tests include =$HOME= existence and size, =$OSG_APP, $OSG_DATA,$OSG_WN_TMP= definitions and availability.  A second set of jobs are submitted on the LRM for running on the worker nodes.  Here the worker node definitions and availability of =$OSG_APP, $OSG_DATA,$OSG_WN_TMP= are tested.  The tests also include the existence of wn-client software (=$OSG_GRID=) and information about the work nodes (cpu,memory,bitness).  Example of these will be given here shortly.




-- Main.JeffPorter - 05 Nov 2008