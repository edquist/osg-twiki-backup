%META:TOPICINFO{author="JohnWeigand" date="1173728305" format="1.1" reprev="1.4" version="1.4"}%
%META:TOPICPARENT{name="MeetingMinutes"}%
---+!! %SPACEOUT{ "%TOPIC%" }%
%TOC%

---++ Introduction
   * Attending: Alain Roy, Stu Martin, Jeff Porter, John Weigand, Suchandra Thapa, Rob Gardner, Charles Bacon
   * Coordinates: Monday Feb 26, 2007; 11am Central; 510-665-5437, #2222
   * Last meeting: MinutesWSGramFeb26


---++ WS Gram logfile issue (Alain)
   * A single logfile is being used for all jobs - not sure if it can be rotated or not, and grows without bounds.   This is for =globus-condor.log=.  This was fixed for pre-WS gram.
   * Question about keeping track of last recorded event - does logfile rotation prohibit this?  Rotate and truncate?
   * Stu will look into this, does not appear to be a serious deep problem.

---++ WS Gram and Managed Fork (Stu, Alain)
   * Can we make managed fork run under WS Gram?  Create another SEG - singular event generator, which manages all jobs by looking at logfile - for this purpose? 
   * Can we make a small update to an OSG 0.6 install to effectively make an managed fork for Gram4 - sounds like yes.
   * Lets follow-up next week.

---++ WS Gram installation (Rob)
   * Included these as a reference for how OSG installs WS Gram.  Will need to make edits to these depending on recommeneded default changes to configuration and setup.  John Weigand has provided this documentation.
   * As installed in OSG, https://twiki.grid.iu.edu/twiki/bin/view/ReleaseDocumentation/InstallAndConfigureGlobusWebServices
   * From Alain/VDT: [[http://vdt.cs.wisc.edu/releases/1.6.1/release.html][VDT 1.6.1 release notes]], [[http://vdt.cs.wisc.edu/releases/1.6.1/][VDT 1.6.1 documentation]], [[http://vdt.cs.wisc.edu/releases/1.6.1/submitting_wsgram_jobs.html][Submitting jobs to Web Services GRAM]]
 
---++ WS Gram scalability tests (Suchandra)
   * See: https://twiki.grid.iu.edu/twiki/bin/view/Main/WSGramTesting 
   * Suchandra is working right now on a time-skew issue on the UC dev cluster.
   * Container was using 512 MB - will update to 1024 MB.
   * Also will change directory that Globus using for writing persistent information to disk local to the service (avoid NFS).

---++ WS Gram configuration (Stu)
   * Use local disk for the Globus account.  John W notes we run WS gram processes from a daemon.  In that case, its the account that runs the daemon.  
   * More discussion: there is an environment variable that is set by VDT for this purpose.  Suchandra and John will discuss with Alain adding note on how to set this to a local disk.
   * Number of container threads - by default, something like 10.  Need to increase this.  Is this being exacerbated by frequent GUMS contacts? Probably, but lets do some tests increasing it first.

---+++ Note from Stu 
Suchandra et all,

Here is my recommendation for next step:

Configuration changes:
   1 change globus account's home dir (or at least ~/.globus) to be on local disk
   2 set 1024 MB memory for the container
   3 configure to use 30 container threads
      - Martin how was this configured in previous testing?  Like this?
         containerThreads 2
         containerThreadsMax 15

*test1*: Then run the globusrun-ws 1 second job submission test. If problems occur where the container no longer processes jobs, then do a kill -QUIT <jvm process> and make that available along with the container's log file.

*test2*: Another separate test to try would be to remove the prima callout completely (only gridmap auth) and rerun.  This would show if that is causing problem that we did not see.  But let's wait on the results from test1/. 

We did not run a test like this with globusrun-ws job every second test, but we did do condor-g tests where 2000 jobs were submitted in about 50 min.  So that is a little faster than your rate.

Are the globusrun-ws client commands being executed on the same host as the GT container is running?  If so, I'd think that would not be a recommended use case.

-Stu

---+++ Martin:
Yes, something like this, maybe containerThreadsMax 20 I modified the default value of the containerThreads-parameter in my tests only once and it didn't make a difference for me. But what made a difference was the value for runQueueThreadCount in $GLOBUS_LOCATION/etc/gram-service/jndi-config.xml. In jobs with file staging and/or file cleanup 30 worked better than 18 on the 4 x Dual Core Processor machine from OSG. For simple jobs that value didn't make a difference. But i guess there's no general recommendation for that value, depends on the number of processors or cores.



---++ WS Gram Condor-G testing (Jeff)
   * Objectives from last meeting: MinutesWSGramFeb26#Jeff_s_update
   * Has client setup - running with GT2 and GT4.  And querying load separately.
   * Looking into config options of Condor-G. Can throttle number jobs to submit to a remote service, frequency for polling job status, etc. 
   * Stu can dig up some default options, will send to Jeff.


---++ Next steps
   * Meet again  next week, same time.



-- Main.RobGardner - 08 Mar 2007

%META:FILEATTACHMENT{name="Recommendations.pdf" attachment="Recommendations.pdf" attr="" comment="GT4+CondorG recommendations" date="1173711324" path="Recommendations.pdf" size="61592" stream="Recommendations.pdf" user="Main.RobGardner" version="1"}%
