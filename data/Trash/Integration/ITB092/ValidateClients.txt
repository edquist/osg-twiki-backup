%META:TOPICINFO{author="JeffPorter" date="1195158633" format="1.1" reprev="1.9" version="1.9"}%
%META:TOPICPARENT{name="ValidatingComputeElement"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%
%BR%
---+ _%INCLUDEHEADING%  %SPACEOUT{ "%TOPIC%" }%_
%EDITTHIS%
%BR%

---++ OSG Client

The OSG Client package provides a set of tools useful for user-access to Grid services.  Below are a series of test you can do after installing the client software via the ClientInstallationGuide and sourcing the =setup.(c)sh= script to test the operability of the software.

---+++ Grid Proxy: *grid-proxy-init* and *voms-proxy-init*

In the following examples it is assumed that your =usercert.pem= and =userkey.pem= files are in =$HOME/.globus/=. If not, you will need to specify them explicitly via "-cert" and "-key" arguments to the xxxx-proxy-init commands.

Obtain a grid-proxy via =grid-proxy-init= 

<pre class=screen>
$ grid-proxy-init
<b>
Your identity: /DC=org/DC=doegrids/OU=People/CN=XXX XXXXX #####
Enter GRID pass phrase for this identity:
Creating proxy ....................................... Done
Your proxy is valid until: Wed Aug 22 04:57:38 2007
</b>
</pre>

Obtain a voms-proxy via =voms-proxy-init=:

<pre class=screen>
$ voms-proxy-init -voms osg
<b>
Cannot find file or dir: /home/condor/execute/dir_14135/userdir/glite/etc/vomses
Enter GRID pass phrase:
Your identity: /DC=org/DC=doegrids/OU=People/CN=XXX XXXXX #####
Cannot find file or dir: /home/condor/execute/dir_14135/userdir/glite/etc/vomses
Creating temporary proxy ................................ Done
Contacting  voms.opensciencegrid.org:15027 [/DC=org/DC=doegrids/OU=Services/CN=host/voms.opensciencegrid.org] "osg" Done
Creating proxy .................................... Done
Your proxy is valid until Tue Oct 30 23:32:54 2007
</b>
</pre>

Note: the apparent error message =Cannot find file or dir: /home/condor/.....= is not an error but an artifact of the software build used in the VDT.  You should ignore that message.

---+++ Simple Client Jobs

Some of the examples here are also documented in CESimpleTest with a emphasis on observiny the CE response via the log files.  Here we simply note the response you should observe from the client.  It is assumed that you have sourced the =$VDT_LOCATION/setup.(c)sh= script , have a valid grid-proxy, and are aware of resources available to you. 

----++++ Submit a remote job: =globus-job-run=

To the default (fork) queue:

<pre class=screen>
$globus-job-run osp1.lbl.gov/jobmanager /bin/hostname

<b>osp1.lbl.gov</b>
</pre>

To the local batch system queue: (specific syntax depends on what batch system is deployed [Condor|PBS|LSF|SGE])

<pre class=screen>
$globus-job-run osp1.lbl.gov/jobmanager-pbs /bin/hostname

<b>osp3.lbl.gov</b>
</pre>

----++++ Copy a file: =globus-url-copy, srmcp, srm-copy=

Copy a file using globus-url-copy from a local system to a remote gsiftp server.

<pre class=screen>
$ls -l /tmp/testdata_10.dat
<b>-rw-r--r--  1 qwerty qwerty 2121000 Nov 15 11:25 /tmp/testdata_10.dat</b>

$globus-url-copy file:///tmp/testdata_10.dat gsiftp://osp4.lbl.gov:2811/tmp/testdata_destination.dat
</pre>

Now copy it back and compare:

<pre class=screen>
$globus-url-copy gsiftp://osp4.lbl.gov:2811/tmp/testdata_destination.dat  file:///tmp/testdata_return.dat
$ls -l /tmp/testdata_return.dat
<b>-rw-r--r--  1 qwerty qwerty 2121000 Nov 15 11:27 /tmp/testdata_return.dat</b>

$diff /tmp/testdata_10.dat /tmp/testdata_return.dat
</pre>

You can repeat the above tests between two remote gsiftp servers using the syntax:
<pre class=screen>
$globus-url-copy  gsiftp://server1:2811/filesystem/filename gsiftp://server2:2811/filesystem2/filename2
</pre>

You can repeat the above tests using =srmcp= instead of =globus-url-copy=. In addition you can access a remote SRM service explicitly. For example:

<pre class=screen>
$srmcp gsiftp://osp4.lbl.gov:2811/tmp/testdata.dat srm://osg-itb.ligo.caltech.edu:8643/pnfs/ligo.caltech.edu/data/star/testdata_dest.dat
</pre>

You may also use the "-debug" flag which will generate diagnostic messages.

----++++ Submit a remote job to WS GRAM: =globusrun-ws=

Basic submission using WS GRAM is done with =globusrun-ws= analgous to =globus-job-run= with GRAM.   A wide range of tests are shown in the  ValidateGramWebServices documentation.  Here is a basic test to verify that the client tools are in order.

<pre class=screen>

$globusrun-ws -submit -F osp1.lbl.gov:9443 -Ft Fork -s -c /bin/hostname
<b>
Delegating user credentials...Done.
Submitting job...Done.
Job ID: uuid:7bfd4324-93b7-11dc-8f33-00304889ddce
Termination time: 11/16/2007 20:15 GMT
Current job state: Active
Current job state: CleanUp-Hold
osp1.lbl.gov
Current job state: CleanUp
Current job state: Done
Destroying job...Done.
Cleaning up any delegated credentials...Done.
</b>
</pre>

---+++ *Condor-G submission*

OSG Client includes a condor package for use as a Condor-G submit host. To test this service 

   * verify service is running:<pre class=screen> $condor_q
 
-- Submitter: qwerty@osp3.lbl.gov : <128.3.30.238:59969> : osp3.lbl.gov
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
 
0 jobs; 0 idle, 0 running, 0 held
</pre>

   * prepare submit file for either GRAM or WS-GRAM <pre class=screen>
#specify resource destination
Universe            = grid
grid_resource       = gt4 https://hostname:9443 [Condor|PBS|SGE]

# specify executable
Executable          = /bin/hostname
Transfer_Executable = false
 
# copy stdout stderr to local files, referenced by job (Process) and submission id (Cluster)
output  =  mytest.out.$(Cluster).$(Process)
error   =    mytest.err.$(Cluster).$(Process)
 
#a single local log file for tracking Condor-g submission
log    = mytest.log
 
# do not send email notification
notification=Never
 
# submit 2 identical jobs: Process=0 and Process=1
Queue 2
</pre>
   * Submit job <pre class=screen>$condor_submit test.submit 
Submitting job(s)..
Logging submit event(s)..
2 job(s) submitted to cluster 51.
</pre>
   * monitor job <pre class=screen>$condor_q
 condor_q
 
 
-- Submitter: porter@osp3.lbl.gov : <128.3.30.238:59969> : osp3.lbl.gov
 ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD               
  51.0   porter         11/15 12:29   0+00:00:00 I  0   9.8  hostname          
  51.1   porter         11/15 12:29   0+00:00:00 I  0   9.8  hostname          
  52.0   porter         11/15 12:29   0+00:00:15 R  0   0.0  gridftp_wrapper.sh
 
3 jobs; 2 idle, 1 running, 0 held
</pre>


Check that job is submitted/runs successfully.

---++ Worker node client
---+++ *srmcp*

Example: 

[neha@fgitb-gk test-submits]$ *which srmcp*

/usr/local/vdt-1.8.0/srm-v1-client/bin/srmcp

[neha@fgitb-gk test-submits]$ *srmcp -debug file:////bin/sh srm://fapl032.fnal.gov:8443/srm/managerv1?SFN=/pnfs/fnal.gov/data/sdss/test17*


%STOPINCLUDE%
%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.JeffPorter - 25 Oct 2007 %BR%
%REVIEW%

%META:TOPICMOVED{by="RobGardner" date="1193339124" from="Integration/ITB_0_7.ValClients" to="Integration/ITB_0_7.ValidateClients"}%
