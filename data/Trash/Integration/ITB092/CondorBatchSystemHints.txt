%META:TOPICINFO{author="StevenTimm" date="1195501985" format="1.1" version="1.5"}%
---+!! *<noop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++Purpose
This Twiki page is for site admins to share the various tricks they have learned in setting up Condor batch systems to work with the Open Science Grid.

---++Hardware configuration
   *  It is strongly recommended that the machine that runs daemons condor_collector and condor_negotiator (your condor master) be different  than the machine which is your Open Science Grid gatekeeper.
   *  It is also strongly recommended that the machine which runs condor_collector and condor_negotiator not be a busy NFS file server.
   *  The Fermi General Purpose Grid cluster has five machines as part of our condor servers:
      1. condor_collector, condor_negotiator, and postgres database
      1. main Globus gatekeeper, condor_schedd plus condor_quill &mdash; handles all inbound grid jobs from OSG
      1. condor_schedd to handle local condor_submits, plus condor_view server &mdash; historically our big NFS server too).
      1. test gatekeeper to test GIP configuration, OSG globus gatekeeper plus condor_schedd
      1. SAMGRID gatekeeper plus condor_schedd
   * We hope to add 3 more:
      1. Dedicated OSG gatekeeper to take jobs from inside of Fermigrid
      1. Dedicated Postgres server to get it off the negotiator/collector machine
      1. Dedicated submit host for outbound grid jobs, to fermigrid and the rest of the OSG
   *  Any production globus-gatekeeper/condor_schedd should have at least 4GB of RAM.  More is better.
   *  Condor collector/negotiator is not so CPU-intensive, but needs a good, fast network connection.   If you don't have a good network connection, or if you have a very sick machine, condor_status will show that there are nodes dropping in and out of the condor pool from time to time.  If so, then definitely split the collector-negotiator to a separate machine and consider setting UPDATE_COLLECTOR_USING_TCP = true.


---++Software configuration

   *  Although the OSG Compute Element install will give you a condor batch system as part of the install, it is better to install it separately. This way you will be able to upgrade the condor independently of the rest of the VDT.  Condor typically has new versions a lot faster than the VDT does.
   *  We have found it useful to use the RPM version of Condor for this reason.  A bit more post-configuration is needed with the RPM though, you have to put in your startup scripts manually.
   *  Earlier versions of condor shipped with open HOSTALLOW_WRITE = *, which meant anyone in the world can join your condor pool.  This is a potential security hole and you need to close it as soon as possible.  There is no reason for OSG functionality to do this.  From condor 6.8.1 this is fixed and you are forced to enter some IP address in HOSTALLOW_WRITE before condor will start.
    *  The OSG Operations group sent out details on how to use stronger authentication than merely IP addresses to configure your condor pool.  This means using either a shared password secret or GSI authentication. Details are at 
[[http://www.cs.wisc.edu/condor/osg_security_recommendations.html]].  Condor as installed by the VDT incorporates the shared pool password mode of authentication as suggested in the document.
   *  It is possible to nfs-mount the condor software so it is seen by all worker nodes.   The default configuration that the VDT will do is to make $VDT_LOCATION/condor/local.nodename
directories for each node, under the main condor installation.  However,
if you are going to do this, it is recommended to change your CONDOR_LOCAL_DIR to be somewhere local on the worker node so you don't have to write your log 
files across NFS, you would only be reading the config files across NFS.
   *  The alternative is to install condor, probably via RPM, on every worker node.  It is a lot more work to do it this way because the configuration files are not shared, but you gain the easy management of RPM, and a condor configuration that will still start up OK even if your nodes come up before the network does.  



%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.StevenTimm - 17 Oct 2007 %BR%
%REVIEW% 


<!-- Main.StevenTimm - 01 Mar 2007 -->
