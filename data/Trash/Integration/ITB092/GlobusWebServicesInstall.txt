%META:TOPICINFO{author="BrianBockelman" date="1486490147" format="1.1" version="1.44"}%
%META:TOPICPARENT{name="Trash.ReleaseDocumentationComputeElementInstall"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

%STARTINCLUDE%
%BR%
---+ _%INCLUDEHEADING%  %SPACEOUT{ "%TOPIC%" }%_
%EDITTHIS%
%BR%

---++ Introduction to WS Gram

WS Gram is the web services resource management component of the Globus toolkit.  The necessary software for WS-Gram using the =grid-mapfile= for authorizing access has already been installed during the OSG installation process.  Much of the configuration has also been completed and is an <i>enabled</i> by default.  There may, however, be configration steps to be performed depending on the authorization mode of the CE: =Local grid-mapfile= , =Compatibility=, or =Full Privilege=.  

---++ Notes on initial installation from VDT

WS GRAM service files and configurations are downloaded during the pacman installation of =OSG:ce= and =OSG:Globus-XXX-Setup= (where XXX is Condor, LSF, PBS, or SGE) and configured with internal calls to =$VDT_LOCATION/vdt/setup/configure_globus_ws=. (Consult the log file, =$VDT_LOCATION/vdt-install.log=, for details.)  

WS GRAM service runs as a non-privileged user, not root.  Therefore, the installation will also:
   * configure the service to run under the ==globus== user, if it exists. Otherwise it will assume the ==daemon== user. Directory and file permissions are set accordingly.
   * clone a cert =containercert.pem/containerkey.pem= from the =hostcert=, install the clone in =/etc/grid-security/=, and set permissions (for ==globus== or ==daemon==) accordingly 
      %NOTE% When you renew your host certificate, be sure to clone it into these =container*.pem= files as well.

In addition, <b>Sudo</b> must be installed and configured on the server.  The required configuration for WS-GRAM is discussed below.  For general information on sudo, see [[http://www.gratisoft.us/sudo/][Sudo Documentation]].

---++ OSG Configuration step

Much of the remaining configuration is completed when you run the OSG configuration script, ==configure-osg.py==.  A configuration switch, <i>wsgram</i>, is available under each of the batch-system specific section (e.g. [PBS], [Condor], ...) in the =config.ini= file. To request WS GRAM configuration set that switch: ==wsgram = True==. 


<!--  Old configure_osg.sh information,  Jeff Porter 6-12-08

<pre class="screen">
Information needed for the WS-GRAM services
WS-GRAM services are being used.

Would you like to use the WS-GRAM service [y] (y/n): 
</pre>

-->

---++ Authorization modes and the sudoer file
WS-GRAM's non-privileged user (<b>globus</b> or <b>daemon</b>) must be able to submit jobs as different users. =Sudo= must be configured to allow this to work (a safer model than running the service as root as is done for <i>pre-ws</i> GRAM).  The configuration requires editing the =/etc/sudoers= file manually.   After running the ==configure-osg.py== script, a file =$VDT_LOCATION/monitoring/sudo-example.txt= is created that contains the text which can be used to modify the =/etc/sudoers= file.  

You __must__ use the same authorization mode for pre-ws and web services.

The modifications are documented in each of the authorization mode topics:
   * [[Trash.ReleaseDocumentationGrid3ModeAuthorization][Local gridmap]]
   * [[CompatibilityModeAuthorization][Compatibility]]  
   * [[Trash.ReleaseDocumentationFullPrivilegeAuthorization][Full Privilege]]  

---++ Running the Service

WS-GRAM runs in a webservices container and, in OSG installs, listens on non-standard port: <b>9443</b>.    The service is run from a startup script, unlike <i>pre-ws</i> GRAM which runs as a xinetd process. The startup script, =globus-ws=, is configured and placed into the =$VDT_LOCATION/post-install= directory.  With =globus-ws= service enabled, starting or stoping the service is done via the ==vdt-control==  commands.  MySQL is also required to be on to support the RFT service:

<pre class="screen">
&gt; source $VDT_LOCATION/setup.sh
&gt; vdt-control --on mysql globus-ws
<b>   or</b>
&gt; vdt-control --off globus-ws
</pre>

Turning the service on via ==vdt-control --on globus-ws== moves the init file into ==/etc/init.d/==, runs the scripts with the ==start== input argument, and enables automatic startup on boot using ==chkconfig== command.  Turning it off reverses these steps: runs the scripts with the ==stop== argument and removes the file from the ==/etc/init.d/== area.

%NOTE% Client access to the service must include the non-standard port designation explicitly (e.g. <i> hostname</i>:9443). 

---++ Performance Enhancement Recommendations 

%RED%

Main.JeffPorter (Jan 2009)  With VDT update XXX, these performance enhancements are automatically enabled where possible.  The one not managed is locating the persistent directory on local disk.  The release documentation should be updated accordingly - when the release is ready.

%ENDCOLOR%

Scaling tests performed against WS-GRAM servers have indicated some configuration details can be modified to improve performance in high use environments. 

   * Increase the Java container parameter containerThreadsMax from the default value="20" to value="100"
      * The containerThreadsMax parameter is located in the file: $GLOBUS_LOCATION/etc/globus_wsrf_core/server-config.wsdd
      * The containerThreadsMax parameter sets the maximum number of threads for the container. More details may be found in Table 2 at: http://www.globus.org/toolkit/docs/4.0/common/javawscore/admin-index.html#id2539200 
   * Increase the maximum heap size that the Java virtual machine can consume to 1024 MB. Set GLOBUS_OPTIONS="-Xmx1024M"
      * The GLOBUS_OPTIONS parameter is set in the file: $VDT_LOCATION/setup.sh
      * The default value -Xmx256M has proven inadequate when stress testing the ws-gram interface.  On an ITB  system with 2 GB of memory on the head node with 600 MB of free memory, setting the value to -Xmx1024M has prevented the Java container from crashing and requiring a restart.  Systems that have limited memory on the head node or have heavy memory usage by other services will need to balance the maximum heap size used by the Java virtual machine with other services.
   * Make sure that the persistance directory is on a local disk
      * Set the value of =org.globus.wsrf.container.persistence.dir= in =$VDT_LOCATION/vdt-app-data/globus/etc/globus-system-properties.in=  
   * Increase the number of file handles available for user that the globus container runs under. Add the following line <pre class="screen">
globus          hard     nofile           16384
</pre> to =/etc/security/limits.conf=

---++ Testing the Service

To test that the service is running and to validate that WS-GRAM is properly configured, you should run through a series of tests from a separate client host. These tests, which include accessing the service with Condor-G, are included in a separate Validation Document, ValidateGramWebServices.

---++ Caveats
---+++ !!Fork jobs hang with Globus WS GRAM installed on NFS

The performance enhancement documents (see above) note that the persistant directory used by the service should be on a local disk. <b>In addition</b>, we've found a specific problem when Globus is installed on an NFS mounted disk.  The logging information automatically go to two files in ==$GLOBUS_LOCATION/var==: ==container-real.log== and ==globus-fork.log== .  The latter file is configured with permission '622' . There are __NO__ problems as long as this file is on a local disk. If it is on an NFS system, however, any Fork jobs hang as the service tries to open/write to this file.   While changing the permissions of this file to 666 fixes the problem, the most maintainable change is to put the log file onto a local disk and point to that file in ==$GLOBUS_LOCATION/etc/globus-fork.conf==.  


<a name="faultcode200"/a>
---+++ !!Unhandled fault code 200 in container-real.log

The log file for the service is ==$VDT_LOCATION/var/container-real.log==.   Messages like that shown below, although not flagged as an error, will not allow web services to function correctly.
<pre class="programlisting">
2007-02-26 13:07:07,441 WARN  exec.StateMachine [RunQueueThread_1,createFaultFromErrorCode:3264]
   Unhandled fault code 200
2007-02-26 13:07:12,438 INFO  exec.StateMachine [RunQueueThread_1,logJobFailed:3522] 
   Job 28e66010-c5c4-11db-971d-c95a3d1cbb7e failed
</pre>

It is generally caused by something wrong in the =/etc/sudoers= file.  It may even be a mis-typed command.  So scrutinize the entries very carefully and always use the =visudo= command to update it.  Double check that you did <i> copy and paste </i> from the generated ==sudo-setup.txt== file and that you are not trying to softlink to a a different installation.

%STOPINCLUDE%
 

%BR%
%COMPLETE3% %BR%
%RESPONSIBLE% Main.CharlesBacon  - 18 Oct 2007 %BR%
%REVIEW% Main.JeffPorter - 12 Jun 2008 %BR%
%REVCOM%  %BR%
%REVFLAG% %Y% %BR%


%META:TOPICMOVED{by="RobGardner" date="1193441210" from="Integration/ITB_0_7.InstallAndConfigureGlobusWebServices" to="Integration/ITB_0_7.GlobusWebServicesInstall"}%
