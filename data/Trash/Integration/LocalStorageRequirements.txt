%META:TOPICINFO{author="BrianBockelman" date="1486493608" format="1.1" version="1.51"}%
---+!! Local Storage Requirements

%TOC%

---++ Introduction

The goal of this page is to provide a recommendation for the definitions and environment variables for the Local Storage accessible to Compute Elements on OSG. It covers the definition of these spaces and correspondences to external schemas (e.g. GLUE, Grid3/OSG).
The paths defined with the names below are sometimes also called _CE storages_. They are disk spaces or SEs accessible from within the CE .
<!-- (the same from each jobmanager of that CE: jobs on the headnode using _fork_, jobs on the worker nodes usong _pbs/condor/lsf/..._, multiple gatekeeper added to the same CE for reliability or scalability reasons will have to have the same values). -->
The keyword <i>UNAVAILABLE</i> will be provided instead of the path when the CE is not supporting a particular CE storage, distinguishing an unconfigured site from one which provides support for only certain CE storages.
The values provided do not refer to any disk space as viewed from outside (SE, !GridFTP server, ...).

<!-- In this document there are some suggestions on how to encourage proper use and deployment (how to make this information available), but in general this is the responsibility of other groups in OSG.
-->
Possible technologies to deploy CE storage include (but are not limited to):
   1. Variables defined in the environment that resolve to the correct path or URL
   2. Path or URLs consistent across the CE (headnodes and WN), published using information provider (e.g. GIP/BDII)
   3. *DEPRECATED* !GridCat

Solutions 2 and 3 may involve a lookup to the information system that can be done before submitting the job (in that case the job will carry along the information to be able to use the CE).

<!-- This page has a sister page, LocalStorageRequirementsDiscussion, that includes the discussion that led to this document. Please read it and feel free to add your ideas to that page if you are interested in proposing changes.
-->

Trash.ReleaseDocumentationLocalStorageConfiguration provides installation/configuration notes and examples.
LocalStorageUse is a document for users containing best practices, notes and examples.
The attached [[%ATTACHURL%/OSG_storage_notes.pdf][OSG_storage_notes.pdf]] is a presentation with pictures clarifying the different concepts of LocalStorage and SE and how these areas may be accessed from inside or outside a CE.

In the following table there is a name matrix:
   * CE Storage corresponds to the names in this document and the variables in =osg-attributes.conf=
   * GLUE is the attribute name as in GLUE Schema 1.2 (The same attribute may appear in more than one place in the Schema)
   * Grid3/OSG is the LDAP attribute name as in Grid3 Schema

| *CE Storage*    |*GLUE*|*Grid3/OSG*|
| OSG_GRID  | Location.Path (*2)  | Gri3Dir    | 
| OSG_APP   | !CE.Info.ApplicationDir (!CE.Info.ApplicationDir) (*1)    | Gri3AppDir    | 
| OSG_DATA  | !CE.Info.DataDir (!CE.VOView.DataDir) (*1)                | Gri3DataDir   | 
| OSG_SITE_WRITE | Location.Path (*2) |   |
| OSG_SITE_READ  | Location.Path (*2) |   |
| OSG_WN_TMP | !CE.Cluster.WNTmpDir (!CE.SubCluster.WNTmpDir)   |  |
| OSG_DEFAULT_SE | !CE.Info.DefaultSE (!CE.VOView.DefaultSE)   |     |

*1. As visible from the table above, GLUE provides the possibility to have multiple values for some of the CE storage, depending on the VO and the Role (VOMS FQAN). In OSG these are currently sitewide information.<br>
*2. The GLUE Schema does not have an specific attribute for SITE_WRITE or SITE_READ, but it provides the location entity (Name/Version/Path sets) to accommodate additional CE local storage. In order to accommodate that, two locations will have to be defined through the GIP:
   1. !LocalID: GRID+OSG, Name:GRID, Version: OSG, Path: <value of GRID>
   1. !LocalID: SITE_WRITE+OSG, Name:SITE_WRITE, Version: OSG, Path: <value of SITE_WRITE>
   1. !LocalID: SITE_READ+OSG, Name:SITE_READ, Version: OSG, Path: <value of SITE_READ>

Each CE administrator will provide for the correct functioning of the client software (Globus and SRM: access to the users proxy, gass_cache mechanism) for all the jobs running on the CE.  Common practice is to use a shared $HOME directory, but the administrator is free to use other mechanisms transparent to the users. Users should have no other assumption about the CE different from what is stated in this document. It is unsafe to make assumptions about the existence and characteristics (size, being shared, ...) of the $HOME directory. In particular, site admins are free to deploy configurations that do not
include any NFS exports from the CE, e.g. as described in OSG document 
<a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382">382</a>.

The following sections describe the different storage areas that may be declared local to a CE.
Each section includes:
   * brief description
   * detailed description
   * use cases (informal)
   * notes

---++ Known Problems

---++ OSG_GRID

This area is read-only, intended for client software provided by OSG.

The CE administrators will install the client software included in the OSG-WN-Client package in this area (see Trash.ReleaseDocumentationWorkerNodeClient for details.)  It is required that relative paths and content are consistent between gatekeeper and worker nodes even if OSG_GRID itself (the base dir) differs between the two.  It is up to the site administrator to either provide a shared directory, or to locally install it on each machine. OSG_GRID may be included directly into the PATH defined for jobs running at the CE.
In cases when Trash.ReleaseDocumentationWorkerNodeClient installation leads to OSG_GRID that differs between CE (i.e. gatekeeper)
and worker nodes, it is required to add appropriate softlinks on the worker node such that jobs that
assume the same base path will work.

Typical uses of this area are:
   * Provide a common set of OSG client software (_globus-job-run_, _globus-url-copy_, _srmcp_, etc.)


---++ OSG_APP

This area is intended for VO-wide software installations.

It is required that relative paths resolve consistently between gatekeeper and worker nodes even if OSG_APP itself (the base dir) differs between the two.  It is strongly recommended that the base dir itself is the same as well, or some legacy software (*1) will not function properly.  This area may be writable only by a subset of users and read-only for all the others: there is no guarantee that every user will have write access.  OSG_APP must point to a POSIX-compliant filesystem for software installation. 
<br>
It is recommended that OSG_APP is writable only via specific users/VO roles in order to improve basic security and robustness. (*2)

Typical uses of this area are:
   * Install and run VO applications

Notes (*#):
   1. Pacman resolves variables an symbolic links saving the full real path. A suggested procedure to avoid problems with it is:
      * Choose any absolute location on your gatekeeper file system for OSG_APP.
      * On each worker node arrange by mount or symlink that OSG_APP has the same path as on the gatekeeper node.
      * Install applications only using the fork jobmanager.
   1. The 'sticky bit' enabled on OSG_APP is recommended for all shared CE storages.

<!-- Furthermore, many shared filesystems trust the integrity of the clients; a compromised client may compromise all of OSG_APP if NFS write access is allowed. The mechanisms to recognize a sw manager role (FQAN) and send only it to the APP server with the possibility of installing software are currently missing in OSG and will require some software development.
-->
---++ OSG_DATA

This area is a transient storage shared between jobs executing on the worker nodes.

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This area is intended to hold data shared among different applications running on different worker nodes and/or data that has to outlive the execution of the jobs on the worker nodes.  It has to allow open/read/write operations by regular programs (that may use it transparently as a local disk space --  NFS, <!-- dcap, drm, -->
AFS, CIFS are OK). This is a subset of a POSIX-compatible filesystem, excluding features such as special file creation (pipes, sockets, locks, links) and the ability to modify file metadata (permissions)(*1).

Gridftp or 'SE like' access from outside of the cluster is required to allow an efficient use of OSG_DATA as staging area (*2).

Since the allocation of this space is transient, it is important that users remove unused data and/or that a simple mechanism to allow cleanups (*3) is added.
The use of OSG_DATA as a writable area from compute nodes is one of the most significant performance bottlenecks in an OSG cluster. Use of OSG_DATA as the "hold all read/write area" (working area for the jobs) is discouraged.
A suggested dataflow using OSG_DATA and providing more scalable and reliable data access is the following:
   * Data is written to OSG_DATA via gridftp or if necessary fork jobs (unpack tarballs, etc.)
   * Job is staged into the cluster
   * Job copies its data to the compute node (OSG_WN_TMP) or reads data sequentially from OSG_DATA if the data is read once. The latter is a significant performance issue if many random reads are necessary on typical network file systems and this should be avoided. It is worth noting that random data access over large data sets is where grid storage shows its potential -- its distributed nature is better suited for handling that type of data access scalably and reliably.
   * Job output is placed in OSG_WN_TMP
   * At the end of job, the results from OSG_WN_TMP are packaged, staged to OSG_DATA and picked up via gridftp or staged out via other mechanisms (such as srmcp).
Functions covered by OSG_DATA are nearly equivalent to those provided by OSG_SITE_READ and OSG_SITE_WRITE together.  The latter solution forces the separation between input and output and may be more efficient for big production sites with specialized hardware, while OSG_DATA may be easier to deploy for small sites.<br>
If you plan to remove OSG_DATA for performance issues, check if your jobmanagers require a shared space and if $HOME is local or it is another OSG_DATA de facto (*4). At present, we discourage site admins from removing OSG_DATA and discourage users from using OSG_DATA. We believe that many users need some more transition time to ween themselves off of their use of OSG_DATA.

Typical uses of this area are:
   * Input datasets for jobs executing on the worker nodes
   * Datasets produced by the jobs executing on the worker nodes
   * Shared data for MPI applications
   * Data staged in or waiting to be staged out

Notes (*#):
   1. The ability to modify file permissions may be required in future revisions -- it is available in SRMv2
   1. The OSG_DEFAULT_SE entry may be used to publish the base GSIftp URL of the SE viewing that space.
   1. An example of a simple space management solution could be a file in each directory ( _.keep_) that includes a number of days (between 0 and _maxdays_) that that data should be kept. If today's date > (_.keep_ modification date+number of days requested), all files in that directory and subdirectories may be removed. This is a "gentleman's agreement" -- keep in mind that none of the data in a transient storage is guaranteed (if the sysadmin needs to remove it, he can do it freely and asking around is a kindness, not a rule)
   1. E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads). If you do decide to eliminate OSG_DATA from your site then you should also eliminate $HOME at the same time, e.g. as described in OSG document <a href="http://osg-docdb.opensciencegrid.org/cgi-bin/ShowDocument?docid=382">382</a>.

---++ OSG_SITE_READ

This area is a transient storage visible from all worker nodes and optimized for high-performance read operations.

It is required that relative paths resolve consistently between gatekeeper and worker nodes.  
This read-only area is intended to hold input data shared among different applications running on different worker nodes.
It allows normal file random access read operations (open, seek, read, close) by regular programs (that may use it transparently as a local disk space). This is provided through a grid file access library, and the users do not have to know the storage location and its underlying implementation (the use will be uniform across OSG).<br>
Users have no write access to this area from the worker node: 
files will be placed there by site administrators or by writing from the Grid side of the SE.  (*1).<br>
If the gatekeeper cannot write to this area, there may be problems with jobmanagers using a shared directory to transfer the executable or some data (*2). 

<!-- This follows the LCG model. This last option may cause problems to many current applications that count on normal file access to a shared space and it may require non-trivial changes to them to use special client programs to access the DEFAULT_SE. Furthermore it may cause problems with jobmanagers using a shared directory to transfer the executable or some data. A dataflow example could be: 

Its features cover the read part of DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.
-->

Typical uses of this area are:
   * Input datasets for jobs executing on the worker nodes
   * Data staged in 

Notes (*#):
   1. If GSIftp can be used to put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes then the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported in that case.
   1. E.g. the current Condor jobmanager uses a shared space (in the gass_cache as default) when it is requested to transfer the executable or some data. As previously mentioned, it is not safe to assume that $HOME is shared between gatekeeper and worker nodes. Furthermore, it is not worth removing a shared space like OSG_DATA for performance issues only to reintroduce it as $HOME (that probably will have even more performance problems because of other loads)

---++ OSG_SITE_WRITE
This area is a transient storage visible from all worker nodes and optimized for high-performance write operations.

It is write-only (or mostly write) transient sorage.
This area is intended to hold the output from jobs running on the CE that are required to persist beyond the job lifetime.
It allows normal random-access file write operations: open, set flags, write (sequential, with multiple write operations), and close. It may not be possible to modify a file once closed. This is provided through a grid file access library specific for the underlying storage, and programs may use it transparently (*1) as a local disk space.<br>
Users may not have read access to this area; in such cases files will be accessed with the help of the site administrator, or through mechanisms external to the CE (e.g., a SE can read from that area (*2)).<br>
<!-- Its features cover the write part of OSG_DATA and it is recommended to big sites to force users not to use DATA as a 'catch-all' directory affecting the performance of the cluster.
-->
Typical uses of this area are:
   * Storage of datasets produced by the jobs executing on the worker nodes
   * Data waiting to be staged out

Notes (*#):
   1. Limitations should be avoided when possible, and be clearly stated.
   1. If GSIftp can be used to put/get files into the SE that are then visible via OSG_SITE_READ or OSG_SITE_WRITE from the worker nodes then the OSG_DEFAULT_SE entry may be used to publish the GSIftp URL of the SE that can do so. However, both put and get must be supported in that case. 

---++ TMP [OBSOLETE!]
This area was intended as a shared temporary work area. Because of its similarities with OSG_DATA, the reduced interest in multimode applications (that anyway can use OSG_DATA), and the possibility of the application abusing of a shared space, this
*has been removed* in the most recent OSG version.

---++ OSG_WN_TMP
This area is a temporary work area that may be purged when the job completes.
It is generally not possible for two jobs running on a cluster to read from (or write to) each other's OSG_WN_TMP areas. The OSG_WN_TMP area is thus fundamentally different from all other areas discussed here in that it is job-specific.

It is required that OSG_WN_TMP points to a POSIX-compliant filesystem that supports links, sockets, locks, pipes and other special files as required. Good I/O is essential for job performance, so a local filesystem is recommended.
Ideally, it should be a temporary directory (or partition) assigned empty for the job, with a well defined amount of space (many jobs require at least 1-2 GB of disk space) and purged at job completion.  The space provided should be dedicated and isolated: jobs overusing their space should not affect each other or affect the OS.<br>
In most cases, it points to a space shared among all jobs currently executing at the worker node, so it is recommended that a job create a unique subdirectory of OSG_WN_TMP (e.g. OSG_WN_TMP/somestring$JOBSLOT) that becomes its working area and that should be removed by the job itself before completion.
<br>
Sites using condor as their batch system typically implement this by letting condor create the
jobs directory prior to launching the job. In those cases OSG_WN_TMP may be defined in the GIP implicitly by pointing to another environment variable, rather than explicitly by pointing to a full path.


Typical uses of this area are:
   * Working directory for jobs (running on the worker nodes)

<!--
Notes (*#):
   1. A mechanism to help automatic cleanup would be the introduction of a 'lock' directories with files named according the temporary directory, containing the PID of the job. If that process is terminated, the directory can be removed
-->

---++ OSG_DEFAULT_SE
This variable represents a Storage Element closely related to the CE.

It is accessible only using SE access methods (Gridftp, SRM). It is accessible from within the cluster (visible from the worker nodes). If accessible from outside, it is the preferred SE for the CE and should be used when doing 2(or more)-step copy of input datasets to the CE (e.g. 3rd party transfer to OSG_DEFAULT_SE, then copy to OSG_WN_TMP using a client program).<br>
In a simple cluster this could be a SE visible from both inside and outside and serving an internally shared space like OSG_DATA. In sites with no shared space, this would allow I/O via grid tools.

Typical uses of this area are:
   * Staging in of large input files (e.g. datasets)
   * Staging out of large output files (e.g. datasets)

<!--
Notes (*#):
  1. This is not guaranteed. To describe better the SE connected to CE storages like DATA, SITE_READ, SITE_WRITE a better mechanism will have to be defined. The GLUE CE-SE binding schema could be a starting point.
-->

---++ Minimum requirements
An OSG site is not required to provide every single area described above but must implement at least one of the options below.
Areas that are not provided should be labelled as "UNAVAILABLE".

Mandatory set options (one of the following):

---+++ 1. OSG_GRID, OSG_APP, OSG_DATA, OSG_WN_TMP 
This is the Grid3 model.  A simple CE (one or few nodes, not a production CE) may even decide to use a single shared disk, pointing both OSG_DATA and OSG_WN_TMP to the same directory, but this is not recommended.

---+++ 2. OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_SITE_WRITE, OSG_WN_TMP
A dataflow example may be:
   * site admin intervention or external transfer 
   * cp: OSG_SITE_READ->OSG_WN_TMP 
   * job execution 
   * cp: OSG_WN_TMP->OSG_SITE_WRITE 
   * external stage-out

---+++ 3. OSG_GRID, OSG_APP, OSG_DEFAULT_SE, OSG_WN_TMP 
This is the LCG model.  Current applications depending on normal file access to a shared space may require non-trivial changes -- there may also be issues with jobmanagers that use a shared directory to transfer the executable or data.
A dataflow example may be: 
   * site admin intervention or external transfer 
   * srmcp: OSG_DEFAULT_SE->OSG_WN_TMP
   * job execution
   * srmcp: OSG_WN_TMP->OSG_DEFAULT_SE
   * external stage-out

---+++ 4. OSG_GRID, OSG_APP, OSG_SITE_READ, OSG_DEFAULT_SE, OSG_WN_TMP 
This is the SRM/dCache model. This is actually a superset of 3, so this CE satisfies 3.  In this model read access is via dcap, while write access is via srmcp only. (Write access via dcap is possible in principle and coulde provide OSG_SITE_WRITE; however, in practice dCache does not allow modification of a file once it is closed).  A typical deployment and use case is thus to allow writes only via srmcp (i.e. OSG_DEFAULT_SE) while reads may use either srmcp (OSG_DEFAULT_SE) or dcap (OSG_SITE_READ). 
The current SRM implementation in dCache does not allow overwriting of logical files, thus guaranteeing that all physical file replicas of the same logical file remain the same. 
A dataflow example may be: 
   * site admin intervention or external transfer 
   * job execution (open/seek/read from OSG_SITE_READ using dcap)
   * srmcp: OSG_WN_TMP->OSG_DEFAULT_SE
   * external stage-out

The set of CE storages provided by a CE *must include* at least one of these four sets (*1 OR 2 OR 3* - 4 includes 3).

Of course, providing a wider selection of CE storages would allow the jobs to select the most proper for their needs -- but it could also allow the jobs to adopt inefficient execution models that could negatively affect the performance of the whole cluster.

See Trash.ReleaseDocumentationLocalStorageConfiguration for installation/configuration notes and examples.


<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->

*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.BurtHolzman - 04 Aug 2005
-- Main.MarcoMambelli - 23 Sep 2005
-- Main.FkW - 25 Apr 2006

%STOPINCLUDE%


   * : Notes on SE and Storages lacal to CE

%META:FILEATTACHMENT{name="OSG_storage_notes.pdf" attachment="OSG_storage_notes.pdf" attr="" comment="Notes on SE and Storages lacal to CE" date="1156358455" path="OSG storage notes.pdf" size="66791" stream="OSG storage notes.pdf" user="Main.MarcoMambelli" version="1"}%
