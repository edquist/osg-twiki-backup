%META:TOPICINFO{author="JohnWeigand" date="1142867349" format="1.0" version="1.14"}%
%META:TOPICPARENT{name="LocalStorageRequirements"}%
<!-- This is the default OSG Integration template. 
Please modify it in the sections indicated to create your topic!
If you have any comments/complaints about this template, then please email me:
rwg@hep.uchicago.edu.  (Adapted from Atlas wiki template, Ed Moyse)
--> 

<!-- By default the title is the WikiWord used to create this topic !-->
<!-- if you want to modify it to something more meaningful, just replace %TOPIC% below with i.e "My Topic"!-->

---+!!<nop>Local Storage Configuration
%TOC%
%STARTINCLUDE%


---++Introduction

This document describes how CE administrator can configure the OSG attributes, including ones referencing "CE storage" (LocalStorageRequirements) during the installation and after (if he needs to change the layout of his CE). 

LocalStorageRequirements describes what the different CE storage are and LocalStorageRequirements#Minimum_requirements specifies the minimum requirements for being OSG-compliant. It is not mandatory to provide all the CE storages.  Also see LocalStorageUse for a very draft idea on how
these should be used.

---++Known Problems

---++Configuring the OSG Attributes

This section covers the configuration of information about your CE that you must make known to OSG. These will be published as pat of the GLUE schema using the GIP (or the Grid3 schema), and used directly or indirectly by other OSG applications (MonALISA, ACDC, !GridCat, ...) and users submitting jobs.
<!-- The current version also provides backward compatibility with applications using Grid3 conventions. -->
At the core is a standard configuration file (<tt>$VDT_LOCATION/monitoring/osg-attributes.conf</tt>), that can be edited or reviewed directly. There is a configuration script (<tt>$VDT_LOCATION/monitoring/configure-osg.sh</tt>) which automates much of the configuration.

The meaning and purpose of the various elements of the attributes in that file are documented further in LocalStorageRequirements and in the [[http://infnforge.cnaf.infn.it/glueinfomodel/index.php/Spec/V12][GLUE documentation]]. New resource administrators may want read that information carefully and determine how to map those elements onto their Resource before proceeding. Guidance on the basic elements and common defaults is provided below. 

---+++!!Gather configuration information

OSG strives to make resources available with minimal requirements; however, in order to provide a basic execution environment, certain information about files and filesystem locations is needed. Filesystem sharing and filesystem mount points available for a cluster requires specific coordination for applications to be installed and to be executed correctly. For this purpose, some special directory hierarchies (mount points) will be required to be defined and allocated in the OSG environment. These directories may be required to be available on the head node/gatekeeper node, available (using the exact path) on each of the worker nodes, simply shared filespace.

* %RED%OSG Location ($OSG_LOCATION)%ENDCOLOR%

This directory is where OSG software will be installed and must be writable by root.
It contains the OSG-specific software as well as the Globus middleware and other middleware applications, including server and client utilities used by the system itself.  Users will use a different client installation.

* %RED%OSG Grid ($OSG_GRID) %ENDCOLOR%

This directory is where OSG Client Software will be installed -- see WorkerNodeClient for description.  It includes
client utilities for Grid middleware, such as VDS and srmcp.  It should be writable by root and readable by all users. It
must be accessible by both gatekeeper and worker nodes (via a shared filesystem, or different installations on local disks using a consistent pathname).

* %RED%Application Directory ($OSG_APP) %ENDCOLOR%

This is the base location for VO-specific application software. Only users with software
installation privileges in their VO should have write privileges to this directories. At least 10 GB of space should be allocated per VO.

* %RED%Data Directories ($OSG_DATA or $OSG_SITE_READ/$OSG_SITE_WRITE)%ENDCOLOR%

	 These directories must be shared from the head node to each of the worker nodes. This is intended
as the space for applications to write input and output data files for running jobs with persistency that must exceed the lifetime of the job which created it. This directory should be writable by all users.  Users will be able to create sub-directories which are private, as provided by the filesystem. At least 10 GB of space should be allocated per worker node; some VOs require much larger allocations.

The following different options are possible: 
	* $OSG_DATA: shared directory with read-write access for all users
	* $OSG_SITE_READ: shared directory with read-only access for all users (data may be prestaged by the administrator or using a SE pointing to the same space)
	* $OSG_SITE_WRITE: shared directory with write-only access for all users (data may be staged out by the administrator or using a SE pointing to the same space)
A CE can provide $OSG_DATA, both $OSG_SITE_READ and $OSG_SITE_WRITE, or none of them if it has a local SE specified in $OSG_DEFAULT_SE. The keyword to say that one hierarchy is not provided is UNAVAILABLE.

* %RED%Temporary Directory ($OSG_WN_TMP)%ENDCOLOR%

	 This is a temporary directory local to the worker node used as a working directory.  At least 10 GB per virtual CPU should be available in this directory (e.g. a !WorkerNode with 2 hyperthreaded CPUs that can run up to 4 jobs, should have 40GB). 
Files placed in this area by a job may be deleted upon completion of the job.

* %RED%Default Storage Element ($OSG_DEFAULT_SE)%ENDCOLOR%

	 The default Storage Element, $OSG_DEFAULT_SE, is a storage element that is close and visible from all the nodes of the CE (worker nodes and head node). Usually, it is local to the CE and accessible from outside with the same or a different URL. The value to be specfied in $OSG_DEFAULT_SE is the full URL, including method, host/port and path of the base dir. 
If the CE has no default SE it can use the value UNAVAILABLE for $OSG_DEFAULT_SE. 

The final question asks about the <b>VO sponsor</b> of your site.
This attempts to determine the VOs paying for the resources of the cluster. The notation
incorporates a VO name followed by a percentage, so that CEs are able to denote multiple VO partners. 

---+++!!Execute the configuration script

Run the following script as root to execute the configuration script.

<pre>
# <b>cd $VDT_LOCATION/monitoring</b>
# <b>./configure-osg.sh</b>
</pre>

For a typical installation, the script will look something like this:

<pre>
Please specify your OSG SITE NAME [_hostname.domain.tld_]: <b><i><a href="http://osg.ivdgl.org/twiki/bin/view/Provisioning/OSGResourceName">UNIQUE_NAME</a></i></b>
Please specify your OSG LOCATION (BASE DIR) [/usr/local/grid]:
Please specify your OSG GRID path [Dir_with_grid_client_sw]: 
Please specify your OSG  APP [Dir_to_install_VO_applications]: 
Please specify your OSG  DATA [UNAVAILABLE]: 
Please specify your OSG  SITE_READ [UNAVAILABLE]: 
Please specify your OSG  SITE_WRITE [UNAVAILABLE]: 
Please specify your OSG  WN_TMP [Working_dir_for_jobs]: 
Please specify your OSG  DEFAULT_SE [UNAVAILABLE]: 

Examples of possible VO Sponsors are usatlas, ivdgl, ligo, 
uscms, sdss...
You can express the percentage of sponsorship using
the following notation:<br/>
 "myvo:50 yourvo:10 othervo:20 local:20"  <font color=red>Please do not use single quotes. It interferes with the database query statement</font>
Please specify the VO sponsor of this site [iVDGL]:

Enter the URL which will describe the policy for this resource
Please specify the Policy URL [POLICY_URL]: 

Please specify the Batch Queuing to be used [condor]:

Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/usr/local/grid</i>
OSG WN Client:	<i>/share/gridclient</i>
Application:	  <i>/app</i>
Data:				<i>/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/tmp</i>
Default SE:		UNAVAILABLE
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

This configure script creates the =$VDT_LOCATION/monitoring/osg-attributes.conf= file (and grid3-info.conf that is a link to it). 
This file is the standard resource information file used by several monitoring services and applications to obtain basic resource configuration information. This file is *required* to be readable from that location for OSG CE's.
The resource owner may choose which information services to run to advertise this information. Configuration of several of the more popular ones is described below in the Monitoring section.

---++Reconfiguration

An already installed OSG site can be reconfigured by editing =osg-attributes.conf=, or by rerunning the =configure-osg= script.

---++Examples
These examples make assumptions about relationships between what is visible from inside of the CE (from the jobs) and what is available from the outside (SE, !GridFTP) for staging operations.
Your configuration may differ considerably.

---+++Single Computer OSG site 
Let's say you want to install a complete OSG site onto a single computer, e.g. as a testbed. You thus have CE, SE, and batch system on the same piece of hardware -- no clustering or multiple pieces of hardware.

For this example, let's assume we are using a partition mounted as  =/osg= (if you prefer, each of =/osg/grid=, =/osg/wntmp=, =/osg/app=, =/osg/data=, =/osg/wngrid= can be on a different partition; anything in between is fine also).  Users are mapped to different unix accounts but all belong to the same =griduser= unix group. 
 
	* Create the subdirectories =/osg/grid=, =/osg/app=, =/osg/data=, =/osg/wngrid=, =/osg/wntmp=
	* Change their group ownership to =griduser=
	* Change their permissions (1770 on /osg/data, /osg/wngrid, /osg/app, /osg/wntmp)
	* install the server (OSG) in =/osg/grid= and the user client (OSG-WN-Client) in =/osg/wngrid=
	* Configure variables (=GRID=/osg/wngrid=, =APP=/osg/app=, =DATA=/osg/data=, =WN_TMP=/osg/wntmp=)
<!--	* Point the gridftp server to =/osg/data= (gsiftp://myserver.domain/mydir/ -> =/osg/data=) -->

Here you will have a summary of =osg-configure.sh= (=osg-attributes.conf=) that looks like:  
<pre>
Please review the information:
Grid Site Name:  <i><a href="#UniqueName">UNIQUE_NAME</a></i>
OSG Location:	 <i>/osg/grid</i>
OSG WN Client:	<i>/osg/wngrid</i>
Application:	  <i>/osg/app</i>
Data:				<i>/osg/data</i>
Site read:		 UNAVAILABLE
Site write:		UNAVAILABLE
WorkerNode Temp: <i>/osg/wntmp</i>
Default SE:		<i>gsiftp://myheadnode.athome.edu:2811</i>
JOB Manager:	  <i>condor</i>
Is this information correct (y/n)? [n]: <b>y</b>
</pre>

The specification for the "Default SE" should be such that one can concatenate it with the setting for "Data"
to arrive at a valid location SURL without any additional, or extraneous "/".
The nodename "myheadnode.athome.edu" is the nodename of the single headnode, and the port number is the port number
at which the gsiftp server is listening.

In addition, the OSG CE expects a home directory (=/osg/users/$USER=) for every user where the proxy and gass-cache are written to as jobs are submitted to it.

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.

---+++Single Headnode Cluster
Let's say you have a cluster with a batch system, and you want to add a single OSG headnode to it that instantiates a CE and SE
on one piece of hardware in the simplest possible way. We do not suggest this as a production installation as it couples CE and SE
and thus does not provide a very reliable OSG site.

Start by following the "Single Computer OSG Site" instructions above.

Mount the file systems mentioned on the single headnode as follows:
<pre>
/osg/grid	 read/write
/osg/wngrid  read/write
/osg/app	  read/write
/osg/data	 read/write
/osg/users	read/write
/osg/wntmp	read/write
</pre>

Make sure that the following file systems are read/write accessible via the gridftp server:
<pre>
/osg/app
/osg/data
</pre>

Mount the file systems mentioned on all worker nodes on the cluster as follows:
<pre>
/osg/wngrid  read only
/osg/app	  read only
/osg/data	 read/write
/osg/users	read/write
/osg/wntmp	read/write
</pre>

In this configuration, WN_TMP (/osg/wntmp) is not a shared file system. It should be local to each and every worker node.
(In fact, it could be local to each and every batch slot.)

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.
However, we do not suggest this configuration unless the cluster is quite small.

---+++Suggested Minimal Configuration
The minimal set of headnodes for an OSG site (site = cluster & headnodes) to be reliable is two:
one for the CE and one for the minimal SE configuration.

To deploy such a configuration, follow the "Single Headnode Cluster" configuration above but separate
components as follows:
<pre>
Headnode 1:
Install the CE

Headnode 2:
Install the gridftp server
Export all file systems
</pre>

In this configuration you must point "myheadnode.athome.edu" in the specification of "Default SE" to the nodename of 
Headnode 2, the one that instantiates the gsiftp server.

This installation implements LocalStorageRequirements#1_OSG_GRID_OSG_APP_OSG_DATA_OSG and is thus OSG compliant.

---+++SRM style CE
There are two versions of SRM covered here: SRM/DRM and SRM/dCache.
The intention here is not to provide a comprehensive description of the configuration of these products,
but to give an overview of how either one of these are incorporated into the OSG infrastructure once deployed.

In principle, replacing =$OSG_DATA= in the "Minimal Suggested Configuration" with either one of these SRM solutions is an
OSG-compliant configuration as specified at LocalStorageRequirements#3_OSG_GRID_OSG_APP_OSG_DEFAULT_S.
However, in practice, too many applications are not yet ready to benefit from SRM, and we thus suggest
sites to deploy SRM in addition to the "Minimal Suggested Configuration" for the time being. A reasonable configuration
might provide a small =$OSG_DATA= area (~100GB) in addition to a large multi-TB SRM space.

SRM/dCache:<br>
For applications that are able to use dcap, SRM/dCache replaces /osg/data.
An ideal workflow is the one described at 
LocalStorageRequirements#4_OSG_GRID_OSG_APP_OSG_SITE_READ . For applications that are not yet able to use dcap
LocalStorageRequirements#3_OSG_GRID_OSG_APP_OSG_DEFAULT_S might be suitable. 
However, in that case the data a job needs must fit
into WN_TMP which often is quite small (~10GB). As a result, it is highly advisable for applications to learn about dcap
if they want to benefit from the large storage available in SRM/dCache.

A site that chooses this configuration needs to configure "Site Read", "Site Write", and "Default SE" as follows:
<pre>
Site read:		 <i>dcap://mydcapnode.athome.edu:22136//pnfs/athome.edu</i>
Site write:		<i>srm://mysrmnode.athome.edu:8443/</i>
Default SE:		<i>gsiftp://myheadnode.athome.edu:2811</i>
</pre>

Note that a file that is copied into the Default SE can not be read from "Site read" as these are two entirely 
different storage elements in this site configuration!

The specifications for "Site read" and "Site write" are chosen such that a user who knows their path inside the
dCache pnfs logical namespace can add that path behind "Site read" or "Site write" without any additional or extraneous
forward slashes.

SRM/DRM:<br>
References to documentation will be forthcoming in future OSG releases.

---++ GIP configuration

See the Generic Information Providers section in the GenericInformationProviders.

<!-- ---++Contributed Examples
Here administratirs can add tips or links to descriptions of their installation. These may include specific references that may not apply to any other CE.
-->

<!-- MAJOR UPDATES
For significant updates to the topic, consider adding your 'signature' (beneath this editing box) !-->



*Major updates*:%BR%
<!--Future editors should add their signatures beneath yours!-->
-- Main.MarcoMambelli - 11 Nov 2005

%STOPINCLUDE%

