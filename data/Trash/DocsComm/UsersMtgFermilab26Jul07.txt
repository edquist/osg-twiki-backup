%META:TOPICINFO{author="AnneHeavey" date="1185469948" format="1.1" reprev="1.2" version="1.2"}%
---+ Notes from users meeting 26-27 Jul 07

8:30
   * Ruth has a slide on OSG scope 
   * Focus on storage areas this year -- we have directories, but need more
   * sw contributed from outside
   * consortium: all instit and projects that contribute (once you register a resource, vo, user, you're part of consort.)
   * project funded to provide staff
   * longer term issues (from nsf presentation)
   * slide on becoming a full osg "citizen" -- fit into "getting started"
   * 
Anand Padmanabhan (now at Urbana Ch) -- troubleshooting
   * problem-oriented documentation, troubleshooting guide ; will work with user support and users on how best to do this
   * idea: make sure Chris has talked with Anand before Aug 1 meeting...
   * CG question: what problems are appropriate to troub as opposed to GOC: Anand: any day to day problem, user problem
   * troubleshooting button gets your problem to both GOC and troubleshoot or VDT -- the man behind the curtain distributes appropriately
   * Mike Diesburg: turnaround time; AP: differs depending on complexity of issue, but will respond quickly RQ: within 24 hrs; but solution may be later!
   * started in May, have had 5 problems
   * AK: Rob will cover when to submit here, when to submit elsewhere
   * Qusetion about whether to go straight to goc or to site sysadmin -- KC: go through goc so they can see patterns;
   * AR: there are some tickets that have been open for a long time; RQ: don't have access to each site, VO SC is responsible; goc can ping; goc did a site survey, goc is waiting on response
   * VOs claim there are sites that that say they support vo but don't. AP: says they can follow up; RQ: there is a test tool, not perfect; would need a cert for each vo to test absolutley but don't want to do that.  There is MIS vo, only cert they hold.  Too much ops overhead.  DB: glidein jobs that work on some nodes not others in one site  RQ: MIS tests will pass on that, CG: spot a poison node, send to GOC; KC: worker nodes are not covered on these tests
   * AP: also have to work with sw devs. As link between sites and vos, need to put in place processes to penalize "bad" sites
   * RQ: sites are not required to support any partic vo
   * CG: but it's meant to be open; 
   * RQ: there are some who only are open to own vo; we can maybe enforce min num of vos, but can't force them to support given vo; 
   * AK: problem is where they say they support, but don't
   * CG: maintian list of ones who nominally support a vo, and those that 
   * and site admins needs to know; can be admin error and list
   * RQ: we are actively removing sites that "don't play game"

Gabriele: info systems  right now (RESS, BDII, VORS) then Chris on info systems, what's been happening lately (gratia)

Gabriele
   * slide of IS goals: realtime info, monitoring tools, ... resource selection
   * propagate info from site: vors and cemon (based on info prov)
   * IS: branch to condor/schedd: can support multiple ress's; any vo/usr can attach to the osg ress today.    Other branch (to condor matchmaker) used by engagement (right branch in slide)
   * ress is scalable to current requirements
   * ress motivations
   * used today by D0 and engagement; do users need it?
   * scalability: what are the req's; can you handle 40k job/day; gg: reqs are published; main stakeholder was D0; if diff needs, can extend architecture; can use ress as source of info and do matchmaking elsewhere like engage does
   * walk through how to get to ress doc: twiki, resource selection, planning doc, d0 requirements
   * user and admin doc sec on the twiki page
   * ~garz/ReSS/ReSS-prd... fancy plot

Chris -- VORS
   * DB: easy way to tell size of site? CG: vors doesn't tell # of WN.  RQ: this is available but not thru vors (lost what, GIP?); keep track of total num; 
   * VORS is snapshot
   * VORS vs RESS: complementary; read about res VORS, num of total jobs RESS (glue schema); info programatically: RESS, BDII have interfaces; real question: if I can implement an interface, what's most effecient way to do: read:VORS, other:RESS (runs on sites 0.6.0)
   * Nik Kur: how many sites rporting correct info: GG or Mats: sites and how big: 1/2 rpt correctly; some report site-wide, some vo-spec, so things will show up mult times; engage isn't using that info, looking for how many jobs can get on sites; thinks size of site is bad metric
   * wants dynamic info: how many wn's avail: GIP provides this; if config'd properly, is reliable

Gratia
   * glexec, glideins -- runs on WN, else on gatek
   * right now only gui interface; can download info in csv; not sure if can do wget on the link
   * is sql db publicly avail? yes, but wouldn't rely on it; it may go away as provide more secure and useful interfaces; "we won't help you with your queries"
   * read-only access to db
   * info: usage by cpu, walltime, jobs...  by site, some canned reports
   * DB: restart issues, provisions for wasted cpu at () level? CG: plan to keep track of "wasted cpu"; FWK: gets daily messages

---++ Session: User Experiences
Mats -- engagement vo, matchmaking with ress, mpi jobs on osg (bad usr exp)
   * want to demonstrate osg infrastructure to other sciences
   * ress for usability, not for hi thruput
   * suppy submit host, sites that show up in ress are verified by them
   * small research groups from campuses, new to grid, lower bar for them
   * rosetta: 3 days job run, 4 wks in wet lab; not grid power user, need easy to use system
   * simple jobs, 1 bin with few I/O, indep betw jobs, good candidate for matchmak
   * use ress infra, condor classads, interact only with condor collector (classads from all sites in there already); engage queries collector every 5 min, run verif jobs every 6 hrs, this is our biggest improvement; takes care of id failures for voms server (goc doesn't cover); check file systems, network setup, sw probes to check if sw is there, 
   * wrapper on condorq and condor-status to give more info. added what sites matching against; what sites passed our verif, what our jobs are doing on these sites; rank sites to figure out how to spread out jobs
questions:
   * tried other res sel? no, but used to use vors for building classads; now use ress; still use vors to get a feel for things; considered pool-based? not yet, but considering; can condor-g talk to >1 collector?  (from steve clark purdue nanohub); no. get list of classads dump them into our collector; are filtering and adding to it (e.g., sw probes, outbound connection check)

MPI jobs - why to consider
   * parallel jobs, allow processes to communicate with each other (HEP runs serial jobs, so not interested)  Lattice guage needs it
   * jobs have to start and run all at same time; if 1 node dies, whole job fails, don't respond well to restarts
   * can't run mpi currently, but should be possible
   * if osg wants to serve this community, mpi will be requirement
   * eb: accel sim at fnal, someone is working on mpi
   * draft report on conf website on exploration; contains proposed action items
   * 3 ways to submit: interactive login (think about looking bad in usability comparisons), fork+local scheduler (will be against site policies soon), jobtype=mpi (mpi wrappers, not many sites have it config'd properly)
   * many diff implementations of MPI standard; MPI is tied to hardware
   * issue: access to (right) compilers, sites need to advertize
   * fkw: if put info into IS, we could do this.  MR: user laziness is issue, has to be easy
   * steve t: default condor doesn't have this; pbs job man work done to support this for teragrid; MR: some sites are already config's for this, so osg should try to get this going to attract other vos.
   * Teragrid is more relaxed about interactive login, spending cash on interconnects, had many job managers to pick from, hard for users, purdue uses TG for MPI
   * discussions about speed of interconnects; would need to advertize that site does, someting about latencies (Gb or better), where libraries are, compiler issue is big; lots of custom codes, lots of mpi code is picky, lots of optimization goes on in code.  On OSG not going to get (?) compiler, WRF compiler exists on some sites but not many. Can't you ship libraries with job?  Need to match binary to interconnect. Some people have vendor mpi install. Shouldn't expect user to have mpi; osg should have it.  Could engagement provide a wrapper? 
   * fkw: provide uniform redhat env; beyond that is very difficult. MR says no, just advertize what you have. CG: encourage sites that have standard farms to make tcpip/mpi capable, or to advertize that they are.  ST: user-based is possible

Petar delayed at airport

Parag M -- D0 and app validation
   * use samgrid to submit, jim is Job Info Mgmt
   * d0 needs to certify sites before consider for production; need to satisfy certain req's
   * certif can take upto few weeks
   * problems found: vo users arent authent (not mapped) : vo support advertized, not implemented (some auth probs are random as policy changes on site)
   * mapped accts don't exist
   * scratch space not local, I/O activity affects performance
   * sites don't always advertize down times
   * cleanup of scratch space on WNs
   * job status reporting discrepancies
   * shortcomings of infrastructure
   * AK says that scientist wants to know exactly where job ran, where data came from. (part of LAN discussion -- can you tell which LAN you're on)
   * need replacement for Monalisa: snapshot of sys, num of jobs running; can we plug this into gratia?
   * CG: categorize probs as things that can be fixed, or inherent lack of robustness?  Factoring out the sam problems, you'd get overoptimistic picture; is it how sam interacts with osg?  cause sam works fine locally
   * use SRM completely?  SRM used to fetch data; local scratch space also used; should you pick more sites that have SRM?

Petar  CHARMM
   * HEP physicist not biophys, but wife (ANa) does. Grew out of getting people like her to use the grid
   * group from NIH, and from OSG who provided help and resources
   * molec dynamics run simulations
   * protein folding and conformational changes leading to functional changes.  require MD on long scales (many micro seconds, run for long time) queues up to 3 days; how to chop this up?  People run a few jobs for several months on local resources? Use grid to make this faster. 1000 proteins, run for 6 mo to compare to real data, SGLD can fold proteins faster (new method for conform search); they're running standard MD and SGLD (self guided langevin dynamics)
   * water coupling to protein is complicated;  MD includes all phys interactions in the simulations (~20 sim programs on market, CHARMM is popular one) other wellknown ones can also be setup without much work; all use data scripting languages.
   * chopped into 12 jobs or threads?  Then set init conds and equiib, then split into MD and SGLD
   * used PANDA model to submit jobs, several threads with many "waves" ; ran own (Torre's?)  autopilot server on Wis farm
   * each wave would submit next wave, not good enough. Wrote daemon to check if metadata from prev jobs have arrived, when they do, submits next wave of jobs
   * problems: globus-url-copy wasn't always available (CG: if WN has outgoing conn, should have this; else, goc ticket) they distr with charmm exec.
   * globus-url-copy could corrupt later waves
   * their job subm daemon is single pt of failure; got lulled into sense of security (good); didn't catch problem (bad)
   * have 3 more threads to complete; have all data are now preparing publication
   * project was pretty easy, understand failure modes, wait times short, hands-off and trouble-free
   * interesting to univ groups who could use this.





-- Main.AnneHeavey - 26 Jul 2007
