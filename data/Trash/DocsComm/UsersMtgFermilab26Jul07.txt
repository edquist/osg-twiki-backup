%META:TOPICINFO{author="KyleGross" date="1225985935" format="1.1" version="1.8"}%
---+ Notes from users meeting 26-27 Jul 07
---++ Thursday morning
---+++ Intro
   * Ruth has a slide on OSG scope 
   * Focus on storage areas this year -- we have directories, but need more
   * sw contributed from outside
   * consortium: all instit and projects that contribute (once you register a resource, vo, user, you're part of consort.)
   * project funded to provide staff
   * longer term issues (from nsf presentation)
   * slide on becoming a full osg "citizen" -- fit into "getting started"
   * 
---+++ Troubleshooting
Anand Padmanabhan (now at Urbana Ch) -- troubleshooting
   * problem-oriented documentation, troubleshooting guide ; will work with user support and users on how best to do this
   * idea: make sure Chris has talked with Anand before Aug 1 meeting...
   * CG question: what problems are appropriate to troub as opposed to GOC: Anand: any day to day problem, user problem
   * troubleshooting button gets your problem to both GOC and troubleshoot or VDT -- the man behind the curtain distributes appropriately
   * Mike Diesburg: turnaround time; AP: differs depending on complexity of issue, but will respond quickly RQ: within 24 hrs; but solution may be later!
   * started in May, have had 5 problems
   * AK: Rob will cover when to submit here, when to submit elsewhere
   * Qusetion about whether to go straight to goc or to site sysadmin -- KC: go through goc so they can see patterns;
   * AR: there are some tickets that have been open for a long time; RQ: don't have access to each site, VirtualOrganizations/VOInfo SC is responsible; goc can ping; goc did a site survey, goc is waiting on response
   * VOs claim there are sites that that say they support vo but don't. AP: says they can follow up; RQ: there is a test tool, not perfect; would need a cert for each vo to test absolutley but don't want to do that.  There is MIS vo, only cert they hold.  Too much ops overhead.  DB: glidein jobs that work on some nodes not others in one site  RQ: MIS tests will pass on that, CG: spot a poison node, send to GOC; KC: worker nodes are not covered on these tests
   * AP: also have to work with sw devs. As link between sites and vos, need to put in place processes to penalize "bad" sites
   * RQ: sites are not required to support any partic vo
   * CG: but it's meant to be open; 
   * RQ: there are some who only are open to own vo; we can maybe enforce min num of vos, but can't force them to support given vo; 
   * AK: problem is where they say they support, but don't
   * CG: maintian list of ones who nominally support a vo, and those that 
   * and site admins needs to know; can be admin error and list
   * RQ: we are actively removing sites that "don't play game"

Gabriele: info systems  right now (RESS, BDII, VirtualOrganizations/VOInfoRS) then Chris on info systems, what's been happening lately (gratia)

Gabriele
   * slide of IS goals: realtime info, monitoring tools, ... resource selection
   * propagate info from site: vors and cemon (based on info prov)
   * IS: branch to condor/schedd: can support multiple ress's; any vo/usr can attach to the osg ress today.    Other branch (to condor matchmaker) used by engagement (right branch in slide)
   * ress is scalable to current requirements
   * ress motivations
   * used today by D0 and engagement; do users need it?
   * scalability: what are the req's; can you handle 40k job/day; gg: reqs are published; main stakeholder was D0; if diff needs, can extend architecture; can use ress as source of info and do matchmaking elsewhere like engage does
   * walk through how to get to ress doc: twiki, resource selection, planning doc, d0 requirements
   * user and admin doc sec on the twiki page
   * ~garz/ReSS/ReSS-prd... fancy plot

Chris -- VirtualOrganizations/VOInfoRS
   * DB: easy way to tell size of site? CG: vors doesn't tell # of WN.  RQ: this is available but not thru vors (lost what, GIP?); keep track of total num; 
   * VirtualOrganizations/VOInfoRS is snapshot
   * VirtualOrganizations/VOInfoRS vs RESS: complementary; read about res VirtualOrganizations/VOInfoRS, num of total jobs RESS (glue schema); info programatically: RESS, BDII have interfaces; real question: if I can implement an interface, what's most effecient way to do: read:VirtualOrganizations/VOInfoRS, other:RESS (runs on sites 0.6.0)
   * Nik Kur: how many sites rporting correct info: GG or Mats: sites and how big: 1/2 rpt correctly; some report site-wide, some vo-spec, so things will show up mult times; engage isn't using that info, looking for how many jobs can get on sites; thinks size of site is bad metric
   * wants dynamic info: how many wn's avail: GIP provides this; if config'd properly, is reliable

Gratia
   * glexec, glideins -- runs on WN, else on gatek
   * right now only gui interface; can download info in csv; not sure if can do wget on the link
   * is sql db publicly avail? yes, but wouldn't rely on it; it may go away as provide more secure and useful interfaces; "we won't help you with your queries"
   * read-only access to db
   * info: usage by cpu, walltime, jobs...  by site, some canned reports
   * DB: restart issues, provisions for wasted cpu at () level? CG: plan to keep track of "wasted cpu"; FWK: gets daily messages

---+++ Session: User Experiences
Mats -- engagement vo, matchmaking with ress, mpi jobs on osg (bad usr exp)
   * want to demonstrate osg infrastructure to other sciences
   * ress for usability, not for hi thruput
   * suppy submit host, sites that show up in ress are verified by them
   * small research groups from campuses, new to grid, lower bar for them
   * rosetta: 3 days job run, 4 wks in wet lab; not grid power user, need easy to use system
   * simple jobs, 1 bin with few I/O, indep betw jobs, good candidate for matchmak
   * use ress infra, condor classads, interact only with condor collector (classads from all sites in there already); engage queries collector every 5 min, run verif jobs every 6 hrs, this is our biggest improvement; takes care of id failures for voms server (goc doesn't cover); check file systems, network setup, sw probes to check if sw is there, 
   * wrapper on condorq and condor-status to give more info. added what sites matching against; what sites passed our verif, what our jobs are doing on these sites; rank sites to figure out how to spread out jobs
questions:
   * tried other res sel? no, but used to use vors for building classads; now use ress; still use vors to get a feel for things; considered pool-based? not yet, but considering; can condor-g talk to >1 collector?  (from steve clark purdue nanohub); no. get list of classads dump them into our collector; are filtering and adding to it (e.g., sw probes, outbound connection check)

MPI jobs - why to consider
   * parallel jobs, allow processes to communicate with each other (HEP runs serial jobs, so not interested)  Lattice guage needs it
   * jobs have to start and run all at same time; if 1 node dies, whole job fails, don't respond well to restarts
   * can't run mpi currently, but should be possible
   * if osg wants to serve this community, mpi will be requirement
   * eb: accel sim at fnal, someone is working on mpi
   * draft report on conf website on exploration; contains proposed action items
   * 3 ways to submit: interactive login (think about looking bad in usability comparisons), fork+local scheduler (will be against site policies soon), jobtype=mpi (mpi wrappers, not many sites have it config'd properly)
   * many diff implementations of MPI standard; MPI is tied to hardware
   * issue: access to (right) compilers, sites need to advertize
   * fkw: if put info into IS, we could do this.  MR: user laziness is issue, has to be easy
   * steve t: default condor doesn't have this; pbs job man work done to support this for teragrid; MR: some sites are already config's for this, so osg should try to get this going to attract other vos.
   * Teragrid is more relaxed about interactive login, spending cash on interconnects, had many job managers to pick from, hard for users, purdue uses TG for MPI
   * discussions about speed of interconnects; would need to advertize that site does, someting about latencies (Gb or better), where libraries are, compiler issue is big; lots of custom codes, lots of mpi code is picky, lots of optimization goes on in code.  On OSG not going to get (?) compiler, WRF compiler exists on some sites but not many. Can't you ship libraries with job?  Need to match binary to interconnect. Some people have vendor mpi install. Shouldn't expect user to have mpi; osg should have it.  Could engagement provide a wrapper? 
   * fkw: provide uniform redhat env; beyond that is very difficult. MR says no, just advertize what you have. CG: encourage sites that have standard farms to make tcpip/mpi capable, or to advertize that they are.  ST: user-based is possible

Petar delayed at airport

Parag M -- D0 and app validation
   * use samgrid to submit, jim is Job Info Mgmt
   * d0 needs to certify sites before consider for production; need to satisfy certain req's
   * certif can take upto few weeks
   * problems found: vo users arent authent (not mapped) : vo support advertized, not implemented (some auth probs are random as policy changes on site)
   * mapped accts don't exist
   * scratch space not local, I/O activity affects performance
   * sites don't always advertize down times
   * cleanup of scratch space on WNs
   * job status reporting discrepancies
   * shortcomings of infrastructure
   * AK says that scientist wants to know exactly where job ran, where data came from. (part of LAN discussion -- can you tell which LAN you're on)
   * need replacement for Monalisa: snapshot of sys, num of jobs running; can we plug this into gratia?
   * CG: categorize probs as things that can be fixed, or inherent lack of robustness?  Factoring out the sam problems, you'd get overoptimistic picture; is it how sam interacts with osg?  cause sam works fine locally
   * use SRM completely?  SRM used to fetch data; local scratch space also used; should you pick more sites that have SRM?

Petar  CHARMM
   * HEP physicist not biophys, but wife (ANa) does. Grew out of getting people like her to use the grid
   * group from NIH, and from OSG who provided help and resources
   * molec dynamics run simulations
   * protein folding and conformational changes leading to functional changes.  require MD on long scales (many micro seconds, run for long time) queues up to 3 days; how to chop this up?  People run a few jobs for several months on local resources? Use grid to make this faster. 1000 proteins, run for 6 mo to compare to real data, SGLD can fold proteins faster (new method for conform search); they're running standard MD and SGLD (self guided langevin dynamics)
   * water coupling to protein is complicated;  MD includes all phys interactions in the simulations (~20 sim programs on market, CHARMM is popular one) other wellknown ones can also be setup without much work; all use data scripting languages.
   * chopped into 12 jobs or threads?  Then set init conds and equiib, then split into MD and SGLD
   * used PANDA model to submit jobs, several threads with many "waves" ; ran own (Torre's?)  autopilot server on Wis farm
   * each wave would submit next wave, not good enough. Wrote daemon to check if metadata from prev jobs have arrived, when they do, submits next wave of jobs
   * problems: globus-url-copy wasn't always available (CG: if WN has outgoing conn, should have this; else, goc ticket) they distr with charmm exec.
   * globus-url-copy could corrupt later waves
   * their job subm daemon is single pt of failure; got lulled into sense of security (good); didn't catch problem (bad)
   * have 3 more threads to complete; have all data are now preparing publication
   * project was pretty easy, understand failure modes, wait times short, hands-off and trouble-free
   * interesting to univ groups who could use this.

---++ THursday after lunch
Ruth representing Alain VDT
   * vdt composition driven by stakeholders
   * 

NanoHub, Steve Clark
   * nanohub is online simulations, ~50 tools (simul apps) run within browser; 15% exec jobs on osg or tg; (50 more apps in pipeline, many good for osg)
   * take code, wrap it using Rappture, get your interface
   * interactive mode throuth rappture interface (upto 5 hrs) or command line (can run for weeks)
   * mpi go to tg, serial to osg
   * biomoca : 9 mo study 7/06 to 3/07; ~190k jobs subm, 179k completed on OSG, 7 sites, (efficiency higher on TG, but latency longer).
   * "hope to publish papers"  ion channel (currently not on osg, but is candidate to do)
   * lessons learned: now 92 osg sites claim to support nanohub, about 1/2 fail
   * nanowire (another app) flagship tool for demo grid computing, runs mpi or not; all serial jobs going to osg, parallel to tg
   * pick randomly from 6-7 sites
   * nanohub uses one vo cert for everybody; only nominal chain of trust for who gets to join VO. Sites can't tell who ran each job. The vo logs it, but the site can't. Ruth said we'd do audit of nanohub to show site can get that info upon request.  KC: on his list to do after next version of SAZ.
   * repositories not open to public; no good provisions yet for prohibiting trojans;  
   * who takes responsib for security: start with Steve C

LIGO Britta  runs ligo workflows on OSG
   * runs for source compact binaries
   * use data from three different detectors
   * too many fork jobs (via vds or pegasus) causes denial of service problem (Steve Timm, Karan, Britta, Abhishek) question about overhead for mult jobs
   * large data set is biggest problem

SDSS and DES, Nikolay K.
   * 1000s of jobs w same executables, diff parameters

Wayne Betts, STAR
   * run sim, reco, user analysis; one simul user to submit all sim jobs; reco: 
   * working towards for srm sites, to stage data from wn to local srm; then wn doesn't need outward network connection (doug o)
   * want to do one time big transfer of package

Rob Q -- GOC
   * cemon/bdii -- most current info https://is.grid.iu.edu

Anne -- doc
   * some ideas from people
   * organize in 8 ways (by role, by function, etc)
   * not clear why some info in web, some in twiki -- needs clarification
   * I asked people to help fill in the VirtualOrganizations/VOInfo page
   * discussion about search -- just use google?  mixed opinions
   * nanohub(purdue) has wiki integrated into web site and own custom search
   * avoid redundancy -- old/obsolete/redundant things must die!
   * add "install client guide" to doc hub, right column; should be both in enduser and admin
   * idea from Kevin Colby about tagging pages by role, function, whatever and pull pages in that way (not full tagging for every page); more work up front on the server end.  Talk to Rob Q and Forrest about that
   * from Fri afternoon: OSG can provide doc on the scale at which condor-g can handle file transfers in/out

Chris -
   * get VirtualOrganizations/VOInfo support center to run this test, they can present to their user comm what they've tested and what results are. (KB)
   * big spectrum of who needs to test what
   * site verification project overlaps some (RQ); want site admin to run this.  Trying to put infrastructure in place to allow right parties to run
   * application owner vs VirtualOrganizations/VOInfo support center -- for umbrella VO; want users to plug their own probe into the infrastructure
   * for me: make sure whatever happens with these probes gets put in doc pages appropriately
   * for v2, make customized dashboard (longterm)
   * is an api, needs doc


Levente -- STAR  monitoring while job is running
   * star unified meta scheduler SUMS
   * almost-automatic resubmit: all gets generated, but user has to say "go"
   * simple wrappers
   * doesn't think it will be adopted by many vo's, but works well for them

---++ Friday morning
---+++ Storage
Abhishek -- place, use, retrieve data on OSG resources
   * takes almost a dozen nodes to install all the server components -- wow (beyond scope of this talk)
   * legacy storage ($OSG_APP etc.) inherited from Grid3 days
   * in talk he has specifics on all these directories
   * discussion about how and when osg_wn_tmp can/should get cleaned out by site; FKW says it's under discussion; AK and FKW: job should not clean everyting up in case it's a shared account. MD: but isn't batch slot unique: depends on jobmanager. Condor does, PBS doesn't; don't necessarily get own batch slot.  FKW: many sites still have to start giving each person uid.  AK: so make your own directory. Move discussion to sysadmin meeting
   * on osg_data page: "fork at gram host": do gridftp or do batch job (use push or pull) (FKW)
   * condor-g should be restricted to files under 1G for head node overloading reasons. Out is more of problem than in.  Is confusing to users to say "use condor, but not for transfers".   Neha: where documented? fkw: probably nowhere; to limit size at site, put own wrappers around what rest of infra does and what you do.  Haven't reached agreement about how best to do.  CG: FWK: known that it needs change
   * SRM: usage is very simple even though does lots of virtual stuff. Just use srmcp command properly. As simple as "cp".  Should work on all sites; who verifies it works on a site? FKW: ticketable offense. GO: wLCG is testing these implementations.  AR: version of srmcp on a WN at a site should be stable; come from OSG stack.  On client, all you need is java and OSG client; KC: current version of client, you may have to choose which java version to point to; shouldn't have to install java separately; fixed in 0_8_0
   * srm-dcache; user in principle contacts only srm server at a site
   * pnfs divides logical namespace (left) from phys (right on slide); typical sites 100 pools
   * he says ready-to-deploy in VDT (but not the CE stack?  -- missed this)
   * use your own or need to negotiate yourself with sites; need list of who to call at each site; FKW: hope it to resolve this; in princ the hooks are there to do via Voms to gums to storage -- not exercised yet.
   * to use own, need to become expert, need lots of storage, need to become site admin
   * diff storage elements besides dcache can implement srm, e.g., castor...
   * is there simple light-weight -- can install on one node; but if only a few TB is it worth it.  Ans: yes, but not supported by OSG (some more filesys based, not hi-volume).    Question of priorities; so will prob be supported in future; evaluate during y2.
   * question of test/valid srm-dcache on itb from Kent (re small installations)
   * (he'll finish his slides at wrapup -- gone overtime)

Andrew Baranovski -- experiences moving data
   * helps d0 w reprocessing effort; talk about model they've used
   * (I didn't listen much to this one)

Dave Dykstra -- Frontier for CMS and Squid
   * squid is popular http cache program on internet -- (same as used on OSG?)
   * issues discussed about squids on each node: memory and disk, security, 
   * RWG: concern about overload WNs; DD: edge? server too, can be config'd w/ small mem, disk sp not as big a prob, 
   * Igor:  caching useful and nec; squid only does http. all have srm w lots of space; can use srm to be a cache would be great.  so equiv to what you do w squid, but not using http, then need to distrib bandwdth among WNs (lots more discussion) MD: 2 issues, get something consistent on WN, then how do users load the cache, how persistent is it, how often to reload.   RWG: need to move this to sysadmin meeting!  CG: move cache on wn, take load off one squid proxy and off NFS;   long term project...

nanohub: Mike McClennan or Gerhardt Klimac  

Mine - new fulltime OSG security person, new at Fermilab
   * each vo needs a security contact person, : know about VirtualOrganizations/VOInfo policies and privilege allocation
   * sites trust VO, not indiv members
   * some people don't know who their VirtualOrganizations/VOInfo sec person is -- how to make known?
   * VirtualOrganizations/VOInfo must list security and admin contacts  (talk to her about various VirtualOrganizations/VOInfo roles)
   * users need to know: what is cert, how to get cert, how to request memb in vo --- some pieces from VO, some from OSG, some from site; AH to talk with her afterwards  (how much info does AUP give?)
   * to get into reg database, no procedure for site to be vetted. REq to come to ops meeting and fill out reg info; we list sec contact for both sites and vos. Sites have host cert's.  There's minimal policy sites have to agree to.  Question of how diligently RA does it's checking duty.

Tanya -- !VirtualOrganizations/VOInfoMRS
   * VirtualOrganizations/VOInfo is to eliminate distrust and to build trust between VirtualOrganizations/VOInfo and resource provider
   * precautions on sending cert proxy to site; 
   * discussion about roles of VirtualOrganizations/VOInfoMRS -- rep vs vo admin etc.

---+++ Discussion sections
Porting existing apps to grid
   * he has link to porting guide, which has links to IBM's globus-centered red books
   * Analyzing your app: GG: some sw relied on env var's: one thing to do is to strip off these dependencies
   * IS: cdf has wrapper around stuff to solve this prob. When job gets there, same as private pool
   * Application chains: what about threshold for having steering script, so that one job runs mult apps in chain wrt split into workflow. How to make decision.  Karan: configure to go either way, can decide at runtime. (pegasus) per-job overhead important. Nikolay: simpler DAG is better, keep dep's out.
   * other porting issues?  discussion about condor standard universe jobs. FKW: no one's done in prod, only way is to control it yourself: use glide-in
   * data movement: what should osg try to get sites to do? priorities?  Abhishek: encourage users to use srmcp, don't focus on whether user should deploy own srm. CG: still need place to get from or go to.  EB: documneted on how to contact/negotiate with site?   FKW: OSG to verify srmcp and globus-url-copy properly installed, espec those with outgoing connections (these connections must be declared).  IS it part of site test?  OSG agreed that site has to advertize what they have, and be truthful; there aren't stipulations about what they should supply.   Kevin Colby: If simpler way to have local srm server, easier to encourage people to use srmcp client.  Client talks to gridftp also.  Always better off not to use site-local storage, if possible. 
   * AK: skimming of data, take big files, get small files out, go in local SE; SRM has problems with this because of 6-10 sec per-file latency. Users need to know overheads, recommendations.  Know models and limitations of tools. Lot of overhead is in authentication, encryption.  
   * How does CMS do? Always merge files before moving.  Is giant pain.Wkflo such that files get produced then merged.
Break for lunch 12:27 till 1:30.

---++ Friday afternoon
Karan for LIGO workflows, and Pegasus
   * pegasus is a workflow compiler (taken from VDS)

Torre Panda
   * workload management, scalability for Atlas in US.
   * making data handling indep of Atlas so that other vos don't have to put layer on top like CHARMM did
   * how many jobs per glide-in, can keep pressure on, may run only one at a time or more

Igor -- glidein and gLexec

Abhishek: finish storage discussion
   * fkw: resolve by 1.0 to have info serv saying dcap avail, and also where it is (currently a hole)
   * how to get opportunistic storage?  fkw: somebody made large request to council, council beat sites into submission, success story!  7TB at Fermilab; 

Wrap-up items
   * 


-- Main.AnneHeavey - 26 Jul 2007
