%META:TOPICINFO{author="ToddTannenbaum" date="1209764012" format="1.1" version="1.2"}%
%META:TOPICPARENT{name="CreateNewReport"}%
<!-- Complete the template. You can either use TWiki style, copy in plain text, or put in HTML. -->
<!-- IF COPYING IN HTML: Please only include the sections between your BODY tags. -->

---+!! 2008Q1 PI Quarterly Report 

%STARTINCLUDE%

---++  <nop>University of Wisconsin - Madison
%EDITTHIS%
<!-- EDIT BELOW THIS LINE ONLY! -->

---+++ Management Activities

Participation in OSG related presentations, workshops -

   1. NSF Cyber-infrastructure and Genomics workshop
   2. DOE Computer Science PIs meeting
   3. EGEE Campus Grid workshop
   4. EGEE, WLCG OSG coordination meeting
   5. Grid policy meeting at OGF
   6. AHM 
   7. Blue print meetings
   8. Several visits to Fermi

---+++ Software Activities

---++++ Middleware Contributions for OSG

In order to improve the ability of OSG sites to gather accounting and auditing information in the face of the increasing use of pilot jobs in OSG, a solution involving integration of GLExec with the condor_procd has been pursued. An initial version of this solution has been made available in version 1.10.0 of the VDT.

The only milestone for this work was to get versions of GLExec and the condor_procd that integrate to support enhanced accounting and auditing for pilot jobs into VDT version 1.10.0. This ensures this software will be made available with the 1.0 release of the OSG software stack. This milestone has been accomplished.

Developing this system has involved adding several new features to the condor_procd and also modifying the GLExec plugin used in OSG to interface with the condor_procd. The GLExec plugin modifications have been made and are maintained by Igor Sfiligoi at FNAL.

Features added to the condor_procd for this work include:

   * The ability to leverage the full configuration language of Condor.  (Previously, the condor_procd received all configuration options via command line arguments.)

   * The ability to leverage the full logging subsystem of Condor. (Previously, the condor_procd relied on a very primitive logging
    mechanism only suitable for debugging by the program's developers.)

   * A helper utility was developed in order to allow external programs (like the GLExec plugin) to use the condor_procd's novel method of using UNIX supplementary group IDs (SGIDs) for keeping track of process families.

   * The condor_procd's functionality was extended to support two new commands, accessible both from the condor_procd's client library and from the procd_ctl command-line tool:
      1. A LIST command that gives a summary view of all processes in a family along with some resource usage information.
      2. A TRACK_BY_GID command that facilitates use of the SGID-based tracking mechanism.

   * A handful of other small changes were additionally required for the condor_procd to integrate with the GLExec plugin.

---++++ VDT

The VDT packaging of dCache was updated to 1.8.0, tested, and released. Many OSG sites have upgraded.

Five updates were made to OSG 0.8. They included security updates (Condor, MySQL, and Syslog-ng).

Major work was accomplished on VDT 1.10.0, which is the basis for the integration testbed testing for OSG 1.0. An update to VDT 1.10.0 will be the basis for OSG 1.0. The release came in early April, but we spent a lot of time work on it during this time period. Features of interest:

   * No longer ship OpenSSL, but use the native SSL.  This allows the operating system vendor or the system administrator to control when and how often to make security-related software updates independently of OSG releases.

   * New Platform support: Debian 4 (for LIGO) and SLES 9.

   * Major update to RSV: bug fixes, new probes, improvements to Condor Cron facility.

   * Major Generic Information Provider update. 

   * Tons of software updates

   * Tons of bug fixes

Staffing: We've been trying to hire a fourth VDT member. So far no success. We're temporarily borrowing someone from the Condor flightworthy group (Alan De Smet), and continuing our hiring process.

Our major focus was, and is, the OSG 1.0 release. 

---+++ Engagement Activities

We are still working with the Economists.  Also trying to work with Zhao Tao to get feff ported to MW so that we can run on OSG.  Finally, spoke with an Astronomy research group @ Wisconsin about running large MPI jobs that simulate thermonuclear reactions -- we are going in the direction of using the SGI Altix machine at Purdue for this activity.

We presented the OSG at a UW campus-wide Computing Research seminar sponsored by the Office of the CIO; attended by approximately 80 faculty and graduate students. 

---+++ Troubleshooting Activities

   * Minor improvements in setting hold reasons for Condor-C and Condor-G

   * Fix condor_preen to not remove MyProxy-related files in spool directory

   * In January, we worked with Parag to diagnose some stuck pre-WS GRAM jobs going from Fermi to Univ. Oklahoma. Part of the problem was a bug in how the GRAM perl module was querying LSF for job status.  We opened Globus Bugzilla ticket 5775 describing the problem.

   * We helped Terrence Martin diagnose some network timeout and Out-of-Memory errors that the gt4 gahp was generating. This was part of his stress testing, submitting several thousand jobs at a time.

   * We worked with Terrence Martin and the Globus team to reduce the memory usage of the gt4 gahp (spurred by the out-of-Memory errors above). We developed some simplifications in how the gt4 gahp uses Globus's java apis that reduced the memory footprint.

   * We helped Sebastien Goasguen at Clemson debug why his Condor-G installation wasn't running.

   * LIGO VO reported occasional loss of stdout/err from gram jobs running on a Condor pool at CIT. The suspected culprit was caching problems in their distributed filesystem (QFS). We suggested some ideas, but couldn't offer a definitive solution.

   * In February, Rob Gardner and Shaowen Wang reported that /bin/date jobs submitted via globus-job-run to a fork jobmanager at UChicago were taking many minutes to run.  We investigated and found two suspected culprits:
      1. A bug in the schedd that can cause scheduler universe jobs to not start until the schedd's next queue scan. Dan fixed this in Condor 7.0.1.
      2. Several hundred old gram state files, which the grid monitor took 2 minutes to process on every scan. The grid monitor is more intelligent about this in Condor 7.0.0.  We also found a couple unrelated problems which we informed them of.

   * During the investigation of the slow jobs at UChicago, we discovered that the VDT-patched version of GRAM can leak Condor user logs if the jobs are removed from the Condor queue by something other than GRAM.  We wrote a patch and gave it to the VDT.

   * We reviewed some proposed changes in how WS GRAM handles Condor user logs. They'll start using one log per job (which can be cleaned up), instead of a single log that can't feasibly be rotated.  We gave it a thumbs-up.

   * In March, Horst Severini reported that SAMGrid jobs were not being matched in the OCHEP cluster at UOklahoma. The problem was a bug with NEGOTIATOR_MATCHLIST_CACHING that was fixed in the Condor 6.8 series. They were running Condor 6.8.0.

   * In late February, Xin Zhao report very slow submission rates on a couple Condor-G machines at BNL. The problem was that they had a large number of jobs (8000 submitted in 4 hours) but hadn't set the gridmanager's throttle to handle that number of jobs efficiently.

   * In March, Jay Packard report a lot of stuck jobs on a Condor-G machine at BNL. They were using a VOMS-extended proxy where the VOMS extension had a much shorter lifetime than the main proxy. When the VOMS extension expired, they refreshed it, but the new main proxy had the same expiration time (which wasn't passed yet). So Condor thought the proxy was unchanged and continued to use the old one, which was cached in memory.

   * In March, Steve Timm reported large numbers of failed Condor-G jobs. The main cause was determined to be a port-range that was too small on one or two gatekeepers. After that was fixed, a small rate of failed jobs continued. We proposed swapping in Condor-G binaries with extra debugging statements to narrow down the cause of the failure. Steve didn't want to use those on their production systems. He proposed that first he reproduce the problem on a test machine. 

%STOPINCLUDE%



%META:TOPICMOVED{by="ForrestChristian" date="1173993132" from="Sandbox.WebTopicTemplateTesting" to="OSGReports.TemplateForReports"}%
