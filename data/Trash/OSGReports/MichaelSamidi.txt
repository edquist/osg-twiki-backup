%META:TOPICINFO{author="MichaelSamidi" date="1164838686" format="1.1" reprev="1.1" version="1.1"}%
---++ Monthly Reports from Michael Samidi

%TOC%

The current OSG WBS is located [[http://projects.fnal.gov/osg/WBS/changeControl/WBSFromProjectPlan.htm][here]]. 

My work falls into this category from the current OSG WBS:
   * 1.3.2.5.2 - Sustained operations of LIGO workflow at UCSD at the level of 25 jobs for one week (2/1/07).

---++ Period: December 2006

---++ Period: November 2006

Moved LIGO data into local cache at UCSD. This local cache is used by Pegasus planner for getting LIGO GWFs, the input data. Normally, the data will be transferred from LIGO repository at Caltech to the remote site.  

Added two addiitonal sites for runnig LIGO workflow:
   1. Fermilab, FNAL_GPFARM (voms-proxy-init) 
   2. STAR_BNL

Configured Pegasus planner to use job clustering in order to reduce the scheduler overhead running small jobs at remote site. I had to upgrade the VDS binary executable at remote site because of the compatibility issue and put the binary under $OSG_APP directory. We're still experimenting with this feature at osg-gw-2_t2_ucsd_edu (UCSD) and OSG_LIGO_PSU production clusters using 25, 50, 75, 100, 500 and 1000 clustered jobs per DAG node.

---++ Period: October 2006

Completed running LIGO workflow containing ~80,000 DAG nodes with a minimum of 700 GB disk space at UCSD production cluster (osg-gw-2_t2_ucsd_edu).

Completed the same workflow at the following sites (limited disk space, < 150 GB):
   1. Purdue_ITaP
   2. UWMilwaukee

Added the following features into LIGO workflow planner for running a workflow of ~80,000 DAG nodes at several OSG production clusters with limited disk space (<150 GB):
  
   1. N arbitrary workflow partitioning
   2. removal of input GWFs when there is no data dependency
 
-- Main.MichaelSamidi - 29 Nov 2006
