%META:TOPICINFO{author="KyleGross" date="1328026232" format="1.1" version="1.86"}%
%META:TOPICPARENT{name="OSGIndividualEffortReports"}%
My work falls into these categories from the current OSG WBS (1 FTE) :

* Integrate and Support Workflow management for LIGO

* Integrate work of the LIGO Data Analysis teams

* Port Gravitation Wave analyses to the OSG

* LIGO software development and testing

* OSG software testing

* Provide requirements to the Pegasus Planner team and test and integrate new functionality

* Support the activities of the OSG-Trash/Trash/Trash/Extensions efforts.

---+ Monthly Reports from %SPACEOUT{ %TOPIC% }%

%TOC%

The current OSG WBS is located [[http://projects.fnal.gov/osg/WBS/changeControl/WBSFromProjectPlan.htm][here]].

---++ Period: April 2007

* Visit to HR.

* Visit to International Scholarship Center.

* Obtain keys to office and the building.

* Obtain access to libraries.

* Gather signatures and get briefd on LIGO policies, Campus security etc.

* Obtain computer account.

* Obtain grid cert.

* Initiate grid proxy.

* Learn how to log onto submit host.

* Attempt to run a job on OSG.

* Attend PEGASUS, DASWG and VDT telecom.

* Study material on Condor/Condor G, Globus and OSG.

* Meeting with Duncan Brown: Introduction to inspiral analysis.

* Introduction to VI editor: wrote RSL script in VI and executed it on OSG.

* Attended Pegasus and DASWG telecons.

* Run a workflow using the LIGO WorkFlowPlanner.

* Run simple jobs on OSG using Pegasus.

* *OSG tutorial*:

   * 
      * Grid intro. 

   * 
      * Network Primer. 

   * 
      * Grid Networks. 

   * 
      * Grid Security. 

   * 
      * Job Management. 

   * 
      * Data Management. 

   * 
      * Lab 1, 2, 3, 4. 

* *OSG Lab 3 review: Grid Resources and Job management*

   * 
      * Wrote simple shell script. 

   * 
      * Wrote submit file, dag file. 

   * 
      * Submitted jobs to OSG sites using condor_submit, condor_submit_dag. 

   * 
      * Monotored jobs using condor_q and tailsites.xml and tc.data files. 

* *Running "NANOHIPE" on OSG sites*:

   * 
      * Used gencdag to generate dag files from dax files. 

   * 
      * Tried to run nanohipe on several OSG and OSG-ITB sites with partial success. 

   * 
      * Tutorial on sites.xml and tc.data files with D. Meyers. 

   * 
      * Generated sites.xml and tc.data files_ 

   * 
      * Learned about and applied cleanup. 

* Attempt to run full Hipe on OSG UWMilwaukee (failed, maybe due to a problem at that site).

* Running full Hipe on OSG_LIGO_PSU (no failed jobs yet, looks promising).

* Learned about cleanup.

* Started error analysis of failed jobs.

* Read VDS properties documentation.

* Read LSC Inspiral Search Pipline documentation.

* Familiarization with MONalisa.

* Get TWiki account.

* Familiarization with TWiki.

* Send weekly agenda for LIGO_PEGASUS telecon.

* Attend International Scholars Services orientati.

* Run NanoHipe on FNAL_GPFARM (successful), on FNAL_FERMIGRID (fail).

* Error analysis of Failure on FERMIGRID (with Steven Timm).

* Inspection of results of NanoHipe run on GPFARM.

* Getting familiar with TWiki, editing on TWiki.

* Reading Condor online material in preparation for Condor Week 2007.

* Attend Condor Week 2007.

* Learning to use monALISA to monitor OSG jobs.

---++ Period: May 2007
 * write agenda for weekly LIGO_Pegasus telecom

* attend weekly LIGO_Pegasus and DASWG telecom

* write minutes for LIGO_Pegasus telecom

* Configure latest CVS version of lal, lalapps and glue in my home directory on ldas-pcdev1 and ldas-pcdev2.

* Generate dag and dax using h(t) data set (hipe).

* Generate dag and dax using small h(t) data set (nano-hipe).

* Attempt to run h(t) nano-hipe usind .dag on ldas (in progress).

* Attempt to run h(t) nano-hipe using .dax and Pegasus on OSG site (in progress).

* Visit to ISI for a day long hands-on tutorial

* generated 363 job workflow with HEAD version of lal, lalapps, glue

* attempt torun workflow on ldas-grid and OSG_LIGO_PSU (to long)

* generated 363 job shorter workflow (mini-hipe) by adjusting parameter space of template bank

* generated mini-hipe with TAG version of lal, lalapps, glue

* successfully ran mini-hipe on ldas-grid, OSG_LIGO_PSU and LIGO_CIT_ITB

* checked operability on FERMIGRID after encountered problem (successful)

* attempt to use Pegasus vds-plan, vds-run rather than gencdag, condor_submit_dag with the aim to conduct depth-first analysis on LIGO WF (failed due to missing mysql and perl dbi at CIT, problem will be solved with the Pegasus 2.0 release)

* Attempt to run small WF WITH dynamic cleanup on OSG sites (failed if the remote site had not updated vds version to 1.48)

* generated WFs of different sizes and run them successfully on ldas-grid (local cluster)

* had cert DN added to PSU and UWMilwaukee gridmapfiles with the aim to use local ht data rather than the staged data when running at those sites (this resulted in problems when running on OSG sites)

* trouble shooting: failed jobs on OSG sites :

* discovered disk space issue at LIGO_CIT_ITB

* re-generated sites.xml and tc.data catalogues

* discovered problem with L1 Data transfer

* generated workflow using H1 and H2 data only

* modified properties file

* checked for typos

* WF analysis:

* modify segment files to generate workflows of different node size

* modify template bank parameters to generate workflows of different duration

* run workflows an ldas-grid and LIGO_CIT_ITB

* compare runtime and diskspace usage

---++ Period: June 2007

   * Write monthly OSG report 

   * Write agenda for weekly LIGO-Pegasus conference call 

* Attend weekly LIGO-Pegasus conference call

* Discovered missing data set at PSU

* Dealt with tape arm problem at CIT

* H(t) data set testing:

* Create 10 segment files to produce LIGO workflows of different job size

* Create 10 dax files corresponding to the segment files

* Use gencdag (Pegasus) to create 10 dag files from the dax

* Run on 10 OSG sites and ldas-grid (local CIT cluster)

* Monitor:

* Record disk usage , total run time, job failures, error messages

* Tested OSG Lab 2: Grid security

* Tested H(T) Version 2 (V2) Workflows with different size frame data on OSG sites for run time, disc space usage failure rate and type

* Trouble shooting gencdag error: re-generate WFs

* Trouble shooting mkdir problem: change Condor Version, change workdir location on remote site

* Set up for testing H(T) WFs with different Mass ranges (template bank sizes) on OSG for run time, failure rate and type

* test H(T) data LIGO workflows with different size template banks on OSG sites

* trouble shooting stdout transfer error (was mkdir problem)

* trouble shooting MIT long idle (solved)

   * V3 data analysis on ldas-grid: 

   * 
      * install head vesion of Glue, Lal, Lalapps on ldas-pcdev2 

   * 
      * modify ini file to use V3_L2 compressed data 

   * 
      * generate dax files with V3 data for testing 

   * 
      * generate dag files with V3 data for testing 

   * 
      * test V3 WFs for run-time on ldas-grid 

   * V3 data analysis on OSG: 

   * 
      * V3 data not available via RLS yet 

* waiting for Stuart Anderson to pin V3 data to disk

* waiting for V3 data to be available to Tier 2 sites like PSU

* generate dag files with V3 data for testing

* Continue V2 testing on OSG

---++ Period: July 2007

   * Write monthly OSG report 

   * Write agenda for weekly LIGO-Pegasus telecon 

   * Attend weekly LIGO-Pegasus and DASWG telecon 

   * Request mor disk space at FNAL_FERMIGRID 

   * Run LIGO WFs with V2 data on OSG sites 

   * Run V3 WFs on ldas-grid 

   * Prepare for V3 WF runs on OSG: 

   * 
      * Change .ini file settings 

   * 
      * generate .dax file 

   * 
      * convert .dax to .dag using Pegasus 

   * Start V3 WF runs on OSG sites 

* Prepare OSG_USER meeting presentation

* V3 data testing:

   * 
      * discovered change in segment lists 4 V3 data 

   * 
      * requested and obtained V3 segment lists 

   * 
      * generated new segments for V3 analysis 

   * 
      * bug in pipeline.py, inspiral.py and inspiral_hipe.in was discovered and fixed by Duncan and Drew 

   * 
      * waiting on: bug fix in pegasus-site-converter, tag of new glue/lal/lalapps versions 

* meeting with ISI

   * 
      * Pegasus presentation 

   * 
      * Pegasus hands on tutorial 

   * 
      * converting from VDS to Pegasus 2.0 tutorial 

   * 
      * discussion of LIGO specific changes in Pegasus and Glue 

* Conversion to tag version s5_1yr_lowcbc_20070711:

   * 
      * install glue, lal, lalapps 

   * 
      * change .ini file section condor 

* Conversion to V3 data

   * 
      * Generate new segment lists 

   * 
      * Genate new .dax/.dag files 

   * 
      * Test V3 data WF on ldas-grid 

* Conversion to Pegasus 2.0

   * 
      * Convert sites.xml 

   * 
      * Convert tc.data 

   * 
      * Change bottom of tc.data to updataed tag version 

   * 
      * Convert Property files 

* Debugging: pegasus-get-sites: tc.data not generated correctly-- manual tc.data fix

   * 
      * pegasus-run tailstatd error--not yet solved 

   * 
      * namespace change-- not yet solved 

   * 
      * schema change--not yet solved 

* Running V3 data WF on OSG

   * 
      * sucessful run at LIGO_CIT_ITB and STAR_BNL 

   * 
      * testing small WF on other sites 

* Attend LIGO all hands meeting

* upload presentation on OSG-USERS meeting site

* debugging:

   * 
      * error: gap in data (OSG but not ldas-grid) 

   * 
      * 
         * re-generate dag 
 

   * 
      * 
         * re-submit dag 
 

   * 
      * 
         * re-run on ldas-grid 
 

   * 
      * chmod failure at UWMilwaukee and UFlorida_HPC 

   * 
      * 
         * re-submit jobs 
 

* attend OSG-USERS meeting at Fermilab

* run 24 LIGO WFs at the same time at 12 (LIGO stable) OSG sites

---++ Period: August 2007

* Write monthly OSG report

* Write agenda for weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus and DASWG telecon

* Find additional OSG sites to run on:

   * 
      * add MIT entry to sites.xml and tc.data 

   * 
      * test 18 new sites (using OSG Production VO Support Matrix): 

   * 
      * 
         * successful 9: UCSDT2, GROW_PROD, MIT_CMS, AGLT2, UTA_DPCC, CIT_CMS_T2, OU_OCHEP_SWT2, NWICG_NotreDame, SPRACE 
 

   * 
      * 
         * unsuccessful 9: GRASE_CCR_U2, UNM_HPC, IU_ATLAS_TIER2, FIU_PG, ASGC_OSG, TTU_ANTAEUS, SMU_PHYS, NERSC_Davinci, NERSC_PDSF 
 

*stdout problem:

   * 
      * change condor version to stable release 6.8.3 

   * 
      * test at 12 sites, problem persists at: STAR_BNL, OSG_LIGO_MIT, UFlorida_PG, UWMilwaukee, TTU_ANTAEUS, ASGC_OSG, FIU_PG 

   * 
      * 
         * Unable to run at: UWMilwaukee, ASGC_OSG, FIU_PG, TTU_ANTAEUS 
 

* Other Problems:

   * 
      * BNL_ATLAS_1, BNL_ATLAS_2 long idles, even small WFs take extremely long to finish 

   * 
      * UFlorida_IHEPA, UFlorida_PG disk space problems, only small WFs can be submitted 

* Familiarization with Gratia

* stdtout race condition testing on GROW_PROD:

   * 
      * test 1: origional version of both Globus and CondorG 

   * 
      * test 2 : patched Globus, origional CondorG 

   * 
      * test 3: origional Globus, patched CondorG 

* Run LIGO WFs on local cluster ldas-grid successfully using Pegasus:

   * 
      * create ldas-grid entry in sites.xml 

   * 
      * create property file for ldas-grid 

   * 
      * create dag from dax using Pegasus 2.0 ( inc sites.xml, property file) 

   * 
      * submit WF via OSG submit machine osg-itb-se.ligo.caltech.edu 

* Investigate gridftp error when transferring files from PSU (8 tests)

* Investigate 'gap in data' error in several LIGO WFs (contacted Duncan Borwn and Scott Koranda)

* attend weekly OSG-USERS telecon

* Run 2 in attempt to reach September milestone:

   * 
      * Max running jobs: 563, max idling jobs: 1063 

   * 
      * Run 48 WF on 17 sites 

* Trouble shooting stdout race condition:

   * 
      * Test 3: with Condor_G patch, without Globus patch, success 

   * 
      * Test 4: with Condor_G patch, with Globus patch, success 

   * 
      * Resolve: Condor_G and Globus patch will be implemented 

* Trouble shooting gap in data problem

   * 
      * Run dag on ldas-grid: no gap in data 

   * 
      * Run Pegasus WF on ldas-grid: gap in Data at lalpps_tmpltbank job 

   * 
      * Run tmpltbank job seperately: no gap in data 

* Trouble shooting gridftp error at PSU and UWMilwaukee: load too heavy on server?

* Bash shell scripting to automate WF submission and monitoring

* Run 3 in attempt to reach September milestone:

   * 
      * Max running jobs: 807, max idling jobs: 1889 

   * 
      * Run 84 WF on 21 sites 

* Write agenda for weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG and OSG-USERS telecon

* Monitor test run 3

* Run on about 20 sites

* Max No running jobs: 807

* MAx No of idling jobs 1889

* Open GOC tickets: sites that claim to support LIGO but are not:

   * 
      * 1) FIU_PG 2) GRASE-BINGHAMTON 3) HAMPTONU 4) MCGILL_HEP 5 ) NERSC-Davinci 6) OU_OCHEP_SWT2 

* Add 4 new sites to sites.xml: 1) ORNL_NSTG 2) Purdue_RCAC 3) GLOW 4) GLOW_CMS

* run on 4 new sites: jobs held at all 4

* Run 4--weekend run: no improvement on No of running jobs

* Run 5--weekrun with job submission staggered: no improvement on No of running jobs

* trouble shooting: gap in data

* trouble shooting gridftp error at PSU and UWM

* develop shell script for remote WF submission and monitoring

* achieved September milestone of running 1000 jobs on OSG

---++ Period: September 2007

* Write monthly OSG report

* Write agenda for weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG and OSG-USERS telecon

* Running on new OSG sites:

* Obtained NERSC account

* MCGILL_HEP does not support LIGO

* GRASE_BINGHAMPTON not a OSG rescource anymore

* HAMPTON_U waiting for response to GOC ticket

* In Progress: attempt to run on OU_OCHEP_SWT2 and FIU_PG

* ITB testing of OSG 0.7.0

* write properties files for 12 ITB sites

* generate dax/dag

   * 
      * test on 11 ITB sites (UCSDT2_ITB1 not in sites.xml or VORS, contacted sys admin) 

* make changes to the python script pipleline.py to make the dax compliant with Pegasus 2.0 (reinstall glue)

* gap in data problem:

   * 
      * use patched version of tmpltbank.c and inspiral.c (reinstall lal) 

   * 
      * regenerate WF 

   * 
      * test on OSG site (in progress) 

* ITB CE OSG 0.7.0 testing continued: TTU_TESTWULF, LIGO_CIT_ITB done, nsf-lite problem at CIT_ITB_1, remaining 3 sites on progress

* Gap in data fix testing continued

* New OSG sites testing continued: NERSC_Davinci, NERSC_PDSF, FIU_PG, OU_OUHEP_SWT2:OU_OUHEP_SWT2 running, idling jobs at NERSC, can't create dir at FIU_PG

* Familiarization with ReSS: read on-line documentation, contacted Gabriele

* Familiarization with SRM: read on-line documentation, contacted Abhishek

* ITB CE OSG 0.7.0 testing continued: FNAL_FERMIGRID_ITB,TTU_TESTWULF, LIGO_CIT_ITB done, nsf-lite problem at CIT_ITB_1, UCSDT2_ITB1 continuously fails with same error message (see below), jobs idle at remaining 3 sites

* ITB SE OSG 0.7.0 testing: started srmcp test on ITB SE sites in question to verify that the LIGO VO is supported

* Trouble shooting (with Karan Vahi) UCSD_T2_ITB1: message in .err: main job specification empty or invalid

* Troubleshooring (with Duncan Brown) pegasus-plan-error regarding filename double nesting

* Continue testing on OSG sites: run-time testing of LIGO workflows of different job size/job length

* Problems running on FIUP_PG: correspondence with Micha Niskin

* Problems running on NERSC: resolved

* Finish Mass testing on some OSG sites: GPFARM, FERNMIGRID, PSU, LIGO_CIT_ITB, UWMilwaukee

* Test new Pegasus feature: new format of tc.data

* Running Einstein at Home on the OSG:

   * 
      * Prepare presentation: Running simple jobs on the OSG 

   * 
      * Study documentation on Einstein at Home 

   * 
      * Study BOINC documentataion 

   * 
      * Familiarization with D-GRID 

   * 
      * Read material on Glide Ins 

   * 
      * Attend telecom with Warren Anderson, Thoms Radke and Robert Engel 

---++ Period: October 2007

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG and OSG-USERS telecon

   * *Week 1* 

* Opened GOC ticket FIU_PG

* Study SQUID documentataion

* Learn how to run extensible VO-centric site testing kit

* Attend OSG-OPS telecon: brief summary of LIGO activity on OSG

* Prepare and send out ppt presentation to Thomas Radke and Robert Engel (D-Grid): Using Pegasus to submit LIGO workflows

* Trouble shooting tmpltbank job fail at CIT_CMS_T2 and UCSDT2

* Continue running LIGO test workflow on OSG

   * *Week 2* 

* Trouble shooting pegasus-plan-error: resolved through implementation of glue patch by Duncan Brown, tested 2 workflows

* Trouble shooting tmpltbank job fail at CIT_CMS_T2: nsf-lite problem

* Trouble shooting tmpltbank job fail at UCSDT2: no read or execute permission

* Trouble shooting UC_ITB: work with Suchandra * Trouble shooting pegasus-plan-error: resolved through implementation of glue patch by Duncan Brown, tested 2 workflows

* Trouble shooting tmpltbank job fail at CIT_CMS_T2: nsf-lite problem

* Trouble shooting tmpltbank job fail at UCSDT2: no read or execute permission

* Trouble shooting UC_ITB: work with Suchandra

* Trouble shooting FIU_PG: involving Condor people

* Einstein at Home on the OSG:

   * 
      * Reading material: 'Expanding the Reach of Grid Computing: Combining Globus- and BOINC-Based Systems', 'Developing Distributed Computing Solutions Combining Grid Computing and Public Computing' 

   * 
      * Joined BOINC-Proj and BOINC-Dev mailing lists 

   * 
      * Obtain information about BOINC Superhost idea and Caching Proxy 

* Write agenda and minutes for weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG and OSG-USERS telecon

* Einstein at Home:

   * 
      * Joined Pulsar Group mailing list 

   * 
      * Prepare for and attend second D-Grid telecon scheduled Friday 10/19/07 

   * 
      * Inquiry about OSG general policies (Suchndra) concerning: wall time limits, preemtion, network access 

* OSG-ITB testing

   * 
      * Read documentation on WS Gram job submission, contact Suchandra 

* OSG-Prod testing

   * 
      * Run workflows on FNAL_Fermigrid, FNAL_GPFARM, OSG_LIGO_MIT, UFlorida_HPC, UCSDT2, FIU_PG 

* Local Cluster testing:

   * 
      * Run two large wfs on ldas-grid 

   * *Week 3* 

* Einstein at Home

   * 
      * Attend second Einstein at Home telecon 

   * 
      * Familiarization with use of VO-centric test kit 

   * 
      * Read literature: BDII, ReSS, LDAP 

   * 
      * Obtain site specific information: AGLT2, BNL_ATLAS_1, BNL_ATLAS_2, STAR_BNL, OSG_LIGO_PSU, OSG_LIGO_MIT, UFlorida_PG, UFlorida_IHEPA, UFlorida_HPC, SPRACE, NERSC_PDSF, NERSC_Davinci, GROW_PROD 

* OSG-PROD testing

   * 
      * Run workflows on FNAL_Fermigrid, OSG_LIGO_MIT, UFlorida_HPC 

   * 
      * Attempt to populate MySQL data base set up by ISI for kickstart loging purposes 

* OSG-ITB

   * 
      * Trouble shooting UC_ITB with Suchandra 

   * 
      * Attempt WS-GRAM submission with Pegasus on FNAL_FERMIGRID_ITB and BNL_ITB_Test1 

   * *Week 4* 

* OSG-PROD activities

   * 
      * Finished Mass Range and Segment testing 

   * 
      * Ran successfully on 3 new sites: Grase_CCR_U2, GLOW, GLOW_CMS 

   * 
      * Repeated tmpltbank (job 8 ) fail at new site ORNL_NSTG and UFlorida_HPC (after $DATA location change), .out, .err emp 

   * 
      * Trouble shooting FIU_PG: condor people involved 

* OSG-ITB activities

   * 
      * Trouble shooting UC_ITB: 3 IM sessions with Suchandra 

   * 
      * Successful WS-GRAM submission to BNL_ITB_Test1, still testing FNAL_FERMIGRID_ITB, GRAM error 47 at CIT_ITB_1 

* Einstein at Home activities

   * 
      * Site information table almost complete, still looking for some site admin contact information 

   * 
      * Sent message to GOC for missing information 

* Open GOC ticket: LIGO can't run at Purdue_Physics, Purdue_Caesar, Purdue_LEAR, Purdue_RCAC

---++ Period: November 2007

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development and OSG-USERS telecon

   * *Week 1* 

* write submission file for submission of EaH as standalone application on OSG

* attempt to run EaH as standalone on AGLT2, NERSC_PDSF (fail)

* write dax for submission of EaH as standalone application on OSG

* convert dax to dag

* attempt to run dag via condor_submit on OSG (fail)

* 11/06, 11/07 attend Inspiral Workshop at UWMilwaukee and give a presentation

* 11/05, 11/08 travel to/from Milwaukee

   * *Week 2* 

* Einstein at Home activity

   * 
      * Trouble shooting: error transfering executables: solved 

   * 
      * Attempt to run EaH application on OSG, fail: Post Script fail status 1 

* OSG-PROD activity

   * 
      * Opened GOC ticket for Purdue_Lear, Purdue_Caesar, Purdue_Physics 

   * 
      * Troubleshooting FIU_PG, GOC ticket still open, UFlorida_HPC--Karan working on secondary stageing 

   * 
      * Testing WS-GRAM submission at AGLT2 (fail), FERMIGRID (fail), OSG_LIGO_MIT (in progress) 

* LDG (LIGO Data Grid)-OSG activity

   * 
      * Attempt to submit workflows remotely to LDG using Pegasus: CIT successful 

   * 
      * Trash/Troubleshooting LLO, LHO, NEMO: LLO, LHO: GRAM jobmanager and globus gatekeeper pointing to wrong condor version, fixed 

   * *Week 3 and Week 4* 

* Einstein at Home activity

   * 
      * Runninng E@H data download on OSG via condor_submit 

   * 
      * Running E@H binary on OSG via condor_submit 

* OSG-PROD activity

   * 
      * New OSG site for LIGO: Purdue_RCAC 

   * 
      * Open GOC tickets: Purdue_Lear--only supports CMS members, Purdue_Caesar--attempt to run fails, Purdue_Physics--trouble shooting with sys admin 

   * 
      * Troubleshooting FIU_PG, CIT_CMS_T2: Patch on gatekeeper (Gridmanager), CIT_CMS_T2, FIU_PG runs successful 

   * 
      * UFlorida_HPC--Karan working on secondary stageing 

   * 
      * Testing WS-GRAM submission testing at UWMilwaukee: in progress 

* LDG (LIGO Data Grid)-OSG activity

   * 
      * Attempt to submit workflows remotely to LDG using Pegasus: CIT, LHO successful 

   * 
      * Trash/Troubleshooting NEMO: troubleshooting with Scott Koranda 

* Write OSG news letter article summarizing Inspiral workshop at UWMilwaukee

---++ Period: December 2007

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development and OSG-USERS telecon

   * *Week 1* 

* Einstein at Home activity

   * 
      * Runninng E@H as standalone application (without BOINC) with Pegasus work-flow planner on OSG sites 

   * 
      * 
         * Successful: NERSC_PDSF, CIT_CMS_T2 
 

   * 
      * 
         * Unsuccessful: AGLT2, UFlorida_PG, UFlorida_IHEPA, TTU_ANTAEUS 
 

* OSG-PROD activity

   * 
      * New OSG site for LIGO: Purdue_Physics 

   * 
      * Closed GOC tickets: FIU_PG, CIT_CMS_T2 after gatekeeper patch (for Gridmanager) 

   * 
      * UWMilwaukee: problem after upgrade to OSG 0.8.0, trouble shooting with Brian Moe, Scott Koranda 

   * 
      * Testing WS-GRAM submission testing at UWMilwaukee: in progress 

* LDG (LIGO Data Grid)-OSG activity

   * 
      * Trash/Troubleshooting NEMO: troubleshooting with Scott Koranda 

* Self teaching: Basic Python programming

* *Week 2*

* Einstein at Home activity

   * 
      * Third telecon discussing progress and strategies 

   * 
      * D-Grid collaborators able to run at UWM via fork manager 

   * 
      * Waiting for Condor upgrade at UWM to test and debug 

* OSG-PROD activity

   * 
      * Trouble shooting UWMilwaukee: problem after upgrade to OSG 0.8.0 

   * 
      * Testing WS-GRAM submission testing at UWMilwaukee: in progress 

   * 
      * Waiting for MOU approval to start trouble shooting with Karan Vahi at: UFlorid_HPC, UWMilwaukee and other sites 

* LDG (LIGO Data Grid)-OSG activity

   * 
      * Trash/Troubleshooting NEMO: troubleshooting with Scott Koranda 

* Continue to learn Basic Python programming

* Read Pegasus and Condor documentation on Glide Ins

* Writing Documentation:

   * 
      * Running Inspiral Hipe on OSG 

   * 
      * Running Inspiral Hipe on LDG 

   * 
      * Installing GLUE, LAL, LALAPPS 

   * 
      * Generating Inspiral DAG/DAX 

* *Week 3*

* Einstein at Home activity

   * 
      * Waiting for Condor upgrade at UWM to test and debug 

   * 
      * Attempt to run at UWM with CIT_CMS_T2 Glide-ins, fails 

* OSG-PROD activity

   * 
      * Update sites.xml, tc.data for sites that upgraded to OSG 0.8.0 

   * 
      * Waiting for MOU approval to start trouble shooting with Karan Vahi at: UFlorid_HPC, UWMilwaukee and other sites 

   * 
      * SRM: simple srmcp to copy files to/from CIT_CMS_T2 fails, trouble shooting with Ilya 

* LDG (LIGO Data Grid)-OSG activity

   * 
      * Addition of ldg_submit_dax (was condor_submit_dax) to GLUE 

   * 
      * move of ldg-sites.xml and pegasus-properties.bundle into GLUE 

   * 
      * Test runs with ldg-4.5 upgrade: CIT, LHO ok, LLO with third party transfer ok, NEMO fail 

* *Week 4*

* Xmas vacation Activity

   * 
      * Eat a lot of unhealthy food 

   * 
      * Do not exercise under any circumstances 

   * 
      * Watch hours of bad TV 

---++ Period: January 2008

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development and OSG-USERS telecon

* *Week 1 and Week 2*

* Participate in Gratia survey

* LIGO activity on OSG

   * 
      * gridftp error at PSU, trouble shooting with Jeff Minelli 

   * 
      * Pre-WS-GRAM submission at OSG 0.8.0 sites: FNAL_GPFARM, FERMIFGRID,BNL_ATLAS_2, GLOW, CIT_CMS_T2 -- success 

   * 
      * Pre-WS-GRAM submission at OSG 0.8.0 sites: BNL_ATLAS_1, OU_OUHEP_SWT2, TTU_ANTAEUS, UWMilawaukee, STAR_BNL--fail (STAR, TTU ticket with Abhishek) 

   * 
      * WS-GRAM submission at FNAL_GPFARM, OSG_LIGO_MIT -- fail at job1 --trouble shooting with Fillippo 

   * 
      * SRM and Pegasus: fail, trouble shooting with Karan 

   * 
      * srmcp from/to osg-itb and osg-itb-se successful 

   * 
      * srmcp from/to osg-itb-se/osg-itb and CIT_CMS_T2 (gsiftp-->srm, srm--->gsiftp) successful 

* LIGO activity on ITB

   * 
      * Test run on LIGO_CIT_ITB -- 2 success 

* LDG Activity

   * 
      * Install glue/lal/lalapps on osg-itb-se 

   * 
      * test ldg_submit_dax on cit, lho, llo, uwm_nemo without data transfer (submit machine: osg-itb-se): 

   * 
      * 
         * cit fail at first condor job 
 

   * 
      * 
         * lho successful 
 

   * 
      * 
         * llo idle at job1 
 

   * 
      * 
         * uwm_nemo ldg_submit_dax error 
 

   * 
      * test ldg_submit_dax on cit, lho, llo, uwm_nemo without data transfer (submit machine: ldas-grid): 

   * 
      * 
         * cit successful 
 

   * 
      * 
         * lho ldg_submit_dax error 
 

   * 
      * 
         * llo ldg_submit_dax error 
 

   * 
      * 
         * uwm_nemo ldg_submit_dax error 
 

* *Week 3*

* LIGO Activity on OSG

   * 
      * Upgrade OSG client on osg-itb-se (0.8.0) 

   * 
      * OSG 0.8.0 testing: FAIL TTU_ANTAEUS, STAR_BNL (GOC tickets opened), SUCCESS at NotreDame, BNL_ATLAS_1 

   * 
      * WS-GRAM testing: FAIL LIGO_CIT_ITB, OSG_LIGO_MIT, NotreDame (trouble shooting MIT, ND), SUCCESS at BNL_ATLAS_1 

   * 
      * SRMCP working between osg-itb-se and srm server at CIT_CMS_T2 

   * 
      * SRM with Pegasus work-flow planner: efforts on hold, waiting for Pegasus wrapper script and MOU approval between LIGO and Pegasus 

   * 
      * Glide-Ins with Pegasus: work in progress 

* *Week 4*

* Register for OSG All Hands Consortium meeting

* Write agenda for LIGO-Pegasus F2F meeting

* Attend LIGO All Hands safety meeting

* Write documentation on how to upgrade OSG client to 0.8.0 as non-root

* LIGO Activity on OSG

   * 
      * OSG 0.8.0 testing: GPFARM with 50 slots for LIGO running 

   * 
      * WS-GRAM submission GPFARM success, AGLT2 fail 

   * 
      * UWM FAIL, trouble shooting with Brian Moe 

   * 
      * Trouble shooting TTU_ANTAEUS, STAR_BNL with GOC 

* LIGO Activity on LDG

   * 
      * Attempt to run with 32 bit binaries: lho, llo fail: ldg_submit_dax error; cit, uwm_nemo can submit but cit idling, uwm_nemo fail at transfer job for binaries 

   * 
      * Attempt to run with 64 bit binaries: uwm_nemo, cit fail: ldg_submit_dax error; llo idling at job1, lho successful 

   * 
      * Attempt use of Glide-Ins on LDG: Fail 

   * 
      * Upgrade LIGO software Glue/Lal/Lalapps 

   * 
      * Test with new sofware 

---++ Period: February 2008

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development, OSG-USERS and OSG-INTEGRATION telecons

   * *Week 1* 

* Preparation for LIGO-Pegasus F2F meeting

   * 
      * Participate in creating the agenda 

   * 
      * Update Wiki information 

   * 
      * Write ppt presentation: LIGO Inspiral work-flows on OSG 

   * 
      * Write ppt presentation: Pegasus and LDG 

   * 
      * Write report on SRM and WS-GRAM effort on OSG 

* LIGO Activity on OSG

   * 
      * GOC Ticket STAR_BNL closed: Condor upgrade on gatekeeper was needed 

   * 
      * Trouble shooting with GOC TTU_ANTAUES: $OSG_DATA instead of $DATA ? 

   * 
      * Troubel shooting UWMilwaukee 

* LIGO Activity on LDG

   * 
      * LLO issue resolved: port 2119 was closed 

   * 
      * ldg_submit_dax (Python script to automate remote submission on LDG) testing on LDG: 

now running at cit, lho, llo, not running at uwm_nemo

* Basic Python Programming

   * *Week 2* 

* Attend LIGO-Pegasus F2F meeting

* Take minutes and post Minutes of LIGO-Pegasus meeting on Wiki

   * 
      * [[http://www.ligo.caltech.edu/~bdaudert/F2F_Minutes.doc F2F_Minutes.doc]] 

* LIGO Activity on OSG

   * 
      * Trouble shooting TTU_ANTAUES (changes to Pegasus site catalogue) 

   * 
      * Credential issue: Invalid CRL: The available CRL has expired, working with Kent to resolve 

* LIGO Activity on LDG

   * 
      * ldg_submit_dax (Python scrip) changes: 

   * 
      * 
         * add -t option 
 

   * 
      * 
         * change results dir, change submit dir 
 

   * 
      * Basic Python Programming 

* Attend ihope meeting 1 (how to run ihope ) with LIGO scientists

   * *Week 3* 

* Attend ihope meeting 2 (plotting) with LIGO scientists

* Trouble shooting with Kent CRL credential issue on local submit host (update VDT, update credentials)

* Trouble shooting with Kent and Gaurang Metha end of file error message during data transfer

* Python Programming

* Meeting with LIGO grad student Drew Keppel to discuss data size reduction of Inspiral Analysis for OSG runs

* Trouble shooting with: JAVA_HOME environment variable issue (02-21)--re-install osg-client

   * *Week 4* 

* Attend ihope meeting 3 with LIGO scientists

* Trash/Troubleshooting TTU-ANTAEUS

* Install ihope software

* Add --nosubmit option to ldg_submit_dax

* Study ihope Python Code

* Attempt to generate dag with ihope -FAIL (.ini file needs fixing by LIGO scientist)

* Make changes to ihope and other software to enable OSG dag creation with ihope (incorporate ldg_submit_dax)

---++ Period: March 2008

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development, OSG-USERS and OSG-INTEGRATION telecons

   * *Week 1* 

* Attend OSG All Hands meeting in Chapel Hill, NC

* *Week 2 and Week3*

* Vacation/ Visa renewal Germany

   * *Week 4* 

* lalapps_ihope development to enable osg submissions of injection runs

---++ Period: April 2008

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development, OSG-USERS and OSG-INTEGRATION telecons

   * *Week 1* 

* lalapps_ihope development to enable osg submissions of injection runs:

   * 
      * Attempt to run ihope original version on cit -- fail (?) 

   * 
      * Attempt osg submission from pcdev1 -- fail 

   * 
      * JAVA out of memory problem on ldas-grid: set JAVA_HEAPMAX env var, use pcdev1 as submit host 

   * 
      * LIGO software installation problem on osg submithost, fix: install python module numpy 

   * 
      * Run lalapps_inspiral_hipe --dax (LSC_DATAFIND env var had to be set as ldas-cit.ligo.caltech.edu) on osg host, pegasus-plan and submit to UWM ---ok 

* Activity on OSG

   * 
      * test runs at TTU_ANTAEUS, CIT_CMS_T2 ---very long idles 2 days running now ... 

* Activity on LDG

   * 
      * Remote submission via Pegasus to cit, lho, llo, uwm_nemo --- very long idles 2 days running now ... 

   * *Week 2* 

* lalapps_ihope development to enable osg submissions of injection runs:

   * 
      * lalapps_ihope modified to generate playground dag via pegasus to run on osg --ok 

   * 
      * Attempt to run ihope original version on cit -- in progress 

   * 
      * Attempt ihope with osg playground submission from ldas-grid -- fail 

   * 
      * 
         * JAVA error ?? 
 

   * 
      * Attempt ihope osg submission from ldas-pcdev1 -- in fail 

   * 
      * 
         * globus-job-run err: GRAM Job submission failed because the job manager failed to open stderr (error code 74) 
 

* Activity on OSG

   * 
      * change LIGO sofware to 32 bit version, rerun CIT_CMS_T2 (ok), AGLT2 (ok), TTU_ANTEUS (batch jobs fail again) 

   * 
      * Trouble shooting TTU: condor_submit test jobs using jobmanager-lsf (/bin/hostname ok, pegasus/kickstart fail) 

* Activity on LDG

   * 
      * Remote submission via Pegasus to cit, lho, llo, uwm_nemo --- retry with 32 bit binaries 

   * 
      * cit, lho ok, 

   * 
      * llo, uwm_nemo fail 

   * *Week 3* 

* Preparation for VO validation of ITB 0.9 towards OSG 1.0.0

* Activity on OSG

   * 
      * TTU_ANTAEUS trouble shooting with Anand 

* lalapps_ihope development to enable osg submissions of injection runs

   * 
      * trouble shooting --running original ihope dag on LDG 

* E@OSG activity

   * 
      * trouble shooting with Thomas: globusrun-ws fail 

   * *Week 4* 

* Write report on OSG-LIGO activity for DASWG/CBC

* Ihope development - troubleshooting sire failures, attempt to run osg playground

* E@OSG deployment at BNL successful, troubleshooting at UWM

* Meeting with Abhishek, Terence, Frank at UCSD--> E@OSG, SRM

* Preparation for ITB testing-site catalogue, properties files

* Trash/Troubleshooting TTU

---++ Period: May 2008

* Write agenda and minutes of weekly LIGO-Pegasus telecon

* Attend weekly LIGO-Pegasus, DASWG, CBC Code development, OSG-USERS and OSG-INTEGRATION telecons

   * *Week 1* 

* Attend Condor week 2008 at UW Madison

   * *Week 2* 

* Meet with Chad Hanna to discuss LDG-OSG progress and future planning

* Re-test LDG remote submission via ldg_submit_dax

* ihope activity

   * 
      * Run ihope dag without condor compiling the code (sire failures) --> all jobs held 

   * 
      * Run ihope dag without sire jobs --> all jobs held 

   * 
      * Further trouble shooting with condor compiled code in progress 

* E@OSG activity

   * 
      * attempt to deploy and run at UWM--fail 

   * 
      * deploy successful at BNL_ATLAS_1, UWMilwaukee and TTU_ANTAEUS 

* one interactive TTU run succcessful

* ITB-testing LBNL_DSD_ITB in progress--> troubleshooting with Jeff Porter

   * *Week 3* 

* IHOPE activity:

   * 
      * troubleshooting ihope on ldg --> head and tag 32 bit binaries submitted from Dev1 -->datafind fails: 

   * 
      * Fix: export _CONDOR_DAGMAN_AUTO_RESCUE=FALSE 

   * 
      * several meetings with Chad to discuss ihope changes to enable pegasus runs on LDG (OSG) 

* E@OSG activity:

   * 
      * Take on leadership of E@OSG project 

   * 
      * Install missing perl module StatusBar on osg-itb-se, submit about 50 E@H jobs remotely to TTU_ANTAEUS 

   * 
      * 15 succeed, rest fail, troubleshooting in progress 

   * 
      * 1 E@UWM job submitted remotely fails 

   * 
      * 1 E@ATLAS_1 remote submission fails due to permission issues in $DATA (job log stoarge) 

* ITB-testing round 2: Can't run at LBNL, CIT:

   * 
      * nsf-lite: $OSG_LOCATION <-- $OSG_GRID location issue: CE <--> WN locations 

   * *Week 4* 

* Write agenda/minutes and lead OSG User Group telecon

* Update DASWG page OSG-LIGO pages: http://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

* E@OSG activity

* Running E@H application at TTU, BNL_ATLAS_1

* Trobleshooting at UCSDT2, UWMilwaukee

* ITB testing

* Testing Completed on FNAL_FERMIGRID_ITB, BNL_ITB_Test1, UC_ITB, LBNL_DSD_ITB

* Waiting for Condor Collector fix at CIT_ITB_1

* Looked at Pegasus new site catalogue schema

* ITB testing completed

---++ Period: June 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecon

* Write minutes for LIGO-Pegasus telecon

* Attend DASWG, Inspiral Coding, OSG-USERS and INTEGRATION telecons

   * *Week 1* 

* Attend OSG Area coordination telecons

* E@OSG activity

   * 
      * Running E@H application at TTU, BNL_ATLAS_1 

   * 
      * Trobleshooting at UCSDT2, UWMilwaukee 

   * 
      * Request some code changes, bug fixes from Robert/Thomas 

   * 
      * Deploy code at Nebraska, start test runs 

* IHOPE activity:

* Attempts to run playground and inj of an ihope dag at CIT_CMS_T2

* Test Pegasus dynamic cleanup at CIT_CMS_T2

* Update DASWG pages (weekly status report for E@OSG)

   * *Week 2* 

* E@OSG activity

   * 
      * Running steady at ATLAS_1, UNL, TTU, AGLT2 

   * 
      * Updated DASWG project page OSG-LIGO 

   * 
      * Working on script that automates submission/monitoring 

* IHOPE activity

   * 
      * ihope run with playground and one injection run at CIT_CMS_T2 successful (submitted from pcdev1) 

   * 
      * SPEW test on OSG from pcdev1: 

   * 
      * 
         * Successful at CIT_CMS_T2, Nebraska, AGLT2 
 

   * 
      * 
         * Fail at FNAL (FERMIGRID, GPFARM) --> need voms proxy, contacted Steven Timm 
 

   * 
      * 
         * Fail at OSG_LIGO_MIT: tmpltbank jobs--> Post script fail with status 1 = remote application concluded with non-zero exitcode 
 

   * 
      * 
         * Fail at BNL_ATLAS_1: tmpltbank jobs--> POST Script failed with status 5 =invocation record has invalid state, unable to parse 
 

   * 
      * collect OSG site info (contacted Rob Quick for pointer) 

* Prepare lightning talk and discussion questions for OSG-USERS F2F

   * *Week 3* 

* OSG-USERS annual F2F meeting

* Vacation

   * *Week 4* 

* E@OSG

   * 
      * deployment attempt at NWICG_NDCCL fails : globusrun-ws -s error, contacted In-Saeng 

   * 
      * deployment at LIGO_UWM_NEMO successful, run fails--> no outgoing network connection from WNs 

* IHOPE

   * 
      * Successful run at Nebraska, AGLT2, CIT_CMS_T2 

   * 
      * Testing pegasus feature: transfer of Pegasus executables as part of work-flow 

* Updated DASWG OSG-LIGO pages

---++ Period: July 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecons

* Write minutes for LIGO-Pegasus telecon

* Attend DASWG, Inspiral Coding, OSG-USERS and LIGO-Pegasus telecons

   * *Week 1* * 

* E@OSG activity

* GEO600-1.1 release

   * Deploy GEO600 1.1 code at 10 OSG site 

   * Modify all config files 

   * Test Runs (fail) and troubleshooting 

* IHOPE activity

   * Tesing dynamic transfer of Pegasus executables (fail) 

   * Testing Dynamic cleanup (fail) 

   * Testing Second Level Staging (fail) 

   * Successful run of SPEW work-flow at FNAL_GPFARM, FNAL_FERMIGRID (voms proxy! 

   * *Week 2* 

* Inquire about Gratia accounting issues

* Meeting with Robert Engel: E@OSG 1.1 release troubleshooting, bug fixes needed (application seg faults)

* Meeting with Chad, Kent, Becky, Robert, Kipp to discuss high mass search on the Grid

* E@OSG activity

   * Running E@H application at BNL_ATLAS_1, TTU_ANTAEUS, AGLT2 

   * Deploy GEO600-devel code at LIGO supported OSG 1.0 sites (~ sites) 

   * Test runs and troubleshooting in progress 

* IHOPE activity:

   * Run SPEW with dynamic staging and cleanup at LIGO_UWM_NEMO (no error but no cleanup done, staging successful) 

   * cvs update of Glue/Lal/Lalapps/pylal 

   * Run Ihope dag with dynamic staging and dynamic cleanup at LIGO_UWM_NEMO (in progress) 

   * WS-GRAM submission of SPEW at AGLT2 and LIGO_UWM_NEMO (in progress) 

   * *Week 3* 

* Gratia testing at 4 OSG sites --> e-mailed results to Anand

* Upgrade LIGO software on osg-itb-se.ligo.caltech.edu

* E@OSG activity

   * Running at 7 OSG sites with devel code --> BNL_ATLAS_1, AGLT2, Pudue_RCAC, TTU_ANTAEUS, SBGrid-Harvard-Exp, SPRACE-CE, CIT_CMS_T2 

   * Monitor daily runs: 

   * 
      * troubleshoot RC 106/107 (at CIT_CMS_T2, SPRACE_CE), RC 127 (at TTU_ANTAEUS)(?) 

   * 
      * Disk usage at remote end large 

* IHOPE activity:

   * download latest Pegasus nightly build 

   * test run with old site schema (but with underlying code changes for new schema) at CIT_CMS_T2 

   * WS-GRAM testing/debugging at AGLT2, LIGO_UWM_NEMO in progress 

   * test Pegasus dynamic cleanup of LIGO sire/coire files 

   * *Week 4* 

* Evaluate new 'Join the OSG pages'

* Test new OSG-USERS Discussion Forum

* Attend OSG Blueprint meeting in Madison: Wed--Fri

* E@OSG activity

   * make code changes to E@OSG job submission script to avoid over submitting 

* Ihope activity

   * test run with Pegasus code changes to accommodate new sites schema 

   * WS-GRAM testing of Inspiral work-flows 
   * Dynamic cleanup testing 

---++ Period: August 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecons

* Write minutes for LIGO-Pegasus telecon

* Attend DASWG, Inspiral Coding, OSG-USERS and LIGO-Pegasus telecons

   * *Week 1* 

* Attend OSG Blueprint meeting in Madison: Aug 1

* Travel from Madison

* Vacation Aug 4/6

* 08/05 E@OSG troubleshooting

   * *Week 2* 

* Attend OSG teleconference concerning LIGO and Globus 4.2 release

* Give report on E@OSG project to OSG executive team

* Start LIGO employee performance review

* 2 1/2 hour USER Group related phone conversation with Abhishek

* In person meeting with Abhishek

* 1hr USER Group related talk with Marcia

* E@OSG

   * Trouble shooting ticket TTU resolved 

   * After maintainance at CIT_CMS_T2 jobs fail with 

   * 
      * RC 253: job did not finish cleanly, lifetime exceeded limit 

   * 
      * RC 255: unknown interrupt during execution, check stdout, stderr & log 

   * GOC reports problems at SBGrid 

   * AGLT2 /bin/date fails after virtualization of gatekeeper 

* Inspiral activity

   * troubleshooting WS-GRAM submission via Pegasus 

   * *Week 3* 

* E@OSG

   * Open GOC ticket: E@OSG troubleshooting 

   * GOC ticket RCAC resolved (WS-GRAM restarted) 

   * GOC ticket SBGrid open 

   * Troubleshooting CIT_CMS_T2 (after maintainance), AGLT2 (after virtualization of gatekeeper) 

   * GEO600-1.1 troubleshooting with Thomas 

   * Start moving GEO600-devel code to tclproxy.ligo.caltech.edu (osg-itb-se to be turned off) 

* Inspiral activity

   * Pegasus nightly build testing 

   * *Week 4* 

* Vacation

---++ Period: September 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecons

* Write minutes for LIGO-Pegasus telecon

* Attend DASWG, Inspiral Coding, OSG-USERS and LIGO-Pegasus telecons

   * *Week 1* * 

* Mon/Tue: Labor day and vacation

* E@OSG

   * *Week 2* * 

* E@OSG activity:

   * Running steady at BNL_ATLAS_1, TTU_ANTAEUS, Nebraska, SPRACE_CE 

   * Contact Ian Stokes-Reeses concerning LIGO's removal from VO trusted list of SBGrid 

   * Trash/Troubleshooting RCAC, CIT_CMS_T2, AGLT2 

* Inspiral Activity

   * Test and troubleshoot SPEW with Pegasus nightly build 09-07 on NEMO, CIT_CMS_T2, BNL_ATLAS_1 

   * Test ihope run (LDG/OSG) with pegasus nightly built, fail 

* Test dynamic cleanup, fail at last cleanup job

   * Test SPEW with new sites schema, fail due to conflict with a python command in ldg_submit_dax 

* OSG USER Group activity

   * Contact Geant4 and GRASE VO concerning their (under) utilization of OSG resources. 

* BOSCO as submit host

   * get pegasus nightly build 

   * Submit SPEW work-flow: fail, condor not running on Bosco: need ldg server not client 

   * install glue/lala/lalapps/pylal: fail: ./00boot checking versions of aclocal, automake, autoheader and autoconf 00boot failed finding program aclocal 

   * *Week 3* * 

* E@OSG activity:

   * AGLT2 issue resolved 

   * RCAC WN deploy attempt 

   * Write progress summary in preparation for E@OSG telecon 

   * Attempt to deploy/run at: NotreDame, UCSDT2, FNAL_GPFARM, NWICG_NDCCL 

* Inspiral Activity

   * New sites schema nightly build trouble shooting 

   * 
      * Test and troubleshoot SPEW with Pegasus nightly build 09-07, ran one SPEW work-flow to completion at LIGO_UWM_NEMO 

   * 
      * Test dynamic cleanup, fail at last cleanup job - fixed, change in properties.bundle needed, but some files leftover at remote end 

   * 
      * Test SPEW with new sites schema, fail due to conflict with a python command in ldg_submit_dax -fixed but need correct local entry in sites catalogue 

* BOSCO as submit host

   * LIGO software installed 

   * SPEW fails at data transfer back to Bosco: connection refused, gsiftp server needed 

   * *Week 4* 

* E@OSG activity

   * Get information on WS-GRAM config at TTU_ANTAEUS from Alan Sill 

   * Send e-mail to WS-GRAM & E@OSG committee concerning WS-GRAM configs at BNL_ATLAS_1, TTU_ANTAEUS 

   * Running stabel at BNL, Nebraska, ANTAEUS, SPRACE, NWICG_NDCCL (only few jobs at a time), AGLT2 

   * Issues at RCAC, attention needed 

   * Attempt to deploy code at Clemson 

   * 
      * Can't authenticate ---> fixed (09-18) 

   * 
      * No W permission in $DATA --> fixed (09-18) 

   * 
      * WS-GRAM not enabled ---> fixed (09-23) 

   * 
      * Code deploy fails: can't find *.rep 

   * 
      * Waiting for port # of gsissh access to deploy interactively on head node 

* Inspiral Activity

   * 
      * Testing dynamic cleanup: setup....in, inspiral...gz left over in run dir, Karan, Chad will look into this, respectively 

   * 
      * Testing new site schema: generated local entry manually, peg-plan error: can't find executables in tc.data 

* BOSCO as submit host

   * 
      * Can't tarnsfer files back to bosco: Waiting for response from Greg concerning wrong libssl version 

* OSG VO Group Activity

   * 
      * Send e-mails to inactive VO asking for status update 

   * 
      * 1 hour phone conversation with Abhishek concerning VO Group related items 

---++ Period: October 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecons

* Write minutes for LIGO-Pegasus telecon

* Attend DASWG, Inspiral Coding, OSG-USERS and LIGO-Pegasus telecons

   * *Week 1* 

* Attend OSG Area Coordinator meeting

* Wednesday: Visit from Karan to discuss future planning for LIGO-Pegasus

* E@OSG activity

   * E@H as Backfill: sent E-mail to Chris Green asking of Gratia can report backfill jobs (09/24) 

   * E@CLEMSON - waiting for gsissh port 

* Inspiral Activity

   * Nightly Build test 09-29: 

   * 
      * old site schema 

   * 
      * 
         * LIGO_UWM_NEMO run successful 
 

   * 
      * 
         * tmpltbank fail at CIT_CMS_T 
 

   * 
      * 
         * file transfer fail at BNL_ATLAS_1 
 

   * 
      * new sites schema 

   * 
      * 
         * Pegasus-plan error due to wrong architecture listed --> fixed 
 

   * 
      * 
         * Work-flow fail due to bug in pegasus-get-sites 
 

* Bosco as submit host

   * file transfer fail (to Bosco) Port 2119 not open 

* OSG VO Group activity

   * Discussion of broad planning matters for year 3 in progress 

   * Send e-mail to various inactive VO for status update 

   * *Week 2* 

* E@OSG activity

* Clemson deploy:

   * Waiting for port # of gsissh access to deploy interactively on head node -- e-mailed Sebastian--> done 

   * Waiting for svn to be installed -->done 

   * Can't run ws-gram with local account (usr britta), no gsissh access to run as ligo--> e-mailed 

* Inspiral Activity

   * Waiting for Karan to fix Pegasus bugs: 

   * pegasus-get-sites bug: condor - fork entries swapped 

   * pegasus-get-sites locations: /localscratch, shared-scratch, local-storage, shared-storage locations do not exits at remote end 

* BOSCO as submit host

   * globus-url-copy gsiftp gsiftp: end of file error 

   * globus-url-copy gsiftp file: works, contacted Anand 

   * trouble shooting with Anand 

   * Phil got complete instruction set of ldg install 

* OSG VO Group Activity

* OSG release planning meeting attended

* OSG area coordinator meeting attended

* Study site-verify.pl script of Craig Prescott (UFl)

* Modify to use without MDS query, use ReSS instead

   * *Week 3* 

* Two telecons with Patrick and Kent concerning LIGO-OSG milestones for year 3

* E@OSG activity

   * Attend telecon concerning wider deployment of E@H onto OSG (10/13 ) 

   * Contact site admins for WS-GRAM config change 

   * 5 test runs at RCAC sucessful (10-14) 

   * Open GOC ticks: CIT_CMS_T2, NWICG_NotreDame 

   * Clemson deploy: waiting for gsissh access to deploy code 

   * Accounting summary ppt presentation: Gratia vs E@H vs DGrid 

   * Compile list of sites potentially used for running E@H as backfill : site, GRAM authentication, WS-GRAM authentication, scheduler 

* Inspiral Activity

   * 1 hr phonecall with Karan discussing new priorities and future approaches 

* BOSCO as submit host

   * Anand: different gridftp instances running 

   * Phil re-installed gridftp server, still end of file error 

   * 10/14 SPEW run on LIGO_UWM_NEMO submitted from BOSCO !!!!! :-) 

* OSG VO Group Activity

   * phone call with Abhishek concerning agenda items 

* Reading : Glidein -WMS

   * *Week 4* 

* Write agenda and lead LIGO-Pegasus and OSG-VO-Group telecons

* Write minutes for LIGO-Pegasus telecon

* Attend Inspiral, DASWG, OSG-Production and OSG-VO-Group telecons

* Attend Eathquake prep seminar: 'Shake out'

* Science milestones telecon # 3

* Open GOC ticket concerning ReSS query and Glue Schema attributes

* E@OSG activity

   * CIT_CMS_T2 trouble shooting: can't find Time/HiRes perl module 

   * 
      * re-deploy code on headnode ok, 

   * 
      * re-deploy code on WN 

   * Open GOC ticket NWICG_NotreDame 

* Inspiral Activity

   * get newest pegasus nightly build 10-16 

   * test new rc-client for conversion of sites catalogue to new schema --> ok 

   * test SPEW on ldg with new site schema: cit, lho good, uwm_nemo, llo fail 

   * test ihope on ldg with new site schema --> troubleshooting 

* Bosco as submit host

   * Spew run on LIGO_UWM_NEMO (OSG) 

   * Get Pegasus nightly 10-16 

   * test rc client to convert sites catalogue to new format 

   * SPEW on cit 

   * ihope on ldg -->troubleshooting

---++ Period: November 2008

* Write agenda and lead LIGO-Pegasus and OSG-USERS telecons

* Write minutes for LIGO-Pegasus telecon

* Attend and lead monhly E@OSG trouble shooting telecon

* Attended newly formed weekly Einstein@Home on the OSG telecon to
  address concerns from the OSG Production/Facilities Team about the
  scientific merits of the Einstein@Home jobs running on the OSG

* Attend DASWG, Inspiral Coding, OSG-USERS, OSG-PRODUCTION and LIGO-Pegasus telecons

* Weekly Update of DASWG page: https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

   * *Week 1* * 

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Shifted all E@H jobs running on the OSG to a separate user account
   to remove the ambiguity of how many jobs are running on the OSG
   version running on the DGRID. Previously these were all lumped
   together under one user. This will allow us to report to the OSG
   the number of science driven credits the OSG is contributing 
   relative to the CPU hours being used there.

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   *   reviewed changes to the CVS repository to track down date 
   and changes (and person making the changes) to help debug
   issue with the head of the repository that were preventing
   the CBC code from building workflows

   *  troubleshooting lalapps-ihope error ( bug in pipeline.py)

   * got newest pegasus nightly build 11-06 and now testing

   * testing new pegasus-get-sites

   * testing Adam Mercer's patch to the LAL / LALapps build

  
   * *Week 2* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Trash/Troubleshooting the UCSDT2 site deployment

   * Trash/Troubleshooting the RCAC site deployment. Seeing Error 2: 
    architecture issue which has been traced to the cluster being
    made up of both 32 bit and 64 bit hosts. Will need to find a
    way to constrain the build and node mapping in the job submission 
    and job execution.

   * Trash/Troubleshooting the Error 106 at RCAC, CIT, SPRACE: 
      preemption with signal 9 is suspected and may be due to the batch
      system not providing sufficient time for jobs to store state info.
   
   *  make minor changes and test cleanup script, add 3 new error codes
    to the cases which require cleaning
   
   *   modifying submission script to prevent job fail at preemption and
    currently testing.

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * troubleshooting lalapps-ihope followup job failure, the followup 
    job does always fail, will test using the -U flag

   * testing newest pegasus nightly build 11-11
   
   * tested new pegasus-get-sites in, needs additional fixes from USC-ISI

   * tested Adam's patch, now implemented and working correctly

   * changed SPEW dax workflow to link inspiral output correctly

   * *Week 3* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * UCSDT2 code deployment successful from GK and WN

   *  UCSDT2  job deployment fail RC 255, trouble shooting in progress

   * CMS sites issue trouble shooting: nfs too busy for application to tar up files in time before job is killed?

   * GOC tickets opened for ws-gram fail at GLOW, OCI_NSF 
 
   
* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * tested new pegasus-get-sites at cit, RCAC, CIT_CMS_T2, ready to tag Pegasus version


   * ihope high mass search, successful run at cit via Pegasus: 874000000-874010000, ~8 hours

 

   * *Week 4* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Daily manual cleanup

   * general trouble shooting concerning increasing number of non-zero return codes across sites: test with different USE_TMP locations

   * Follow up on ticket OCI_NFS

   * Research preemption/ priority setting (condor defaults and non-default settings at SPRACE, CIT_CMS_T2 )

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * ihope high mass search,  run at cit, uwm_nemo, OSG_LIGO_MIT via Pegasus: 874000000-874010000, fail, need 32 bit binaries

   * Attempt to install 32 bit binaries fails: lalapps configure does not find correct lal libraries (??)

   * Attempt to install 32 bit binaries using tag: release-5-2-90, fail with same error

---++ Period: December 2008

* Write agenda and lead LIGO-Pegasus and OSG-VO-Group telecons

* Write minutes for LIGO-Pegasus telecon

* Attend and lead monhly E@OSG trouble shooting telecon

* Attended weekly Einstein@Home on the OSG telecon to
  address concerns from the OSG Production/Facilities Team about the
  scientific merits of the Einstein@Home jobs running on the OSG

* Attend DASWG, Inspiral Coding, OSG-VO-Group, OSG-PRODUCTION and LIGO-Pegasus telecons

* Weekly Update of DASWG page: https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

   * *Week 1* * 

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * GT2/GT4 code deployment on 8 OSG sites

   * Daily manual cleanup 

   * Follow up on Purdue failures: reduce run time to 2 hours to avoid condor_vacate 

   * CIT_CMS_T2  troubleshooting: Restart container, reduce run time to 2 hours

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * Evaluate RSV pages at http://rsv.grid.iu.edu for usability  with Pegasus  work-flow planner

   * install 32 bit binaries (after libmetaio issues were resolved), same error at lalapps configure step, no        
    error  without  --enable-condor

   * retry LAL and LALAPPS configure with --with-gcc-flags --disable-xml

   * *Week 2* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * gt2/gt4 deployment at 4 additional sites successful (10 new sites all together)

   * Fermilab has nfs issues,  contacted Steve, need to re-deploy code

   * Daily manual cleanup st WS-GRAM sites

   * GT2 test runs at 10 new  sites:          
   
      * logs don't come back automatically
      
      * Jobs run and  finish as DONE, RC=0,  but state=executing, Error staging out files ?

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:


   * Install 32/64 bit binaries (need to confiugure lal/lalapps with options --with-gcc-flags --disable-xml) (12/04)

   * Added LIGO_UWM_NEMO, OSG_LIGO_MIT to ldg-sites.xml and committed to cvs

   * Obtained Pegasus nighty build 12/05 

   * SPEW on RCAC successful

   * ihope high mass search successful (12-11, runtime 6 hours 40 mins ):
     
      * gps times: 874000000-874010000
     
      * [sites]
      
      * imrinj=OSG_LIGO_MIT (via Pegasus)
     
      * phenominj=LIGO_UWM_NEMO (via Pegasus)
    
      * eobinj=cit (via Pegasus)
     
      * spininj=cit (via Pegasus)
     
      * playground=local
     
      * full_data=local


   * *Week 3 and 4* *

* Vacation Dec 17 -- 

---++ Period: January 2009

* Write agenda and lead LIGO-Pegasus and OSG-VO-Group telecons

* Write minutes for LIGO-Pegasus telecon

* Attend and lead monhly E@OSG trouble shooting telecon

* Attended weekly Einstein@Home on the OSG telecon to
  address concerns from the OSG Production/Facilities Team about the
  scientific merits of the Einstein@Home jobs running on the OSG

* Attend DASWG, Inspiral Coding, OSG-VO-Group, OSG-PRODUCTION and LIGO-Pegasus telecons

* Weekly Update of DASWG page: https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

   * *Week 1* * 

* sick leave 01/05, 01/06

* GENERAL

   * Attended Inspiral, DASWG, OSG-Production and High Mass telecons

   * Wrote agenda and led LIGO-Pegasus, OSG-VO-Group telecon

   * Wrote minutes for LIGO-Pegasus telecon


   * Updated GOC ticket: Can't run ws-gram job at OCI-NSF, possible fix 
     tomorrow

   * SWT2_CPB ticket resolved: gridftp problem

   * Opened GOC ticket: cann't create voms proxy

   * Phone talk with Abhishek, update on VO-Group activity 

   * Meeting with Kent, general catch up


* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Resume E@H GT4 activity on 7 sites after GSI credential expiration   
     on osg-itb-se


     
* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * Opened ticket with LONI support: Can't run at LONI: LONI OSG machine 
      down, no ETA for fix



   * *Week 2* *

* GENERAL

   * Open GOC tickets: can't create voms proxy, Can't run ws-gram job at OCI-NSF

   * Closed GOC ticket: $WNTMP at UFlorida_HPC not listed in VORS; (issue unresolved)

   * Open ticket with LONI support: Can't run at LONI: LONI OSG machine

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
      software to support both WS-GRAM and pre-WS-GRAM.
      Current status:
      
      * Code deployment at 10 sites successful
  
      * Test runs at 10 sites performed. For results, please  see
         https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

   * Maintain GT4 activity on 7 OSG sites:

      * Recent Average Credit (RAC): 61,491.82780
   
      * E@H rank based on RAC: 14

      * E@H rank based on accumulated Credits: 90 (+ 14 from last week)

   * Daily manual cleanup

   * Troubleshooting: high failure rates at RCAC, CIT_CMS_T2, SPRACE

* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES

   * In preparation for the S6 science run, injection (only) work-flows using one weeks worth of data for the highmass search are run on several OSG sites. The aim is to determine suitable OSG sites to run the analysis on within the time frame of a week and without any failures. Current status:

      * Attempts to submit test work-flows have been made at 7 OSG sites

         * OSG_LIGO_MIT, MIT_CMS - running without any failures so far

         * Purdue_RCAC, UCSDT2 -- 1 job failed

         * BNL_ATLAS_1 : pegasus-plan error

         * UFlorida_HPC: can't run there due to $WNTMP being later binding = not listed in VORS

  * *Week 3* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
       software to support both WS-GRAM and pre-WS-GRAM.
       Current status:

      * Code deployment at 12 sites successful

      * GT2 test runs at 10 sites performed. For results, please  see
          https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

      * Deploy code at CIT_CMS_t2, AGLT2

      * Trash/Deployment fails at RCAC, SPRACE (.rep issue)

      * Attempt GT4 submission with new code at AGLT2, CIT_CMS_T2, fail

   * Maintain GT4 activity on 7 OSG sites with old code:

      * Recent Average Credit (RAC):78,084.67861

      * E@H rank based on RAC: 13 (+1)

      * E@H rank based on accumulated Credits: 75 (+ 15 from last week)

      * Daily manual cleanup needed

      * high failure rates at Purdue_RCAC, CIT_CMS_T2, SPRACE


* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * In preparation for the S6 science run, injection (only) work-flows
      using one weeks worth of data for the highmass search are
      run on several OSG sites. The aim is to determine suitable OSG
      sites to run the analysis on within the time frame of a week
      and without any failures. Current status:

      * Checked out tagged Pegasus version after stage_in fail of
         previous runs (Pegasus aware of version issue after new
         release)

      * Test run at OSG_LIGO_MIT: stage_in of gwf files fail, can't
         find files on nemo data server, needs further investigation 
 
 * *Week  4* *

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
       software to support both WS-GRAM and pre-WS-GRAM.
       Current status:

      * Trash/Deployment at AGLT2, RCAC, CIT_CMS_T2, SPRACE fail

      * AGLT2, fail: a perl module was not installed correctly,Re-Deploy

      * Robert cut stable version GEO600-1.2.x (01/27)

         * check deployment code on osg-itb-se
         * deploy run code on osg-itb-se
         * deployment attempt at Nebraska fails

   * Maintain GT4 activity on 7 OSG sites with old code:

      * Recent Average Credit (RAC):	74,192.77984

      * E@H rank based on RAC: 13

      * E@H rank based on accumulated Credits: 69

   * Daily manual cleanup

   * High failures at RCAC, CIT_CMS_T2, SPRACE

   * red.unl.edu (Nebraska) GEO600-devel dir vanished - GT2 deploy
      (01/27)


* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * In preparation for the S6 science run, injection (only) work-flows
      using one weeks worth of data for the highmass search are
      run on several OSG sites. The aim is to determine suitable OSG
      sites to run the analysis on within the time frame of a week
      and without any failures. Current status:

      * Checked out tagged Pegasus version after stage_in fail of
         previous runs

      * Test run at OSG_LIGO_MIT: fail due to issue at nemo-dataserver
         (fixed 01/22 ?), OSG_LIGO_MIT down (01/23 --> contacted
         Filippo) , open GOC (01/27)--> MIT declared as
         unavailable until further notice (01/28)

---++ Period: February 2009

* Write agenda and lead LIGO-Pegasus and OSG-VO-Group telecons

* Write minutes for LIGO-Pegasus telecon

* Attend and lead monhly E@OSG trouble shooting telecon

* Attended weekly Einstein@Home on the OSG telecon to
  address concerns from the OSG Production/Facilities Team about the
  scientific merits of the Einstein@Home jobs running on the OSG

* Attend DASWG, Inspiral Coding, OSG-VO-Group, OSG-PRODUCTION and LIGO-Pegasus telecons

* Weekly Update of DASWG page: https://www.lsc-group.phys.uwm.edu/daswg/projects/OSG-LIGO.html

   * *Week 1* * 

* Attend OSG Area Coordinator meeting (02/05)

* Design first draft of poster for OSG All Hands meeting

* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
       software to support both WS-GRAM and pre-WS-GRAM.
       Current status:

      * Robert cut stable version GEO600-1.2.x (01/27)

         * code deployment at osg-job.ligo.caltech.edu fails (error in
           log files)

         * Trash/Deployment at TTU_ANTAEUS, Nebraska successful (head node,
           with Roberts config files)

         * Nebraska test runs fail: need WN deploy
           (can't find certain perl modules due to
           architecture difference of head node and WNs)
         * WN deploy at Nebraska fails: can't find
           $GLOBUS_USER_HOME/deployment dir

      * Attempt to deploy code on osg-job.ligo.caltech.edu fails
        (02/02)

      * Maintain GT4 activity on 7 OSG sites with old code:

         * Recent Average Credit (RAC): 61,600.41514 (down 13,000)

         * E@H rank based on RAC: 13 (no change)

         * E@H rank based on accumulated Credits: 66 +3

         * Daily manual cleanup

         * High failures at SPRACE, CIT_CM_T2 down, RCAC experienced
           problems last week
         
         * No activity on Nebraska


* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * In preparation for the S6 science run, injection (only) work-flows
      using one weeks worth of data for the highmass search are
      run on several OSG sites. The aim is to determine suitable OSG
      sites to run the analysis on within the time frame of a week
      and without any failures. Current status:

      * Test runs at 6 LDG/OSG sites fail at stage in of data:

         * some of the data on tape found faulty, Dan fixed it (02/02)

         * GUC of pegasus tar ball fails at some sites (contacted Karan
            02/02)

         * Stuart pinned 2 month worth of S5 h(t) data (02/02)

      * Test PEGASUS rmps on spud: SPEW work-flow to cit,
         LIGO-UWM_NEMO, BNL_ATLAS_1, Purdue_RCAC (with Dev1 binaries)

         * cit, LIGO-UWM_NEMO successful
         
         * BNL_ATLAS_1 successful without stage in of Pegasus
           executables
         
         * Purdue-RCAC fail (site related issues)

   * *Week 2* * 

* GENERAL

   * Open GOC tickets: Can't run ws-gram job at

     OCI-NSF, RSV wish list, voms-proxy issue

   * Updated DASWG page: OSG-LIGO


* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
       software to support both WS-GRAM and pre-WS-GRAM.
       Current status:

      * Robert cut stable version GEO600-1.2.x (01/27), fixed bug 02/07

         * Attempt to deploy code on osg-job.ligo.caltech.edu:
           (02/06) --osg-jon down until next week
           ssh osg-job.ligo.caltech.edu
           ssh: connect to hostosg-job.ligo.caltech.edu
           port 22: No route to host
 
         * 02/09 (head node) Trash/Deployment successful at: Purdue-RCAC, 
           NEMO, 
           SPRACE, BNL_ATLAS_1, CIT_CMS_T2, AGLT2, OUHEP_OSG, STAR_BNL

         * Test runs + trouple shooting 02/10, 02/11:
            
            * Nebraska test runs fail: need WN deploy
              (can't find certain perl modules due to
              architecture difference of head node and WNs)
              WN deploy at Nebraska fails: can't find
              $GLOBUS_USER_HOME/deployment dir
           
            * OUHEP_OSG ~20 successful jobs --> GT2
           
            * STAR_BNL (GT2)(thttp proxy ?)
           
            * NEMO (GT2)(http proxy?), 
           
            * CIT_CMS_T2 (GT2/GT4) 
           
            * SPRACE  (GT4, GT2)fail 

      * Maintain GT4 activity on 7 OSG sites with old code:

         * Recent Average Credit (RAC): 52,183.45522 (down 9000) 

         * E@H rank based on RAC: 12 (+1)

         * E@H rank based on accumulated Credits: 60 (+6)

         * Daily manual cleanup

         * High failures at SPRACE, RCAC, CIT_CM_T2,
           ANTAEUS down (02/06 ++)

         * No activity on Nebraska


* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * In preparation for the S6 science run, injection (only) work-flows
      using one weeks worth of data for the highmass search are
      run on several OSG sites. The aim is to determine suitable OSG
      sites to run the analysis on within the time frame of a week
      and without any failures. Current status:

      * GUC of pegasus tar ball fails at some sites (contacted Karan
        02/02, contacted Xin at BNL_ATLES_1 02/10)

      * Test runs at LDG/OSG without pegasus stage-in, data pinned to
        disk at Caltech (02/02):

         * UCSDT2, lsu: tmpltbank fail: can't glob frame files, checked  
                         files in remote dir--> ok, LIGO software bug 
                         again??

         * llo (61 hr), NEMO (28 hr)

         * CIT_CMS hangs at data transfer

         * BNL_ATALS_1, lho running


      * Test PEGASUS RPMs on spud

         * get highmas code onto spud:(02/06) 32 bit code

         * test ihope run on LIGO-UWM_NEMO, cit from spud ---> condor 
           problem on spud, Phil contacted Jamie Frey, 
         *   waiting for response

   * *Week 3* *

* GENERAL

   * Third draft of poster for OSG All Hands meeting --> edited by Kent  
     --> printed  02/18

   * Attended LIGO-Pegasus, Coding, DASWG, OSG-VO-GROUP, High
     Mass telecons, OSG-INTEGRATION telecons

   * Wrote agenda and led LIGO-Pegasus and OSG-VO-Group telecons

   * Open GOC tickets:RSV wish list
                      BNL_ATLAS_1 gridftp with http source

   * Closed tickets: OCI-NSF (try running in near future, re-open if 
                              problems)
                     UCSDT2 - architecture differences cause problem 
                              when running tmpltbank
                     LONI, OSG_LIGO_MIT - declared  not operational 
                                          until further notice


* EINSTEIN AT HOME ON THE GRID ACTIVITIES:

   * Effort is underway to upgrade the job submission management
       software to support both WS-GRAM and pre-WS-GRAM.
       Current status:

      * Robert cut stable version GEO600-1.2.x (01/27), fixed bug 02/07

         * More bugs were discovered - testing on hold
  
         * Robert is working on fixes and first tests 

      * Maintain GT4 activity on 7 OSG sites with old code:

         * Recent Average Credit (RAC): 48,946.37260 (down ~ 4000)

         * E@H rank based on RAC: 13 (-1)

         * E@H rank based on accumulated Credits: 58 (+2)

         * Daily manual cleanup

         * High failures at SPRACE, RCAC, CIT_CM_T2,RACAC, BNL_ATLAS_1, 
                            ANTAEUS, AGLT2 (down 02/17, 02/18) 

         * Headnode deploy of devel code at Nebraska success

         * Test runs fail (need WN deploy)

         * WN deploy attempt


* BINARY INSPIRAL APPLICAITON ON THE GRID ACTIVITIES:

   * In preparation for the S6 science run, injection (only) work-flows
      using one weeks worth of data for the highmass search are
      run on several OSG sites. The aim is to determine suitable OSG
      sites to run the analysis on within the time frame of a week
      and without any failures. Current status:

      * GUC of pegasus tar ball fails at some sites --goc tic opened (ATLAS_1)

      * Test runs at LDG/OSG without pegasus stage-in, data pinned to
         disk at Caltech (02/02):

         * 12 sites attempted

         * 2 successes: llo (61 hr), NEMO (28 hr)
           
         * Trouble Shooting: UCSDT2 architechture issue

      * Test PEGASUS rmps on spud

         * condor issue at spud resolved (magically by itself ??)

         * Submit ihope dag (LIGO_UWM_NEMO) form spud

* CIT-Cluster work:

   * osg-job.lligo.caltech.edu
   
      * Attempt to intstall LIGO software: Missing metaio, libframe