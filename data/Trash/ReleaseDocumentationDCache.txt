%META:TOPICINFO{author="NehaSharma" date="1279559103" format="1.1" reprev="1.20" version="1.20"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="2"}%

---++ Preparation

The following are the recommended system minimums for an OSG dCache installation. All nodes should have Scientific Linux 4.2 or later.

*Admin Nodes*
   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) with a hot spare on the Admin Nodes with the billing or srm databases running the XFS file system. These should be backed up regularly.</p> 
   * <p>Gigabit or better network links</p> 

*Pool Nodes*
   * <p>Dual CPU or Dual Core Intel Xeon, 2.8GHz or better or equivalent Opteron (e.g., quad Opteron 270)</p> 
   * <p>4 GB RAM or more</p> 
   * <p>Raided (mirrored) system disks, hot swappable with spare recommended</p> 
   * <p>Raided data disks (RAID 5) hot swappable with hot spare recommended running the XFS file system. External RAIDs are highly recommended.</p> 
   * <p>Gigabit or better network links</p> 

*PNFS Node*
   * <p>8 GB RAM</p> 
   * <p>Postgres databases on raided disk (RAID 5) with back-up performed regularly.</p> 
   * <p>Disk should be used exclusively for the pnfs database.</p> 

*Chimera Node*

*Tuning/Hardware Layout*
   * [[http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=1837&version=1&filename=dcache_tuning.pdf"][ Recommended Linux TCP/IP Parameter Tuning]] 
   * [[https://indico.desy.de/getFile.py/access?contribId=19&sessionId=5&resId=0&materialId=slides&confId=138][A Good Guide for dCache Hardware Layout]] 
   *  [[http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server][PostgreSQL Tuning]]
   *  [[http://www.dcache.org/manuals/index.shtml"][dCache Manual]]

---++ Installation
VDT provides a package for installing dCache on an Open Science Grid site. To install dCache using the VDT-dCache package, do following -
   * [[http://vdt.cs.wisc.edu/components/dcache.html][(optional) Visit the official VDT-dCache website]]
   * [[http://vdt.cs.wisc.edu/software/dcache/server/2.4.8][Download the tarball]] 
   * [[http://vdt/cs/wisc.edu/extras/2.4.8/InstallingDcacheForOSG.README.html][Follow the Installation Instructions]]

---+++ Integration with the information system
 The Storage Element does not collect or publish information independently. Integration of the SE with the central information systems takes place during the Compute Element installation/configuration. Read [[ReleaseDocumentation/GenericInformationProviders][Generic Information Providers]] for more information.
 
---+++ Integration with Gratia dCache probes
Please follow [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes][Gratia dCache Probes Installation]] to install and configure Gratia dCache Probes.

---+++ Setup Replica Manager
If enabled, the Replica Manager feature of dCache automatically makes extra copies of files that are written into the storage element. The purpose is to increase performance for the subsequent reading of files and to offer some protection against data loss in non tape-backed systems. In the VDT-dCache package the Replica Manager is turned on by default with parameters of a minimim of two and a maximum of three replicas per file. However, dCache pools must be explicitly added to the "ResilientPools" group in the <nop>PoolManager.conf file in order for replication to take effect. See [[https://twiki.grid.iu.edu/twiki/bin/view/Documentation/StorageDcacheOverview][Overview of Storage Resource Manager (SRM) and dCache for OSG]] for details of the dCache pools and <nop>PoolManager. Only files written to resilient pools will be replicated.  

For instructions on the replica manager setup, please see [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/ReplicaManagerSetup][Configuration of Storage Elements for Replica Management]].

---+++ Setup Space Reservation for Opportunistic Use
The Space Reservation feature of dCache allows users to have a guarantee of a given amount of storage for a period of time. For opportunistic use, the concept is for users to make a space reservation in the context of supporting a job running on a compute element. The reservations will be for relatively small amounts of storage for short duration. See the [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/StorageInOSGDraft][Use of Storage in OSG]] document for details. In the VDT packaging of dCache, Space Reservation is turned on by default. However, dCache pools must be explicitly added to the "public" pool group in the <nop>PoolManager.conf file in order for space reservation to take effect. Files written using space reservations will always be written to the associated pools.

For instructions on the opportunistic storage setup, please see [[https://twiki.grid.iu.edu/twiki/bin/view/ReleaseDocumentation/OpportunisticStorageSetup][ Configuration of Storage Elements for Opportunistic Use]], or, if using tags, [[Storage.OpportunisticStorageUse]]

---++ Post Install 
---+++ Setup gPlazma 
After dCache is installed, authorization for access must be configured. Please refer to the [[http://www.dcache.org/manuals/Book/config/cf-gplazma.shtml][gPlazma chapter]] of the dCache book on how to configure authorization for your site's policies.   


---+++ GIP
[[InformationServices/DcacheGip]]

---+++ dCache Generic Information Provider

The purpose of the dCache Generic Information Provider is to discover and publish all storage related information corresponding to your dCache based Storage Element. After you have installed the OSG CE, please refer to documentation on how to configure the GIP for the SE on the [[https://twiki.grid.iu.edu/twiki/bin/view/InformationServices/DcacheGip][dCacheGIP]] page. 

*Note:* At the moment, to get the most up-to-date, bug-free version, you'll need to switch from the GIP distributed with ITB 0.9.0 to SVN. The instructions on how to do this are available on the [[https://twiki.grid.iu.edu/twiki/bin/view/InformationServices/GipInstall][GipInstall]] page. 

---+++RSV

---++Validation

---+++ dCache Validation Test Suite
---++++ Description
The dCache validation test suite can be used by storage admins to perform a good first-level check of basic functionality testing of a dCache instance. It is developed and maintained at Fermilab. If all tests within the testsuite are successful, it means the basic functionality exists on the server side. 

 The suite consists of three major parts:

   * Fermi SRM Client Test Suite. The main purpose of the Fermi SRM Client Test Suite is to run various srmclient commands developed at Fermilab against a dCache based Storage Element (SRM V2.2 only). Various tests that are run as part of this test suite include:
      * <u>srmcp</u> (Get/Put operations, with and without Space Reservation)
      * <u>srmmkdir</u> (Create new directories)
      * <u>srmmv</u>  (Move directory from one location to another)
      * <u>srmls</u> (List contents of a directory)
      * <u>srm-get-permission</u> (Get permissions on a file/directory)
      * <u>srm-check-permission</u> (Check permissions on a file/directory)
      * <u>srm-set-permission</u> (Set permissions on a file/directory)
      * <u>srm-reserve-space</u> (Make a space reservation)
      * <u>srm-release-space</u> (Release a space reservation)
      * <u>srmrm</u> (Remove file)
      * <u>srmrmdir</u> (Remove directory)

%ICON{warning}%  If your dcache configuration doesn't support opportunistic storage some of the commands should fail (e.g srmreservespace, srmreleasespace, srmcp with space token option). See the above section on how to set up opportunistic storage.

   * SRM Space Management test suite. It tests space allocation/release with various options including:
      * retention_policy
      * guaranteed_size
      * desired_size
      * lifetime
      * access_latency

%ICON{warning}%If your dcache configuration doesn't support opportunistic storage you should skip this test. See the "Replica Manager" section above on how to set up the replica manager.

   * Replica Manager test suite. It performs the following tasks:
      * copy multiple files into storage
      * verify report of disk usage
      * check the number of replicas for each files
      * delete these files from storage
      * verify consistency of information in Pnfs, pools and Replica Manager
      * verify report of disk usage

%ICON{warning}%If your dcache configuration doesn't support the replica manager you should skip this test. See the above section on how to set up the replica manager.

The validation suite is an rpm package that can be downloaded from  [[http://vdt.cs.wisc.edu/software/dcache/tools//testing/][VDT dcache tools]]. You will have to modify the test configuration according to your dcache installation. A detailed description of configuration is provided in README file contained in the rpm.

---++++ Installation
The validation suite is an rpm package. After installing it, change the ownership of the installation root directory to the uid of the user who will be running the test with voms-proxy certificate.
<verbatim>
wget http://vdt.cs.wisc.edu/software/dcache/tools//testing/dcache_validation-0.1-0.noarch.rpm
rpm -i --prefix <your_home_area> dcache_validation-0.1-0.noarch.rpm
chown -R <your_user_name>.<your_group_name> <your_home_area>/dcache_validation-0.1
</verbatim>

---++++ Running and viewing results
The detailed instructions how to run the test, what software should be installed on your machine and how to see the results are provided in  dcache_validation-0.1/README. An example of the test suite results can be found [[https://osg-ress-2.fnal.gov:8443/dcache_validation_tests][here]]. 
%ICON{warning}% In order to see this example you have to have your user certificate installed in your browser.

---+++ Registration for SRM Monitoring at LBNL

To register your SE with LBNL Monitoring system:
go to http://datagrid.lbl.gov/sitereg/
and follow the instructions to register. 

Daily functional monitoring for all SRM interfaces is done around 9am Pacific time.


---+++ Testing Opportunistic Storage

The Fermi client commands for using space reservations are described in [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/SpaceResClientCommands][Using Opportunistic Storage]]. These commands can be run from any client machine which has them installed (see above). When there is a Compute Element associated with a Storage Element, jobs containing the commands described therein should be created and run on worker nodes. To test opportunistic storage from a Compute element please run the [[%ATTACHURL%/oppstor_test.py.txt][oppstor_test.py]] script (remove the .txt extension) from a worker node. This script does several tests of making, using, and releasing space reservations using the Fermi clients. To run it, you only need a proxy and to have the Fermi clients in the path. Use the command

<verbatim>
jython oppstor_test.py  srm://srmnode.oursite.edu:8443 /pnfs/oursite.edu/data/oppstorage/test
</verbatim>

with the URL of the srm server and desired path for the written files.

Removal of files used in opportunistic storage that are no longer needed allows the space to be recycled. To do this please use the SRM PNFS Space Reclaimer of the OSG Storage Operations Toolkit (see below).



[[ReleaseDocumentation.ValidateDcacheGratia]]

[[Integration/ITB090/InstallationITBStorageElement]]

[[ReleaseDocumentation/GPlazmaInteropTesting]]


---++ References

[[http://s-2.sourceforge.net/][S2]] - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp

<br />%STOPINCLUDE% 


---++ *Comments*
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:27 |
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:42 |
| I hope that I have fixed all the links | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
| We will need to add simple validation test commands | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = NehaSharma

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = TanyaLevshina
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


############################################################################################################
-->

%META:FILEATTACHMENT{name="dcache_tier2.png" attachment="dcache_tier2.png" attr="" comment="" date="1257462558" path="dcache_tier2.png" size="157327" stream="dcache_tier2.png" tmpFilename="/usr/tmp/CGItemp11636" user="TanyaLevshina" version="1"}%
