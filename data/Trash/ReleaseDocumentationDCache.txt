%META:TOPICINFO{author="TanyaLevshina" date="1285968226" format="1.1" reprev="1.32" version="1.32"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC{depth="3"}%

---++ Information

dCache is a disk-caching system jointly developed by [[http://www.desy.de/index_eng.html][Deutsches Elektronen-Synchrotron (DESY)]] and [[http://www.fnal.gov][Fermi National Accelerator Laboratory (Fermilab)]].  It has a [[https://srm.fnal.gov/twiki/bin/view/SrmProject/WebHome][Storage Resource Manager]] interface and meets the [[https://sdm.lbl.gov/srm-wg/][SRM V2.2]] specification. Some of the main advantages of using dCache are:
   * It strictly separates the name space from the data repositories, thus increasing fault tolerance for the storage element.
   * It allows several copies of a single file for distributed data access
   * It provides automatic file replication under high load ("hotspot detection")
   * It supports multiple transfer protocols: gridftp, xrootd, dcap, and http
   * It provides a [[https://twiki.grid.iu.edu/bin/view/Storage/DCacheChimera][single name space]] across the entire pool of disk servers, making it look like a single file system to the users. 
   * It provides multiple ways ([[http://www.dcache.org/manuals/Book-1.9.9/start/intouch-web.shtml][Web Interface]],[[http://www.dcache.org/manuals/Book-1.9.9/start/intouch-admin.shtml][Admin Interface]],[[http://www.dcache.org/manuals/Book-1.9.9/start/intouch-gui.shtml][GUI]]) to administer/monitor the storage element.
   * It provides mechanisms for internal clean-up and load balancing, which allows efficient use of storage space and handling of users' requests.
   * It has a large customer base with years of experience, so help is readily available.

%TWISTY{%TWISTY_OPTS_REVIEW%}%
   * Please check documentation template  and use all mandatory sections
   * Please get rid of any  opinions such as "makes an excellent grid Storage Element" 
   * I don't understand how this link https://srm.fnal.gov/twiki/bin/view/SrmProject/WebHome help you with the point you try to make.
   *  links are needed:
       * meets the SRM v2.2 specification 
       * Virtual Data Toolkit
   * https://plone4.fnal.gov/P0/DCache/dcachedoc/cell-descriptions - have you checked this link - look obsolete to me
   * dCache is not using PNFS - please change it
%ENDTWISTY%
---+++ Architecture
Please refer to the attached file (dCacheHighLevel.pdf) for a High Level View of dCache Architecture.

Main dCache components are listed in the table below:
| *Component Name* | *Purpose* |
|Location Manager|Instructs a newly started domain to which domains it should connect to|
|Chimera| Name server|
|PNFS Manager|Manages the name space functionality|
|Info Provider|Provides a description of a !dCache instance using the GLUE information model|
|!gPlazma|Manages authorization of the clients|
|!Storage.PoolManager|Selects which pool is used for an incoming request|
|!ResilientManager|Keeps track of the number of replicas of each file within a certain subset of pools and makes sure this number is always within a specified range|
|!SrmSpaceManager|Provides a standardized web service interface for managing a storage resource. Provides means to reserve space, initiate file storage or retrieval, and replicate files to another SRM|
|Doors ||
|  dcap door|Supports dcap protocol|
|  gsiftp door|Supports !GridFTP protocols v1 and !GridFTP v2. |
|  gsidcap door|Supports dCap protocol with a GSI authentication wrapper|
|  xrootd door|Supports xroot file transfer protocol|
|  NFSv4.1| Supports NFSv4.1 protocol|
|Pools| Store retrieved files and provide access to the data. A machine may have multiple pools| 

---++ Preparation
*Operating System* <br></br>
All nodes should have Scientific Linux 4.2 or later <br></br>

*Hardware Recommendations* <br></br>
Admin Nodes
   * Dual CPU or Dual Core Intel Xeon, 2.8GHz or better
   * 4 GB RAM or more
   * Raided (mirrored) system disks, hot swappable with spare recommended 
   * Raided data disks (RAID 5) with a hot spare on the Admin Nodes with the billing or srm databases running the XFS file system. These should be backed up regularly
   * Gigabit or better network links <br></br>
Pool Nodes
   * Dual CPU or Dual Core Intel Xeon, 2.8GHz or better or equivalent Opteron (e.g., quad Opteron 270)
   * 4 GB RAM or more
   * Raided (mirrored) system disks, hot swappable with spare recommended
   * Raided data disks (RAID 5) hot swappable with hot spare recommended running the XFS file system. External RAIDs are highly recommended.
   * Gigabit or better network links <br></br>
PNFS Node
   * 8 GB RAM
   * Postgres databases on raided disk (RAID 5) with back-up performed regularly 
   * Disk should be used exclusively for the pnfs database<br></br>
Chimera Node
   * 8 GB RAM
   * Postgres databases on raided disk (RAID 5) with back-up performed regularly 
   * Disk should be used exclusively for the Chimera database<br></br>
   *Note * <p> All user interaction with dCache will use the !PnfsManager. This means that the throughput of user interactions will depend on
     the speed of !PnfsManager, and so on the speed of the namespace. If these components are slow then dCache will be slow. 
     Therefore, when buying machines, the "chimera node" should be one of the fastest machines.
     If possible, buy enough memory so the machine's free memory (after starting the services) is big enough to store the entire namespace 
     in RAM. !PostGreSQL will use the kernel filesystem block-cache to cache database information. If there's sufficient free memory to store 
     the entire namespace then database reads never need to fetch information from disk, which will improve dCache performance.</p>

*Hardware/Tuning Layout*
   * [[http://cd-docdb.fnal.gov/cgi-bin/RetrieveFile?docid=1837&version=1&filename=dcache_tuning.pdf"][ Recommended Linux TCP/IP Parameter Tuning]] 
   * [[https://indico.desy.de/getFile.py/access?contribId=19&sessionId=5&resId=0&materialId=slides&confId=138][A Good Guide for dCache Hardware Layout]] 
   *  [[http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server][PostgreSQL Tuning]]
   *  [[http://www.dcache.org/manuals/index.shtml"][dCache Manual]]
%TWISTY{%TWISTY_OPTS_REVIEW%}%
  * first document is from 2006 - are you sure that it is still valid? Second one is from 2007
  * the last link refers to all dcache documentation
%ENDTWISTY%
---++ Installation
VDT provides a package for installing dCache on an Open Science Grid site. There are several advantages to using the VDT-dCache package.
   * It provides a convenient and easy deployment of SRM/dCache to create an OSG Storage Element (SE).
   * It allows you to use a single configuration file for multi-node installations.
   * It reduces number of installation steps to 20% as compared to non VDT-dCache based install.
   * It provides a "dry run" capability for the installation.
   * It provides the dCache Gratia storage and transfer probe, which are very useful for monitoring purpose.
   * It provides Init.d scripts for various dCache services.
   * It provides detailed instructions on installation, upgrade & clean up of a dCache installation.
   * It comes with great Installation and debugging support.

To install dCache using the VDT-dCache package, do following -
   * [[http://vdt.cs.wisc.edu/components/dcache.html][(optional) Visit the official VDT-dCache website]]
   * [[http://vdt.cs.wisc.edu/software/dcache/server/2.4.8][Download the tarball]] 
   * [[http://vdt.cs.wisc.edu/extras/2.4.8/InstallingDcacheForOSG.README.html][Follow the Installation Instructions]]

---+++ dCache Information System & its Integration with OSG's Generic Information Provider
For dCache versions < 1.9, the information system has to be configured manually. Please read the instructions on how to do this [[https://twiki.grid.iu.edu/twiki/bin/view/InformationServices/DcacheGip][here]]. For dCache version >= 1.9, the in-built [[http://www.dcache.org/manuals/dCache-info-20080813.pdf][info service]] is configured and started automatically. 

Once you have the dCache based info service configured, up and running, next step is to integrate your SE with OSG's Generic Information Provider.
At present, the SE does not collect or publish information independently. Integration of the SE with the central information systems takes place during the Compute Element installation/configuration. You will need to login on to the CE node and edit the SE related section in the $VDT_LOCATION/osg/etc/config.ini file

Example settings are shown below
<pre class="file">
se_available = %(disable)s
default_se = fndca1.fnal.gov
grid_dir = /usr/local/grid
app_dir = /grid/app
data_dir = /grid/data
worker_node_temp = /local/stage1
site_read = dcap://fndca1.fnal.gov:24525//pnfs/fnal.gov/usr/fermigrid/volatile
site_write = srm://fndca1.fnal.gov:8443//pnfs/fnal.gov/usr/fermigrid/volatile
srm_hosts = fndca1.fnal.gov
srm_dir = /pnfs/fnal.gov/usr/fermigrid/volatile/fermilab
srm_webservice_path = srm/managerv2
</pre>

You can read about the various SE related Glue Schema attributes [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GenericInformationProviders#Explanation_of_GIP_Glue_Attribut][here]]

---+++ Integration with Gratia dCache probes
Please follow [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GratiaDcacheProbes][Gratia dCache Probes Installation]] to install and configure Gratia dCache Probes.

---+++ Replica Manager
If enabled, the Replica Manager feature of dCache automatically makes extra copies of files that are written into the storage element. The purpose is to increase performance for the subsequent reading of files and to offer some protection against data loss in non tape-backed systems. In the VDT-dCache package the Replica Manager is turned on by default with parameters of a minimim of two and a maximum of three replicas per file. However, dCache pools must be explicitly added to the "ResilientPools" group in the <nop>Storage.PoolManager.conf file in order for replication to take effect. See [[https://twiki.grid.iu.edu/twiki/bin/view/Documentation/StorageDcacheOverview][Overview of Storage Resource Manager (SRM) and dCache for OSG]] for details of the dCache pools and <nop>Storage.PoolManager. Only files written to resilient pools will be replicated.  

---+++ Space Reservation for Opportunistic Use
The Space Reservation feature of dCache allows users to have a guarantee of a given amount of storage for a period of time. For opportunistic use, the concept is for users to make a space reservation in the context of supporting a job running on a compute element. The reservations will be for relatively small amounts of storage for short duration. See the [[https://twiki.grid.iu.edu/twiki/bin/view/Trash/StorageStorageInOSGDraft][Use of Storage in OSG]] document for details. In the VDT packaging of dCache, Space Reservation is turned on by default. However, dCache pools must be explicitly added to the "public" pool group in the <nop>Storage.PoolManager.conf file in order for space reservation to take effect. Files written using space reservations will always be written to the associated pools.
For instructions on the opportunistic storage setup, please see [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/OpportunisticStorageUse][ Configuration of Storage Elements for Opportunistic Use]], or, if using tags, [[Storage.OpportunisticStorageUse]]

---+++ gPlazma 
After dCache is installed, authorization for access must be configured. Please refer to the [[http://www.dcache.org/manuals/Book/config/cf-gplazma.shtml][gPlazma chapter]] of the dCache book on how to configure authorization for your site's policies.   

---++Validation
---+++ Simple 5 command validation
<pre class="screen">
srmping -2 -debug srm://[SRM-HOST]:8443/srm/managerv2
</pre>
<pre class="screen">
srmcp -2 file:///[ABSOLUTE-LOCATION-SOURCE-FILE] srm://[SRM-HOST]:8443/srm/managerv2?SFN=[PNFS-PATH-DESTINATION-LOCATION]
</pre>
<pre class="screen">
srmcp -2 srm://gwdca04.fnal.gov:8443/srm/managerv2?SFN=[PNFS-PATH-SOURCE-LOCATION] file:///[ABSOLUTE-LOCATION-DESTINATION-FILE]
</pre>
<pre class="screen">
srm-reserve-space -debug -2 --retention_policy=REPLICA --access_latency=ONLINE -lifetime=6000 -desired_size=50 -guaranteed_size=50 srm://[SRM-HOST]:8443
</pre>
<pre class="screen">
srmcp -2 -debug -space_token=[SPACE-TOKEN] file:///[ABSOLUTE-LOCATION-SOURCE-FILE] srm://[SRM-HOST]:8443/srm/managerv2?SFN=[PNFS-PATH-DESTINATION-LOCATION]
</pre>
*Note* In the space reservation command, the size is set to 50bytes, so make sure size of your test file is less than that.

---+++ dCache Test Suite
---++++ Description
The dCache validation test suite can be used by storage admins to test basic functionality of a dCache based SE.The suite consists of three parts -

   * Fermi SRM Client tests <br></br>
      Following srmclient commands developed at Fermilab are run against dCache based SE (SRM V2.2 only). 
      * <u>srmcp</u> (Get/Put operations, with and without Space Reservation)
      * <u>srmmkdir</u> (Create new directories)
      * <u>srmmv</u>  (Move directory from one location to another)
      * <u>srmls</u> (List contents of a directory)
      * <u>srm-get-permission</u> (Get permissions on a file/directory)
      * <u>srm-check-permission</u> (Check permissions on a file/directory)
      * <u>srm-set-permission</u> (Set permissions on a file/directory)
      * <u>srm-reserve-space</u> (Make a space reservation)
      * <u>srm-release-space</u> (Release a space reservation)
      * <u>srmrm</u> (Remove file)
      * <u>srmrmdir</u> (Remove directory)

%ICON{warning}%  If your dcache configuration doesn't support opportunistic storage some of the commands should fail (e.g srmreservespace, srmreleasespace, srmcp with space token option). See the "Space Reservation for Opportunistic Use" section above on how to set up opportunistic storage.

   * SRM Space Management tests (Skip this if the SE you are about to test doesn't support opportunistic storage)<br></br>
      It tests space allocation/release with following options:
      * retention_policy
      * guaranteed_size
      * desired_size
      * lifetime
      * access_latency

   * Replica Manager tests (Skip this if the SE you are about to test doesn't support Replica Manager)<br></br>
     It performs the following tasks:
      * copy multiple files into storage
      * verify report of disk usage
      * check the number of replicas for each files
      * delete these files from storage
      * verify consistency of information in PNFS, Pools and Replica Manager
      * verify report of disk usage
      See the "Replica Manager" section above on how to set up the replica manager.

---++++ Installation
The validation test suite is an rpm package that can be downloaded from  [[http://vdt.cs.wisc.edu/software/dcache/tools//testing/][VDT dcache tools website]]. You will have to modify the test configuration according to your dCache installation. A detailed description of configuration is provided in README file contained in the rpm. After installing it, change the ownership of the installation root directory to the uid of the user who will be running the test with voms-proxy certificate.
<pre class="screen">
wget http://vdt.cs.wisc.edu/software/dcache/tools//testing/dcache_validation-0.1-0.noarch.rpm
rpm -i --prefix <your_home_area> dcache_validation-0.1-0.noarch.rpm
chown -R <your_user_name>.<your_group_name> <your_home_area>/dcache_validation-0.1
</pre>

---++++ Execution
The execution instructions are present in the README file packaged in the rpm. An example of the test suite results can be found [[https://osg-ress-2.fnal.gov:8443/dcache_validation_tests][here]]. <br></br>
%ICON{warning}% In order to see this example you have to have your user certificate installed in your browser.

---+++ oppstor_test.py (Tool for testing Opportunistic Storage)
The Fermi client commands for using space reservations are described in [[https://twiki.grid.iu.edu/twiki/bin/view/Storage/SpaceResClientCommands][Using Opportunistic Storage]]. These commands can be run from any client machine which has them installed (see above). When there is a Compute Element associated with a Storage Element, jobs containing the commands described therein should be created and run on worker nodes. To test opportunistic storage from a Compute element please run the [[%ATTACHURL%/oppstor_test.py.txt][oppstor_test.py]] script (remove the .txt extension) from a worker node. This script does several tests of making, using, and releasing space reservations using the Fermi clients. To run it, you only need a proxy and to have the Fermi clients in the path. Use the command
<pre class="screen">
jython oppstor_test.py  srm://[SRM-HOST]:8443[PNFS-PATH]
</pre>
where <br></br>
SRM-HOST - FQDN of SRM server <br></br>
PNFS-PATH - Desired PNFS path for written files

Removal of files used in opportunistic storage that are no longer needed allows the space to be recycled. To do this please use the SRM PNFS Space Reclaimer of the OSG Storage Operations Toolkit (see below).

---+++ S-2
[[][S2]] is a SRM V2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp

---++ Registration
Please follow the instructions mentioned [[http://datagrid.lbl.gov/stereg][here]] to register your SE with LBNL monitoring system.
The monitoring system performs daily functional monitoring for all SRM interfaces around 9am Pacific time.

---++ References
[[https://twiki.grid.iu.edu/bin/view/Documentation/StorageDcacheOverviewHardwareLayout][dCache Hardware Layout]]
<br></br>
[[https://twiki.grid.iu.edu/bin/view/Storage/OpportunisticStorageUse][Space Reservation and Opportunistic Storage Use]]
<br></br>
[[https://twiki.grid.iu.edu/bin/view/Storage/DCacheChimera][dCache Chimera]]
<br></br>
[[https://twiki.grid.iu.edu/bin/view/Storage/PoolManager][Storage.PoolManager.conf]]

<br />%STOPINCLUDE% 


---++ *Comments*
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:27 |
| Under Installation, neither the Installation Procedure or Upgrade Procedure links are valid. | Main.JamesWeichel | 09 Sep 2009 - 18:42 |
| I hope that I have fixed all the links | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
| We will need to add simple validation test commands | Main.TanyaLevshina | 04 Nov 2009 - 15:20|
| I have updated/fixed this document and its ready for review | Main.NehaSharma | 20 July 2010 - 10:13|
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = NehaSharma

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Integration|Operations|Tier3)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = TanyaLevshina
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->

   * [[%ATTACHURL%/dCacheHighLevel.pdf][dCacheHighLevel.pdf]]: High Level View of dCache

%META:FILEATTACHMENT{name="dcache_tier2.png" attachment="dcache_tier2.png" attr="" comment="" date="1257462558" path="dcache_tier2.png" size="157327" stream="dcache_tier2.png" tmpFilename="/usr/tmp/CGItemp11636" user="TanyaLevshina" version="1"}%
%META:FILEATTACHMENT{name="dCacheHighLevel.pdf" attachment="dCacheHighLevel.pdf" attr="" comment="High Level View of dCache" date="1285952441" path="dCacheHighLevel.pdf" size="326931" stream="dCacheHighLevel.pdf" tmpFilename="/usr/tmp/CGItemp21946" user="NehaSharma" version="1"}%
