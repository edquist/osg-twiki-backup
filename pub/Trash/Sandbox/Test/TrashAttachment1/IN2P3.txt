B-2 - Description du projet et résultats attendus : (8 pages maximum en arial 11, simple interligne)On décrira le déroulement prévisionnel et les diverses phases intermédiaires ainsi que les méthodologies employées. L'originalité et le caractère ambitieux du projet devront être explicités. L'interdisciplinarité et l'ouverture à diverses collaborations seront à justifier en accord avec l'orientation du projet. La capacité de ou des équipes «  porteuse(s) » devra être attestée par la qualification et les productions scientifiques antérieures de leurs membres. Leur rôle dans les différentes phases du projet devront être précisés et la valeur ajoutée des collaborations entre les différentes équipes sera argumentée. Les moyens demandés devront être en accord avec les objectifs scientifiques du projet. Interopérabilité des grilles de calculLe calcul pour les 4 expériences du LHC s'opèrera sur les 6 continents et devrait fonctionner de manière cohérente. A ce jour, cette cohérence est gérée par les logiciels des expériences de manière très désordonnée et peu transparente. D'autre part, cette gestion rajoute une couche supplémentaire au dessus des diverses grilles afin d'assurer cette interopérabilité. Ce mode de fonctionnement est totalement inefficace et suggères une main d'œuvre lourde pour les expériences dans le domaine du développement et de la maintenance.  La figure ci-dessous montre le profil de gestion de cette interopérabilité par l'expérience ATLAS.A ce jour, plusieurs grilles et progiciels de grille existent, tels: UNICORE , ARC , EDG/LCG/gLite , Globus , Condor , SRB . Ils sont capables d'apporter quelques services de grilles fondamentales comme la soumission de « taches » et leur gestion, la gestion des données et le service d'information « Grid information services ». L'émergence et le vaste déploiement des différents progiciels pose le problème de l'Interopérabilité.  Malheureusement, à ce jour, aucune communauté GRID n'apporte de solutions simples qui résolvent le problème , qui puissent être utilisés et implémentés par tous et qui soient acceptés par toutes les communautés GRID. L'existence de diverses solutions de grilles inter opérables est admise dans notre communauté et nous pensons de même que la solution d'une grille unique n'est ni productive ni d'ailleurs possible. Il s'agit donc d'encourager des interfaces ouvertes pouvant s'adapter à toute la communauté, à toutes les expériences et à tous les partenaires des diverses grilles. L'interopérabilité doit se faire en établissant des interfaces avec des standards communs. Cette interopérabilité est, bien entendu, un niveau de service basé sur les services fondamentaux de la grille et de ses interfacesL'effort que nous souhaiterions investir serait dans l'implémentation et le support de nouveaux standards. Le traitement des données du LHC sera assuré par un réseau de centres de calcul organisé suivant l'architecture de grille. La partie Européenne de cette grille s'organise autour du projet LCG qui met en œuvre le middleware gLite développé dans le cadre du projet Européen EGEE . Les partenaires des pays nordiques implémentent le calcul LHC sur la grille NorduGrid et ceux des Etats-Unis sur la grille OSG. Le CCIN2P3 possède déjà des contacts privilégiés avec le Fermi National Accelerator Laboratory (FNAL) grâce à l'exploitation commune des données de l'expérience D0. FNAL est également le plus important centre Tier-1 pour l'expérience CMS et sera dès 2007 le laboratoire principal pour l'analyse physique des données du LHC aux Etats-Unis.Nous nous proposons d'exploiter et de renforcer cette collaboration, de manière à développer et à tester l'interopérabilité entre les grilles LCG et OSG. Ces développements comprendront les points suivants :_	Le transfert des données_	Les mécanismes de réplications et de référencement des données_	L'interopérabilité des systèmes d'informations et de soumissions des tâches_	Le contrôle et le monitorage des grilles_	L'Utilisation des statistiques et du comptage (accounting)Le but du projet est de développer un environnement permettant d'exploiter les ressources des deux grilles de manière transparente depuis n'importe quel site distant et de mettre cet environnement à la disposition de la communauté LHC.Relations Tier-1 - Tier-1 dans le cadre de LCGLe modèle de calcul distribué du LHC, prévoit de nombreux échanges entre le Tier-0 basé au CERN et les Tiers-1, ainsi que des Tiers-1 vers les Tiers-2. Il apparaît indispensable de pouvoir développer aussi des relations entre les Tiers-1 directement. Ce type d'échanges est indispensable pour assurer la redondance et la tolérance aux pannes. En effet, il est important de minimiser l'impact de la perte d'accès aux données dans un Tier-1 que celle-ci soit due à une perte de connectivité avec le centre en question (panne réseau, panne du Tier-1) ou bien à la perte physique des données en raison d'un problème matériel dans le système de stockage ou tout simplement du à un goulot d'étranglement non prévu.Une série de tests appelés « Service Challenges » sont prévus dans le cadre du projet LCG et sont étalés dans le temps pour tester divers défis.  Parmi ces tests, le SC3, teste les transferts de fichiers, les services de transfert et les taux de données transférés. Ce test démarrera en juillet 2005 et s'achèvera en septembre 2005. Il faudra démontrer la véracité des transferts des données au taux de 150 MB/s par Tier-1 de disque à disque et de 60 MB/s de bande à bande. Le taux de transfert externe au CERN devrait atteindre 1 GB/s. Quelques sites Tier2 participeront mais l'idée est de tester sur une période courte les transferts Tier-0Tier-1Tier-1. Un test d'une durée plus longue qui impliquerait tous les Tier1s  et  les Tier2s avec des taux de transferts double de ceux de SC3 est prévu, SC4, mais pour l'instant n'est pas encore d'actualité. Ce test devrait être la phase qui déterminerait le fonctionnement « optimum » de l'ensemble du calcul LHC.Pour faire un tel test, il est nécessaire de mettre en place dès que possible les éléments pour le faire fonctionner. En particulier, ces possibilités de basculement d'un Tier-1 vers un autre sont prévues au niveau des modèles de calcul, mais n'ont pas encore été implémentées et testées.D'autre part, il sera important de mettre en place des mécanismes permettant de gérer les priorités d'accès aux données. En effet, en plus des accès aux données provenant du Tier-1 lui-même, il faudra aussi gérer les accès en provenances des nombreux Tiers-2 (ou même Tiers-3) rattachés aux Tier-1. Pour l'heure, aucun mécanisme n'est prévu ni dans le cadre des échanges de données entre Tier1 ni dans le cadre de l'interopérabilité d'au moins 2 grilles de calcul. La volonté de collaboration de FermiLab (USA) avec l'Europe et en particulier avec la France à travers le CCIN2P3 est opportunité inespérée pour faire partie des premiers qui auront contribué et assuré  le succès de cette entreprise bien compliquée. Elle nous préparera de même à pallier aux problèmes que nous suspectons (bouchons réseaux, pannes de transferts, gestion des priorités des centres Tier1 vs Tier2s et des taches de soumissions des utilisateurs, ...). Ces problèmes sont relevés mais pour l'instant non traités.Transferts massifs de données et protocoles réseauxToute l'infrastructure des grilles de calcul repose sur des réseaux performants et fiables. En particulier au niveau des transferts de données sur de longue distance. Pour la réussite du projet LCG, nous devons pouvoir compter sur la disponibilité permanente de débits allant de quelques gigabits/s à quelques dizaines de gigabits/s entre les centres Tier-1 ou vers le Tier-0 du CERN. L'exploitation continue de telles bandes passantes est un problème complexe qui fait intervenir la technologie du réseau lui même, les interfaces et serveurs qui doivent pouvoir gérer les flux entrants et sortants et finalement les protocoles eux-mêmes qui doivent être optimisé.Nous souhaitons utiliser les infrastructures RENATER et GEANT pour tester et développer les outils nécessaires. Nos relations avec le FNAL nous offrent l'opportunité de pouvoir mettre sur pied des tests en vraie grandeur basés sur de véritables données scientifiques.Axe de travail : protocoles réseaux Cet axe de travail s'intéresse plus particulièrement aux mouvements de données et aux aspects performance des communications dans le contexte très large échelle de traitements de données massives du projet LHC.Ce sous-projet consistera à définir et à expérimenter un service de transport et les protocoles de transport associés offrant des performances optimisées, maîtrisées et prévisibles dans le cadre d'une aggrégation de ressources de stockages interconnectées par un réseau intercontinental très haut débit hétérogène.Depuis les dix dernières années, avec la généralisation de la fibre optique, l'infrastructure de coeur des réseaux évolue très rapidement en débit et en fiabilité. On assiste à la périphérie des réseaux de coeur surdimensionnés à l'explosion de réseaux d'accès très haut dèbit qui deviennent de plusen plus flexibles. A terme, les utilisateurs des grands centre de calcul pourront demander de labande passante à la demande.Les débits maximaux à l'accès sont passés en cinq ans de 2Mb/s à 10 Gb/s. Les cartes 10Gb/s sont disponibles et permettent de connecter les serveurs sur des réseaux locaux 10Gb/s.Néanmoins, l'interconnexion grande distance qui relie les ressources distribuées induit une latence incompressible et peut exhiber des performances, médiocres et variables et parfois  incompatibles avec les algorithmes de calcul et les mécanismes de stockage si les traitements protocolaires sont inappropriés à l'infrastructure ou mal implantés et les congestions mal contrôlées.Les services de transferts de l'information sur un réseau reposent en effet actuellement sur les protocoles et les logiciels de communications TCP/IP bâtis il y a plus de trente ans selon le modèle en couches et le principe de bout en bout. Ces protocoles n'offrent pas de mécanisme de contrôle des délais de transfert aux applications, ne s'intéressent qu'aux transferts mémoire à mémoire et non pas aux transferts disques à disques et sont basés sur des mécanismes de contrôle de congestion peut dynamiques. Le contexte des grilles de données est très différent de celui de l'Internet et ses principes de partage des ressources selon un mode "best effort" ne sont pas adaptés comme l'on démontré de nombreux travaux.Dans le contexte très haut débit, il apparaît que les pertes de performance  (latenceet pertes de données) dues à diverses formes de congestions et de dépassement de capacité se produisent très souvent par rafale et au niveau des systèmes d'extrémité et des liens d'accès. Ainsi, les mécanismes de contrôle de congestion dérivés de l'algorithme de TCP et basés sur la détection de perte de paquets s'avèrent peu dynamiques et inappropriés.Depuis quelques années, des solutions de transport spécifiques sont étudièes dans le contexte des grilles haute performance. Ces travaux visent à augmenter la fonction réponse du protocole TCP pour les liens à très haut produit débit délai tout en conservant les propriétés d'équité inter-flux, d'équité inter-protocoles (TCP friendlieness), d'équité selon les distances (RTT fairness) et  d'équilibre de TCP. Actuellement aucun protocole proposé ne conserve simultanément toutes ces propriétés. Il est fort probable qu'à terme aucun ne puisse être plus universel que TCP lui-même et qu'il faudra adapter le mécanisme de contrôle de congestion au contexte (infrastructure, applications). Pour cette raison, il est nécessaire d'étudier systématiquement l'impact des protocoles sur l'infrastructure et l'application. Il est aussi nécessaire de travailler aux aspects « reproductibilité » des résultats. Ceci implique de définir et de déployer des mécanismes d'ajustement ne nécessitant pas systématiquement la présence d'un informaticien expert pour obtenir les performances optimales d'une configuration donnée.Dans le cadre de ce projet, nous proposons de mener, en collaboration avec les équipes de l'IN2P3 et de RENATER une recherche innovante permettant :-	d'une part de mettre les équipes françaises au niveau des derniers travaux théoriques et expérimentaux menés au niveau international.-	de développer, d'expérimenter et de diffuser une solution logicielle flexible et optimisée pour le transfert des données massives du LHC.-	de participer à la définition d'un standard pour un service de transfert des données adapté aux  grilles de données.Ces travaux s'appuieront sur une  approche expérimentale innovante, basée sur l'émulation haut débit pour le calibrage des équipements d'extrémités et l'expérimentation en vraie grandeur avec le trafic réel  (volumes et contraintes temporelles) des grilles de données de physique sur une véritable infrastructure de production transcontinentale. Cette méthode a pour objectif de permettre l'étude réaliste des comportements aux limites des protocoles et des capacités des équipements actuels et de les valider dans un contexte réel très exigeant.L'émulation offre un environnement matériel stable et bien maîtrisé permettant le contrôle précis des conditions d'expériences.  Cet outil exploite une grappe de 200 PC du projet national GRID5000, interconnectant de nombreux équipements réels (commutateurs, émulateurs matériel, routeurs et systèmes d'extrémités) et active des composants logiciels et matériels qui émulent des latences, des taux de pertes de liens ainsi que des topologies d'infrastructures variées. Validation des modèles développésL'environnement du LHC fournit déjà  un ensemble de données scientifique simulées très fidèles par rapport à la réalité qui permet de tester et de valider les modèles développés en vraie grandeur. Il sera possible par exemple, de produire un lot de données de plusieurs centaines de Téraoctets dans l'un des Tiers-1, de le répliquer dans un autre Tier-1 en utilisant les protocoles de transfert spécifiquement optimisés dans le cadre de ce projet, et finalement de valider l'ensemble de la chaîne en exécutant des tâches d'analyse de physique sur ces données avec retour des résultats vers l'utilisateurs. L'intérêt ici est de pouvoir tester la chaîne complète de traitement sur un lot de données substantiel et de valider l'ensemble.Durant la phase 2006-2007, la validation se fera essentiellement sur des lots de données simulées. A partir de fin 2007-2008 des données réelles des quatre expériences LHC seront disponibles pour tester le système dans un environnement soumis à la pression des utilisateurs. Comme le montre le diagramme ci-dessous, une tache complète pour une expérience est de procéder aux 3 facettes de l'analyse de physique : la simulation, la reconstruction et l'analyse proprement dite qui gère les algorithmes d'identification des objets que le physicien souhaite étudier ou découvrir. Ces taches mettent en jeu diverses parties d'un logiciel ayant une architecture fort développée pour à la fois gérer les entrées sorties des données (données réelles ou simulées), les stocker et classer dans des bases de données les méta-données, soumettre les taches, permettre des visualisations en temps réel etc. .... La validation du logiciel de l'expérience et de la chaîne complète d'analyse de physique nécessite un apprentissage du logiciel sur les systèmes d'exploitation mis à sa disposition, une compréhension de la gestion « automatique » des données des deux côtés (Tier-1 Tier1), un apprentissage du support utilisateur, un réglage fin du calcul et l'apprentissage progressif des physiciens et de l'interaction entre la communauté des physiciens utilisateurs et des fournisseurs du service calcul dans son ensemble.Les équipes participant au projetLe projet présenté ici fait intervenir trois équipes françaises et un partenaire aux Etats-Unis.Le Centre de calcul de l'IN2P3 et du DAPNIA (CCIN2P3)Représenté par son directeur Dominique Boutigny, le CCIN2P3 pilote le présent projet. Il héberge le Tier-1 pour le calcul des quatre expériences LHC en France. Après une première expérience européenne dans le domaine de grille de calcul dans le cadre du projet DataGRID, le CCIN2P3 s'est fortement impliqué dans les développements liés aux grilles de calcul en France à travers les projets EGEE et LCG. Il possède une très grande expérience dans le domaine de l'exploitation des ressources informatiques et de la mise en place des grandes architectures de calcul.Le CC-IN2P3 bénéficie d'un contexte particulièrement favorable au développement d'une expertise dans le domaine des grilles de calcul. Avec ses experts en administration système, en télécommunications, en stockage de données, et en gestion de la production du calcul scientifique, il réunit le panel de compétences nécessaires à l'appréhension et à l'intégration des technologies de grille. La richesse et la diversité de son infrastructure technique, que ce soit en termes de CPUs (plusieurs milliers), de stockage (plusieurs To de disque et de l'ordre du Po en stockage de masse), ou de réseau, donne les moyens au CC d'être un acteur significatif des projets de grille.L'objectif de ces deux projets consiste à mettre en place une infrastructure de grille de production à l'échelle mondiale et couvre donc tous les aspects de ce domaine. Ils fournissent ainsi au CC-IN2P3 l'opportunité d'exploiter tout son savoir-faire, tout en se forgeant une sérieuse expertise dans le déploiement et l'exploitation d'une infrastructure de grille. Dans ce cadre, le CC intervient aussi bien au niveau organisationnel que technique.D'un point de vue organisationnel, la participation du CC-IN2P3 se traduit par une contribution importante à la coordination de l'exploitation de plus de 130 centres de ressource. Concrètement, le CC assure, d'une part,  la direction technique au niveau français pour le projet LCG, et d'autre part, les rôles de « regional operations center » (ROC) et de « core infrastructure center » (CIC) pour le projet EGEE. Les missions de ces deux derniers sont respectivement la coordination de la production des sites français (9 actuellement) et la mise en place de l'exploitation globale de la grille, en étroite collaboration avec 4 autres centres européens. Pour répondre à ses différentes missions, le CC-IN2P3 a du mettre en place une structure adaptée à un travail de coordination distribuée. D'un point de vue technique, il met actuellement à disposition de la grille LCG/EGEE, aussi bien ses ressources de calcul et de stockage que des services clefs nécessaires aux communautés d'utilisateurs pour exploiter les ressources de la grille. Ce travail a permis de  placer le CC dans les 10 sites les plus importants de la grille EGEE/LCG qui compte plus de 130 sites repartis sur toute la planète.Le site possède une grande expérience des transferts de données à  très grande vitesse. Les besoins de la physique des hautes énergies ont nécessités le  développement d'outils de transferts très performants et l'utilisation de nouveaux protocoles. Un débit de transferts de données de production de l'ordre de 300Mbps maintenu sur plusieurs heures est actuellement courant entre les Etats-Unis et le CCIN2P3. Dernièrement sa participation aux Service Challenge LCG a montré sa capacité à mettre en place une infrastructure  _permettant un débit de transferts de données provenant du CERN/Genève de 800Mbps de moyenne en continu pendant plus de 10 jours; soit un transfert de prés de 80Toctets de données. (http://www2.cnrs.fr/presse/communique/667.htm)Le CCIN2P3 hébergera le matériel nécessaire pour mener à bien le projet (serveurs, matériel réseau spécifique, etc.)Le CCIN2P3 met à la disposition du projet son infrastructure et ses équipes techniques. Le GIP RENATER (www.renater.fr)Représenté par son directeur Dany Vandrommele GIP RENATER apportera au projet les infrastructures de communications nécessaires aux échanges avec le CERN et les autres Tier-1, en particulier le Fermi Lab. Le GIP RENATER assure la maîtrise d'ouvrage du réseau national de télécommunications pour la technologie, l'enseignement et la recherche. Ce réseau est une infrastructure mutualisée qui interconnecte toutes les universités et les centres de recherche publique de France. Il fournit également l'accès à tous les autres réseaux recherche dans le reste du mande, via le réseau Pan-européen GEANT. Dany Vandromme est également membre du directoire de la société DANTE (qui assure la maîtrise d'ouvrage du réseau GEANT) et membre du comité exécutif du projet GN2. Pour répondre au mieux aux attentes de la communauté HEP, RENATER installe, dans le cadre du déploiement de RENATER-4, une fibre optique entre le CC-IN2P3 et le CERN. Par ailleurs, des fibres seront également déployées entre Lyon et Paris, où se trouve également le nœud français de GEANT2. Toutes ces fibres seront illuminées par RENATER, ce qui permettra de déployer autant de circuits que nécessaires, sur le même support optique. Chaque circuit, correspondant à une longueur d'onde (lambda), aura une capacité de transmission de 10 Gb/s. Les connexions du Tier-1 de Lyon seront dont assurées prioritairement par la liaison Lyon-Genève, et secondairement par GEANT, qui assurera également la connectivité inter-Tier-1 (y compris avec le Fermi Lab).L'équipe RESO du Laboratoire de l'Informatique du Parallélisme (LIP) RESO est une équipe du laboratoire de l'Informatique du Parallélisme (LIP) UMR N°5568 de l'Ecole Normale Supérieure de Lyon.RESO s'intéresse aux protocoles et services des réseaux très haut débit courtes et longues distances. L'objectif de RESO est d'étudier et de proposer des solutions de transport de bout en bout originales et innovantes visant à répondre aux besoins spécifiques des applications de grille et aptes à passer à l'échelle de plus hauts débits et de flux plus hétérogènes et exigeants.Les travaux fondamentaux sont articulés selon les deux axes de recherche suivants:    * Les architectures Logicielles Optimisées  pour communications efficaces dans les systèmes de type grappe, serveur ou équipements d'accès.    * Les protocoles et traitements pour le transport performant  de flux hétérogènes.Les études et les résultats sont appliqués au domaine des Grilles de calcul et de données Haute-Performance. RESO développe des solutions de « Services réseau » pour les grilles.Le Fermi National Accelerator laboratory (FNAL)Intervient dans le projet en tant que partenaire étranger. Il héberge l'infrastructure du Tier-1 de l'expérience CMS aux Etats-Unis. Le FNAL est l'un des laboratoires leader mondial en physique des hautes énergies, il héberge en particulier l'accélérateur actuellement  le plus puissant au monde (Tevatron) et les expériences CDF et D0. L'expérience D0 dans laquelle la France participe, exploite déjà le concept des grilles de calcul et pourra servir de banc d'essai pour les tests d'interopérabilité.Le FNAL, par l'intermédiaire de son centre de calcul, est fortement impliqué dans les développements informatiques pour le LHC ainsi que dans le développement de la grille américaine Open Science Grid.Collaboration entre les différentes équipesLa collaboration entre les différentes équipes est cruciale pour la réussite du projet. _	Le CCIN2P3 apporte l'infrastructure de calcul avec des capacités de stockage sur disque de plusieurs centaines de Téraoctets maintenant et plusieurs Pétaoctets en 2008. _	RENATER apporte l'infrastructure réseau avec la possibilité de contrôler et d'ajuster la configuration des matériels. La possibilité de tester le réseau international avec des flux de données importants pendant de longues périodes permet comprendre le comportement des réseaux dans des situations extrêmes. _	L'équipe RESO du LIP apporte son expertise dans le domaine des protocoles réseau et bénéficie, à travers le présent projet, à la fois de l'infrastructure réseau mise en place et  d'applications réelles pour tester en vraie grandeur les performances atteintes par ses développements.Retombées attenduesLes retombées du projet consisteront en plusieurs publications dans des journaux scientifiques dans le domaine des STIC ainsi que des communications dans des conférences internationales. Nous attendons également du projet qu'il permette aux équipes françaises qui développent la grille de calcul pour le LHC d'acquérir une position de leader dans ce domaine et d'être ainsi dans les meilleures conditions pour participer pleinement à l'exploitation scientifique des données du LHC dès la mise en route de l'accélérateur. Ce rôle est crucial, puisqu'on attend des découvertes majeures (boson de Higgs, Supersymétrie...) dès les premiers mois d'exploitation des données.Calendrier du projet et jalonsLes différentes étapes du projet protocoles réseaux seront les suivantes:- Une première partie  (T0 à T24)consistera en une analyse des besoins de transport des applications physiques déployées sur LHC. Pour cela, un outil de capture de la signature de communication d'une application conçu par l'équipe RESO sera validé sur des applications réelles et les données acquises seront analysées pour modéliser les trafics émis et reçus.  (RESO + IN2P3)- Parallèlement une étude des spécificités de l'infrastructure étudiée, débouchera sur une modélisation de l'interconnexion réseau d'une grille et de ses sources d'indéterminisme (systèmes d'extrèmités, réseau local, routeur d'accès, firewall, coeur...) qui sera validée ensuite par les travaux expérimentaux (RESO +  RENATER). - La deuxième partie sera une étude comparative expérimentale des nouveaux protocoles de transport vis à vis des besoins identifiés et des propriétés attendues.  On cherchera à définir un benchmark de service et de protocoles de transport pour les grilles de données. (RESO) ( T0 à T24)-	La troisième partie comprendra le développement, la validation et l'évaluation expérimentale d'un prototype de service transport dans l'environnement eWAN puis sur l'infrastructure réelle avec un échantillon de trafics type (T12 à T36) (RESO + IN2P3)Propositions d'experts et confidentialité_	Chaque porteur de projet devra fournir une liste de 3 à 5 noms d'experts français ou étrangers (avec coordonnées complètes : adresse postale et adresse électronique) susceptibles d'évaluer le projet avec lesquels il n'a ni conflit d'intérêt, ni collaborations en cours._	Les membres du Comité d'évaluation et du Comité stratégique sont astreints à la confidentialité.Programme non thématique 2005C - Moyens financiers et humains demandés par chaque équipe partenaire du projetChaque équipe partenaire remplira une fiche de demande d'aide selon les modèles proposés ci-dessous (laboratoire public ou fondation ; entreprise ou association) en fonction de son appartenance.On présentera une brève justification scientifique des moyens demandés pour chacune des équipes impliquées dans le projet.Ressources nécessairesLes ressources nécessaires pour mener le projet à bien sont de trois types :Matériel_	X serveurs de type y équipé de ...._	Matériel réseau...._	Disque....Ressources humainesRESO : 1 bourse de thèse (3ans) (environ 30Keuros /an)1 ingénieur expert pendant 3 ans ( environ 40K euros /an)15K euros de fonctionnement par an 15K euros d' équipement par an (cartes 10Gb/s, serveur haute performance, analyseur de trames...)Soit un total de 100K euros par an.L'embauche sur CDD de trois ( ???) personnes est indispensable à la réussite du projet, le profil de ces personnes est le suivant :_	Deux ingénieurs en informatique niveau IR pour mener bien les développements liés à l'interopérabilité des grilles de calculs. Ceux-ci travailleront en étroite collaboration avec les équipes de développements LCG et EGEE de l'IN2P3 et du CERN. L'un travaillera sur l'aspect purement logiciel du projet et l'autre sur l'aspect exploitation._	Un ingénieur en informatique niveau IR pour travailler sur la partie réseau du projet, cette personne devra développer la collaboration entre le CCIN2P3 et RENATER, il sera également chargé d'implémenter et de tester les protocoles et les applications réseaux développées par l'équipe RESO du LIP.Pour RENATER, un ingénieur Réseau tel que défini dans le second alinéa du paragraphe précédent.Missions et frais d'accueil de chercheurs étrangersPour assurer la parfaite coordination des développements, il faudra prévoir 2 mois par an de missions cumulées aux Etats-Unis et 2 mois de prise en charge de frais de séjours de collaborateurs américains en France (les voyages étant à la charge du FNAL). Programme non thématique 2005Fiche de demande d'aide - Laboratoire public / FondationAcronyme ou titre court du projetResponsable scientifique (coordinateur ou partenaire) (nom, prénom) : Estimation du coût marginal du projet pour le laboratoire :Les valeurs obtenues dans les cellules du tableau P à W serviront à renseigner le tableau « estimation du coût complet » ci-dessousAnnée 1Année 2Année 3Total (Euros)Nbre h/mCoût h/mCoût totalNbre h/mCoût h/mCoût totalNbre h/mCoût h/mCoût totalDépenses de personnel (1)(catégorie 1)(catégorie 2)...(P)Equipements (2),(4)(Q)Achats de petits matériels, de consommables etc (2)(R)Prestations de service(2),(3)(S)Frais de missions (2)(T)Frais généraux (4 % des dépenses)(U)Total (Euros)(V)Aide demandée (Euros)(W)(1)	Personnel non statutaire directement affecté au projet exprimé en hommes mois. Les dépenses éligibles se limitent aux salaires et aux charges sociales. Pour cet appel les doctorants ne doivent pas être pris en compte. Exemple : post-doc (catégorie 1), ingénieur d'études (catégorie 2)(2)	Y compris TVA non récupérable.(3)	Le montant des prestations de service est limité à 50% du montant global du fonctionnement demandé. (4)	Matériel dont la valeur unitaire est supérieure à 4000 euros HTEvaluation (pour information) du coût complet du projet pour le laboratoireEQUIPEMENT (1)  (2)FONCTIONNEMENTTOTALEquipement + fonctionnementDépenses de personnelPrestations de service (1)Autres dépenses de fonctionnement (1)Total fonctionnement(a)(b)(c)(d)(e) = (b) + (c) + (d)(f) = (a) + (e)  = (Q) ci-dessus= somme (P) x (4)+ (3) x (4)= (S) ci-dessus= somme (R+T+U) ci-dessus  (X)(1)	Coût HT majoré le cas échéant de la TVA non récupérable(2) 	Équipement : matériel dont la valeur unitaire est supérieure à 4 000 euros HT(3)	Dépense du personnel rémunéré par d'autres sources de financement (charges sociales comprises) affecté au projet, au prorata de leur implication dans le projet  (y compris les doctorants)(4)	Taux d'environnement de l'établissementProgramme non thématique 2005Fiche de demande d'aide - Entreprise / AssociationAcronyme ou titre court du projetResponsable scientifique (coordinateur ou partenaire) (nom, prénom) : Estimation (en coûts complets H.T.) du projet pour l'entreprise :Année 1Année 2Année 3Total (Euros)Nbre h/mCoût h/mCoût totalNbre h/mCoût h/mCoût totalNbre h/mCoût h/mCoût totalAmortissements d'équipements de R&DDépenses de personnel (1)(catégorie 1)(catégorie 2)...Prestations de service (2) :- Information scientifique et technique- Propriété industrielle- Faisabilité technique - Conception, analyse de la valeur- Essais, tests, caractérisation- Prototypage - Etudes économiques- Autres prestationsFrais de missionAutres dépenses de fonctionnementDépenses liées à l'utilisationd'autres équipements de R&Dque ci-dessus (3)Autres dépenses (3)Frais (assistance, encadrement, coût de structure)  (4)Total H.T. (Euros)(X)Aide demandée (Euros)(W)(1)	Personnel directement affecté au projet, chiffré en hommes mois par catégories de personnel.(2)	Chiffré par  types de prestations(3)	Justifiées selon une procédure de facturation interne(4)	Ces frais seront remboursés jusqu'à un plafond défini par les règles propres à l'ANR. Ce plafond est calculé en fonction des éléments donnés dans le tableau (frais de personnel, équipement, ...)Note : En cas de décision de financement de ce projet, le porteur de projet devra alors fournir un dossier complémentaire comportant un planning de déroulement du projet et des documents administratifs et financiers (entreprises, laboratoires et/ou associations) dont la liste lui sera précisée. Programme non thématique 2005D - Récapitulatif global de la demande financière pour le projet Acronyme ou titre court du projet :a-Total de l'aide demandée (reporter les valeurs (W) des fiches des différents partenaires)Aide demandéeCoordinateur (Partenaire 1)Partenaire 2......Total à reporter sur la première page du dossierb-Estimation (pour information) du coût complet de cette demande(reporter les valeurs (X) des fiches des différents partenaires)Coût completCoordinateur (Partenaire 1)Partenaire 2......Total à reporter sur la première page du dossierContrats sur les trois dernières années (effectués et en cours)Nom du membre participant à cette demande% d'implicationIntitulé de l'appel à projetsSource de financementMontant attribuéTitre du projetNom du coordinateurDate début -Date finDemandes de contrats en cours d'évaluation Nom du membre participant à cette demande% d'implicationIntitulé de l'appel à projetsSource de financementMontant demandéTitre du projetNom du coordinateurSecteurs disciplinaires¬	Sciences et technologies de l'information et de la communication (STIC),¬	Sciences pour l'ingénieur,¬	Chimie,¬	Physique,¬	Mathématiques et interactions,¬	Sciences de l'univers et géo-environnement,¬	Sciences agronomiques et écologiques,¬	Biologie et santé,¬	Sciences humaines et sociales
