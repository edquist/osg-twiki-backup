<html>

<head>

</head>

<body>

    <h1>OSG User Guide for XSEDE users</h1>

    <p>
    <a href="#overview">Overview</a><br>
    <a href="#computations_match">Computation that is a good match for OSG</a><br>
    <a href="#system_configuration">System Configuration</a><br>
    <a href="#system_access">System Access</a><br>
    <a href="#development">Application Development</a><br>
    <a href="#running">Running Your Application</a><br>
    <a href="#help">How to get help using OSG</a><br>
    </p>

<a name="overview">
<h2>Overview</h2>

<p>
The <a href="http://www.opensciencegrid.org/">Open Science Grid</a> (OSG) promotes science by
</p>

<ul>
    <li>enabling a framework of distributed computing and storage resources</li>
    <li>making available a set of services and methods that enable better access
    to ever increasing computing resources for researchers and communities</li>
    <li>providing resource sharing principles and software that enable distributed
    high throughput computing (DHTC) for users and communities at all scales. </li>
</ul>

<p>
The Open Science Grid does not own
the computing, storage, or network resources used by the scientific community. The
resources accessible through the OSG are contributed by the community,
organized by the OSG, and governed by the <a
href="http://www.opensciencegrid.org">OSG Consortium</a>; an overview is available
at <a
href="http://osg-docdb.opensciencegrid.org/0008/000839/004/OSG%20Intro%20v23.pdf">An
Introduction to OSG</a>.  Today, the OSG community brings together over 100
sites that provide computational and storage resources. Usage metrics are
available at the <a href="http://display.grid.iu.edu/">OSG Usage Display.</a>
Current OSG resources are summarized below:</p>

<table>
    <tr>
        <td>Number of Grid interfaced processing resources on the production infrastructure</td>
        <td>131</td>
    </tr>
    <tr>
        <td>Number of Grid interfaced data storage resources on the production infrastructure</td>
        <td>61</td>
    </tr>
    <tr>
        <td>Number of Campus Infrastructures interfaced to the OSG</td>
        <td>9 (GridUNESP, Clemson, FermiGrid, Purdue, Wisconsin, Buffalo, Nebraska, Oklahoma, SBGrid)</td>
    </tr>
    <tr>
        <td>Number of National Grids interoperating with the OSG</td>
        <td>3 (EGI, NGDF, XSEDE)</td>
    </tr>
    <tr>
        <td>Number of Cores accessible to the OSG infrastructure</td>
        <td>~70,000</td>
    </tr>
    <tr>
        <td>Size of Disk storage accessible to the OSG infrastructure</td>
        <td>~29 Petabytes</td>
    </tr>
    <tr>
        <td>CPU Wall Clock usage of the OSG infrastructure</td>
        <td>Average of 56,000 CPU days / day during May 2011</td>
    </tr>
</table>


<p>
OSG supports XSEDE users by providing a <i>Virtual Cluster</i> that forms an
abstraction layer to access the distributed OSG infrastructure.
This interface allows XSEDE users to view the OSG as a single cluster to manage
their jobs, provide the inputs and retrieve the outputs. XSEDE users access the
OSG via the OSG-XSEDE login host which appears as a resource in the XSEDE
infrastructure.
</p>

<a name="computations_match">
    <h2>Computation that is a good match for OSG</h2>

<p>High throughput workflows with simple system and data
dependencies are a good fit for OSG. The
<a href="http://research.cs.wisc.edu/condor/manual/v7.6/index.html">Condor manual</a> has an
overview of <a href="http://research.cs.wisc.edu/condor/manual/v7.6/1_Overview.html">highthroughput computing</a>.
</p>

<p>Jobs submitted into the OSG Virtual Cluster will be
executed on machines at several remote physical clusters. These machines may
differ in terms of computing environment  from the submit node. Therefore it is
important that the jobs are as self-contained as possible by generic binaries
and data which can be either carried with the job, or staged on demand. Please
consider the following guidelines:</p>

<ul>
    
    <li>Software should preferably be single threaded,
using less than 2 GB memory and each invocation should run for 4-12 hours.
There is some support for jobs with longer run time, more memory or multi-threaded
codes. Please contact the support listed below for more information about these
capabilities.<br></li>

<li>Compute sites in the OSG can be configured to use
pre-emption, which means jobs can be automatically killed if higher priority
jobs enter the system. Pre-empted jobs will restart on another site, but it is
important that the jobs can handle multiple restarts.<br></li>

<li>Binaries should preferably be statically linked.
However, dynamically linked binaries with standard library dependencies, built for
a 64-bit Red Hat Enterprise Linux (RHEL) 5 machines will also work. Also,
interpreted languages such as Python or Perl will work as long as there are no
special module requirements.<br></li>

<li>Input and output data for each job should be &lt;
10 GB to allow them to be pulled in by the jobs, processed and pushed back to
the submit node. Note that the OSG Virtual Cluster does not currently have a
global shared file system, so jobs with such dependencies will not work.<br></li>

<li>Software dependencies can be difficult to
accommodate unless the software can be staged with the job.<br></li>

</ul>

<p>The following are examples of computations which are <b>not</b> good matches for OSG:</p>

<ul>
<li>Tightly
coupled computations, for example MPI based communication, will not work well
on OSG due to the distributed nature of the infrastructure.</li>

<li>Computations
requiring a shared file system will not work as there is no shared filesystem
between the different clusters on OSG.</li>

<li>Computations
requiring complex software deployments are not a good fit. There is limited
support for distributing software to the compute clusters, but for complex
software, or licensed software, deployment can be a major task.</li>

</ul>

<a name="system_configuration">
<h2>System Configuration</h2>

<p>The OSG Virtual Cluster is a Condor pool overlay on
top of OSG resources. The pool is dynamically sized based on the demand, the
number of jobs in the queue, and supply, resource availability at the OSG
resources. It is expected that the average number of resources, on average,
available to XSEDE users will be in the order of 1,000 cores.</p>


<p>One important difference between the OSG Virtual
Cluster and most of the other XSEDE resources is that the OSG Virtual Cluster
does not have a shared file system. This means that your jobs will have to
bring executables and input data.</p>

<p>Local storage space at the submission site is
controlled by quota. Your home directory has a quota of 10 GBs and your work
directory /local-scratch/$USER has a quota of 1 TB. There are no global quotas
on the remote compute nodes, but expect that about 10 GBs are available as
scratch space for each job.</p>

<a name="system_access">
<h2>System Access</h2>

<p>The OSG Virtual Cluster supports
Single Sign On through the XSEDE User Portal, and also from the command line
using gsissh with a grid certificate for authentication. Please see the <a
href="https://www.xsede.org/accessing-resources">XSEDE GSISSH documentation</a>
for more information.</p>

<p>To log in via the XSEDE User Portal,
log in to the portal and use the login link on the &quot;accounts&quot; tab
there.</p>

<p>To log in via gsissh:</p>

<pre>gsissh osg-xsede.grid.iu.edu</pre>

<p>To transfer files, you can use GridFTP or gsiscp, Examples:</p>

<pre>
globus-url-copy file:///a/local/file gsiftp://osg-xsede.grid.iu.edu/local-scratch/username/filename

gsiscp /a/local/file osg-xsede.grid.iu.edu/local-scratch/username/filename
</pre>

<a name="development">
<h2>Application Development</h2>

<p>Most of the clusters in OSG are running Red Hat
Enterprise Linux (RHEL) 5, or some derivative thereof, on an x86_64
architecture. For your application to work well in this environment, it is
recommend that the application is compiled on a similar system, for example on the
OSG Virtual Cluster login system,osg-xsede.grid.iu.edu . It is also recommended
that the application be statically linked, or alternatively dynamically linked against
just a few standard libraries. What libraries a binary depends on can be
checked using the Unix ldd command-line utility:</p>


<pre>
ldd a.out
    a.out is a static executable
</pre>

<p>In the case of interpreted languages like Python and
Perl, applications have to either use only standard modules, or be able to ship
the modules with the jobs. Please note that different compute nodes might have
different versions of these tools installed. </p>

<a name="running">
<h2>Running Your Application</h2>

<p>The OSG Virtual Cluster is based on Condor and the
<a href="http://research.cs.wisc.edu/condor/manual/v7.6/">Condor manual</a>
provides a reference for command
line tools. The commonly used tools are:</p>

<ul>
    <li><b>condor_submit</b> - Takes a Condor submit file and add the job to the queue</li>
    <li><b>condor_q</b> - Lists the jobs in the queue. Can be invoked with your username to
    limit the list of jobs to your jobs: condor_q $USER</li>
    <li><b>condor_status</b> - Lists the available slots in the system. Note that this is a
    dynamic list and if there are no jobs in the system, condor_status can return an empty list</li>
    <li><b>condor_rm</b> - Remove a job from the queue. If you are running a DAG, please
    condor_rm the id of the DAG to remove the whole workflow.</li>
</ul>

<h3>Submitting a Simple Job</h3>

<p>Below is a basic job description for the Virtual Cluster.</p>

<pre>
universe = vanilla

# requirements is an expression to specify machines that can run jobs
requirements = (FileSystemDomain != &quot;&quot;) &amp;&amp; (Memory &gt;= 1) &amp;&amp; (Arch == &quot;X86_64&quot;)

executable = /bin/hostname

arguments = -f

should_transfer_files = YES
WhenToTransferOutput = ON_EXIT

output = job.out
error = job.err
log = job.log

notification = NEVER

queue
</pre>

<p>Create a file named job.condor containing the above text and then run:</p>

<pre>
condor_submit job.condor
</pre>

<p>You can check on the job using the condor_q command</p>

<h3>Sample Jobs and Workflows</h3>

<p>
A set of sample jobs and workflows can be found under <i>/opt/sample-jobs</i>
on the osg-xsede.grid.iu.edu host. README files are included with details
for each sample.
</p>

<p>
<i>/opt/sample-jobs/single/</i> contains a single Condor job example. Single
jobs can be used for smaller set of jobs or if the job structure is simple,
such as parameter sweeps.
</p>

<p>
<a href="http://research.cs.wisc.edu/condor/manual/v7.6/2_10DAGMan_Applications.html">DAGMan</a>
is a Condor workflow tool. It allows the creation of a directed
acyclic graph of jobs to be run, and then DAGMan submits and manages the
jobs. DAGMan is also useful if you have a large number of jobs, even
if there are no job inter-dependencies, as DAGMan can keep track of
failures and provide a restart mechanism if part of the workflow fails.
A sample DAGMan workflow can be found in <i>/opt/sample-jobs/dag/</i>
</p>

<p>
<a href="htt://pegasus.isi.edu">Pegasus</a> is a workflow system which
can be used for more complex workflows. It plans abstract workflow
representations down to an executable workflow and users Condor DAGMan
as a workflow executor. Pegasus also provides debugging and monitoring
tools that allow users to easily track failures in their workflows.
Workflow provenance information is collected and can be summarized with
the provided statistical and plotting tools. A sample Pegasus workflow
can be found in <i>/opt/sample-jobs/pegasus/</i>
</p>


<a name="help">
<h2>How to get help using OSG</h2>

<p>XSEDE users of OSG may get technical support by
contacting OSG User Support staff at email
<a href="mailto:osg-xsede-support@opensciencegrid.org">osg-xsede-support@opensciencegrid.org</a>
Users may also contact the <a href="https://portal.xsede.org/help-desk">XSEDE helpdesk</a></p>.



</body>

</html>
