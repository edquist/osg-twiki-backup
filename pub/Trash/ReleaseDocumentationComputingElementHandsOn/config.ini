;===================================================================
;                       IMPORTANT
;===================================================================
;
; 
; You can get documentation on the syntax of this file at:
; https://twiki.grid.iu.edu/twiki/bin/view/Integration/ITB090/ConfigurationFileFormat
; You can get documentation on the options for each section at:
; https://twiki.grid.iu.edu/twiki/bin/view/Integration/ITB090/ConfigurationFileHelp
;


[DEFAULT]
; Use this section to define variables that will be used in other sections
; For example, if you define a variable called dcache_root here
; you can use it in the gip section as %(dcache_root)s  (e.g. 
; my_vo_1_dir = %(dcache_root)s/my_vo_1
; my_vo_2_dir = %(dcache_root)s/my_vo_2

; Defaults, please don't modify these variables
unavailable = UNAVAILABLE
default = UNAVAILABLE

; Name these variables disable and enable rather than disabled and enabled
; to avoid infinite recursions
disable = False
enable = True

; You can modify the following and use them
localhost = uct3-edge5.uchicago.edu
admin_email = sthapa@ci.uchicago.edu
osg_location = UNAVAILABLE

;===================================================================
;                       Site Information
;===================================================================

[Site Information]
; The group option indicates the group that the OSG site should be listed in,
; for production sites this should be OSG, for vtb or itb testing it should be
; OSG-ITB
; 
; YOU WILL NEED TO CHANGE THIS
group = OSG-ITB

; The host_name setting should give the host name of the CE  that is being 
; configured, this setting must be a valid dns name that resolves
; 
; YOU WILL NEED TO CHANGE THIS
host_name = %(localhost)s

; The site_name setting should give the registered OSG site name (e.g. OSG_ITB)
; 
; YOU WILL NEED TO CHANGE THIS
site_name = UC_ITB_2

; The sponsor setting should list the sponsors for your cluster, if your cluster
; has multiple sponsors, you can separate them using commas or specify the 
; percentage using the following format 'osg, atlas, cms' or 
; 'osg:10, atlas:45, cms:45'  
; 
; YOU WILL NEED TO CHANGE THIS
sponsor = osg:100

; The site_policy setting should give an url that lists your site's usage
; policy 
site_policy = http://osg-vtb.uchicago.edu/policies.html

; The contact setting should give the name of the admin/technical contact
; for the cluster
; 
; YOU WILL NEED TO CHANGE THIS
contact = Suchandra Thapa

; The email setting should give the email address for the technical contact
; for the cluster 
; 
; YOU WILL NEED TO CHANGE THIS
email = %(admin_email)s

; The city setting should give the city that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
city = Chicago

; The country setting should give the country that the cluster is located in
; 
; YOU WILL NEED TO CHANGE THIS
country = IL

; The longitude setting should give the longitude for the cluster's location
; if you are in the US, this should be negative
; accepted values are between -180 and 180
; 
; YOU WILL NEED TO CHANGE THIS
longitude = -87.60194

; The latitude setting should give the latitude for the cluster's location
; accepted values are between -90 and 90
; 
; YOU WILL NEED TO CHANGE THIS
latitude = 41.7992


;===================================================================
; For the following job manager sections (LSF, SGE, PBS, Condor)
; you should delete the sections corresponding to job managers that 
; you are not using.  E.g. if you are just using LSF on your site,
; you can delete 
;===================================================================


;===================================================================
;                              PBS
;===================================================================


[PBS]
; This section has settings for configuring your CE for a PBS job manager

; The enabled setting indicates whether you want your CE to use a PBS job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the pbs install directory
home = %(unavailable)s

; The pbs_location setting should give the location of pbs install directory
; This should be the same as the home setting above
pbs_location = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-pbs) 
job_contact = %(localhost)s/jobmanager-pbs

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                             Condor
;===================================================================


[Condor]
; This section has settings for configuring your CE for a Condor job manager

; The enabled setting indicates whether you want your CE to use a Condor job 
; manager
; valid answers are True or False
enabled = %(enable)s

; The home setting should give the location of the condor install directory
home = /opt/osg-1.2/condor

; The condor_location setting should give the location of condor install directory
; This should be the same as the home setting above
condor_location = %(home)s

; The condor_location setting should give the location of condor config file,
; This is typically  etc/condor_config within the condor install directory
condor_config = %(unavailable)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-condor) 
job_contact = %(localhost)s/jobmanager-condor

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(enable)s

;===================================================================
;                              SGE
;===================================================================


[SGE]
; This section has settings for configuring your CE for a SGE job manager

; The enabled setting indicates whether you want your CE to use a SGE job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the sge install directory
home = %(unavailable)s

; The sge_location setting should give the location of sge install directory
; This should be the same as the home setting above
sge_location = %(home)s

; The sge_root setting should give the location of sge install directory
; This should be the same as the home setting above
sge_root = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-sge) 
job_contact = %(localhost)s/jobmanager-sge

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              LSF
;===================================================================


[LSF]
; This section has settings for configuring your CE for a LSF job manager

; The enabled setting indicates whether you want your CE to use a LSF job 
; manager
; valid answers are True or False
enabled = %(disable)s

; The home setting should give the location of the lsf install directory
home = %(unavailable)s

; The lsf_location setting should give the location of lsf install directory
; This should be the same as the home setting above
lsf_location = %(home)s

; The job_contact setting should give the contact string for the jobmanger 
; on this CE (e.g. host.name/jobmanager-lsf) 
job_contact = %(localhost)s/jobmanager-lsf

; The util_contact should give the contact string for the default jobmanager
; on this CE (e.g. host.name/jobmanager)
util_contact = %(localhost)s/jobmanager

; The wsgram setting should be set to True or False depending on whether you
; wish to enable wsgram on this CE
wsgram = %(disable)s

;===================================================================
;                              Managed Fork
;===================================================================


[Managed Fork]
; The enabled setting indicates whether managed fork is in use on the system
; or not. You should set this to True or False
enabled = %(enable)s

; The condor_location setting should give the location of condor install directory
; This should be the same as the home setting above
condor_location = /opt/osg-1.2/condor

; The condor_location setting should give the location of condor config file,
; This is typically  etc/condor_config within the condor install directory
condor_config = %(default)s

;===================================================================
;                              Misc Services
;===================================================================


[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = %(unavailable)s

; If you wish to use the ca certificate update service, set this setting to True, 
; otherwise keep this at false
; Please note that as of OSG 1.0, you have to use the ca cert updater or the rpm
; updates, pacman can not update the ca certs
use_cert_updater = %(enable)s

;This setting should be set to the host used for gums host.  
; If your site is not using a gums host, you can set this to %(unavailable)s
gums_host = %(unavailable)s

; This setting should be set to one of the following, gridmap, prima, xacml 
; to indicate whether gridmap files, prima callouts, or prima callouts with xacml
; should be used
authorization_method = gridmap

; This setting indicates whether the osg index page generation will be run,
; by default this is not run
enable_webpage_creation = %(disable)s

;===================================================================
;                            RSV
;===================================================================


[RSV]
; The enable option indicates whether rsv should be enable or disabled.  It should
; be set to True or False
enabled = %(enable)s

; The rsv_user option gives the user that the rsv service should use.  It must
; be a valid unix user account
; 
; If rsv is enabled, and this is blank or set to unavailable it will default to 
; rsvuser
rsv_user = rsv

; The enable_ce_probes option enables or disables the RSV CE probes.  If you enable this,
; you should also set the ce_hosts option as well.
;
; Set this to true or false. 
enable_ce_probes = %(enable)s

; The ce_hosts options lists the FQDN of the CEs that the RSV CE probes should check. 
; This should be a list of FQDNs separated by a comma (e.g. my.host,my.host2,my.host3)
;
; This must be set if the enable_ce_probes option is enabled.  If this is set to 
; UNAVAILABLE or left blank, then it will default to the hostname setting for this CE
ce_hosts = %(default)s

; The enable_gridftp_probes option enables or disables the RSV gridftp probes.  If 
; you enable this, you must also set the ce_hosts or gridftp_hosts option as well.
;
; Set this to True or False. 
enable_gridftp_probes = %(enable)s

; The gridftp_hosts options lists the FQDN of the gridftp servers that the RSV CE 
; probes should check. This should be a list of FQDNs separated by a comma 
; (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts must be set if the enable_gridftp_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gridftp_hosts = %(localhost)s

; The gridftp_dir options gives the directory  on the gridftp servers that the 
; RSV CE  probes should try to write and read from. 
;
; This should be set if the enable_gridftp_probes option is enabled. It will default
; to /tmp if left blank or set to UNAVAILABLE 
gridftp_dir = %(default)s

; The enable_gums_probes option enables or disables the RSV gums probes.  If 
; you enable this, you must also set the ce_hosts or gums_hosts option as well.
;
; Set this to True or False. 
enable_gums_probes = %(disable)s

; The gums_hosts options lists the FQDN of the CE that uses GUMS server that the 
; RSV GUMS probes should check. This should be a list of FQDNs separated by a 
; comma (e.g. my.host,my.host2,my.host3)
;
; This or ce_hosts should be set if the enable_gums_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
gums_hosts = %(default)s

; The enable_srm_probes option enables or disables the RSV srm probes.  If 
; you enable this, you must also set the srm_hosts option as well.
;
; Set this to True or False. 
enable_srm_probes = %(disable)s

; The srm_hosts options lists the FQDN of the srm servers that the 
; RSV SRM probes should check. This should be a list of FQDNs separated 
; by a comma (e.g. my.host,my.host2,my.host3).  You can specify the port 
; on a host using host:port (e.g. localhost:8443 ). 
;
; This or _hosts must be set if the enable_srm_probes option is enabled.  If 
; this is set to UNAVAILABLE or left blank, then it will default to the hostname 
; setting for this CE
srm_hosts = %(default)s

; The srm_dir options gives the directory  on the srm servers that the 
; RSV SRM probes should try to write and read from. 
;
; This must be set if the enable_srm_probes option is enabled. 
srm_dir = %(unavailable)s

; This option gives the webservice path that SRM probes need to along with the 
; host: port. For dcache installations, this should work if left blank or left out. 
; However Bestman-xrootd SEs normally use srm/v2/server as web service path, and so 
; Bestman-xrootd admins will have to pass this option with the appropriate value 
; (for example: "srm/v2/server") for the SRM probes to work on their SE.
srm_webservice_path = %(unavailable)s

; Use the use_service_cert option indicates whether to use a service 
; certificate with rsv 
;
; NOTE: This can't be used if you specify multiple CEs or GUMS hosts
use_service_cert = %(enable)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the public key file (pem) for your service 
; certificate
; 
; If this is left blank or set to UNAVAILABLE  and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvcert.pem
rsv_cert_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  
; This should point to the private key file (pem) for your service 
; certificate
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /etc/grid-security/rsvkey.pem
rsv_key_file  = %(default)s

; You'll need to set this if you have enabled the use_service_cert.  This 
; should point to the location of the rsv proxy file.
;
; If this is left blank or set to UNAVAILABLE and the use_service_cert 
; setting is enabled, it will default to /tmp/rsvproxy
rsv_proxy_out_file = %(default)s

; If you don't use a service certificate for rsv, you will need to specify a 
; proxy file that RSV should use in the proxy_file setting.
; This needs to be set if use_service_cert is disabled
proxy_file = %(unavailable)s

; This option will enable RSV record uploading to central RSV collector at the GOC
;
; Set this to True or False
enable_gratia = %(disable)s

; The print_local_time option indicates whether rsv should use local times instead of 
; GMT times in the local web pages produced (NOTE: records uploaded to central RSV 
; collector will still have UTC timestamps)
;
; Set this to True or False
print_local_time = %(disable)s

; The setup_rsv_nagios option indicates whether rsv try to connect to a locat
; nagios instance and report information to it as well
;
; Set this to True or False
setup_rsv_nagios = %(disable)s

; The rsv_nagios_conf_file option indicates the location of the rsv nagios 
; file to use for configuration details.  This is optional
;
rsv_nagios_conf_file = %(default)s

; The setup_for_apache option indicates whether rsv try to create a webpage
; that can be used to view the status of the rsv tests.  Enabling this is 
; highly encouraged.
;
; Set this to True or False
setup_for_apache = %(disable)s


;===================================================================
;                            Storage 
;===================================================================

[Storage]
;
; Several of these values are constrained and need to be set in a way
; that is consistent with one of the OSG storage models
;
; Please refer to the OSG release documentation for an indepth explanation 
; of the various storage models and the requirements for them

; If you have a SE available for your cluster and wish to make it available 
; to incoming jobs, set se_available to True, otherwise set it to False
se_available = %(disable)s

; If you indicated that you have an se available at your cluster, set default_se to
; the hostname of this SE, otherwise set default_se to UNAVAILABLE
default_se = %(unavailable)s

; The grid_dir setting should point to the directory which holds the files 
; from the OSG worker node package, it should be visible on all of the computer
; nodes (read access is required, worker nodes don't need to be able to write) 
; 
; YOU WILL NEED TO CHANGE THIS
grid_dir = /opt/wn-1.2

; The app_dir setting should point to the directory which contains the VO 
; specific applications, this should be visible on both the CE and worker nodes
; but only the CE needs to have write access to this directory
; 
; YOU WILL NEED TO CHANGE THIS
app_dir = /opt/share/app

; The data_dir setting should point to a directory that can be used to store 
; and stage data in and out of the cluster.  This directory should be readable
; and writable on both the CE and worker nodes
; 
; YOU WILL NEED TO CHANGE THIS
data_dir = /opt/share/data

; The worker_node_temp directory should point to a directory that can be used 
; as scratch space on compute nodes, it should allow read and write access on the 
; worker nodes but can be local to each worker node
; 
; YOU WILL NEED TO CHANGE THIS
worker_node_temp = /tmp

; The site_read setting should be the location or url to a directory that can 
; be read to stage in data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_read = /opt/share/data

; The site_write setting should be the location or url to a directory that can 
; be write to stage out data, this is an url if you are using a SE 
; 
; YOU WILL NEED TO CHANGE THIS
site_write = /opt/share/data

;===================================================================
;                              Monalisa
;===================================================================

[MonaLisa]
; Set the enabled setting to True if you have monalisa installed and wish to 
; use it, otherwise set it to False 
enabled = %(disable)s

; If you want monalisa to use it's vo modules, set the use_vo_modules setting
; to true, otherwise set this to False
use_vo_modules = %(enable)s

; The ganglia_support setting should be enabled if you are using ganglia on
; your cluster and you wish monalisa to use it as well
ganglia_support = %(disable)s

; If you've enabled ganglia support, you should enter the hostname of the 
; ganglia server in the ganglia_host option
ganglia_host = %(unavailable)s

; If you've enabled ganglia support, you should enter the port that ganglia
; is running on
ganglia_port = %(default)s

; Set this to the monitor group that monalisa will report, by default this 
; will be set to the group set in the Site Information section
monitor_group = %(default)s

; This setting should be set to Y or N depending on whether you
; want monalisa to autoupdate itself, by default this is set to N
auto_update = %(enable)s

; This setting should be set to the user account that monalisa will
; run under, by default this is set to daemon
user = %(default)s

;===================================================================
;                             Squid
;===================================================================

[Squid]
; Set the enabled setting to True if you have squid installed and wish to 
; use it, otherwise set it to False 
enabled = %(disable)s

; If you are using squid, specify the location of the squid server in the 
; location setting, this can be a path if squid is installed on the same
; server as the CE or it can be a hostname
location = %(unavailable)s

; If you are using squid, use the policy setting to indicate which cache
; replacement policy squid is using
policy = %(unavailable)s

; If you are using squid, use the cache_size setting to indicate which the 
; size of the disk cache that squid is using
cache_size = %(unavailable)s

; If you are using squid, use the memory_size setting to indicate which the 
; size of the memory cache that squid is using
memory_size = %(unavailable)s


;===================================================================
;                              GIP
;===================================================================

[GIP]

; ========= These settings must be changed ==============

;; This setting indicates the batch system that GIP should query
;; and advertise
;; This should be the name of the batch system in lowercase
batch = condor
;; Options include: pbs, lsf, sge, or condor

; ========= These settings can be left as is for the standard install ========

;; This setting indicates whether GIP should advertise a gsiftp server
;; in addition to a srm server, if you don't have a srm server, this should
;; be enabled
;; Valid options are True or False
advertise_gsiftp = %(enable)s

;; This should be the hostname of the gsiftp server that gip will advertise
gsiftp_host = %(localhost)s

;; This setting indicates whether GIP should query the gums server.
;; Valid options are True or False
advertise_gums = %(disable)s

;
; NOTE ABOUT PREVIOUS GIP OPTIONS:
; There used to be many more options in the GIP section, mostly involving the
; configuration of the site's subclusters and storage elements.  These options
; have been moved into separate sections - one section per subcluster or SE.
; However, backward compatibility with the older format has been retained, and
;

;===================================================================
;                          Subclusters
;===================================================================

; For each subcluster, add a new subcluster section.
; Each subcluster name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each subcluster section must start with
; the words "Subcluster", and cannot be named "CHANGEME".

; There should be one subcluster section per set of homogeneous nodes in the
; cluster.

; This data is used for our statistics collections in the OSG, so it's important
; to keep it up to date.  This is important for WLCG sites as it will be used
; to determine your progress toward your MoU commitments!

; If you have many similar subclusters, then feel free to collapse them into
; larger, approximately-correct groups.

; See example below:

[Subcluster Main]
; should be the name of the subcluster
name = Main
; number of homogeneous nodes in the subcluster
node_count = 1
; Megabytes of RAM per node.
ram_mb = 4096
; CPU model, as taken from /proc/cpuinfo.  Please, no abbreviations!
cpu_model = Dual Core AMD Opteron(tm) Processor 285
; Should be something like:
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; Vendor's name -- AMD or Intel?
cpu_vendor = AMD
; Approximate speed, in MHZ, of the chips
cpu_speed_mhz = 2592
; Must be an integer.  Example: cpu_speed_mhz = 2400
; Platform; x86_64 or i686
cpu_platform = i686

; Number of CPUs (physical chips) per node
cpus_per_node = 2
; Number of cores per node.
cores_per_node = 4
; For a dual-socket quad-core, you would put cpus_per_node=2 and
; cores_per_node=8

; Set to true or false depending on inbound connectivity.  That is, external
; hosts can contact the worker nodes in this subcluster based on their hostname.
inbound_network = TRUE
; Set to true or false depending on outbound connectivity.  Set to true if the
; worker nodes in this subcluster can communicate with the external internet.
outbound_network = TRUE

; Non-mandatory attributes
; The amount of swap per host in MB
;  swap_mb = 4000
; The per-core SpecInt 2000 score.  This is usually computed for you.
;  SI00 = 2000
; The per-core SpecFloat 2000 score.  This is usually computed for you
;  SF00 = 2000

; Here's a full example.  Remember, globally unique names!
; [Subcluster Dell Nodes UNL]
; name = Dell Nodes UNL
; node_count = 53
; ram_mb = 4110
; swap_mb = 4000
; cpu_model = Dual-Core AMD Opteron(tm) Processor 2216
; cpu_vendor = AMD
; cpu_speed_mhz = 2400
; cpus_per_node = 2
; cores_per_node = 4
; inbound_network = FALSE
; outbound_network = TRUE


;===================================================================
;                             SE
;===================================================================

; For each storage element, add a new SE section.
; Each SE name must be unique for the entire grid, so make sure to not
; pick anything generic like "MAIN".  Each SE section must start with
; the words "SE", and cannot be named "CHANGEME".

; There are two main configuration types; one for dCache, one for BestMan

; Don't forget to change the section name!  One section per SE at the site.
[SE CHANGEME]

; The first part of this section shows options which are mandatory for all SEs.
; dCache and BestMan-specific portions are shown afterward.

; Set to False to turn off this SE
enabled = False

; Name of the SE; set to be the same as the OIM registered name
name = SE_CHANGEME
; The endpoint of the SE.  It MUST have the hostname, port, and the server
; location (/srm/v2/server in this case).  It MUST NOT have the ?SFN= string.
srm_endpoint = httpg://srm.example.com:8443/srm/v2/server
; dCache endpoint template: httpg://srm.example.com:8443/srm/managerv2

; How to collect data; the most generic implementation is called "static"
provider_implementation = static
; WLCG sites with a SE *must* use bestman, dcache, or dcache19
; Implementation and version of your SRM SE; usually dcache or bestman
implementation = bestman
; Version refers to the SE version, not the SRM version.
version = 2.2.1.foo
; dCache example: version = 1.9.1
; Default paths for all of your VOs; VONAME is replaced with the VO's name.
default_path = /mnt/bestman/home/VONAME
; Set a specific path for VOs which don't use the default path.
; Comma-separated list of VO:PATH pairs.  Not required.
; vo_dirs=cms:/mnt/bestman/cms, dzero:/mnt/bestman2/atlas

; For BestMan-based SEs, uncomment and fill in the following.
;  provider_implementation = bestman
;  implementation = bestman

; Set to TRUE if the bestman provide can use 'df' on the directory referenced
; above to get the freespace information.  If set to false, it probably won't
; detect the correct info.
;  use_df = True

; For dCache-based SEs, uncomment and fill in the following
; How to collect data; set to 'dcache' for dcache 1.8 (additional config req'd
; for this case), 'dcache19' for dcache 1.9, or 'static' for default values.
; provider_implementation = dcache19
;  implementation = dcache
;  If you use the dcache provider, see 
; http://twiki.grid.iu.edu/bin/view/InformationServices/DcacheGip
; If you use the dcache19 provider, you must fill in the location of your
; dCache's information provider:
;  infoprovider_endpoint = http://dcache.example.com:2288/info
; SE implementation name; leave as 'dcache'

; Here are working configs for BestMan and dCache
; [SE dCache]
; name = T2_Nebraska_Storage
; srm_endpoint = httpg://srm.unl.edu:8443/srm/managerv2
; provider_implementation = static
; implementation = dcache
; version = 1.8.0-15p6
; default_path = /pnfs/unl.edu/data4/VONAME

; [SE Hadoop]
; name = T2_Nebraska_Hadoop
; srm_endpoint = httpg://dcache07.unl.edu:8443/srm/v2/server
; provider_implementation = bestman
; implementation = bestman
; version = 2.2.1.2.e1
; default_path = /user/VONAME


;===================================================================
;                            Install Locations
;===================================================================

[Install Locations]
; The osg option is used to give the location of the directory where the 
; osg ce software is installed
osg = %(osg_location)s

; The globus option is used to give the location of the directory where the 
; globus software is installed, it is the globus subdirectory of the osg 
; install location normally
globus = %(osg)s/globus

; This is the location of the file that contains the gridftp logs, it is usually
; the globus/var/log/gridftp.log file in the osg install directory 
gridftp_log = %(globus)s/var/log/gridftp.log
