"Connecting Researchers to Resources"
For campus grids, there are three parts to the subject of "Connecting Researchers to Resources":
1) Resources: Some sufficiently enticing and easy to use computational or storage resource on campus that can be used for scientific computing.
2) Researchers: Finding and maintaining the interest of campus researchers who have a need for computing in order to get their science done.
3) Connecting: Selecting the right level of engagement for researcher based on the present and future need of the science.
In this section, we are not going to tackle the acquisition of resources but rather finding and growing the needs of researchers and connecting them to the types of scientific computing that suit them best.  All of the opinions expressed are based upon the experience garnered by doing these activities at the University of Nebraska-Lincoln.

In Nebraska, we find new researchers using the following means:
1) Top-down approach: We enjoy strong support from the Office of Research, which encourages scientists to partner with the Holland Computing Center when additional computing is needed and does not allow new grants to purchase their own computing resources outside HCC.
2) "User Recommendation": Users often are recommended by their peers, whether they are new research group recommended by colleagues who use HCC or by new students who are joining a research group that is already utilizing HCC.
3) Active Engagement: Some research groups have joined after we have directly talked to heads of departments, deans, or research group leads.  We feel this is one method to "break in" to a new campus or department where we have no active users.
4) Education: HCC staff teach CSE classes almost every semester.  Topics in the past have included system administration, parallel programming, cluster computing, and grid computing.  Students who believe they need scientific computing (or whose advisers believe this) often take these classes and get involved with HCC through their class project.  We also use these classes as a recruitment tool for student workers.  We also offer a local workshop approximately once a year.
We have never examined our "retention rate" for active users, or thoroughly examined the reasons why active users become inactive.

[Before we go into details of how we go about connecting researchers to resources, we make a few definitions; one can skip this section if they are familiar with all the keywords
Primary types of resources:
1) Commodity Linux clusters: Clusters composed of low-to-mid-range server hardware; commodity ethernet network; small number (<=16) of cores per node; 1-3 GB RAM per core.
2) Tightly-coupled clusters: Commodity Linux clusters with a low-latency network.
3) Specialty resources: Machines serving a specific niche purpose not well suited for general usage or non-dedicated applications.  Examples include GPGPU-equipped machines, SGI Altix / large memory single-system-image machines, and possibly machines with non-x86 architectures.

We also divide the jobs up into general classes:
1) High throughput: A large number of single-core jobs; usually a large number of jobs (hundreds to tens of thousands) form a single workflow, which might have trivial or complex interdependencies (a large number of jobs should be able to be run simultaneously).  Parallelism is achieved through running additional jobs.
2) High performance: MPI or other massively parallel jobs.  These tend to take up significant amounts of computing resources - many, perhaps hundreds, of nodes.  Usually, a small number (<10) batch system jobs per workflow
3) High throughput, high performance: Workflows that mix the characteristics of high performance and throughput; usually multi-core jobs running on a single machine.
4) Specialty - jobs that can only run on specialty resources.
(TODO: It seems silly to define these ourselves instead of referencing them, as it is certain we will forget an important characteristic of one of these categories and offend someone.  At the least, this can be a part of a footnote or appendix)]

We find the following pairings have made the most sense:
1) Specialty jobs can only be run on specialty resources.  The users with specialty jobs often have more computing expertise; they know what they want and they can take care of themselves if they have access to the resource.  No interest in distributed or grid computing.  Care must be taken to take the added cost for hardware support and purchase into consideration.
2) HPC jobs: Generally, HPC jobs can only run on HPC resources.  The portability of these jobs are low, as the researcher is often interested in intimately tuning the jobs to the machine (we've had experts state that it takes 1-2 months to "break in" a new machine for their code); it can take many recompiles per machine to get the desired performance.  These workflows may not scale well by adding additional cores to the jobs, which is why compiler settings are so important.  Amongst these jobs, there is usually low interest in distributed computing, and hence a low probability of success for Engagement with campus grids effort.  The only potential successes we foresee are from converting those users whose jobs are really HTC to use HTC methods; for example, there are embarrassingly parallel workflows ideal for HTC that are implemented in MPI because that was the only tool the researcher was familiar with.
3) HTPC jobs are only run on tightly-coupled clusters at Nebraska.  These have the potential to run on commodity cluster and a core stakeholders (CMS) may express interest in this.  Nebraska will probably wait for guidance and leadership from OSG Satellite for running HTPC jobs on the grid.  There are a small number of local users whose jobs might fit this description; they may eventually be a target for the campus grid.  Currently, the cost for porting is too high and the potential for increased resources for these users is too low.
4) HTC jobs: These jobs can and are run almost anywhere - HPC, HTC, or even specialty resources.  These workflows scale by adding additional jobs - there is less focus to highly tune the job to each machine they run on.  These users are more interested in distributed computing and their jobs have a higher probability to be successfully ported to grids.

At Nebraska, we hope to offload as many researchers off to a campus grid or the OSG as possible.  Porting a job to the campus grid increases utilization of all local resources and increases the resources available to a single research group.  We currently *do not* attempt to port a job to the campus grid if they meet any of the following criteria:
1) Software requires licensing or license server.
2) Software requires multiple cores per job.
3) Workflow can be done within the desired timeframe regardless of how busy the cluster is.  I.e., almost any local user should be able to finish a 1,000 compute hour workflow overnight through fair-share; if the turn-around of 8 hours is acceptable for the user, there will likely never be a need to use distributed clusters.
4) Workflows which are highly data-intensive (more than 1GB of input per job)
(1) and (2) are software limitations that can be solved, but the solutions are complex enough that the costs outweigh the benefits.  (3) is difficult as we predict our clusters will become increasingly over-subscribed; if in doubt, we don't apply it.  It is important to consider the potential science benefit versus the cost of HCC support time; there are cases where a HTC workflow meets all the criteria for running on the OSG except actually being "large enough".

When a researcher has HTC jobs that don't meet any of the exclusion criteria above, we envision the following steps for a successful campus grid user:
1) Run application interactively on any HCC cluster, and run the workflow from start to finish for one path (i.e., if the workflow is a sweep through 3,000 input parameter sets, verify they can run it on 1 parameter set).  The HCC effort involved is usually the sysadmins installing new software dependencies or any HCC employee helping with Unix basics.
2) Port the application to the Condor cluster.  Express the workflow dependencies are expressed as a Condor DAGMan.  Each individual Condor job should encompass a "reasonable" amount of work (i.e., should be between 30 minutes and 8 hours long, with an average of 2 hours).  Data dependencies should be well defined and expressed in the condor job; the user should not depend on a shared file system.  An HCC application expert or integration expert will be able to help here.
   * It is possible that, after this step is completed, the user is satisfied for quite awhile.  It is possible they may not continue to step 3 until their computing needs increase or usage of the condor cluster increases.
3) Port the application to the campus grid.  Currently, this is done by individual engagement with the integration expert.  Usually, this involves:
   a) Getting the user a grid certificate and adding them to the GPN (or Engage?) VO.  Training with OSG grid basics.
   b) Deploying their application code to all sites supporting GPN.
   c) Modifying their condor submit script slightly to use OSG-MM instead of "normal" Condor.  We already have implemented a mostly automated tool to do this.
      * We are in the process of evaluating GlideInWMS, which should make this step even easier.
We believe (2) is a crucial step in order to allow a user to fully debug their application locally (where it is easier to separate condor errors from application errors than separate condor-g errors from application errors).  It also provides the cleanest transition from a single local resource to a OSG-like resource, especially when the data dependencies are correctly expressed.

TODO: This barely touches on the topic of storage or data management.
